{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b139c416",
   "metadata": {},
   "source": [
    "# HTML Parser Code-Beautiful Soup -Data Preprocessing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2faf5d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Amazon Unveils Graviton4: A 96-Core ARM CPU with 536.7 GBps Memory Bandwidth\n",
      "Author: Anton Shilov\n",
      "Date Published: 2023-11-29T21:30:00Z\n",
      "URL: https://www.anandtech.com/show/21172/amazon-unveils-graviton4-a-96core-cpu-with-5367-gbs-memory-bandwidth\n",
      "Content: Nowadays many cloud service providers design their own silicon, but Amazon Web Services (AWS) started to do this ahead of its rivals and by now its Annapurna Labs subsidiary develop processors that can well compete with those from AMD and Intel. This week AWS introduced its Graviton4 SoC, a 96-core ARM-based chip that promises to challenge renowned CPU designers and offer unprecedented performance to AWS clients.\"By focusing our chip designs on real workloads that matter to customers, we are able to deliver the most advanced cloud infrastructure to them,\" said David Brown, vice president of Compute and Networking at AWS. \"Graviton4 marks the fourth generation we have delivered in just five years, and is the most powerful and energy efficient chip we have ever built for a broad range of workloads.\"The AWS Graviton4 processor packs 96 cores that offer on average 30% higher compute performance compared to Graviton3 and is 40% faster in database applications as well as 45% faster in Java applications, according to Amazon. Given that Amazon did not reveal many details about its Graviton4, it is hard to attribute performance increases to any particular characteristics of the CPU.Yet,NextPlatformbelieves that the processor uses Arm Neoverse V2 cores, which are more capable than V1 cores used in previous-generation AWS processors when it comes to instruction per clock (IPC). Furthermore, the new CPU is expected to be fabricated using one of TSMC's N4 process technologies (4nm-class), which offers a higher clock-speed potential than TSMC's N5 nodes.\"AWS Graviton4 instances are the fastest EC2 instances we have ever tested, and they are delivering outstanding performance across our most competitive and latency sensitive workloads,\" said Roman Visintine, lead cloud engineer at Epic. \"We look forward to using Graviton4 to improve player experience and expand what is possible within Fortnite.”In addition, the new processor features a revamped memory subsystem with a 536.7 GB/s peak bandwidth, which is 75% higher compared to the previous-generation AWS CPU. Higher memory bandwidth improves performance of CPUs in memory intensive applications, such as databases.Meanwhile, such a major memory bandwidth improvement indicates that the new processor employs a memory subsystem with a higher number of channels compared to Graviton3, though AWS has not formally confirmed this.Graviton4 will be featured in memory-optimized Amazon EC2 R8g instances, which is particularly useful to boost performance in high-end databases and analytics. Furthermore, these R8g instances provide up to three times more vCPUs and memory than Graviton 3-based R7g instances, enabling higher throughput for data processing, better scalability, faster results, and reduced costs. To ensure security of AWS EC2 instances, Amazon equipped all high-speed physical hardware interfaces of Graviton4 CPUs.Graviton4 R8g is currently in preview, these instances will be available widely in the coming months.Sources:AWS,NextPlatform\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/21172/amazon-unveils-graviton4-a-96core-cpu-with-5367-gbs-memory-bandwidth\n",
      "Title: Arm Acquires Minority Stake in Raspberry Pi\n",
      "Author: Anton Shilov\n",
      "Date Published: 2023-11-03T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/21120/arm-acquires-minority-stake-in-raspberry-pi\n",
      "Content: Arm Holdings has acquired a minority stake in Raspberry Pi Ltd, reinforcing a partnership that began in 2008. This strategic investment is designed to support development of Raspberry Pi's low-cost low-power Arm-based platforms aimed at edge computing and IoT applications, leveraging Raspberry Pi's ability to deliver affordable, high-performance computing globally.;\"Arm and Raspberry Pi share a vision to make computing accessible for all, by lowering barriers to innovation so that anyone, anywhere can learn, experience and create new IoT solutions,\" said Paul Williamson, SVP and GM, Internet of Things Line of Business, Arm.Raspberry Pi's single-board-computers (SBCs) for students, enthusiasts, and commercial edge and IoT developers have historically been based exclusively on system-on-chips featuring Arm cores. This is a big deal for Arm as Raspberry Pi has soldmore than 40 million SBC units as of 2022, a huge number for Arm, which has not seen much success with IoT, despite high expectations of Softbank.The use of Arm technology has been quite beneficial for Raspberry Pi's product development, providing the necessary performance, energy efficiency, and software ecosystem to facilitate accessible computing for a wide range of users, from students to professional developers. But in the recent years competing SBCs based on RISC-V SoCs began to emerge, posing threat to Raspberry Pi's domination and to Arm's place in emerging edge computing, edge AI, and IoT markets.In a bid to strengthen Raspberry Pi's positions, Arm is infusing the company with cash, possibly trying to speed up development of more versatile and competitive solutions (either in terms of performance, or in terms of power).\"With the rapid growth of edge and endpoint AI applications, platforms like those from Raspberry Pi, built on Arm, are critical to driving the adoption of high-performance IoT devices globally by enabling developers to innovate faster and more easily,\" said Williamson. \"This strategic investment is further proof of our continued commitment to the developer community, and to our partnership with Raspberry Pi.\"\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/21120/arm-acquires-minority-stake-in-raspberry-pi\n",
      "Title: Report: AMD and NVIDIA Set to Offer Arm-Based Processors for PCs\n",
      "Author: Anton Shilov\n",
      "Date Published: 2023-10-25T12:30:00Z\n",
      "URL: https://www.anandtech.com/show/21106/amd-and-nvidia-set-to-offer-armbased-processors-for-pcs\n",
      "Content: Apple's release of Arm-based system-on-chips for its desktops and laptops three years ago demonstrated that such processors could offer competitive performance and power consumption. On the Windows-based PC front, only Qualcomm has offered Arm-based SoCs for notebooks in recent years, but it looks like it will soon be joined by AMD and NVIDIA, two notable players in the PC space, according to aReutersreport.While neither AMD nor NVIDIA has confirmed plans to offer Arm-based SoCs for client PCs, it will not be the first time these companies either planned or offered Arm-powered processors for the Windows ecosystem. Furthermore, this move will align with Microsoft's broader vision to push the adoption of Arm-based processors into Windows PCs to make them more competitive against Apple's offerings.Microsoft has spearheaded initiatives promoting the utilization of Arm-based processors, intending to diversify and evolve beyond the prevailing x86 architecture, predominantly controlled by Intel, for years. At first, the company attempted to do this in the late 2000s but saw no major success for years. However, AMD tried to develop a competitive Arm-based SoC (but had to fold development due to insufficient budgets), whereas NVIDIA offered its Tegra processor.Microsoft's renewed attempt to push Arm into Windows PCs kicked off in 2016 when the company signed a deal with Qualcomm, under which the latter produced PC-oriented Snapdragon application processors. In contrast, Microsoft tailored its Arm version of Windows specifically for these SoCs.\"Microsoft learned from the 90s that they do not want to be dependent on Intel again, they do not want to be dependent on a single vendor,\" said Jay Goldberg, chief executive of D2D Advisory, in a conversation with Reuters. \"If Arm really took off in PC (chips), they were never going to let Qualcomm be the sole supplier.\"That deal between Microsoft and Qualcomm is set to expire in 2024 when AMD, NVIDIA, and others will be able to release their Windows PC-oriented SoCs. But it remains to be seen what exactly the two companies will offer.AMD, Intel's arch-rival on the x86 CPU market for decades, may be inclined to expand its offerings. However, this move will not fit into its contemporary data center-oriented strategy. Furthermore, NVIDIA has also focused primarily on data center business in recent years.On the one hand, both companies are interested in making high-volume products of PCs. On the other hand, both earn tons of money on AI, HPC, and data center CPUs and GPUs, and both use all production capacity they can to meet demand for their high-margin offerings. Yet, historically, companies that solely concentrated on enterprise hardware went extinct or became parts of more prominent entities.Source:Reuters\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/21106/amd-and-nvidia-set-to-offer-armbased-processors-for-pcs\n",
      "Title: Arm Total Design to Facilitate Development of Custom Datacenter SoCs\n",
      "Author: Anton Shilov\n",
      "Date Published: 2023-10-19T00:00:00Z\n",
      "URL: https://www.anandtech.com/show/21099/arm-total-design-to-facilitate-development-of-custom-datacenter-socs\n",
      "Content: Arm this week introduced its Arm Total Design initiative, which is aimed at accelerating development of custom datacenter-oriented system-on-chip (SoC) designs usingNeoverse Compute Subsystems(CSS). The collaborative ecosystem unites various developers in a bid to speed up time-to-market and reduce development costs of custom SoCs for AI, cloud, and high-performance computing markets. ATD promises to enable development of datacenter processors that will offer formidable competition for x86 CPUs.The Arm Total Design ecosystem is a conglomerate of ASIC design houses, IP vendors, EDA tool providers, foundries, and firmware developers that is aimed to facilitate rapid and cost-efficient delivery of custom silicon for datacenters based on Arm Neoverse cores for AI, HPC, cloud, and networking workloads. The ecosystem provides preferential access to Neoverse CSS to its partners, fostering innovation, and facilitating faster time-to-market strategies, while also lowering development costs.The initiative aims to harness collective industry expertise at every stage of custom SoC development, thereby promoting the broad availability of specialized, Arm Neoverse-based solutions.Central to this initiative is the delivery of pre-integrated, validated IP and EDA tools, courtesy of collaborative efforts from partners such as Cadence, Rambus, and Synopsys. Such strategic collaborations are instrumental in speeding up silicon design process as it simplifies incorporation of essential components like memory, security, and various peripherals.In addition to the abovementioned companies, Arm Total Design leverages design services prowess of such companies ADTechnology, Alphawave Semi, Broadcom, Capgemini, Faraday, Socionext, and Sondrel. These companies bring their expertise to the table, providing robust support to the ecosystem given their experience with Neoverse CSS as well as other Arm IPs and technologies.Since the expected future of datacenter processors is multi-chiplet, the Arm Total Design ecosystem not only makes custom chips more accessible, but also poised to support AMBA CHI C2C, and UCIe standards. Intel Foundry Services and TSMC also participate in the Arm Total Design ecosystem bringing in leading-edge process technologies and advanced packaging techniques.Complementing the hardware-focused aspects, the initiative also places a strong emphasis on commercial software and firmware support for Neoverse CSS, drawing upon the specialized contributions from AMI.On paper, Arm Total Design emerges as a major alliance that could significantly alter the landscape of custom datacenter silicon development. By combining a diverse spectrum of industry leaders under one roof, it promises a seamless and efficient pathway towards realizing the potentials of Neoverse CSS. This collaborative venture aspires to unlock new levels of performance and features for AI, edge, and HPC SoCs based on the Arm technology, offering a decidedly more group-oriented approach to chip design than the vertically integrated strategies employed by industy heavyweights such as Intel and AMD.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/21099/arm-total-design-to-facilitate-development-of-custom-datacenter-socs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qualcomm Swaps Out Arm for RISC-V for Next-Gen Google Wear OS Devices\n",
      "Author: Anton Shilov\n",
      "Date Published: 2023-10-18T23:00:00Z\n",
      "URL: https://www.anandtech.com/show/21098/qualcomm-swaps-out-arm-for-risc-v-for-next-wear-soc\n",
      "Content: As part of a broad collaborative agreement with Google, Qualcomm this week said that that it will be adopting the RISC-V instruction set architecture (ISA) for a future Snapdragon Wear platform. Working together, the two companies will be bootstrapping a RISC-V ecosystem for Wear OS devices, with Qualcomm providing the hardware while Google expands its wearables OS and associated ecosystem of tools to support the new processor architecture.Qualcomm's Wear processors have been the de facto chip of choice for Wear OS devices since the launch of Google's wearables platform almost a decade ago, with Qualcomm employing multiple generations of Arm CPU designs. This makes Qualcomm's decision to develop a RISC-V wearables SoC especially significant, as it not only represents one of the highest profile adoptions of RISC-V in a consumer platform to date, but it means that, depending on Qualcomm's specific product plans, this could see the overall Wear OS market make a hard turn from Arm to RISC-V in relatively short order.As laid out in the relatively brief announcement from Qualcomm, the company will focus on development of RISC-V-based hardware suitable for wearable devices. While the company isn't disclosing detailed technical specifications of their in-development products, given the company's significant chip-design background, this likely includes customized RISC-V general purpose cores as well as sensors.Notably here, the announcement is for \"aRISC-V based wearables solution,\" rather than a complete pivot to RISC-V with multiple solutions. Wearables as a whole are a much smaller market than smartphones, so Qualcomm has historically not offered a particularly deep lineup of hardware – meaning that even one chip is significant. Still, this also means that Qualcomm is not formally dropping Arm from its Snapdragon Wear platform at this time.Qualcomm's decision to embrace RISC-V for a future wearables SoC is significant news for the up-and-coming ISA, as this marks one of the highest profile adoptions of RISC-V in consumer gear to date. The open standard ISA has seen success over the last several years in the microcontroller market, with chip vendors adopting RISC-V CPU cores – often in place of Arm Cortex-M designs – as a means of having more control over their CPU core designs, and avoid paying ISA royalties in the process. Conversely, RISC-V has seen very limited adoption in the application processor space thus far, owing to the more complex chip designs and the overall smaller market. So Qualcomm's plans to use RISC-V in their Snapdragon Wear platform, which has traditionally been based on Arm Cortex-A designs, marks a significant milestone for the adoption of RISC-V into higher-performing mobile devices.Similarly, Google's backing of the ISA by porting Wear OS to RISC-V is a major milestone on the software front. Bootstrapping a platform based on a new ISA is not just about the hardware, but the software as well, as there needs to be well-developed operating systems and applications to make the hardware useful. All of which requires significant tooling to enable that development. Google, for its part, is no stranger to embracing multiple ISAs – Android has long supported Arm, x86, and even MIPS – and the company already announced earlier this year that they're working to make RISC-V a \"tier-1\" platform for Android, so the company's efforts with Wear OS will go hand-in-hand with that.Between the two companies, Google and Qualcomm essentially make up the software and hardware backend of the Wear OS ecosystem. Google's Wear OS, in turn, is used by a range of popular smart watches, including those from Samsung, Fossil Group, Motorola, and Casio.\"Qualcomm Technologies have been a pillar of the Wear OS ecosystem, providing high performance, low power systems for many of our OEM partners,\" said Bjorn Kilburn, GM of Wear OS by Google. \"We are excited to extend our work with Qualcomm Technologies and bring a RISC-V wearable solution to market.\"Meanwhile, the decision to use RISC-V for wearables also has the potential to be a big change for the business side of Qualcomm. The company is currently butting heads with Arm over licensing and royalty rates, particularly in regards to their acquired Nuvia IP. That relationship has already devolved to lawsuits, including Arm looking to block Qualcomm's use of Nuvia-designed Arm CPU cores.In short, swapping out Arm for RISC-V would allow Qualcomm to do away with paying royalties to Arm for Snapdragon Wear chips. The current royalties aren't thought to be extravagant – Qualcomm is using Cortex-A53 here – but a penny saved is a penny booked for Qualcomm's quarterly earnings. If nothing else, the very public announcement about the development of a RISC-V Snapdragon Wear SoC can be considered a shot across Arm's bow, as a reminder that Qualcomm could eventually do the same thing with bigger and higher royalty bearing chips.\"We are excited to leverage RISC-V and expand our Snapdragon Wear platform as a leading silicon provider for Wear OS,\" said Dino Bekis, vice president and general manager, Wearables and Mixed Signal Solutions, Qualcomm Technologies. \" Our Snapdragon Wear platform innovations will help the Wear OS ecosystem rapidly evolve and streamline new device launches globally.\"\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/21098/qualcomm-swaps-out-arm-for-risc-v-for-next-wear-soc\n",
      "Title: Arm's Clients and Partners Signal Interest to Invest $735 Million Ahead of IPO\n",
      "Author: Anton Shilov\n",
      "Date Published: 2023-09-07T01:00:00Z\n",
      "URL: https://www.anandtech.com/show/20043/arms-clients-and-partners-to-invest-735-million-in-the-company\n",
      "Content: According to fresh SEC filings from Arm, the chip IP designer has secured a slew of industry investors ahead of the company's impending IPO. Aiming for a strong start to whatReuters reports is projected to be a $52 billion IPO valuation, Arm has been seeking out major industry customers as cornerstone investors, successfully lining up nearly a dozen companies from their efforts. Altogether, AMD, Apple, Cadence, Google, Intel, MediaTek, NVIDIA, Samsung, Synopsys, and TSMC have signaled an interest to purchase up to an aggregate of $735 million of Arm's American Depositary Shares (ADS), SoftBank, the owner of Arm, disclosed in a filing with theSecurities and Exchange Commission.While the exact number of shares to be purchased has not been disclosed – and may very well change ahead of the IPO as the current inquiries are non-binding – at the upper-end price of $51/share, a $735 million purchase would represent just over 15% of the 95.5 million Arm shares that SoftBank intends to offer as part of the IPO. Or, measured against the projected $52 billion valuation of the company, this would leave the cornerstone investors owning a collective 1.4% of Arm.The list of companies that plan to purchase Arm shares is pretty impressive as it contains not only Arm's partners and clients like Apple, Cadence, Google, Samsung, and TSMC, but also customer-rivals, such as AMD and Intel, who both use Arm IP in some of their chips while competing with Arm designs in other chips. Meanwhile, some of Arm's other big customers are notably absent from the cornerstone investor group, including Qualcomm and Amazon.Overall, the cornerstone investors represent a mix of fabless chip designers and tool vendors, as well as all three of the world's leading fabs themselves. For Intel's part, the company is establishing its Intel Foundry Services group to produce chips for fabless chip designers, and virtually all of them use Arm's cores. Therefore, close collaboration with Arm is something that IFS needs to have, and a good way of making friends with Arm is to own a piece of it.\"80% of TSMC wafers have an Arm processor in them,\" said Stuart Pann, Senior Vice President and General Manager of Intel Foundry Services, at the Goldman Sachs Communacopia & Technology Conference, reportsTom's Hardware. \"The fact that our organization, the IFS organization, is embracing Arm at this level, investing in Arm, doing partnerships with Arm should give you a signpost that we are absolutely serious about playing this business. Because if you are not working with Arm, you cannot be a foundries provider.\"Interestingly, the head of Intel's foundry unit even said that IFS will have to focus more on Arm and RISC-V going forward as both instruction set architectures are going to drive chip volumes and volumes is what Intel wants at its fabs.Meanwhile Apple, one of the founders of Arm back in the 1990, extended its license agreement with Arm beyond 2040, which is a testament that the company is confident of the ISA and its development, at least for now. Keeping in mind that for now all of Apple's products use at least one Arm's CPU core, it is not reasonable that the companies are going to remain partners for the foreseeable future.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/20043/arms-clients-and-partners-to-invest-735-million-in-the-company\n",
      "Title: Arm to Be Public Once More, Files for IPO on Nasdaq\n",
      "Author: Ryan Smith\n",
      "Date Published: 2023-08-21T22:15:00Z\n",
      "URL: https://www.anandtech.com/show/20020/arm-to-be-public-once-more-files-for-ipo-on-nasdaq\n",
      "Content: The ongoing saga of ownership of Arm appears to be finally nearing its end, as Arm has announced this afternoon that the company has made its long-awaited filing for an initial public offering (IPO) on the Nasdaq exchange. Share prices and listing dates have not been set as of this time, but Arm has secured the ARM ticker symbol for the new offering.The SoftBank-owned chip IP designer, whose designs are at the core of virtually every smartphone and countless other embedded computers, has been hanging in a state of limbo since early 2022, whenNVIDIA’s acquisition of the company was called off due to regulatory pressure. At the time, SoftBank announced that they would instead take Arm public – a much more challenging and less profitable endeavor – using the last 18 months to prepare for an IPO.SoftBank originally acquired Arm in 2016as a growth vehicle for the investment firm, paying roughly $32 billion for the chip designer. The company then began shopping Arm around in 2020 after other SoftBank investments such as WeWork turned sour, and SoftBank looked to shore up its balance sheets. Ultimately, the group found a potential buyer in NVIDIA, who was offering $40 billion for Arm, only for that exchange to never come to pass as regulators deemed Arm too critical of a company to be held by NVIDIA – or presumably any other single tech company, for that matter.The failure of the NVIDIA acquisition has left Arm in a state of limbo for the past year and a half. While there’s little doubt that SoftBank will be able to find investors on the open market, there’s a good deal more doubt over whether SoftBank would be able to sell any stake in Arm at a profit, given their relatively high 2016 buy-in and the fact that NVIDIA’s top offer was only 25% above that. SoftBank’s plans seemed to have softened in the meantime – the IPO filing indicates that SoftBank will be retaining voting control over Arm, so they’re not divesting themselves of Arm entirely – but the company is still looking to turn a profit on Arm, and IPO timing is an important factor in accomplishing that.At this point, Arm has not announced how many shares of the company will be sold or at what price, as those details will be determined later. Meanwhile,according to a report from Reuters on Friday, SoftBank re-acquired the outstanding 25% stake of Arm held by its Vision Fund unit in a deal valuing Arm at $64 billion. This is consistent with other reports that SoftBank is aiming for an IPO valuation between $60 billion and $70 billion, far better than NVIDIA’s offer and a well over what the investment firm acquired Arm for in the first place. These reports also claim thatSoftBank is courting NVIDIA, Intel, and other tech companies as initial investors, which would result in Arm being partially held by what amounts to a quasi-consortium of tech companies.A successful IPO should also provide some stability for the engineering side of Arm, though it won’t alleviate investment pressures entirely. As a public company, investors will be pushing Arm to further grow the company and raise revenues – a familiar spot for the previously-public chip designer – but now Arm will be able to develop products without the looming prospect of being sold to another company, and the change in priorities that would come from that. Ultimately, Arm is going to have to find ways to drive growth without making customers flinch from royalty rates, a tricky task given the success of RISC-V MCUs and other alternative processor designs.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/20020/arm-to-be-public-once-more-files-for-ipo-on-nasdaq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Unveils 2023 Mobile CPU Core Designs: Cortex-X4, A720, and A520 - the Armv9.2 Family\n",
      "Author: Gavin Bonshor\n",
      "Date Published: 2023-05-29T00:30:00Z\n",
      "URL: https://www.anandtech.com/show/18871/arm-unveils-armv92-mobile-architecture-cortex-x4-a720-and-a520-64bit-exclusive\n",
      "Content: Throughout the world, if there's one universal constant in the smartphone and mobile device market, it's Arm. Whether it's mobile chip makers basing their SoCs on Arm's fully synthesized CPU cores, or just relying on the Arm ISA and designing their own chips, at the end of the day, Arm underlies virtually all of it. That kind of market saturation and relevance is a testament to all of the hard work that Arm has done in the last few decades getting to this point, but it's also a grave responsibility – for most mobile SoCs, their performance only moves forward as quickly as Arm's own CPU core designs and associated IP do.Consequently, we've seen Arm settle into a yearly cadence for their client IP, and this year is no exception. Timed to align with this year's Computex trade show in Taiwan, Arm is showing off a new set of Cortex-A and Cortex-X series CPU cores – as well as a new generation of GPU designs – which we'll see carrying the torch for Arm starting later this year and into 2024. These include the flagship Cortex-X4 core, as well as Arm's mid-core Cortex-A720. and the new little-core Cortex-A520.Arm's latest CPU cores build upon the foundation of Armv9 and their previous Total Compute Solution (TCS21/22) ecosystem. For their 2023 IP, Arm is rolling out a wave of minor microarchitectural improvements through its Cortex line of cores with subtle changes designed to push efficiency and performance throughout, all the while moving entirely to the AArch64 64-bit instruction set. The latest CPU designs from Arm are also designed to align with the ongoing industry-wide drive towards improved security, and while these features aren't strictly end-user facing, it does underscore how Arm's generational improvements are to more than just performance and power efficiency.In addition to refining its CPU cores, Arm has undertaken a comprehensive upgrade of its DynamIQ Shared Unit core complex block, with the DSU-120. Although the modifications introduced are subtle, they hold substantial significance in terms of improving the efficiency of the fabric holding Arm CPU cores together, along with extending Arm's reach even further in terms of performance scalability with support for up to 14 CPU cores in a single block – a move designed to make Cortex-A/X even better suited for laptops.With three new CPU cores and a new core complex, there's a lot to cover. So let's dive right in.Arm TCS23 at a High Level: Pushing Efficiency & Going Pure 64-bitExpanding on the enhancements introduced in the Armv9.1 architecture last year, Arm is progressing through its scheduled development cycle with the latest Armv9.2 architecture. The primary objective of this cycle is to eliminate support for 32-bit applications and transition to a comprehensive 64-bit platform. Underpinning this transition is Arm's strategic framework, \"Total Compute Solutions\" (TCS), which revolves around three core principles: compute performance, security, and developer access. This approach forms the foundation for Arm's methodology and guides its efforts in delivering optimal performance, robust security measures, and streamlined developer capabilities.Arm's focus on phasing out the 32-bit instruction set has been one it has been working towards for several years. For their latest TCS23, they have finally created a fully 64-bit cluster to capitalize on the benefit of a complete 64-bit mobile ecosystem, excising AArch32 (32-bit instruction) support entirely.. So whether it's a big, mid, or little core, for Arm's latest generation of IP there is only AArch64.Developing a dynamic system-on-a-chip (SoC) that caters to a broad spectrum of mobile devices, ranging from cutting-edge flagship smartphones to entry-level models, necessitates a meticulous and consistent approach to maintaining competitiveness in a rapidly expanding market. In the realm of flagship devices, for instance, Qualcomm's Snapdragon 8 Gen2 SoC stands out, leveraging a cluster of Arm's Cortex-X3, Cortex A715/710, and Cortex-A510 cores. The upcoming iteration of Qualcomm's Snapdragon 8 Gen3 and other SoC manufacturers are poised to harness the power of Arm's TSC23 core cluster and intellectual property to further enhance performance in the subsequent generation of flagship mobile devices.Arm's latest DynamIQ Shared Unit, DSU-120, offers support for up to 14 CPU cores in a cluster, which opens the door to a significant number of different CPU core combinations. We'll see what SoC vendors have opted for later this year, but one probably configuration is a 1+5+2 (X4+720+520), which is likely a configuration for a high-end smartphone. Compared to a last-generation 1+3+4 cluster (X3+715+510), Arm is claiming an uplift of 27% in compute performance within GeekBench 6 MT and a more considerable uplift of between 33% and 64% in the Speedometer 2.1 benchmark depending on software optimizations implemented.Focusing more on the approach to 64-bit migration, last year Arm announced their first AArch64-only CPU core, the Cortex-A715. Consequently, last year saw the release of the first 64-bit only products, such as MediaTek's Dimensity 9200 SoC, as well as Google's Pixel 7 – which was 64-bit only as a platform choice rather than an architectural restriction.That said, actual AArch64 adoption/use within the larger software ecosystem has been slower than expected, primarily due to the Chinese market being slow to make the switch from 32-bit to 64-bit. Google has actually been key with its application storage (Google Play) by requiring its developers to submit 64-bit apps as far back as 2019, while also allowing the use of 32-bit applications on devices without native 64-bit support. Other markets haven't been as quick in doing so, but Arm claims that it is 'nudging' companies such as OPPO, Vivi, and Xiaomi to adopt AArch64 faster, which is believed to have the desired effect.With the initial Armv9 architecture, Arm made improvements to security through the use of its Memory Tagging Extension (MTE) (Armv8.5), which is a hardware-based implementation that uses Pointer Authentication (PA) extensions to help protect from memory vulnerabilities. Memory-based vulnerabilities have been a consistent threat to hardware-based security for many years, and it is something Arm is continually developing within its IP to help mitigate these types of attacks. For reference, Google's Chromium Project claimed that around 70% of high-severity bugs are from memory.One of the related security features of the latest Armv9.2 architecture is the introduction of a new QARMA3 Pointer Authentication Code (PAC) algorithm. Arm claims the newer algorithm reduces the CPU overhead of PAC to less than 1%, even on their little cores, giving developers and handset vendors even less of a reason to not enable the security feature. Most of these improvements revolve around hardware integrity and security, with a combination of MTE and native benefits through the 64-bit instruction and architecture, all designed to make devices even more secure going into 2023 and beyond. This fits with Arm's ethos to encourage a full switch to 64-bit over a hybrid 64 and 32-bit marketplace.Finally, looking at performance, Arm claims that their latest generation CPU and core complex architecture has made solid gains in power efficiency. At iso-performance, Cortex-X4 offers upwards of a 40% reduction in power consumption versus Cortex-X3, while Cortex-A720 and A520 save 20-22% over their respective predecessors. On the DSU-120 hub itself, Arm claims an 18% improvement in power efficiency.Of course, most of these power savings are going to instead be invested in additional performance. But it goes to show what SoC and handset vendors can aim for in this generation if they focus singularly on power efficiency and battery life.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18871/arm-unveils-armv92-mobile-architecture-cortex-x4-a720-and-a520-64bit-exclusive\n",
      "Title: Intel IFS Partners Up With Arm To Develop Improved Arm IP Designs for Intel's 18A Node\n",
      "Author: Gavin Bonshor\n",
      "Date Published: 2023-04-13T17:00:00Z\n",
      "URL: https://www.anandtech.com/show/18811/intel-ifs-partners-up-with-arm-to-design-socs-on-intel-s-upcoming-18a-node\n",
      "Content: In 2016, Intel's now-defunct Custom Foundry business and Arm teamed up to bring Arm's Artisan Physical IP and POP IP for its ARM Cortex-A processor cores to Intel's 10nm process. What was meant to be a long-term deal and collaboration subsequently turned out to be anything but, as Intel closed its Custom Foundry Business in 2018 due to issues with its manufacturing. Fast forward to 2021, and Intel's CEO Pat Gelsinger laid out plans as part of its IDM 2.0 strategy and created a new Foundry Service called Intel Foundry Services (IFS).Since the launch of Intel's Foundry Services, it has signed agreements and partnerships with several companies, including MediaTek, and the acquisition of Tower Semiconductors for $5.4 billion. Intel's latest deal involves a new collaboration with Arm which are based in Cambridge, UK, to enable Arm's IP and technologies to be optimized and built on Intel's upcoming 18A manufacturing process.Intel Foundry Services and Arm's partnership includes a design technology co-optimization (DTCO) agreement, where Arm's current and impending SoC-based IP is optimized for power, package area, performance, and cost on Intel's 18A process node. In other words, Arm and Intel are going to be developing versions of various Arm IP blocks that have been optimized for use on Intel's 18A process.While the announcement of the deal doesn't expressly state whether or not Arm will directly be developing the cores itself, it does open the doors for further collaborations in the future. This could include other companies using Arm's IP to create SoCs, including MediaTek and even Qualcomm, who have individual agreements with Intel Foundry Services.Render of Intel's Fab Campus in Ohio, $100 billion worth of investment in IFSOne essential part of Intel's Foundry Services was to create a standalone business offering a combination of different packaging and process technologies available to the industry. This included sourcing sites for new manufacturing facilities, including a planned location in Germany and a $20 billion investment into two new chip factories outside Ohio in the United States as part of Intel's IDM 2.0 strategy. The latest deal with Arm to optimize its designs for manufacturing on Intel's 18A node is just one part.Despite the failure of its previous Intel Custom Foundry, which had numerous issues with its 10 and 7 nm nodes, the latest Intel Foundry Services does offer confidence to the market through this new deal with ARM. One of the most significant benefits of Intel's recent agreement with Arm allows Intel to act as an impartial fabricator and manufacturer of chips outside of its Client Computing Group (CCG) and other arms, including its graphics (AXG) division. Not just Arm but Arm's current partners will also be able to take full advantage of Intel's open system foundry model, which includes packaging, software, and chiplets as part of the agreement.The move, including a combination of Intel's IDM 2.0 strategy, as well as the new deal with Arm to develop and optimize its SoCs for the Intel 18 A node, not only looks to build market confidence in IFS but it's a shot across the bow of the Taiwan Semiconductor Manufacturing Company (TSMC). TSMC is currently the market leader for chip manufacturing and leading-edge process nodes for chiplets designs.It's worth noting that allowing optimizations for Arm IP in SoCs in an area Intel has struggled explicitly with in the past will enable IFS to grow and potentially expand the business into areas such as the Internet of Things (IoT) and automotive.Source:IntelandArm\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18811/intel-ifs-partners-up-with-arm-to-design-socs-on-intel-s-upcoming-18a-node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Announces Neoverse V2 and E2: The Next Generation of Arm Server CPU Cores\n",
      "Author: Ryan Smith\n",
      "Date Published: 2022-09-15T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/17575/arm-announces-neoverse-v2-and-e2-the-next-generation-of-arm-server-cpu-cores\n",
      "Content: Just under four years ago, Arm announced their Neoverse family of infrastructure CPU designs. Deciding to double-down on the server and edge computing markets by designing Arm CPU cores specifically for those markets – and not just recycling the consumer-focused Cortex-A designs – Arm set about tackling the infrastructure market in a far more aggressive manner. Those efforts, in turn, have increasingly paid off handsomely for Arm and its partners, whom thanks to the likes of products like Amazon’s Graviton and Ampere Altra CPUs have at long last been able take a meaningful piece of the server CPU market.But as Arm CPUs finally achieve the market penetration that eluded them in the previous decade, Arm needs to make sure it isn’t resting on its laurels. Of the company’s three lines of Neoverse core designs –the efficient E, flexible N, and high-performance V – the company is already on its second generation of N cores, aptly dubbed theN2. Now, the company is preparing to update the rest of the Neoverse lineup with the next generation of V and E cores as well, announcing today the Neoverse V2 and Neoverse E2 cores. Both of these designs are slated to bring the Armv9 architecture to HPC and other server customers, as well as significant performance improvements.Arm Neoverse V2: Armv9 Graces High-Performance ComputingLeading the charge for Arm’s new CPU core IP is the company’s second-generation V-series design, the Neoverse V2. The complete V2 platform, codenamed Demeter, marks Arm’s first iteration on their high-performanceV-series cores, as well as the transition of this core lineup from the Armv8.4 ISA to Armv9. And while this is only Arm’s second go at a dedicated high-performance core for servers, make no mistake: Arm aims to be ambitious. The company is claiming that Neoverse V2 CPUs will offer the highest single-threaded integer performance available in the market, eclipsing next-generation designs from both AMD and Intel.While this week’s announcement from Arm is not a full-on deep-dive of the new architecture – and, more annoyingly, the company is not talking about specific PPA metrics – Arm is offering a high-level look at some of the changes and features that will be coming with the V2 platform. To be sure, the V2 IP is already finished and shipping to customers today (most notably NVIDIA), but Arm is playing coy to some degree with what they’re saying about V2 before the first chips based on the IP ship in 2023.First and foremost, the bump toArmv9brings with it the full suite of features that come with the latest Arm architecture. That includes the security improvements that are a cornerstone feature of the architecture (and especially handy for cloud shared environments) along with Arm’s newer SVE2 vector extensions.On the latter, Arm is making an interesting change here by reconfiguring the width of their vector engines; whereas V1 implemented SVE(1) using a 2 pipeline 256-bit SIMD, V2 moves to 4 pipes of 128-bit SIMDs. The net result is that the cumulative SIMD width of the V2 is not any wider than V1, but the execution flow has changed to process a larger number of smaller vectors in parallel. This change makes the SIMD pipeline width identical to Arm’s Cortex parts (which are all 128-bit, the minimum size for SVE2), but it does mean that Arm is no longer taking full advantage of thescalablepart of SVE by using larger SIMDs. I expect we’ll find out why Arm is taking this route once they do a full V2 deep dive, as I’m curious whether this is purely an efficiency play or something more akin to homogenizing designs across the Arm ecosystem.Past that, it’s likely worth noting that while Arm’s presentation slides put bfloat16 and int8 matmul down as features, these are notnewfeatures. Still, Arm is promising that V2’s SIMD processing will provide microarchitecture efficiency improvements over the V1.More broadly, V2 will also be introducing larger L2 cache sizes. The V2 design supports up to 2MB of private L2 cache per core, double the maximum size of V1. V2 will also be introducing further improvements to Arm’s integer processing performance, though the company isn’t going into further detail at this point. From an architectural standpoint, the V1 borrowed a fair bit from the Cortex-X1 CPU design, and it wouldn’t be too surprising if that was once again the case for the V2, borrowing from the X2. In which case consumer chips like the Snapdragon 8 Gen1 and Dimensity 9000 should provide a loose reference on what to expect.For the Demeter platform Arm will be reusing theirCMN-700 mesh fabric, which was first introduced for the V1 generation. CMN-700 is still a modern mesh design with support for up to 144 nodes in a 12x12 configuration, and is suitable for interfacing with DDR5 memory as well as PCIe 5/CXL 2 for I/O. As a result, strictly speaking the V2 isn’t bringing anything new at the fabric level – even the 512MB of SLC could be done with a V1 + CMN-700 setup – but this does mean that the CMN-700 mesh and its features is now a baseline moving forward with V2.The Neoverse V2 core, in turn, is going to be the cornerstone of the upcoming generation of high-performance Arm server CPUs. The de facto flagship here will beNVIDIA’s Grace CPU, which will be one of the first (if not the first) V2 design to ship in 2023. NVIDIA had previously announced that Grace would be based on a Neoverse design, so this week’s announcement from Arm finally confirms the long-held suspicion that Grace would be based on the next-generation Neoverse V core.NVIDIA, for its part, has their fall GTC event scheduled to take place in just a few days. So it’s likely we’ll hear a bit more about Grace and its Neoverse V2 underpinnings as NVIDIA seeks to promote the chip ahead of its release next year.Neoverse E2: Cortex-A510 For Use With N2Alongside the Neoverse V2 announcement, Arm is also using this week’s briefing to announce the Neoverse E2 platform. Unlike the V2 reveal, this is a much smaller scale announcement, and Arm is only offering a handful of technical details. Ultimately, E2’s day in the sun will be coming a bit later on.That said, the E2 platform is being delivered to partners with an eye towards interoperability with the existing N2 platform. For this, Arm has paired the Cortex-A510 CPU, Arm’s little/high-efficiency Cortex CPU core, and paired that with the CMN-700 mesh. This is intended to give server operators/vendors further flexibility by providing an alternative CPU core to the N2, while still offering the modern I/O and memory features of Arm’s mesh. Underscoring this, the E2 system backplane is even compatible with the N2 backplane.Neoverse Next: Poseidon, N-Next, and E-NextFinally, Arm’s announcement this week provides a glimpse at the company’s future roadmap for all three Neoverse platforms, where, unsurprisingly, Arm is working on updated versions of each of the platforms.Notably, all three platforms call for adding PCIe 6 support as well asCXL 3.0support. This would come from the next iteration of Arm’s CMN mesh network, which as Arm already does today, is shared between all three platforms.Meanwhile, it’s interesting to see the Poseidon name once again pop up in Arm’s roadmaps. Going back to Arm’svery first Neoverse roadmap, Poseidon was the name attached to Arm’s 5mn/2021 platform, a spot since taken by N2 and V1/V2 in various forms. With V2 not landing in hardware until 2023, Poseidon/V3 is still years off, but there’s likely some significance to Arm keeping the codename (such as new microarchitecture).But first out of the gate will be the N-Next platform – the presumable Neoverse N3. With the Neoverse N platform a generation ahead of the rest (N2 was first announced in 2020), it’ll be the next platform due for a refresh. N3 is due to be available to partners in 2023, with Arm broadly touting generational performance and efficiency improvements.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17575/arm-announces-neoverse-v2-and-e2-the-next-generation-of-arm-server-cpu-cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NVIDIA-Arm Acquisition Officially Nixed, SoftBank to IPO Arm Instead\n",
      "Author: Ryan Smith\n",
      "Date Published: 2022-02-08T11:00:00Z\n",
      "URL: https://www.anandtech.com/show/17245/nvidiaarm-acquisition-officially-nixed-softbank-to-ipo-arm-instead\n",
      "Content: NVIDIA’s year-and-a-half long effort to acquire Arm has come to an end this morning, as NVIDIA and Arm owner SoftBank have announced that the two companies are officially calling off the acquisition. Citing the current lack of regulatory approval of the deal and the multiple investigations that have been opened up into it, NVIDIA and SoftBank are giving up on their acquisition efforts, as the two firms no longer believe it will be possible to receive the necessary regulatory approvals needed to close the deal. In lieu of being able to sell Arm to NVIDIA (or seemingly anyone else), SoftBank is announcing that they will instead be taking Arm public.First announced back in September of 2020,SoftBank and NVIDIA unveiled what was at the time a $40 billion deal to have NVIDIA acquire the widely popular IP firm. And though the two companies expected some regulatory headwind given the size of the deal and the importance of Arm’s IP to the broader technology ecosystem – Arm’s IP is in many chips in one form or another – SoftBank and NVIDIA still expected to eventually win regulatory approval.However, after 17 months, it has become increasingly clear that government regulators were not apt to approve the deal. Even with concessions being made by NVIDIA, European Union regulators ended upopening an investigation into the acquisition, Chinese regulators have held off on approving the deal, andUS regulators moved to outright block it. Concerns raised by regulators centered around NVIDIA gaining an unfair advantage over other companies who use Arm’s IP, both by controlling the direction of its development and by their position affording NVIDIA unique access to insights about what products Arm customers were developing – some of which would include products being designed to compete with NVIDIA’s own wares. Ultimately, regulators have shown a strong interest in retaining a competitive landscape for chips, with the belief that such a landscape wouldn’t be possible if Arm was owned by a chip designer such as NVIDIA.As a result of these regulatory hurdles, NVIDIA and SoftBank have formally called off the acquisition, and the situation between the two companies is effectively returning to status quo. According to NVIDIA, the company will be retaining its 20 year Arm license, which will allow the company to continue developing and selling chips based around Arm IP and the Arm CPU architecture. Meanwhile SoftBank has received a $1.25 billion breakup fee from NVIDIA as a contractual consequence of the acquisition not going through.In lieu of selling Arm to NVIDIA, SoftBank is now going to be preparing to take Arm public. According to the investment group, they are intending to IPO the company by the end of their next fiscal year, which ends on March 23rdof 2023 – essentially giving SoftBank a bit over a year to get the IPO organized. Meanwhile,according to Reuters, SoftBank’s CEO Masayoshi Son has indicated that the IPO will take place in the United States, most likely on the Nasdaq.Once that IPO is completed, it will mark the second time that Arm has been a public company. Arm was a publicly-held company prior to the SoftBank acquisition in 2016, whenSoftBank purchased the company for roughly $32 billion. And while it’s still too early to tell what Arm will be valued at a second time around, it goes without saying that SoftBank would like to turn a profit on the deal, which is why NVIDIA’s $40 billion offer was so enticing. Still, even with the popularity and ubiquity of Arm’s IP across the technology ecosystem, it’s not clear at this time whether SoftBank will be able to get something close to what they spent on Arm, in which case the investment firm is likely to end up taking a loss on the Arm acquisition.Finally, the cancellation of the acquisition is also bringing some important changes to Arm itself. Simon Segars, Arm’s long-time CEO and major proponent of the acquisition, has stepped down from his position effective immediately. In his place, the Arm board of directors has already met andappointed Arm insider Rene Haas to the CEO position. Haas has been with Arm since 2013, and he has been president of the Arm IP Products Group since 2017.Arm’s news release doesn’t offer any official insight into why Arm is changing CEOs at such a pivotal time. But with the collapse of the acquisition, Arm and SoftBank may be looking for a different kind of leader to take the company public over the next year.Sources:NVIDIA,Arm\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17245/nvidiaarm-acquisition-officially-nixed-softbank-to-ipo-arm-instead\n",
      "Title: United States FTC Files Lawsuit to Block NVIDIA-Arm Acquisition\n",
      "Author: Ryan Smith\n",
      "Date Published: 2021-12-03T00:45:00Z\n",
      "URL: https://www.anandtech.com/show/17101/united-states-ftc-files-lawsuit-to-block-nvidiaarm-acquisition\n",
      "Content: In the biggest roadblock yet to NVIDIA’s proposed acquisition of Arm, the United States Federal Trade Commission (FTC) has announced this afternoon that the regulatory body will be suing to block the merger. Citing concerns over the deal “stifling the innovation pipeline for next-generation technologies”, the FTC is moving to scuttle the $40 billion deal in order to protect the interests of the wider marketplace.The deal with current Arm owner SoftBank wasfirst announced in September of 2020, where at the time SoftBank had been shopping Arm around in an effort to either sell or spin-off the technology IP company. And while NVIDIA entered into the deal with bullish optimism about being able to close it without too much trouble, the company has since encountered greater political headwinds than expected due to the broad industry and regulatory discomfort with a single chip maker owning an IP supplier used by hundreds of other chip makers. The FTC, in turn, is the latest and most powerful regulatory body to move to investigate the deal – voting 4-0 to file the suit – following theEuropean Union opening a probe into the merger earlier this fall. TheWhile the full FTC complaint has yet to be released, per apress release put out by the agency earlier today, the crux of the FTC’s concerns revolve around the advantage over other chip makers that NVIDIA would gain from owning Arm, and the potential for misconduct and other unfair acts against competitors that also rely on Arm’s IP. In particular, the FTC states that “Tomorrow’s technologies depend on preserving today’s competitive, cutting-edge chip markets. This proposed deal would distort Arm’s incentives in chip markets and allow the combined firm to unfairly undermine Nvidia’s rivals.”To that end, the FTC’s complaint is primarily focusing on product categories where NVIDIA already sells their own Arm-based hardware. This includes Advanced Driver Assistance Systems (ADAS) for cars, Data Processing Units (DPUs) and SmartNICs, and, of course,Arm-based CPUs for servers. These are all areas where NVIDIA is an active competitor, and as the FTC believes, would provide incentive for NVIDIA to engage in unfair competition.More interesting, perhaps, is the FTC’s final concern about the Arm acquisition: that the deal will give NVIDIA access to “competitively sensitive information of Arm’s licensees”, which NVIDIA could then abuse for their own gain. Since many of Arm’s customers/licensees aredirectly reliant on Arm’s core designs(as opposed to justlicensing the architecture), they are also reliant on Arm to add features and make other alterations that they need for future generations of products. As a result, Arm’s customers regularly share what would be considered sensitive information with the company, which the FTC in turn believes could be abused by NVIDIA to harm rivals, such as by withholding the development of features that these rival-customers need.NVIDIA, in turn, has announced that they will be fighting the FTC lawsuit, stating that “As we move into this next step in the FTC process, we will continue to work to demonstrate that this transaction will benefit the industry and promote competition.”Ultimately, even if NVIDIA is successful in defending the acquisition and defeating the FTC’s lawsuit, today’s announcement means that the Arm acquisition has now been set back by at least several months. NVIDIA’s administrative trial is only scheduled to begin on August 9, 2022, almost half a year after NVIDIA initially expected the deal to close. And at this point, it’s unclear how long a trial would last – and how long it would take to render a verdict.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17101/united-states-ftc-files-lawsuit-to-block-nvidiaarm-acquisition\n",
      "Title: European Union Regulators Open Probe Into NVIDIA-Arm Acquisition\n",
      "Author: Ryan Smith\n",
      "Date Published: 2021-10-27T20:40:00Z\n",
      "URL: https://www.anandtech.com/show/17038/european-union-regulators-open-probe-into-nvidiaarm-acquisition\n",
      "Content: Following an extended period of regulatory uncertainly regarding NVIDIA’s planned acquisition of Arm, the European Union executive branch, the European Commission, has announced that they have opened up a formal probe into the deal. Citing concerns about competition and the importance of Arm’s IP, the Commission has kicked off a 90 day review process for the merger to determine if those concerns are warranted, and thus whether the merger should be modified or blocked entirely. Given the 90 day window, the Commission has until March 15thof 2022 to publish a decision.At a high level, the EC’s concerns hinge around the fact that Arm is an IP supplier for both NVIDIA and its competitors. Which has led the EC to be concerned about whether NVIDIA would use its ownership of Arm to limit or otherwise degrade competitors’ access to Arm’s IP. This is seen as an especially concerning scenario given the breadth of device categories that Arm chips are in – everything from toasters to datacenters. As well, the EC will also be examining whether the merger could lead to NVIDIA prioritizing the R&D of IP that NVIDIA makes heavy use of (e.g. datacenter CPUs) to the detriment of other types of IP that are used by other customers.It is worth noting that this is going to be a slightly different kind of review than usual for the EC. Since NVIDIA and Arm aren’t competitors – something even the EC notes – this isn’t a typical competitive merger. Instead, the investigation is going to be all about the downstream effects of a major supplier also becoming a competitor.Overall, the need for a review is not terribly surprising. Given the scope ofthe $40 billion deal, the number of Arm customers (pretty much everyone), and the number of countries involved (pretty much everyone again), there was always a good chance that the deal could be investigated by one or more nations. Still, the EC’s investigation means that, even if approved, the deal will almost certainly not close by March as previously planned.\"Semiconductors are everywhere in products and devices that we use everyday as well as in infrastructure such as datacentres. Whilst Arm and NVIDIA do not directly compete, Arm's IP is an important input in products competing with those of NVIDIA, for example in datacentres, automotive and in Internet of Things. Our analysis shows that the acquisition of Arm by NVIDIA could lead to restricted or degraded access to Arm's IP, with distortive effects in many markets where semiconductors are used. Our investigation aims to ensure that companies active in Europe continue having effective access to the technology that is necessary to produce state-of-the-art semiconductor products at competitive prices.\"-Executive Vice-President Margrethe Vestager\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article already exists: https://www.anandtech.com/show/17038/european-union-regulators-open-probe-into-nvidiaarm-acquisition\n",
      "Title: The Arm DevSummit 2021 Keynote Live Blog: 8am PT (15:00 UTC)\n",
      "Author: Ryan Smith\n",
      "Date Published: 2021-10-19T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/17020/the-arm-devsummit-2021-keynote-live-blog-8am-pt-1500-utc\n",
      "Content: This week seems to be Arm's week across the tech industry. Following yesterday's Arm SoC announcements from Apple, today sees Arm kick off their2021 developer's summit, aptly named DevSummit. As always, the show is opening up with a keynote being delivered by Arm CEO Simon Segars, who will be using the opportunity to lay out Arm's vision of the future.Arm chips are already in everything from toasters to PCs – and Arm isn't stopping there. So be sure to join us at 8am PT (15:00 UTC) for our live blog coverage of Arm's keynote.10:57AM EDT- We're here for this year's Arm developer summit keynote10:58AM EDT- Like pretty much everything else this year, this is once again a virtual show in light of the coronavirus pandemic10:58AM EDT- So Arm's schedule and content is tweaked a bit to account from that10:59AM EDT- Arm is not unique in this, but the switch to virtual shows means that there's a much greater focus on videos that are produced ahead of time11:00AM EDT- Which means keynotes have been going from just talk-and-applause to more flashier, Apple-like presentations11:00AM EDT- And here we go11:01AM EDT- Starting with a pre-roll video11:01AM EDT- 200 billion Arm-based chips have been deployed11:03AM EDT- Oh, no, apparently that was a fake-out. The stream hosts are talking for a bit, before Simon's keynote actually gets started11:03AM EDT- Okay, *now* here we go11:03AM EDT- And here's Simon11:04AM EDT- Simon is starting off with a focus on the convergence of both the hardware and software ecosystems11:05AM EDT- Simon is recapping his own history with computing and how he got started11:05AM EDT- Back in the days of the Sinclair ZX81 and its 1KB of memory11:06AM EDT- Simon is thanking Sinclair himself for inspiring a generation of techies11:07AM EDT- Simon eventually went on to become an EE, and joining Arm11:08AM EDT- Continuing the walk down memory lane, Simon is now recapping the first embedded in-circuit emulation capabilities that were added to Arm in the 90s11:08AM EDT- And ultimately, the steps needed to have a CPU become a building block of a bigger chip11:09AM EDT- \"At our core, we're an engineering-focused organization\"11:09AM EDT- Now on to the not-too-distant future11:10AM EDT- Armv9 will be the core of Arm's next decade of computing IP. It's already shipping, and will show up in more and more IP and chips as time goes on11:10AM EDT- Arm designed Armv9 to run the full spectrum of compute, from HPC down to micro sensors11:11AM EDT- While also helping developers innovate on top of Arm's own IP11:11AM EDT- And of course, all of this saves time for chip design11:12AM EDT- Meanwhile, security and cyberattacks remain an ongoing issue for the world and for the tech industry building this hardware11:13AM EDT- Addressing this problem requires fundamentally rethinking matters11:14AM EDT- Simon believes that Arm shares in the responsibility for improving security11:14AM EDT- One such step Arm is taking is with Confidential Compute support in Armv911:15AM EDT- Moving on to the subject of decarbonization11:15AM EDT- Arm is committed to achieving carbon neutrality by 202311:16AM EDT- And of course, Arm's focus on energy efficiency is a way to help reduce power consumption (and thus CO2 generation)11:17AM EDT- And now a short testimonial from CloudFlare and why they use Arm11:18AM EDT- Moving on again to the changing nature of workloads. Starting with IoT11:19AM EDT- Simon is discussing one such example of a power line sensor that can detect if a line is damaged11:20AM EDT- Arm this week has announced Arm Total Solutions for IoT11:20AM EDT- 5G is another focus area for Arm11:20AM EDT- Arm is expecting a lot more than just handsets to be attached to 5G networks11:21AM EDT- Including the pieces necessary to further push edge computing11:21AM EDT- Arm is announcing the Arm 5G Solutions Lab to help develop the future of Arm-powered 5G hardware11:22AM EDT- What does Arm think is the most important technology of the next 50 years? AI11:22AM EDT- \"AI is entering a new stage of development\"11:23AM EDT- Arm expects AI to \"enhance the full spectrum of computing\"11:23AM EDT- The first devices incorporating Arm Ethos AI accelerators are starting to ship11:24AM EDT- Now a word on the Arm-NVIDIA merger11:25AM EDT- Simon thinks the combination of the two companies will be in a good position to tackle the future of AI11:25AM EDT- Arm's focus for the future will be supplying the tools needed to help hardware and software developers develop AI systems in a quick and efficient manner11:26AM EDT- \"Our purpose is to unlock the power of technology\"11:27AM EDT- And that's a wrap from Simon. Arm's VP of IoT, Mohamed Awad, is up in a moment11:28AM EDT- Simon's keynote was just under 30 minutes, so there's a couple of minutes of talking heads here before Awad's presentation starts11:32AM EDT- And here's Mohamed Awad, with a presentation titled \"Designing with Systems in Mind\"11:33AM EDT- Starting things off with robots11:34AM EDT- Recapping Asimov's 3 laws of robotics11:34AM EDT- And what modern laws look like11:35AM EDT- Slowly, science fiction about robotics will become less fictional11:36AM EDT- How Arm brings up systems, from definition and design to the final hardware development11:37AM EDT- With Arm providing the tools11:37AM EDT- These days Arm technology is baked into a immense number of specialized mobile devices11:38AM EDT- One area Arm is looking to improve the capabilities of mobile hardware is with their Memory Tagging Extensions for developers, to help them secure their software11:38AM EDT- And what Arm has learned in mobile they are applying to infrastructure11:39AM EDT- For Neoverse, Arm has not only worked on scaling up for cores, but also how those cores will be connected to other hardware, and how developers will create software for them11:41AM EDT- \"Establishing Arm in the cloud hasn't been easy\"11:41AM EDT- Awad credits designing things with a focus on whole systems as helping them to break into the cloud market11:42AM EDT- \"It's intelligence that will enable IoT to boost productivity\"11:42AM EDT- \"We must be honest about how hard it is to build IoT systems today\"11:43AM EDT- \"These systems are complex and massively fragmented\"11:43AM EDT- So Arm aims to help simplify the process11:43AM EDT- Such as Project Cassini11:44AM EDT- Arm has kicked off a related initiative: Project Centauri, a similar initiative for M-class systems11:45AM EDT- Arm partners shipped over 25B chips last year11:45AM EDT- Arm is doubling down on Corstone11:45AM EDT- Which will further help to accelerate SoC development times11:46AM EDT- One of Arm's customers, Alif, was able to achieve over an 800x performance improvement in ML versus their past M-class design11:47AM EDT- Now on to Arm Total Solutions for IoT, which was formally announced yesterday11:48AM EDT- Arm Virtual Hardware: develop on virtual hardware to deploy to physical hardware11:48AM EDT- Basically, an even greater focus on developing inside of a simulator11:49AM EDT- Arm partners are already using virtual hardware for their development processes11:49AM EDT- Including Amazon Lab126 and Himax11:50AM EDT- And now a short testamonial from AWS about using simulations for development11:52AM EDT- And Google's TensorFlow Mobile group11:53AM EDT- By making the virtual hardware available when the corstone is available, it allows software and hardware developers to get started right away, without waiting for silicon11:54AM EDT- \"We're fully committing to this Total Solutions approach\"11:54AM EDT- And that extends into Arm's IoT roadmap11:54AM EDT- Virtual hardware will be available at no cost while it's in beta11:55AM EDT- Arm believes this will take years off of product design cycles11:55AM EDT- And that's a wrap. Thank you for joining us.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17020/the-arm-devsummit-2021-keynote-live-blog-8am-pt-1500-utc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Apple Announces M1 Pro & M1 Max: Giant New Arm SoCs with All-Out Performance\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-10-18T20:00:00Z\n",
      "URL: https://www.anandtech.com/show/17019/apple-announced-m1-pro-m1-max-giant-new-socs-with-allout-performance\n",
      "Content: Today’s Apple Mac keynote has been very eventful, with the company announcing a new line-up of MacBook Pro devices, powered by two different new SoCs in Apple’s Silicon line-up: the new M1 Pro and the M1 Max.The M1 Pro and Max both follow-up onlast year’s M1, Apple’s first generation Macsilicon that ushered in the beginning of Apple’s journey to replace x86 based chips with their own in-house designs. The M1 had been widely successful for Apple, showcasing fantastic performance at never-before-seen power efficiency in the laptop market. Although the M1 was fast, it was still a somewhat smaller SoC – still powering devices such as the iPad Pro line-up, and a corresponding lower TDP, naturally still losing out to larger more power-hungry chips from the competition.Today’s two new chips look to change that situation, with Apple going all-out for performance, with more CPU cores, more GPU cores, much more silicon investment, and Apple now also increasing their power budget far past anything they’ve ever done in the smartphone or tablet space.The M1 Pro: 10-core CPU, 16-core GPU, 33.7bn Transistors in 245mm²The first of the two chips which were announced was the so-called M1 Pro – laying the ground-work for what Apple calls no-compromise laptop SoCs.Apple started off the presentation with a showcase of the packaging, there the M1 Pro is shown to continue to feature very custom packaging, including the still unique characteristic that Apple is packaging the SoC die along with the memory dies on a single organic PCB, which comes in contrast to other traditional chips such as from AMD or Intel which feature the DRAM dies either in DIMM slots, or soldered onto the motherboard. Apple’s approach here likely improves power efficiency by a notable amount.The company divulges that they’ve doubled up on the memory bus for the M1 Pro compared to the M1, moving from a 128-bit LPDDR4X interface to a new much wider and faster 256-bit LPDDR5 interface, promising system bandwidth of up to 200GB/s. We don’t know if that figure is exact or rounded, but an LPDDR5-6400 interface of that width would achieve 204.8GB/s.In a much-appreciated presentation move, Apple actually showcased the die shots of both the M1 Pro and M1 Max, so we can have an immediate look at the chip’s block layout, and how things are partitioned. Let’s start off with the memory interfaces, which are now more consolidated onto two corners of the SoC, rather than spread out along two edges like on the M1. Because of the increased interface width, we’re seeing quite a larger portion of the SoC being taken up by the memory controllers. However, what’s even more interesting, is the fact that Apple now apparently employs two system level cache (SLC) blocks directly behind the memory controllers.Apple’s system level cache blocks have been notable as they serve the whole SoC, able to amplify bandwidth, reduce latency, or simply just save power by avoiding memory transactions going off-chip, greatly improving power efficiency. This new generation SLC block looks quite a bit different to what we’ve seen on the M1. The SRAM cell areas look to be larger than that of the M1, so while we can’t exactly confirm this right now, it could signify that each SLC block has 16MB of cache in it – for the M1 Pro that would mean 32MB of total SLC cache.On the CPU side of things, Apple has shrunk the number of efficiency cores from 4 to 2. We don’t know if these cores would be similar to that of the M1 generation efficiency cores, or if Apple adopted the newer generation IP from the A15 SoC – we had noted that the new iPhone SoC had some larger microarchitectural changes in that regard.On the performance core side, Apple has doubled things up to 8 cores now. Apple’s performance cores were extremely impressive on the M1, however were lagging behind other 8-core SoCs in terms of multi-threaded performance. This doubling up of the cores should showcase immense MT performance boosts.On the die shot, we’re seeing that Apple is seemingly mirroring two 4-core blocks, with the L2 caches also being mirrored. Although Apple quotes 24MB of L2 here, I think it’s rather a 2x12MB setup, with an AMD core-complex-like setup being used. This would mean that the coherency of the two performance clusters is going over the fabric and SLC instead. Naturally, this is speculation for now, but it’s what makes most sense given the presented layout.In terms of CPU performance metrics, Apple made some comparisons to the competition – in particular the SKUs being compared here were Intel’s Core i7-1185G7, and the Core i7-11800H, 4-core and 8-core variants of Intel’s latest Tiger Lake 10nm 'SuperFin' CPUs.Apple here claims, that in multi-threaded performance, the new chips both vastly outperform anything Intel has to offer, at vastly lower power consumption. The presented performance/power curves showcase that at equal power usage of 30W, the new M1 Pro and Max are 1.7x faster in CPU throughput than the 11800H, whose power curve is extremely steep. Whereas at an equal performance levels – in this case using the 11800H's peak performance – Apple says that the new M1 Pro/Max achieves the same performance with 70% lower power consumption. Both figures are just massive discrepancies and leap ahead of what Intel is currently achieving.Alongside the powerful CPU complexes, Apple is also supersizing their custom GPU architecture. The M1 Pro now features a 16-core GPU, with an advertised compute throughput performance of 5.2 TFLOPs. What’s interesting here, is that this new much larger GPU would be supported by the much wider memory bus, as well as the presumably 32MB of SLC – this latter essentially acting similarly to what AMD is now achieving with their GPU Infinity Cache.Apple’s GPU performance is claimed to vastly outclass any previous generation competitor integrated graphics performance, so the company opted to make direct comparisons to medium-end discrete laptop graphics. In this case, pitting the M1 Pro against a GeForce RTX 3050 Ti 4GB, with the Apple chip achieving similar performance at 70% less power. The power levels here are showcased as being at around 30W – it’s not clear if this is total SoC or system power or Apple just comparing the GPU block itself.Alongside the GPU and CPUs, Apple also noted their much-improved media engine, which can now handle hardware accelerated decoding and encoding of ProRes and ProRes RAW, something that’s going to be extremely interesting to content creators and professional videographers. Apple Macs have generally held a good reputation for video editing, but hardware accelerated engines for RAW formats would be a killer feature that would be an immediate selling point for this audience, and something I’m sure we’ll hear many people talk about.The M1 Max: A 32-Core GPU Monstrosity at 57bn Transistors & 432mm²Alongside the M1 Pro, Apple also announced a bigger brother – the M1 Max. While the M1 Pro catches up and outpaces the laptop competition in terms of performance, the M1 Max is aiming at delivering something never-before seen: supercharging the GPU to a total of 32 cores. Essentially it’s no longer an SoC with an integrated GPU, rather it’s a GPU with an SoC around it.The packaging for the M1 Max changes slightly in that it’s bigger – the most obvious change is the increase of DRAM chips from 2 to 4, which also corresponds to the increase in memory interface width from 256-bit to 512-bit. Apple is advertising a massive 400GB/s of bandwidth, which if it’s LPDDR5-6400, would possibly be more exact at 409.6GB/s. This kind of bandwidth is unheard of in an SoC, but quite the norm in very high-end GPUs.On the die shot of the M1 Max, things look quite peculiar – first of all, the whole top part of the chip above the GPU essentially looks identical to the M1 Pro, pointing out that Apple is reusing most of the design, and that the Max variant simply grows downwards in the block layout.The additional two 128-bit LPDDR5 blocks are evident, and again it’s interesting to see here that they’re also increasing the number of SLC blocks along with them. If indeed at 16MB per block, this would represent 64MB of on-chip generic cache for the whole SoC to make use of. Beyond the obvious GPU uses, I do wonder what the CPUs are able to achieve with such gigantic memory bandwidth resources.The M1 Max is truly immense – Apple disclosed the M1 Pro transistor count to be at 33.7 billion, while the M1 Max bloats that up to 57 billion transistors. AMD advertises 26.8bn transistors for the Navi 21 GPU design at 520mm² on TSMC's 7nm process; Apple here has over double the transistors at a lower die size thanks to their use of TSMC's leading-edge 5nm process. Even compared to NVIDIA's biggest 7nm chip, the 54 billion transistor server-focused GA100, the M1 Max still has the greater transistor count.In terms of die sizes, Apple presented a slide of the M1, M1 Pro and M1 Max alongside each other, and they do seem to be 1:1 in scale. In which case, the M1 we already know to be 120mm², which would make the M1 Pro 245mm², and the M1 Max about 432mm².Most of the die size is taken up by the 32-core GPU, which Apple advertises as reaching 10.4TFLOPs. Going back at the die shot, it looks like Apple here has basically mirrored their 16-core GPU layout. The first thing that came to mind here was the idea that these would be 2 GPUs working in unison, but there does appear to be some shared logic between the two halves of the GPU. We might get more clarity on this once we see software behavior of the system.In terms of performance, Apple is battling it out with the very best available in the market, comparing the performance of the M1 Max to that of a mobile GeForce RTX 3080, at 100W less power (60W vs 160W). Apple also includes a 100W TDP variant of the RTX 3080 for comparison, here, outperforming the NVIDIA discrete GPU, while still using 40% less power.Today's reveal of the new generation Apple Silicon has been something we’ve been expecting for over a year now, and I think Apple has managed to not only meet those expectations, but also vastly surpass them. Both the M1 Pro and M1 Max look like incredibly differentiated designs, much different than anything we’ve ever seen in the laptop space. If the M1 was any indication of Apple’s success in their silicon endeavors, then the two new chips should also have no issues in laying incredible foundations for Apple’s Mac products, going far beyond what we’ve seen from any competitor.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17019/apple-announced-m1-pro-m1-max-giant-new-socs-with-allout-performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Hot Chips 2021 Live Blog: DPU + IPUs (Arm, NVIDIA, Intel)\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-08-23T21:20:00Z\n",
      "URL: https://www.anandtech.com/show/16903/hot-chips-2021-day-1-dpu-ipus-live-blog-arm-nvidia-intel\n",
      "Content: 05:27PM EDT- Welcome to Hot Chips! This is the annual conference all about the latest, greatest, and upcoming big silicon that gets us all excited. Stay tuned during Monday and Tuesday for our regular AnandTech Live Blogs.05:30PM EDT- Just waiting for this session to start, should be a couple of minutes05:30PM EDT- Arm up first with its Neoverse N2 cores05:34PM EDT- Roadmap, objectives, core architecture, system architecture, performance, conclusions05:34PM EDT- Second generation infrastructure followiung N105:34PM EDT- 4-128 core designs05:35PM EDT- 5G infrastructure to cloud data centers05:35PM EDT- Arm sells IP and definitions05:35PM EDT- SBSA/SBBR support05:36PM EDT- Marvell is already using N2, up to 36 in an SoC05:36PM EDT- High speed packet processing05:36PM EDT- All about SpecINT score with DDR5 and PCIe 5.005:37PM EDT- N2 with Arm v905:37PM EDT- Two lots of Scalable Vector Extensions, SVE, SVE205:37PM EDT- BF16 support, INT8 mul05:38PM EDT- Side channel security, SHA, SM3/405:38PM EDT- *SHA3/SHA51205:38PM EDT- Persistent memory support05:38PM EDT- memory partition and monitoring05:39PM EDT- Gen on gen improvements with virtualization05:39PM EDT- +40% IPC uplift05:39PM EDT- Similar power/area as N1, maximizes perf/Watt05:39PM EDT- an intense PPA trajectory05:40PM EDT- 3.6 GHz max core frequency05:40PM EDT- N1 on 7nm, vs N2 on 5nm05:41PM EDT- uArch - Most structures are biggers05:41PM EDT- bigger05:42PM EDT- Fetch more per cycle on the front end - increase branch prediction accuracy05:42PM EDT- Enhanced security to prevent side-channel05:43PM EDT- More bigger structures on the back end05:44PM EDT- N2 has Correlated Miss Caching (CMC) prefetching05:45PM EDT- Latency improvement on L2 as a result of CMC05:45PM EDT- 32% IPC improvement at iso-frequency05:46PM EDT- SPEC2006 was 40% mentioned earlier05:47PM EDT- Coherent Mesh Network - CMN700 - chiplets and multi-socket05:47PM EDT- Also CXL support05:48PM EDT- improvements over 600 - double mesh links, 3x cross sectional BW05:48PM EDT- Programmable hot-spot re-routing05:49PM EDT- Composable Datacenter SoCs - chiplets and IO dies and super home dies05:51PM EDT- balancing memory requests05:51PM EDT- control for capacity or bandwidth05:52PM EDT- Cbusy - throttling outstanding transactions to the CPU - affects hardware prefetcher aggressiveness05:53PM EDT- Cbusy and MPAM meant to work together05:54PM EDT- Resulting in best performance05:56PM EDT- Compared to the market with N205:56PM EDT- integer performance only05:57PM EDT- 'Real world workload' numbers based on pre-silicon models05:58PM EDT- Up to 256 cores of N2 should be fun05:59PM EDT- hit the market in the next few months05:59PM EDT- Q&A05:59PM EDT- Q: Is N1/N2 at iso-freq - what freq on slide 10? A: a range of power modes, quoted 2-2.5 GHz which is what customers will use06:01PM EDT- Q: Cbusy for a heterogeneous multi-die system? A: All IPs will get the CBusy information and throttle requests,06:03PM EDT- Q: MPAM cache partitioning? weight? A: It can do. But also support fine grain threshholds for control - you can tune based on capacity without overpartitioning06:03PM EDT- Second talk of the session - NVIDIA DPU06:04PM EDT- Idan Burstein, co-authored NVMoF06:04PM EDT- Architecture and platform use-cases06:05PM EDT- Data center is going through a revolution06:05PM EDT- Fully disaggregate your server between compute, memory, acceleration, storage, and software. Requires accelerated networking and DPUs to control it all06:06PM EDT- 10-20x bandwidth deployed per server requires better networking06:06PM EDT- a Datacenter infrastructure workload06:08PM EDT- Moving infrastructure workloads to the CPU is a bad idea06:08PM EDT- Need appropriate offload06:08PM EDT- Data pass acceleration needed06:09PM EDT- Bluefield-206:09PM EDT- Roadmap06:09PM EDT- Currently shipping BF-2, announced BF-3 with double bandwidth06:09PM EDT- BF-4 is 4x BF-306:09PM EDT- BF-4 also uses NVIDIA AI06:10PM EDT- 22 billion transistors06:10PM EDT- PCIe 5.0 x3206:10PM EDT- 400 Gb/s Crypto06:10PM EDT- 300 equivalent x86 cores06:10PM EDT- 16 cores of Arm A7806:10PM EDT- DDR5-5600, 128-bit bus06:11PM EDT- supports 18m IOPs06:11PM EDT- Connect-X 706:11PM EDT- DOCA Framework06:11PM EDT- Program on DOCA on BF-2, scales immediately to BF-3 and BF-406:12PM EDT- 3 different programmable engines06:12PM EDT- 16x Arm A78 - server level processor06:12PM EDT- 16 cores, 256 threads (SMT16?) datapath accelerator06:12PM EDT- ASAP - As soon as possible programmable packet processor flow pipeline06:13PM EDT- Bluefield-4X06:13PM EDT- Bluefield-3X06:13PM EDT- Not ASAP, ASAP-squared06:15PM EDT- Isolated boot domain in RT OS06:16PM EDT- PCIe tuned for DPU06:17PM EDT- Differentiating between the datapaths - software defined networking stack06:18PM EDT- accelerating the full path from host to network06:18PM EDT- it says bare metal host - can it do virtual hosts?06:19PM EDT- Encryption, Tunneling, NAT, Routing, QoS, Emulation06:19PM EDT- 100G DPDK06:19PM EDT- Million packets per second06:20PM EDT- vs AMD EPYC 7742 64-core06:20PM EDT- This is Bluefield 206:20PM EDT- TCP flow with 100G IPSEC06:22PM EDT- Storage processing acceleration06:22PM EDT- Data in-flight encryption06:24PM EDT- NVMe over Fabric06:27PM EDT- Cloud-native supercomputing with non-blocking MPI performancew06:27PM EDT- Accelerated FFT performance across multi-node HPC06:28PM EDT- DPU isolates Geforce Now - 10 million concurrent users06:28PM EDT- +50% more users per server06:28PM EDT- Push more concurrent users06:29PM EDT- Bluefield 3X has onboard GPU06:30PM EDT- Support for CUDA06:30PM EDT- GPU + DPU + network connectivity, fully programmable on a single PCIe card06:31PM EDT- Q&A Time06:31PM EDT- Q: Cortex A rather than N1/N2 A: A78 was the most performing core at the time06:31PM EDT- Q: Add CXL to future Bluefield? A: Can't comment. See CXL as important06:33PM EDT- Q: RT-OS cores? A: Designed internally, arch is RISC-V compatible06:34PM EDT- Q: Can DPU accelerate RAID construction? A: Yes it can - trivial and complex06:36PM EDT- That's the end, next up is Intel06:36PM EDT- Bradley Burres06:37PM EDT- Driving network across the datacenter for Intel06:38PM EDT- Same five minute intro about IPUs as the previous talks06:40PM EDT- IPU over time has been gaining more control to free up CPU resources. Move these workloads to the IPU = more performance!06:41PM EDT- 'solving the infrastructure tax'06:41PM EDT- Mount Evans06:41PM EDT- Developed with a CSP06:41PM EDT- Baidiu or JD ?06:42PM EDT- 16 Neoverse N1 cores06:42PM EDT- 200G Ethernet MAC06:42PM EDT- PCIe 4.0 x1606:42PM EDT- NVMe storage with Optane recognition06:42PM EDT- Advanced crypto and compression acceleration06:42PM EDT- Software, Hardware, Accelerator co-design06:43PM EDT- solving the long-tail infrastructure issue06:44PM EDT- Dataplane on the left, compute on the right06:44PM EDT- Support 4 socket systems with one Mount Evans06:44PM EDT- RDMA and ROCE v206:44PM EDT- QoS and telemetry up to 200 million packets per second06:45PM EDT- Inline IPSec06:46PM EDT- N1s at 3 GHz06:46PM EDT- three channels of dual mode LPDDR4 - 102 GB/s bandwidth06:46PM EDT- Engines for crypto06:47PM EDT- Intel didn't just glue assets together06:49PM EDT- P4 programmable pipeline06:51PM EDT- Most applications for IPU is 'brownfield' - has to be dropped in to current infrastructure06:54PM EDT- Now talking system security with isolation and recovery independent of workloads and tenants06:55PM EDT- QoS, uptime06:56PM EDT- malicious driver detection06:56PM EDT- Futureproofing06:56PM EDT- Compliant to NSA standards and FIPS140. Said something about 2030?06:57PM EDT- More info at the intel On event06:57PM EDT- Q&A Time06:58PM EDT- Q: PPA with Arm vs IA A: IP available and schedule picked Arm06:59PM EDT- Q: SBSA compliant? A: Yes06:59PM EDT- Q: TDP? A: work within PCIe power07:00PM EDT- Q: Work with SPR given both crypto? A: Yes07:00PM EDT- Q: Does Mount Evans replace server PCH? A: No, orthogonal07:02PM EDT- Q: specific to Xeon A: Use with any CPU07:02PM EDT- THat's a wrap, time for a keynote!07:02PM EDT- .\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16903/hot-chips-2021-day-1-dpu-ipus-live-blog-arm-nvidia-intel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Sponsored Post: Keep Your App’s Memory Safe with Arm Memory Tagging Extension (MTE)\n",
      "Author: Sponsored Post\n",
      "Date Published: 2021-06-14T16:30:00Z\n",
      "URL: https://www.anandtech.com/show/16759/sponsored-post-keep-your-apps-memory-safe-with-arm-memory-tagging-extension-mte\n",
      "Content: Subtle memory bugs, including buffer overruns and pointer errors, create ticking time bombs inside your applications. Malicious actors can exploit these bugs to execute unauthorized code, take over systems to add them to malware botnets, or simply cause applications and systems to crash. The notorious Morris Worm of 1988 was one of the earliest examples of a malicious application exploiting a buffer overflow. Announcements of memory safety issues creating potential exploits arrive with alarming frequency, either from security researchers or found loose in the wild.The impact on users can be substantial. Rogue applications can take advantage of unsafe memory in order to gain access to sniff out sensitive data, such as user credentials and passwords, enabling access to higher levels of privilege in the system. This allows bad actors to gain access to confidential data or make the system part of a larger botnet. It’s not always outside forces that cause problems – sometimes unsafe memory results in unpredictable system crashes due to memory leaks and related issues, frustrating users. It’s estimated that two-thirds of all Android vulnerabilities happen due to unsafe memory practices.Arm Memory Tagging ExtensionSoftware-based solutions, including Address Sanitizer (Asan), help mitigate these memory issues by integrating memory corruption detection into modern compilers. However, Asan requires adding software instrumentation to application code, which can significantly slow down app runtime and increase memory usage, particularly problematic in mobile and embedded systems.What’s needed is a solution to detect and minimize memory bugs with minimal impact on performance and memory use. Properly implementing a hardware-based method for detecting potentially unsafe memory usage results in smaller memory usage and better performance, while improving system reliability and security.Arm introduced its memory tagging extension as a part of the Armv8.5 instruction set. MTE is now built into Armv9 compliant CPUs recently announced by Arm, such as the Cortex-X2, Cortex-A710, and Cortex-A510. Future CPUs based on Armv9 will also integrate MTE. These all include memory tagging as a basic part of the architecture.The idea behind memory tagging is pretty simple: add a small set of bits to chunks of memory to identify them as safe for application usage. Arm implements memory tagging as a two-phase system, known as the lock and the key:Address tagging. This adds four bits to the top of every pointer in the process. Address tagging only works with 64-bit applications since it uses top-byte-ignore, which is an Arm 64-bit feature. Address tags act as a virtual “key.”Memory tagging. Memory tags also consist of four bits, but are linked with every aligned 16-byte region in the application’s memory space. Arm refers to these 16-byte regions astag granules.These four bits aren’t used for application data and are stored separately. The memory tag is the “lock”.A virtual address tag (key) must match the memory tag (lock). Otherwise, an error occurs.Figure 1.Shows an example of lock and key access to memorySince the address tag must match the memory tag, the first thing you might notice is that 4-bits is only 16 variations. This makes MTE a stochastic process, which means that it is possible for a key to incorrectly match up to a different lock. The likelihood of this happening is less than 8%, according to Arm.Since address and memory tags are created and destroyed on the fly frequently, memory allocation units work to make sure that sequential memory tags always differ. MTE supports random tag generation as well. The combination of the memory allocator understanding that sequential tags must be different plus the random tag generation feature means the actual frequency of tag clashes is quite low. Furthermore, running MTE across a fleet of millions (or billions) of devices can provide robust error detection for system and application software.Underlying ArchitectureArmv8.5 and v9 implement a new memory type, which Arm dubs Normal Tagged Memory. The CPU can determine the safety of a memory access, by comparing an address tag to the corresponding memory tag. Developers can choose whether or not a tag mismatch results in a synchronous exception or reported asynchronously, which allows the application to continue. Figure 2 shows how MTE is implemented in ARM CPU designs.Figure 2.Arm Total Compute Solution (Armv9)Asynchronous mismatch details accumulate in a system register. This means the OS can isolate mismatches to specific execution threads and make decisions based on ongoing operations.Synchronous exceptions can directly identify the specific load or store instruction causing tag mismatches. Arm added a variety of new instructions to the instruction set to manipulate tags, handle pointer and stack tagging, and for low-level system use.Implementing Arm MTEMTE is handled in hardware; load and store instructions have been modified to verify that the address tag matches the memory tag, and hardware memory allocation ensures the randomization of address and memory tag creation. This has differing implications for OS developers and end-user application programmers.Arm enhanced its AMBA 5 coherent interconnect to support MTE. Tag check logic is typically built into the system-level cache, with tag checking and tag caching occurring ahead of the DRAM interface. Figure 3 shows an example block diagram.Figure 3: Example block diagram showing how MTE might be implemented in an SoC design. (Source: Arm)Operating systems must be modified in order to fully support MTE. Arm initially prototyped MTE by creating a version of the Linux kernel which implemented tags. Google has expressed its intent to add MTE to Android and is working with SoC developers to ensure compatibility.End-user application developers have it a bit easier assuming operating system support for MTE. Since MTE occurs behind the scenes in the OS and hardware, applications require no source code modifications. MTE tagging for heap memory requires no extra effort. However, tagging memory on existing runtimes using stack memory requires compiler support, so existing binaries need to be recompiled. This is straightforward since mobile app developers frequently push out updates anyway. Figure 4 shows the software development timeline when implementing MTE.Figure 4:Software development timeline with MTEEnsuring memory is protected may require aligning memory objects to the Tag Granule (16-byte alignment). This can increase stack and memory utilization, though the impact seems to be fairly minimal.Why Use Arm MTE?MTE offers several quality-of-life improvements for developers. MTE allows programmers to find memory-related bugs quickly, speeding up the application debugging and development process. Since memory bugs can be found and quashed sooner, issues such as memory leaks, memory race conditions, and other memory-related crashes become more infrequent. This in turn improves the end-user experience.Memory safety bugs account for about two-thirds of all common vulnerabilities and exposure (CVE) bugs, so MTE allows companies to ship applications faster with fewer bugs. End users may often be reluctant to upgrade to new hardware or operating system software, but MTE gives them tangible reasons to upgrade, including improved stability and overall security.Further InformationYou can find more detailed information on Arm’s memory tagging extensions in a variety of sources.Arm’s white paperdescribing its implementation of MTE.Google’s Konstantin Serebryany’s detailed paper on howMTE improves memory safety with C/C++.How Armimplemented MTE in the Linux kernel.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16759/sponsored-post-keep-your-apps-memory-safe-with-arm-memory-tagging-extension-mte\n",
      "Title: Arm Announces New Mali-G710, G610, G510 & G310 Mobile GPU Families\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-05-25T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/16694/arm-announces-new-malig710-g610-g510-g310-mobile-gpu-families\n",
      "Content: Alongside with today’s extremely large and comprehensive CPU line-up announcement including the new Cortex-X2, Cortex-A710, Cortex-A510, new DSU-110 and new interconnects, we’re seeing the announcement of Arm’s newest Mali GPU line-up. Similar to the CPU family, we’re also seeing an extensive line-up announcement with the new Mali-G710 flagship series, the G510 middle-range, and the new ultra-area efficient Mali-G310.The new GPU series follows up in the same Valhall GPU family that wasstarted back in 2019 with the Mali-G77and seen minor improvements with theMali-G78 in last year’sannouncements and seen silicon adoption in this year’s SoC’s such as the Kirin 9000, Exynos 2100 or the new MediaTek Dimensity SoCs.At the high-end, the Mali-G710 is a direct successor to the Mali-G78 and is a relatively straightforward generational improvement in terms of what it’s aiming for: the highest possible performance that Arm’s architects can achieve in a Mali GPU. The Mali-G610 is a branding exercise that differentiates the same microarchitecture as the G710 at lower core counts, aiming to aid partners to better differentiate flagship products from the “premium” segment.The Mali-G510 is a successor to the 2019 Mali-G57 and is a major upgrade to Arm’s mid-range portfolio, bringing extremely large generational performance boosts as well as power efficiency gains over the predecessor.Finally, the new Mali-G310 is a new Valhall based low-end entry that represents a multi-generation architectural bump over the old-in-the-tooth Bifrost based Mali-G31 and targets the low-end area-efficiency focused market where we see hundred of billions of low-cost devices and other embedded markets such as smart TVs.As gross overview, the highlight today for most readers will be focused around the new Mali-G710 flagship GPU. The improvements that the company is promising is roughly a +20% boost in performance in a ISO-process node GPU configuration compared to a comparable Mali-G78 GPU. Similarly, at similar performance, the new GPU design promises a -20% reduction in power consumption and thus also energy efficiency gain.Recently, Arm has also made a focus on Machine Learning on the GPU and here the new design is promising a larger +35% boost in performance.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16694/arm-announces-new-malig710-g610-g510-g310-mobile-gpu-families\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Announces Mobile Armv9 CPU Microarchitectures: Cortex-X2, Cortex-A710 & Cortex-A510\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-05-25T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16693/arm-announces-mobile-armv9-cpu-microarchitectures-cortexx2-cortexa710-cortexa510\n",
      "Content: It’s that time of the year again, and after last month’s unveiling of Arm’snewest infrastructure Neoverse V1 and Neoverse N2CPU IPs, it’s now time to cover the client and mobile side of things. This year, things Arm is shaking things up quite a bit more than usual as we’re seeing three new generation microarchitectures for mobile and client: The flagship Cortex-X2 core, a new A78 successor in the form of the Cortex-A710, and for the first time in years, a brand-new little core with the new Cortex-A510. The three new CPUs form a new trio of Armv9 compatible designs that aim to mark a larger architectural/ISA shift that comes very seldomly in the industry.Alongside the new CPU cores, we’re also seeing a new L3 and cluster design with the DSU-110, and Arm is also making a big upgrade in its interconnect IP with the new cache coherent CI-700 mesh network and NI-700 network-on-chip IPs.The Cortex-X2, A710 and A510 follow up on last year's X1, A78 and A55. For the new Cortex-X2 and A710 in particular, these are direct microarchitectural successors to their predecessors. These parts, while iterating on generational improvements in IPC and efficiency, also incorporate brand-new architectural features in the form of Armv9 and new extensions such as SVE2.The Cortex-A510, Arm's new little core, is a larger microarchitectural jump, as it represents a new clean-sheet CPU design from Arm’s Cambridge CPU design team. A510 brings large IPC improvements while still having a continued focus on power efficiency, and, perhaps most interestingly, retains its characteristic in-order microarchitectural.An Armv9 CPU Family – AArch64 only for all practical purposes*The new CPU family marks one of the largest architectural jumps we’ve had in years, as the company is now baselining all three new CPU IPs on Armv9.0. We've extensivelycovered the details of the new Arm architectureback in late March. Cornerstone features of the new ISA include the new enrollment of prior optional/missing Armv8.2+ features that weren’t guaranteed in mobile and client designs (mostly due to the older A55 cores), and the introduction of new SVE2 SIMD and vector extensions.One big change we’ve been expecting for quite some time now is that we’ll be seeing a deprecation of the 32-bit AArch32 execution mode in upcoming Arm Cortex-A mobile cores. The clock has been ticking for 32-bit apps ever sinceGoogle’s announcedin 2019 that the Google Play store will require for 64-bit app uploads, and the company will stop serving 32-bit applications to 64-bit compatible devices later this summerWhile Arm is declaring that shift to happen in 2023, for all intents and purposes it’s already happening next year for most global users. Both the Cortex-X2 flagship core and the Cortex-A510 little cores are AArch64-only microarchitectures that are no longer able to execute AArch32 code.With that said, sharp readers will note that two out of three CPUs isn't acompleteshift, and the reason for that is because the Cortex-A710 actually still supports AArch32. Arm states that the reason for this is primarily to meet the needs of the Chinese mobile market, which lacks the homogeneous ecosystem capabilities of the global Play Store markets, and Chinese vendors and their domestic app market require a little more time to facilitate the shift towards 64-bit only. This means we’ll have an odd scenario next year of having SoCs on which only the middle cores are able to execute 32-bit applications, with those apps being relegated to the middle A710 cores and missing out on the little A510 cores’ power efficiency or the X2 cores’ performance.On the big core side, the new Cortex-X2 and Cortex-A710 are successors to the Cortex-X1 and Cortex-A78. Both designs are mostly designed by Arm’s Austin design team, and represent the 4thgeneration of this microarchitecture family, which hadstarted off with the Cortex-A76 several years ago. These cores should be the last of this microarchitecture family before Arm hands things off to a completely new design with next year’s new Sophia cores.In terms of design philosophy, the X2 and A710 generally keep the same overarching goals the X1 and A78 had defined: The X-series continues to focus on advancing performance by increasing microarchitectural structures and by Arm being willing to make compromises on power within reasonable limits. Meanwhile the A710 continues to focus on advancing performance and efficiency through smarter design and with a large focus on maximizing the power, performance, and area (PPA) balance of the IP.One point Arm makes in the above slide is having optimized critical paths and physical design for sustained voltage operations – this is more of a goal the company is striving for in the next generations of “middle” cores rather than something that’s specifically reflected in the Cortex-A710.This year, we are also finally seeing a new little core.We had covered the Cortex-A55 back in 2017, and since then we haven’t had seen any updates to Arm’s little cores, to the point of it being seen as large weakness of last few generations of mobile SoCs.The new Cortex-A510 is a clean-sheet design from Arm’s Cambridge design team, leveraging a lot of the technologies that had been employed in the company’s larger cores, but implemented into a new in-order little microarchitecture. Yes – we’re still talking about an in-order core, and Arm still sees this to be the best choice in terms of extracting the best efficiency and “Days of use” of mobile devices.Even though it’s a in-order core, Arm made a comparison that the new design is extremely similar to a flagship core of 2017 – namely the Cortex-A73, achieving very similar IPC and frequency capabilities whilst consuming a lot less power.The new design also comes with a very interesting shared complex approach and shares the L2 and FP/SIMD pipelines with a second core, a design approach Arm calls “merged core” and undoubtedly will remind readers of AMD’s CMT approach in Bulldozer cores 10 years ago, even though there are quite important differences in the approaches.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16693/arm-announces-mobile-armv9-cpu-microarchitectures-cortexx2-cortexa710-cortexa510\n",
      "Title: Qualcomm Shows Off Snapdragon Dev Kit for Windows on Arm Development\n",
      "Author: Ryan Smith\n",
      "Date Published: 2021-05-24T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/16697/qualcomm-shows-off-snapdragon-dev-kit-for-windows-on-arm-development\n",
      "Content: Alongsidetoday’s Snapdragon 7c Gen 2 SoC announcement, Qualcomm is also unveiling a new Windows 10-focused development kit. Collaborating with Microsoft, the two companies have put together the Snapdragon Developer Kit for Windows 10, which true to its name, is designed to serve as a dev kit for application authors to more easily test Windows 10 on Arm programs. The pint-sized PC is expected to be available this summer.Overall, while devices based on Qualcomm’s Windows-capable Snapdragon SoCs have been around for a couple of years now, neither Qualcomm nor Microsoft have put together an official development kit for the platform. And though the idea of a development kit is somewhat foreign in the PC landscape where there is no one PC platform (x86 or otherwise), Qualcomm’s Windows on Arm (WoA) efforts hail from the mobile world, where dev kits and reference devices are common. So in an effort to better meet the needs of WoA application developers, whom until now have been stuck doing testing on laptops and tablets like the Surface Pro X, Qualcomm and Microsoft are putting together a proper mini-PC for developer testing.At this point, Qualcomm isn’t saying too much about the PC itself, in part to give Microsoft something to announce as part of their Build conference later this week. However, given the timing of the announcement – as well as Qualcomm’s own comments on ensuring the dev kit remains affordable – it wouldn’t be too surprising to see the kit based around the new Snapdragon 7c Gen 2 SoC. Though the slowest of Qualcomm’s offerings, the 7c Gen 2 is also the cheapest option, and more than sufficient for basic compatibility testing.Meanwhile a side-shot of the PC at least gives us a basic idea of what to expect for I/O. The right side of the box ha a single USB port, along with a SD card slot and a third, unknown card slot (SIM?).The Snapdragon Developer Kit will go on sale this summer, with Microsoft selling the dev kit directly through their online store.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16697/qualcomm-shows-off-snapdragon-dev-kit-for-windows-on-arm-development\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Announces Neoverse V1, N2 Platforms & CPUs, CMN-700 Mesh: More Performance, More Cores, More Flexibility\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-04-27T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16640/arm-announces-neoverse-v1-n2-platforms-cpus-cmn700-mesh\n",
      "Content: 2020 has been an extremely successful year for Arm’s infrastructure and enterprise endeavours, as it was the year where we’ve seen fruition of the company’s “Neoverse” line of CPU microarchitectures hit the market in the form of Amazon’snew Graviton2 designas well asAmpere’s Altraserver processor. Arm had first introduced the Neoverse N1 back in early 2019 and if you weren’t convinced of the Arm server promise with the Graviton2, the more powerful and super-sized Altra certainly should have turned some heads.Inarguably the first generation of Arm servers that are truly competitive at the top end of performance, Arm is now finally achieving a goal the company has had in their sights for several years now, gaining real market share against the x86 incumbents.Fast-forward to 2021, the Neoverse N1 design today employed in designs such as the Ampere Altra is still competitive, or beating the newest generation AMD or Intel designs – a situation that which a few years ago seemed farfetched. We recommend catching up on these important review pieces over the last 2 years to get an accurate picture of today’s market:Arm Announces Neoverse N1 & E1 Platforms & CPUs: Enabling A Huge Jump In Infrastructure PerformanceAmazon's Arm-based Graviton2 Against AMD and Intel: Comparing Cloud ComputeThe Ampere Altra Review: 2x 80 Cores Arm Server Performance MonsterAMD 3rd Gen EPYC Milan Review: A Peak vs Per Core Performance BalanceIntel 3rd Gen Xeon Scalable (Ice Lake SP) Review: Generationally Big, Competitively Small(Note: Y axis left chart starts at 50%)Arm is very open that their main priority with the Neoverse line of products is gaining cloud footprint deployment market share, and as an example of the new-found success is an estimate into Amazon’s own AWS instance additions throughout 2020, where the new Arm-based Graviton2 is said to be the dominant hardware deployment, picking up the majority of share that’s being lost by Intel.Looking towards 2022 and BeyondToday, we’re pivoting towards the future and the new Neoverse V1 and Neoverse N2 generation of products. Armhad already tested the new products last September, teasing a few characteristics of the new designs, but falling short of disclosing more concrete details about the new microarchitectures. Followinglast month’s announcement of the Armv9 architecture, we’re now finally ready to dive into the two new CPU microarchitectures as well as the new CMN-700 mesh network.As presented back in September, this generation of Neoverse CPU microarchitectures differ themselves in that we’re talking about two quite different products, aimed at different goals and market segments. The Neoverse V1 represents a new line-up for Arm, with a CPU microarchitecture that is aiming itself for more HPC-like workloads and designs oriented towards such markets, while the Neoverse N2 is more of a straight-up successor to the Neoverse N1 and infrastructure and cloud deployments in the same way that the N1 sees itself today in products such as the Graviton or Altra processors.For readers who are familiar with Arm’s mobile CPU microarchitectures, there’s definitely very large similarities between the designs – even though Arm’s marketing seems to be oddly reluctant to make such kind of comparisons, which is why I made the above chart which more clearly tries to depict the similarities between design generations.The original Neoverse N1 as seen in the Graviton2 and Altra Q processors had been a derivative, or better said, a sibling microarchitecture, to theCortex-A76, which had been employed in the 2019 generation of Cortex-A76 mobile SoCs such as theSnapdragon 855. Naturally, the Neoverse designs had server-oriented features and changes that aren’t present in the mobile counterparts.Similarly to how the N1 was related to the A76, the new generation V1 and N2 microarchitectures are related to newer designs in the Cortex-portfolio. The V1 is related to the Cortex-X1 which we’ve seen in this year’s new mobile SoCssuch as the Snapdragon 888 or Exynos 2100. The Neoverse N2 on the other hand is related to an upcoming new Cortex-A microarchitecture which we expect to hear more about in the following few months. Throughout the piece today we’ll make a few more references to this generational disconnect between the V1 and N2, and it’s important to remember that the N2 is a newer design, albeit aimed at different performance and efficiency points.This decoupling of design goals between the V1 and N2 for Arm comes through the company’s attempt to target more specific markets where the end products might have different priorities, much like how in the mobile space the new Cortex-X series prioritises per-core performance while the Cortex-A series continues to focus on the best PPA. Similarly, the V1 focuses on maximised performance at lower efficiency, with features such as wider SIMD units (2x256b SVE), while the N2 continues the scale-out philosophy of having the best power-efficiency while still moving forward performance through generational IPC improvements.In today’s piece, we’ll be diving into the new microarchitectural changes of the V1, N2, as well as Arm’s newest generation mesh interconnect IP, the CMN-700, which is expected to serve as the foundation of the next-generation Arm infrastructure processors.Table of contents:A Successful 2020 for Arm - Looking Towards 2022The Neoverse V1 Microarchitecture: X1 with SVE?The Neoverse V1 Microarchitecture: Platform EnhancementsThe Neoverse N2 Microarchitecture: First Armv9 For EnterpriseThe SVE Factor - More Than Just Vector SizePPA & ISO Performance ProjectionsThe CMN-700 Mesh Network - Bigger, More FlexibleEventual Design Performance ProjectionsFirst Thoughts & End Remarks\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16640/arm-announces-neoverse-v1-n2-platforms-cpus-cmn700-mesh\n",
      "Title: NVIDIA Unveils Grace: A High-Performance Arm Server CPU For Use In Big AI Systems\n",
      "Author: Ryan Smith\n",
      "Date Published: 2021-04-12T16:20:00Z\n",
      "URL: https://www.anandtech.com/show/16610/nvidia-unveils-grace-a-highperformance-arm-server-cpu-for-use-in-ai-systems\n",
      "Content: Kicking off another busy Spring GPU Technology Conference for NVIDIA, this morning the graphics and accelerator designer is announcing that they are going to once again design their own Arm-based CPU/SoC. Dubbed Grace – after Grace Hopper, the computer programming pioneer and US Navy rear admiral – the CPU is NVIDIA’s latest stab at more fully vertically integrating their hardware stack by being able to offer a high-performance CPU alongside their regular GPU wares. According to NVIDIA, the chip is being designed specifically for large-scale neural network workloads, and is expected to become available in NVIDIA products in 2023.With two years to go until the chip is ready, NVIDIA is playing things relatively coy at this time. The company is offering only limited details for the chip – it will be based on a future iteration of Arm’s Neoverse cores, for example – as today’s announcement is a bit more focused on NVIDIA’s future workflow model than it is speeds and feeds. If nothing else, the company is making it clear early on that, at least for now, Grace is an internal product for NVIDIA, to be offered as part of their larger server offerings. The company isn’t directly gunning for the Intel Xeon or AMD EPYC server market, but instead they are building their own chip to complement their GPU offerings, creating a specialized chip that can directly connect to their GPUs and help handle enormous, trillion parameter AI models.NVIDIA SoC Specification ComparisonGraceXavierParker(Tegra X2)CPU Cores?82CPU ArchitectureNext-Gen Arm Neoverse(Arm v9?)Carmel(Custom Arm v8.2)Denver 2(Custom Arm v8)Memory Bandwidth>500GB/secLPDDR5X(ECC)137GB/secLPDDR4X60GB/secLPDDR4GPU-to-CPU Interface>900GB/secNVLink 4PCIe 3PCIe 3CPU-to-CPU Interface>600GB/secNVLink 4N/AN/AManufacturing Process?TSMC 12nmTSMC 16nmRelease Year202320182016More broadly speaking, Grace is designed to fill the CPU-sized hole in NVIDIA’s AI server offerings. The company’s GPUs are incredibly well-suited for certain classes of deep learning workloads, but not all workloads are purely GPU-bound, if only because a CPU is needed to keep the GPUs fed. NVIDIA’s current server offerings, in turn, typically rely on AMD’s EPYC processors, which are very fast for general compute purposes, but lack the kind of high-speed I/O and deep learning optimizations that NVIDIA is looking for. In particular, NVIDIA is currently bottlenecked by the use of PCI Express for CPU-GPU connectivity; their GPUs can talk quickly amongst themselves via NVLink, but not back to the host CPU or system RAM.The solution to the problem, as was the case even before Grace, is to use NVLink for CPU-GPU communications. Previously NVIDIA has worked with the OpenPOWER foundation to get NVLink into POWER9 for exactly this reason, however that relationship is seemingly on its way out, both as POWER’s popularity wanes and POWER10 is skipping NVLink. Instead, NVIDIA is going their own way by building an Arm server CPU with the necessary NVLink functionality.The end result, according to NVIDIA, will be a high-performance and high-bandwidth CPU that is designed to work in tandem with a future generation of NVIDIA server GPUs. With NVIDIA talking about pairing each NVIDIA GPU with a Grace CPU on a single board – similar to today’s mezzanine cards – not only does CPU performance and system memory scale up with the number of GPUs, but in a roundabout way, Grace will serve as a co-processor of sorts to NVIDIA’s GPUs. This, if nothing else, is a very NVIDIA solution to the problem, not only improving their performance, but giving them a counter should the more traditionally integrated AMD or Intel try some sort of similar CPU+GPU fusion play.By 2023 NVIDIA will be up to NVLink 4, which will offer at least 900GB/sec of cummulative (up + down) bandwidth between the SoC and GPU, and over 600GB/sec cummulative between Grace SoCs. Critically, this is greater than the memory bandwidth of the SoC, which means that NVIDIA’s GPUs will have a cache coherent link to the CPU that can access the system memory at full bandwidth, and also allowing the entire system to have a single shared memory address space. NVIDIA describes this as balancing the amount of bandwidth available in a system, and they’re not wrong, but there’s more to it. Having an on-package CPU is a major means towards increasing the amount of memory NVIDIA’s GPUs can effectively access and use, as memory capacity continues to be the primary constraining factors for large neural networks – you can only efficiently run a network as big as your local memory pool.CPU & GPU Interconnect BandwidthGraceEPYC 2 + A100EPYC 1 + V100GPU-to-CPU Interface(Cummulative, Both Directions)>900GB/secNVLink 4~64GB/secPCIe 4 x16~32GB/secPCIe 3 x16CPU-to-CPU Interface(Cummulative, Both Directions)>600GB/secNVLink 4304GB/secInfinity Fabric 2152GB/secInfinity FabricAnd this memory-focused strategy is reflected in the memory pool design of Grace, as well. Since NVIDIA is putting the CPU on a shared package with the GPU, they’re going to put the RAM down right next to it. Grace-equipped GPU modules will include a to-be-determined amount of LPDDR5x memory, with NVIDIA targeting at least 500GB/sec of memory bandwidth. Besides being what’s likely to be the highest-bandwidth non-graphics memory option in 2023, NVIDIA is touting the use of LPDDR5x as a gain for energy efficiency, owing to the technology’s mobile-focused roots and very short trace lengths. And, since this is a server part, Grace’s memory will be ECC-enabled, as well.As for CPU performance, this is actually the part where NVIDIA has said the least. The company will be using a future generation of Arm’s Neoverse CPU cores, where the initial N1 design has already beenturning heads. But other than that, all the company is saying is that the cores should break 300 points on the SPECrate2017_int_base throughput benchmark, which would be comparable to some of AMD’s second-generation 64 core EPYC CPUs. The company also isn’t saying much about how the CPUs are configured or what optimizations are being added specifically for neural network processing. But since Grace is meant to support NVIDIA’s GPUs, I would expect it to be stronger where GPUs in general are weaker.Otherwise, as mentioned earlier, NVIDIA big vision goal for Grace is significantly cutting down the time required for the largest neural networking models. NVIDIA is gunning for 10x higher performance on 1 trillion parameter models, and their performance projections for a 64 module Grace+A100 system (with theoretical NVLink 4 support) would be to bring down training such a model from a month to three days. Or alternatively, being able to do real-time inference on a 500 billion parameter model on an 8 module system.Overall, this is NVIDIA’s second real stab at the data center CPU market – and the first that is likely to succeed. NVIDIA’sProject Denver, which was originally announced just over a decade ago, never really panned out as NVIDIA expected. The family of custom Arm cores was never good enough, and never made it out of NVIDIA’s mobile SoCs. Grace, in contrast, is a much safer project for NVIDIA; they’re merely licensing Arm cores rather than building their own, and those cores will be in use by numerous other parties, as well. So NVIDIA’s risk is reduced to largely getting the I/O and memory plumbing right, as well as keeping the final design energy efficient.If all goes according to plan, expect to see Grace in 2023. NVIDIA is already confirming that Grace modules will be available for use in HGX carrier boards, and by extension DGX and all the other systems that use those boards. So while we haven’t seen the full extent of NVIDIA’s Grace plans, it’s clear that they are planning to make it a core part of future server offerings.First Two Supercomputer Customers: CSCS and LANLAnd even though Grace isn’t shipping until 2023, NVIDIA has already lined up their first customers for the hardware – and they’re supercomputer customers, no less. Both the Swiss National Supercomputing Centre (CSCS) and Los Alamos National Laboratory are announcing today that they’ll be ordering supercomputers based on Grace. Both systems will be built by HPE’s Cray group, and are set to come online in 2023.CSCS’s system, dubbed Alps, will be replacing their current Piz Daint system, a Xeon plus NVIDIA P100 cluster. According to the two companies, Alps will offer 20 ExaFLOPS of AI performance, which is presumably a combination of CPU, CUDA core, and tensor core throughput. When it’s launched, Alps should be the fastest AI-focused supercomputer in the world.An artist's rendition of the expected Alps systemInterestingly, however, CSCS’s ambitions for the system go beyond just machine learning workloads. The institute says that they’ll be using Alps as a general purpose system, working on more traditional HPC-type tasks as well as AI-focused tasks. This includes CSCS’s traditional research into weather and the climate, which the pre-AI Piz Daint is already used for as well.As previously mentioned, Alps will be built by HPE, who will be basing on their previously-announced Cray EX architecture. This would make NVIDIA’s Grace the second CPU option for Cray EX, along with AMD’s EPYC processors.Meanwhile Los Alamos’ system is being developed as part of an ongoing collaboration between the lab and NVIDIA, with LANL set to be the first US-based customer to receive a Grace system. LANL is not discussing the expected performance of their system beyond the fact that it’s expected to be “leadership-class,” though the lab is planning on using it for 3D simulations, taking advantage of the largest data set sizes afforded by Grace. The LANL system is set to be delivered in early 2023.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16610/nvidia-unveils-grace-a-highperformance-arm-server-cpu-for-use-in-ai-systems\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Announces Armv9 Architecture: SVE2, Security, and the Next Decade\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-03-30T18:00:00Z\n",
      "URL: https://www.anandtech.com/show/16584/arm-announces-armv9-architecture\n",
      "Content: It’s been nearly 10 years since Arm had first announced the Armv8 architecture in October 2011, and it’s been a quite eventful decade of computing as the instruction set architecture saw increased adoption through the mobile space to the server space, and now starting to become common in the consumer devices market such as laptops and upcoming desktop machines. Throughout the years, Arm has evolved the ISA with various updates and extensions to the architecture, some important, some maybe glanced over easily.Today, as part of Arm’s Vision Day event, the company is announcing the first details of the company’s new Armv9 architecture, setting the foundation for what Arm hopes to be the computing platform for the next 300 billion chips in the next decade.The big question that readers will likely be asking themselves is what exactly differentiates Armv9 to Armv8 to warrant such a large jump in the ISA nomenclature. Truthfully, from a purely ISA standpoint, v9 probably isn’t an as fundamental jump as v8 was over v7, which had introduced a completely different execution mode and instruction set with AArch64, which had larger microarchitectural ramifications over AArch32 such as extended registers, 64-bit virtual address spaces and many more improvements.Armv9 continues the usage of AArch64 as the baseline instruction set, however adds in a few very important extensions in its capabilities that warrants an increment in the architecture numbering, and probably allows Arm to also achieve a sort of software re-baselining of not only the new v9 features, but also the various v8 extensions we’ve seen released over the years.The three new main pillars of Armv9 that Arm sees as the main goals of the new architecture are security, AI, and improved vector and DSP capabilities. Security is a very big topic for v9 and we’ll go into the new details of the new extensions and features into more depth in a bit, but getting DSP and AI features out of the way first should be straightforward.Probably the biggest new feature that is promised with new Armv9 compatible CPUs that will be immediately visible to developers and users is the baselining of SVE2 as a successor to NEON.Scalable Vector Extensions, or SVE, in its first implementation wasannounced back in 2016and implemented for the first time inFujitsu’s A64FX CPU cores, now powering theworld’s #1 supercomputer Fukagu in Japan. The problem with SVE was that this first iteration of the new variable vector length SIMD instruction set was rather limited in scope, and aimed more at HPC workloads, missing many of the more versatile instructions which still were covered by NEON.SVE2 was announced back in April 2019, and looked to solve this issue by complementing the new scalable SIMD instruction set with the needed instructions to serve more varied DSP-like workloads that currently still use NEON.The benefit of SVE and SVE2 beyond addition various modern SIMD capabilities is in their variable vector size, ranging from 128b to 2048b, allowing variable 128b granularity of vectors, irrespective of what the actual hardware is running on. Purely from a view of vector processing and programming, it means that a software developer would only ever have to compile his code once, and if in the future a CPU would come out with say native 512b SIMD execution pipelines, the code would be able to already take advantage of the full width of the units. Similarly, the same code would be able to run on more conservative designs with a lower hardware execution width capability, which is important to Arm as they design CPUs from IoT, to mobile, to datacentres. It also does this all whilst remaining within the 32b encoding space of the Arm architecture, whereas alternative implementations such as on x86 have to add on new extensions and instructions depending on vector size.Machine learning is also seen as an important part of Armv9 as Arm sees more and more ML workloads to become common place in the next years. Running ML workloads on dedicated accelerators naturally will still be a requirement for anything that is performance or power efficiency critical, however there still will be vast new adoption of smaller scope ML workloads that will run on CPUs.Matrix multiplication instructionsare key here and will represent an important step in seeing larger adoption across the ecosystem as being a baseline feature of v9 CPUs.Generally, I see SVE2 as probably the most important factor that would warrant the jump to a v9 nomenclature as it’s a more definitive ISA feature that differentiates it from v8 CPUs in every-day usage, and that would warrant the software ecosystem to go and actually diverge from the existing v8 stack. That’s actually become quite a problem for Arm in the server space as the software ecosystem is still baselining software packages on v8.0, which unfortunately is missing the all-important v8.1 Large System Extensions.Having the whole software ecosystem move forward and being able to assume new v9 hardware has the capability of the new architectural extensions would help push things ahead, and probably solve some of the current situation.However v9 isn’t only about SVE2 and new instructions, it also has a very large focus on security, where we’ll be seeing some more radical changes.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16584/arm-announces-armv9-architecture\n",
      "Title: The Ampere Altra Review: 2x 80 Cores Arm Server Performance Monster\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-12-18T11:00:00Z\n",
      "URL: https://www.anandtech.com/show/16315/the-ampere-altra-review\n",
      "Content: As we’re wrapping up 2020, one last large review item for the year is Ampere’s long promised new Altra Arm server processor. This year has indeed been the year where Arm servers have had a breakthrough;Arm’s new Neoverse-N1 CPU corehad been the IP designer’s first true dedicated server core, promising focused performance and efficiency for the datacentre.Earlier in the year we had the chance to test out the first Neoverse-N1 siliconin the form of Amazon’s Graviton2inside of AWS EC2 cloud compute offering. The Graviton2 seemed like a very impressive design, but was rather conservative in its goals, and it’s also a piece of hardware that the general public cannot access outside of Amazon’s own cloud services.Ampere Computing, founded in 2017 by former Intel president Renée James, built uponinitial IP and design talent of AppliedMicro’s X-Gene CPUs, and withArm Holdings becoming an investor in 2019, is at this moment in time the sole “true” merchant silicon vendor designing and offering up Neoverse-N1 server designs.To date, the company has had a few products out in the form of the eMAG chips, but withrather disappointing performance figures- understandable given that those were essentially legacy products based on the old X-Gene microarchitecture.Ampere’s new Altra product line, on the other hand is the culmination of several years of work and close collaboration with Arm – and the company first “true” product which can be viewed as Ampere pedigree.Today, with hardware in hand, we’re finally taking a look at the very first publicly available high-performance Neoverse based Arm server hardware, designed for nothing less than maximum achievable performance, aiming to battle the best designs from Intel and AMD.Mount Jade Server with Altra QuicksilverAmpere has supplied us with the company’s server reference design, dubbed “Mount Jade”, a 2-socket 2U rack unit sever. The server came supplied with two Altra Q80-33 processors, Ampere’s top-of-the-line SKU with each featuring 80 cores running at up to 3.3GHz, with TDP reaching up to 250W per socket.The server was designed with close collaboration with Wiwynn for this dual socket, and with GIGABYTE for the single socket variant, as previously hinted bythe two company’s announcements of leading hyperscale deployments of the Altra platforms.The Ampere-branded Mount Jade DVT reference motherboard comes in a typical server blue colour scheme and features 2 sockets with up to 16 DIMM slots per socket, reaching up to 4TB DRAM capacity per socket, although our review unit came equipped with 256GB per socket across 8 DIMMs to fully populate the chip’s 8-channel memory controllers.This is also our first look at Ampere’s first-generation socket design. The company doesn’t really market any particular name to the socket, but it’s a massive LGA4926 socket with a pin-count in excess of any other commercial server socket from AMD or Intel. The holding mechanism is somewhat similar to that of AMD’s SP3 system, with a holding mechanism tensioned by a 5-point screw system.The chip itself is absolutely humongous and amongst the current publicly available processors is the biggest in the industry, out-sizing AMD’s SP3 form-factor packaging, coming in at around 77 x 66.8mm – about the same length but considerably wider than AMD’s counterparts.Although it’s a massive chip with a huge IHS, the Mount Jade server surprised me with its cooling solution as the included 250W type cooler only made contact with about 1/4ththe surface area of the heat spreader.Ampere here doesn’t have a recessed “lip” around the IHS for the mounting bracket to hold onto the chip like on AMD or Intel systems, so the actual IHS surface is actually recessed in relation to the bracket which means you cannot have a flat surface cooler design across the whole of the chip surface.Instead, the included 250W design cooler uses a huge vapour chamber design with a “pedestal” to make contact with the chip. Ampere explains that they’ve experimented with different designs and found that a smaller area pedestal actually worked better for heat dissipation – siphoning heat off from the actual chip die which is notably smaller than the IHS and chip package.The cooler design is quite complex, with vertical fin stacks dissipating heat directly off the vapour chamber, with additional large horizontal fins dissipating heat from 6 U-shaped heat pipes that draw heat from the vapour chamber. It’s definitely a more complex and high-end design than what we’re used to in server coolers.Although the Mount Jade server is definitely a very interesting piece of hardware, our focus today lies around the actual new Altra processors themselves, so let’s dive into the new Q80-33 80-core chip next.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16315/the-ampere-altra-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Apple Announces Event for November 10th: Arm-Based Macs Expected\n",
      "Author: Ryan Smith\n",
      "Date Published: 2020-11-02T17:30:00Z\n",
      "URL: https://www.anandtech.com/show/16212/apple-announces-keynote-for-november-10th-armbased-macs-expected\n",
      "Content: We don’t normally publish news posts about Apple sending out RSVPs for product launch events, but this one should be especially interesting.This morning Apple has sent notice that they’re holding an event next Tuesday dubbed “One more thing.” In traditional Apple fashion, the announcement doesn’t contain any detailed information about the content expected; but as Apple has already announced their updated iPads and iPhones, the only thing left on Apple’s list for the year is Macs. Specifically, their forthcoming Arm-powered Macs.Aspreviously announced by Apple back at their summer WWDC event, the company is transitioning its Mac lineup from x86 CPUs to Arm CPUs. With a two-year transition plan in mind, Apple is planning to start the Arm Mac transition this year, and wrapping things up in 2022.For the new Arm Macs, Apple will of course be using their own in-house designed Arm processors, the A-series. As we’ve seen time and time again from the company, Apple’s CPU design team is on the cutting-edge of Arm CPU cores, producing the fastest Arm CPU cores for several years running now, and more recently even overtaking Intel’s x86 chips in real-world Instruction Per Clock (IPC) rates. Suffice it to say, Apple believes they can do better than Intel by designing their own CPUs, and especially with the benefits of vertical integration and total platform control, they might be right.Apple has been shipping early Arm Macs to developers since the summer, using modified Mac Minis containing their A12Z silicon. We’re obviously expecting something newer, but whether it’s a variant ofApple’s A14 SoC, or perhaps something newer and more bespoke specifically for their Macs, remains to be seen.In the meantime, because this is a phased transition, Apple will be selling Intel Macs – including new models – alongside the planned Arm Macs. So although Apple will no doubt focus on their new Arm Macs, I wouldn’t be the least bit surprised to see some new Intel Macs announced alongside them. Apple will be supporting Intel Macs for years to come, and in the meantime they need to avoid Osborning their x86 systems.As always, we’ll have a live blog of the events next Tuesday, along with a breakdown of Apple’s announcements afterwards. So please be sure to drop in and check that out.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16212/apple-announces-keynote-for-november-10th-armbased-macs-expected\n",
      "Title: Arm Announces Cortex-A78AE, Mali-G78AE and Mali-C71AE Autonomous System IPs\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-09-29T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16114/arm-announces-cortexa78ae-malig78ae-and-malic71ae-autonomous-system-ips\n",
      "Content: Functional safety is an area of computing that is becoming ever more important as we see more and more embedded technologies integrated into our daily lives. Arm’s Automotive Enhanced (AE) line of IP had been launched back in 2018 with the release of the Cortex-A76AE.Fast-forward a few years, it’s time for a new set of AE IP, with Arm now introducing the new Cortex-A78AE, bringing a higher performance CPU core, and also for the first time introducing an AE class GPU and ISP in the form of the Mali-G78AE and Mali-C71AE. With the move, Arm also says that it is diversifying beyond just the automotive sector and widening the scope to industrial and other autonomous systems.Hercules-AE is Cortex-A78AEStarting off with the CPU, the new IP isn’t exactly new as we’ve first heard about the new Hercules-AE design last year duringNvidia’s announcement of their “Orin” automotive SoC.The new Cortex-A78AE, as its name implies, is based offthe Cortex-A78 microarchitecturewhich we’ve extensively covered in in our in-depth Tech Day article earlier this summer.Compared to the previous generation Corex-A76AE this means a 30% uplift in IPC and higher performance.What’s new this year in regards to the functional safety features of the IP is the introduction of a new operating “hybrid mode” that represents a new architecture for how to achieve ASIL-B compliance, but with a lesser performance impact compared to the existing Split Mode operating mode.Functional safety currently is achieved on AE CPUs by running in either “Split Mode” or “Locked Mode”. Locked mode is quite straightforward and includes running pairs of cores in lock-step with additional logic controlling that the computational results between the pairs are consistent at all times. Effectively this cuts your throughput in half as you are always duplicating work done.Split mode maintaining ASIL-B functional safety still requires the cores to be periodically checked for correct operation which makes them temporarily unavailable. The problem lies at the DSU-level (Dynamic Shared Unit – the L3 cache) as for this to be checked it will make the whole CPU cluster unavailable, and this has a larger performance impact on the system.The new hybrid mode adds additional logic on the part of the DSU to enable it failure detection without having to make it unavailable to the CPUs, and thus ensuring continuous operation and computational throughput. It’s to be noted that this redundancy on the part of the DSU means additional control logic, but does not include actually duplicating the L3 SRAMs which would incur a large area penalty.The new hybrid mode thus would represent a higher performing design configuration for ASIL-B workloads with a comparatively smaller cost in area overheard in the DSU. If a vendor chooses to implement Hybrid Mode or remains with the simpler Split Mode configuration is a design-time choice that takes into account the extra area requirements. ASIL-D operation in Locked Mode naturally continues to require the extra area investment.As noted, the Cortex-A78AE had already been licensed quite some time ago and Nvidia’s new Orin SoC with 12 cores is the first publicly known design to employ the new cores.Mali-G78AE - Finally introducing virtualisationAlongside the Cortex-A78AE, Arm is also for the first time announcing a functional safety capable GPU in the form of the new Mali-G78AE. Based onthe mobile Mali-G78 GPU core, we should be expecting similar performance and power efficiency figures from the IP- scaling up from 1 to 24 cores.The important addition of the new IP is the inclusion of full-fledged hardware virtualisation, a critical feature for autonomous systems that to date had been lacking in the company’s GPU IP.Hardware virtualisation is important to be able to separate safety critical software from other non-critical workloads, ensuring that if anything were to go wrong (such as for example some odd workload crashing the GPU driver), that the safety critical components continue to operate without issue.Samsung’sExynos Auto V9is an example of such a SoC design where previously it had to deploy 3 GPU clusters (MP12+MP3+MP3) to ensure independent workload operation that would not impact critical systems.With hardware virtualisation, a newer design with the Mali-G78AE wouldn’t require multiple GPU clusters and instead be able to use a single GPU, flexibly partitioning inner-GPU execution resources between concurrent multiple workloads. The IP supports four such partitions. Beyond the hardware partitioning, software virtualisation also allows time-split operation of workloads within the same partition.The new IP supports functional safety to an ASIL-B standard – this is both a design limitation, however Arm also says that this is currently what customers are demanding. It would be possible to achieve higher ASIL-D ability in a design if you put down two identical GPUs in your design and compare outputs.Beyond the announcement of the new CPU and GPU IP, we’re also seeingan extension of the company’s existing Mali-C71 ISP IPwith the new C71AE which adds support for ASIL-B and SIL2 integrity checking.Also part of the announcement today is Arm’s enablement for reference autonomous platforms:Beyond these new amazing hardware technologies, we are working to enable the developers of autonomous systems. For software developers we are enabling familiar cloud native technologies in autonomous applications to ease development, while Arm development solutions accelerate software development and validation while shortening the path to deployment. For developers of autonomous silicon, our physical IP, training and design reviews help reduce risk.Related Reading:Arm Unveils Arm Safety Ready Initiative, Cortex-A76AE ProcessorArm Announces Cortex-A65AE for Automotive: First SMT CPU CoreARM Announces Mali-C71: Their First Automotive-Grade Image Signal ProcessorNVIDIA Details DRIVE AGX Orin: A Herculean Arm Automotive SoC For 2022\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16114/arm-announces-cortexa78ae-malig78ae-and-malic71ae-autonomous-system-ips\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Announces Neoverse V1 & N2 Infrastructure CPUs: +50% IPC, SVE Server Cores\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-09-22T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16073/arm-announces-neoverse-v1-n2\n",
      "Content: Arm’s ambitions for the server market has been a very long journey that’s taken years to materialise. After many doubts and false start attempts, today in 2020 nobody can deny that sever chips powered by the company’s CPU IP are not only competitive, but actually class-leading on several metrics.Amazon’s Graviton264-core Neoverse N1 server chip is the first of what should become awider range of designsthat will be driving the Arm server ecosystem forward and actively assaulting the infrastructure CPU market share that’s currently dominated by the x86 players such as Intel and AMD.The journey has been a long one, but has had its roots back inroadmaps publicly planned laid out by the company back in 2018. Fast-forward to 2020, not only have we seen products withthe first-generation Neoverse N1infrastructure CPU IP hit the market in commercial and publicly available form, but we’ve seen the company exceed their targeted 30% generational gain by a factor of 2x.The Neoverse V1: A New Maximum Performance Tier Infrastructure CPUToday, we’re ready to take the next step towards the next generation of the Neoverse platform, not only revealing the CPU microarchitecture previously known as Zeus, but a whole new product category that goes beyond the Neoverse N-series: Introducing the new Neoverse V-series and the Neoverse V1 (Zeus), as well as a new roadmap insertion in the form of the Neoverse N2 (Perseus).The new Neoverse V1 introduces the new V-series into Arm’s infrastructure IP portfolio, and essentially this represents the company’s push for higher absolute performance, no matter the cost.Earlier this spring we covered thecompany’s new mobile Cortex-X1 CPU IPwhich represented significant business model change for Arm: Instead of offering only a single one-fits-all CPU microarchitecture which licensees had to make due with in a wider range of designs and performance points, we’ve now seen a divergence of the microarchitectures, with one IP offering now focusing on pure maximum performance (Cortex-X1), no matter the area or power cost, while the other design (Cortex-A78) focuses on Arm’s more traditional maximised PPA (Power, Performance, Area) design philosophy.The Zeus microarchitecture in the form of the Neoverse V1 is essentially the infrastructure counterpart to what Arm has achieved in the mobile IP offering with the Hera Cortex-X1 CPU IP: A focus on maximum performance, with a lesser regard to power and area.This means that the V1 has significantly larger caches, cores structures, using up more area and power to achieve unprecedented performance levels.In terms of generational performance uplift, it’s akin to Arm throwing down the gauntlet to the competition, achieving a ground-breaking +50 IPC boost compared to Neoverse N1 that we’re seeing in silicon today. The performance uplift potential here is tremendous, as this is merely a same-process ISO-frequency upgrade, and actual products based on the V1 will also in all likelihood also see additional performance gains thanks to increased frequencies through process node advancements.If we take the conservatively clocked Graviton2 with its 2.5GHz N1 cores as a baseline, a theoretical 3GHz V1 chip would represent an 80% uplift in per-core single-threaded performance. Not only would such a performance uptick vastly exceed any current x86 competition in the server space in terms of per-core performance, it would be enough to match the current best high-performance desktop chips from AMD and Intel today (Though we have to remember it’ll compete against next-gen Zen3 Milan and Sapphire Rapids products).Neoverse N2 is Perseus – Continues the PPA FocusAlongside the Neoverse V1 platform, we’ve seen a roadmap insertion that previously wasn’t there. The Perseus design will become the Neoverse N2, and will be the effective product-positioning successor to the N1. This new CPU IP represents a 40% IPC uplift compared to the N1, however still maintains the same design philosophy of maximising performance within the lowest power and smallest area.It can be a bit confusing when it comes to the microarchitectural generations that we’re talking about here, so I made a graph to illustrate what we could call generational siblings between Arm’s mobile, and server CPU IP:Although this is just a general rough outline of Arm’s products, the important thing to note that there’s similarities between generations of Cortex and Neoverse products as they’ve being developed in tandem at similar moments in time during their design. The Neoverse N1 was developed in conjunction with the Cortex-A76, and thus the two microarchitectures can be regarded as sibling designs as they share a lot of similarities.The Neoverse V1 can be regarded as a sibling design to the Cortex-X1, likely sharing a lot of the supersized core structures that had been developed for these two flagship CPUs.The Neoverse N2 is a bit more special as it represents the sibling design to a next-generation Cortex-A core which is the follow-up to the A78. Arm says they’ll be licensing out this “Perseus” design by the end of the year and that customers already are engaging on beta RTL – we’re likely to hear more about this generation of products at next year’s TechDay event. The N2 would be lagging behind the V1 by one year and subsequently it'll take more time to see this in products.As a note, all of the above designs are all based in Austin and can be regarded as in the same microarchitecture family that had been started off with the Cortex-A76. If I’m not mistaken, next-generation “Poseidon” designs will be on a fresh new microarchitecture started by Arm’s Sophia-Antipolis design team – although Arm does note that there’s a lot more collaboration and blur between the different teams nowadays. Here Arm already notes a +30% IPC uplift for this generation of designs, likely to hit products in 2023.An Undisclosed Architecture with SVE: Armv9?One very notable characteristic of both the Neoverse V1 and N2 are the fact that these now supportSVE (Scalable Vector Extensions), with the V1 having two native 256-bit pipelines and the N2 being a 2x128-bit design. The advantage of SVE over other SIMD ISAs is the fact that code written in it can scale with the varying execution width of a microarchitecture, something that’s just not possible with today’s Neon or AVX SIMD instructions.Fujitsu’s A64FX chip and custom coremicroarchitecture had been to date the only CPU announced and available with SVE, meaning the V1 and N2 will be Arm’s first own designs actually implementing SVE.Today’s announcements around this part of the V1 and N2 CPUs raised more questions than it answered, as the company wasn’t willing to disclose whether this support referred to the first-generation SVE instruction set, or whether they already supported SVE2.In fact, the company wouldn’t confirm even the base architecture of the designs, whether this were Armv8 designs or one of the subsequent iterations. This is extremely unusual for the company as it’s traditionally transparent on such basic aspects of their IPs.What I think is happening here is thatthe V1 andN2 might bebothArmv9 designs, and the company will be publicly revealing the new ISA iteration sometime between today’s announcement and mid next year at the latest – of course this is all just my own interpretation of the situation as Arm refused to comment on the topic.Update:Actually it does seem that Arm had already publicly upstreamed theinitial compiler entries to GCC for Zeus back in June, confirming that at least the Neoverse V1 is an Armv8.4+SVE(1) design. I still think the N2 might be a v9+SVE2 design.At the end of the day, what we end up are two extremely compelling new microarchitectures that significantly push Arm’s positioning in the infrastructure market. The Neoverse N2 is an obvious design that focuses on Arm’s PPA metrics, and the company sees customers designing products that are primarily focused on “scale-out” workloads that requite a lot of CPU cores. Here we could see designs up to 128 cores.The Neoverse V1 will see designs with lesser core-counts as the CPUs are just bigger and more power hungry. Arm sees the 64-96 range being what’s most likely to be adopted by licensees. These are the premier products that will be going against the best of what Intel and AMD have to offer- and if the performance projections pan out (as they usually do for Arm), then we’re in for a brutally competitive fight unlike we’ve seen before.The first publicly known design confirmed to employ the new Neoverse V1 cores isSiPearl’s “Rhea” chipthat looks to feature 72 cores in a 7nm TSMC process node. Ampere’s “Siryn” design would also be a candidate for applying the V1 microarchitecture, targeted for a 2022 release on TSMC’s 5nm node.Today’s announcement has been more of a teaser or unveiling, with the company planning to go into more details about the architecture and microarchitectures of the designs at a later date.Arm's DevSummitis scheduled for October 6-8th - and might be where we'll hear a bit more about the new architecture.Related Reading:Arm Announces Neoverse N1 & E1 Platforms & CPUs: Enabling A Huge Jump In Infrastructure PerformanceArm's New Cortex-A78 and Cortex-X1 Microarchitectures: An Efficiency and Performance DivergenceAmazon's Arm-based Graviton2 Against AMD and Intel: Comparing Cloud ComputeNext Generation Arm Server: Ampere’s Altra 80-core N1 SoC for Hyperscalers against Rome and XeonArm Announces Neoverse Infrastructure IP Branding & Future Roadmap\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16073/arm-announces-neoverse-v1-n2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: It’s Official: NVIDIA To Acquire Arm For $40 Billion\n",
      "Author: Ryan Smith\n",
      "Date Published: 2020-09-14T03:15:00Z\n",
      "URL: https://www.anandtech.com/show/16080/nvidia-to-acquire-arm-for-40-billion\n",
      "Content: Following a number of rumors and leaks, NVIDIA this evening announced that it is buying Arm Limited for $40 billion. The cash and stock deal will see NVIDIA buy the semiconductor and IP design firm from SoftBank and its associated SoftBank Vision Fund, with NVIDIA taking an eye towards expanding Arm’s IP licensing business while also using Arm’s technology to further pierce into the datacenter market. The deal is just being formally announced today and will likely not close for some time, as it is expected to be required to clear multiple regulatory hurdles in the UK, US, China, and other governments across the globe.The groundbreaking deal will see NVIDIA take over Arm Limited from SoftBank,who previously acquired the then-independent Arm in 2016 for $32 billion. At the time, SoftBank acquired Arm primarily as an investment vehicle, expecting the successful company to continue to grow as the number of chips shipped on the Arm architecture continued to explode. However, the investment firm has been under pressure in recent months as some of its other investments have taken big hits – particularly WeWork and Uber – and while SoftBank isn’t officially commenting on why it’s selling Arm after all of this time, there’s ample reason to believe that the firm is selling off one of its more valuable assets in order to shore up its balance sheets.The $40 billion transaction means that SoftBank will come out ahead on their investment, but only barely – their Arm investment has significantly underperformed relative to the broader technology industry. The deal will see SoftBank receive $12 billion in cash, along with $21.5 billion in NVIDIA stock. That transaction will give SoftBank a relatively sizable ownership stake in NVIDIA, though according to the companies the total stake is expected to be under 10 percent. Finally, the remaining $6.5B valuation of the deal will come from a further $1.5B in equity that NVIDIA will be paying out to Arm employees, as well as a $5B “earn-out” payment to be paid if Arm meets certain financial targets.As for NVIDIA, the Arm acquisition marks their largest acquisition to date, easily eclipsing the Mellanox acquisition that closed just a short few months ago. Over the last half-decade NVIDIA has undergone significant growth – both in regards to revenue and market capitalization – thanks in big part to NVIDIA’s newfound success in the server and datacenter market with their deep learning accelerators. While the company is well off of its 52-week high that it set earlier this month, NVIDIA now has a sizable $330B market cap that they are leveraging to make this deal possible.And according to the company, it’s that success in the server market that is driving their interest in and plans for Arm. NVIDIA expects the server market to remain a high-growth opportunity, and that by acquiring Arm they can leverage Arm’srecent success with Neoverseand other server products to reach an even bigger chunk of that market.To be sure, NVIDIA isn’t announcing any specific hardware plans today – the deal is easily still a year and a half off from closing – but NVIDIA has made it clear that following their success in the GPU/accelerator and networking markets, they see Arm as the perfect complement to their current product lineup, giving them a capable CPU architecture to round-out their technology portfolio. Even with Arm, NVIDIA will not be capable of complete vertical integration, but while the company today still has to rely on third-party vendors (e.g. AMD and Intel) for some of the most important silicon that goes into servers incorporating their accelerators, with an Arm-based server CPU, NVIDIA can offer a nearly complete package on its own.Of course, Arm is more than just a server CPU designer; the company has its fingers in everything from toasters to supercomputers thanks to its very broad range of IP, and any discussion about acquiring Arm has to include what happens to those businesses. Arm’s core business is licensing IP, and NVIDIA is telling the public (and partners) that this won’t change – that the company will continue to license out IP to other companies. The rationale for this is multifaceted – NVIDIA needs to win over everyone from regulators to customers to investors – but at the end of the day the company is in no position to compete with a lot of Arm’s customers, nor would they want to. Even in the server space NVIDIA couldn’t hope to address everything from microservers to supercomputers, never mind embedded controllers and smartphones. So NVIDIA is taking a complementary approach to the acquisition, using Arm’s server technology to augment their own, all the while continuing to license out IP.In fact, the company is looking at growing the amount of IP that Arm licenses by including IP currently held by NVIDIA; technologies such as GPUs, AI accelerators, and network processors. It’s an idea that NVIDIA hasplayed around with once beforewithout much success, but Arm comes with a much better business model and much more experience in licensing than NVIIDA ever had. Just what this expansion entails remains to be seen, but the obvious routes include licensing out GeForce graphics IP for use in SoCs (potentially replacing Arm’s Mali offerings), as well as licensing out bits and pieces of NVIDIA’s tensor core and InfiniBand technologies.Still, NVIDIA knows that they face an uphill battle in convincing Arm’s traditional customers that NVIDIA has their best interests at heart. The Arm deal is less than desirable for the industry as a whole, as Arm has traditionally only sold IP and related core designs, remaining fully divorced from full-scale chip design and sales. However with SoftBank seemingly set on selling Arm, there are few companies in a position to buy Arm, and even fewer that would be willing to take it on-board as a long-term investment. Ultimately, Arm being acquired by a chipmaker makes for strange bedfellows all around, and it falls on NVIDIA to convince customers that their acquisition of Arm will help the ecosystem by combining the companies engineering resources, and that they are earnest about continuing to design and sell top-shelf IP that other companies – even NVIDIA’s competitors – will get reasonable access to.The other (and perhaps more immediate) challenge for NVIDIA is convincing regulators across the globe to approve the deal. NVIDIA is pitching the deal as being complementary, combining two companies that otherwise have minimal overlap. None the less, minimal is not the same as “none”, and besides the immediate and obvious overlap with Mali and GeForce GPU technologies, regulators will no doubt take a great deal of interest in the future of IP licensing. The smartphone revolution of the past decade and a half has been built on top of Arm architectures – never mind the billions of devices with Arm-based microcontrollers – so many parties have a vested interest in keeping that going.To that end, while NVIDIA is just starting discussions with regulators – the deal was secret and not being discussed with partners nor regulators until this evening – the company isalready making concessions and guarantees to the British government to get its approval. This includes committing to keeping Arm headquartered in Cambridge, and continuing to do a significant amount of their engineering work there. The company is alsoannouncing that they will be building one of their AI “centers of excellence” in Cambridge. Besides providing an environment for cutting-edge AI research and training, NVIDIA will be building an Arm & NVIDIA-powered supercomputer at the site. While specific plans for the supercomputer are not being announced, the company recently finished building the world’s 7th-fastest supercomputer,Selene, using its DGX Pod infrastructure, so NVIDIA has significant capabilities here.Otherwise, no such overtures have been made to the US or China, however the situations in those countries are very different since they are not Arm’s traditional home. What (if anything) NVIDIA will need to do to sell regulators in those countries remains to be seen, but it’s worth noting that nothing about this deal resolves the current export impasse with China; even after the deal closes, Arm will still face the same restricts in exporting its technology to China.Finally, while this deal will see NVIDIA buying Arm wholesale, the two companies have confirmed that Arm’s ongoing efforts to sell off its IoT Services Group will continue. As a non-IP business NVIDIA has no interest in it, and as a result it will still be spun-off.Ultimately, the Arm deal will be a significant shift in the industry, one far bigger than the $40 billion price tag indicates. In one form or another, Arm and its IP are at the heart of billions of chips sold every year, a reach that few other companies can match. For NVIDIA, acquiring Arm will cement their position as a top-tier chip designer, all the while giving them an even larger technology portfolio to go after the server market, and new opportunities to sell IP to other vendors. But true success will likely hinge on how well NVIDIA can court the rest of the technology industry: Arm’s reach is only as big as its customers, and so it’s up to NVIDIA to convince it’s customers that they can still count on Arm’s neutrality even after the change in ownership.Gallery:NVIDIA To Acquire Arm: Press DeckSource:NVIDIA\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16080/nvidia-to-acquire-arm-for-40-billion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Announces Cortex-R82: First 64-bit Real Time Processor\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-09-03T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16056/arm-announces-cortexr82-first-64bit-real-time-processor\n",
      "Content: Arm is known for its Cortex range of processors in mobile devices, however the mainstream Cortex-A series of CPUs which are used as the primary processing units of devices aren’t the only CPUs which the company offers. Alongside the microcontroller-grade Cortex-M CPU portfolio, Arm also offers the Cortex-R range of “real-time” processors which are used in high-performance real-time applications. The last time we talked about a Cortex-R product was theR8 release back in 2016. Back then, the company proposed the R8 to be extensively used in 5G connectivity solutions inside of modem subsystems.Another large market for the R-series is storage solutions, with the Cortex-R processors being used in HDD and SSD controllers as the main processing elements.Today, Arm is expanding its R-series portfolio by introducing the new Cortex-R82, representing the company’s first 64-bit Armv8-R architecture processor IP, meaning it’s the first 64-bit real-time processor from the company.To date, previous generation R processors were based on the predecessor Armv7-R or ArmV8-R 32-bit architecture,such as the Cortex-R52. This was for years still fine and sufficient for the use-cases in which these processors were deployed. In modern products however, we’re seeing designs in which larger memory addressing is becoming necessary. Modern SSDs right now for example routinely use up to 2GB of DRAM memory on their controllers, which comes near to the 32-bit 4GB memory addressing limit of the R8 CPUs.The new Cortex-R82 enhances the core to enable up to a 2x performance improvement over the R8, uses a wider physical address space up to 1TB which is coherent with the rest of the system.Arm currently doesn’t divulge much about the microarchitecture of the R82 and how it differs from the R8, but we imagine there to be some significant changes with the shift to the Armv8-R architecture.An important addition here from the architecture and µarch side is the optional inclusion of NEON units for SIMD processing, including new dot-product instructions. This would enable for higher performance parallel processing compute capacity on the processor itself, allowing customers such as SSD-controller designers more flexibility as to their designs.Another big change to the microarchitecture is the inclusion of an MMU, which allows the Cortex-82 to actually serve as a general-purpose CPU for a rich operating system such as Linux. This is actually quite a huge change in regards to the target market possibilities of the -R series going forward if the processor can run its own OS all by itself.Arm’s product presentations focus on storage controllers, able to run both real-time workloads like they have been until now, but add in a rich OS for more complex algorithms and higher-level applications that aren’t as feasible on a bare-metal and real-time operating system.A chip designer for example can tape-out a drive controller with multiple R82 cores (the design scales up to a 8-core cluster), and flexibly change the processing resources between the real-time application and the higher-level computation workload.As the storage market evolves, one of the biggest requirements we’ve seen from our partners is flexibility. The new features of the Cortex-R82 processor give partners the possibility to design multi-core implementations of up to 8 cores, and adjust the types of workload running on the storage controller based on external demands in software. For example, parking lots will regularly use video surveillance to recognize license plate information which is later used for billing. During the day vehicle registration plate data is collected, meaning most cores are being used for intensive storage. At night, these cores will be used to process the data for billing and will adjust to carry out the data analysis and machine learning needed. As storage controllers are becoming more diverse to address different markets and features, Cortex-R82 delivers an architecture to provide this extreme flexibility – reducing costs and time to market.Arm states that 85% of current storage solutions use Cortex-R processors, so the new R82 likely will represent a big jump in performance and open new possibilities for vendors to design new differentiating features in future designs.Related Reading:ARM Announces the Cortex-R52 CPU: Deterministic & Safe, For ADAS & MoreARM Announces New Cortex-R8 Real-Time ProcessorARM Announces ARMv8-M Instruction Set For Microcontrollers – TrustZone Comes to Cortex-M\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16056/arm-announces-cortexr82-first-64bit-real-time-processor\n",
      "Title: Ampere Altra 1P Server Pictured: GIGABYTE’s 2U with 80 Arm N1 Cores, PCIe 4.0 and CCIX\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-08-03T21:00:00Z\n",
      "URL: https://www.anandtech.com/show/15949/ampere-altra-1p-server-pictured-gigabytes-2u-with-80-arm-n1-cores-pcie-40-and-ccix\n",
      "Content: With the news of Apple moving to Arm SoCs replacing Intel in a few key products, and the success of the new Graviton2 SoC in Amazon’s Web Services, the news-o-sphere is awash with excitement about a new era of Arm-based computing. One of the companies looking to deploy Arm into the cloud is Ampere, with its new Altra and Altra Max compute processors.We’ve coveredAltra in some detail, with the aim to offer better-than-Graviton performance and functionality to hyperscalers that don’t have access to Graviton2 (because it’s an Amazon only chip). In June,the company launched its processor list, going from 24 cores all the way up to 80 cores running at 3.3 GHz for 250 W. This processor list is quite possibly the easiest-to-follow naming scheme of any processor list in recent memory. Alongside all those high-performance cores there are 128 PCIe 4.0 lanes, support for CCIX, eight channel DDR4-3200 memory, and a 128-core version coming early next year. It’s a shot well past Graviton, aimed squarely at Xeon and Epyc.At the announcement of Altra, Ampere stated that it would be developing reference designs for Altra – a single socket called Mt. Snow, and a dual socket called Mt. Jade. GIGABYTE is the first OEM partner to showcase its single socket design, with a dedicated video on the product as part of theGIGABYTE Virtual Show 2020, which replaced its Computex plans.The R272-P30 server is the Mt. Snow single socket 2U design, built upon GIGABYTE’s MP32-AR0 motherboard, which is an EATX form factor. The motherboard is laid out in order to improve server airflow with a transposed socket, which also helps with supporting all of the sixteen DDR4-3200 memory slots. The Altra socket is a rather substantial LGA4926 socket, indicating 4926 pins within the bracket, with the bracket held on by five Torx screws (EPYC uses three by comparison). Supporting the socket is an 8-phase server-grade power delivery, which seems minuscule by consumer standards but is probably overkill here.The motherboard has two PCIe 4.0 x16 full-length slots and four PCIe 4.0 x8 full-length slots, plus another PCIe 4.0 x8 half-length slot. Management is through the popular Aspeed AST2500 BMC implementation, while onboard Ethernet uses a dual Intel I350 solution. There are four slimline U.2 ports, as well as eight min-SAS breakout headers and an OCP 2.0 PCIe 3.0 slot for add-in OCP solutions.The server as presented by GIGABYTE had seven PCIe breakout cards installed, leading to 24x PCIe 4.0 NVMe x4 storage at the front of the chassis. Ultimately this solution is for a fast-storage cloud deployment, and is one of the first Altra-based servers that GIGABYTE is developing. Ampere is a key partner with NVIDIA in order to provide CUDA-on-Arm solutions, so we suspect a GPGPU variant might be in the works as well.Ampere 1st Gen Altra 'QuickSilver'Product ListAnandTechCoresFrequencyTDPPCIeDDR4PriceQ80-33803.3 GHz250 W128x G48 x 3200?Q80-30803.0 GHz210 W128x G48 x 3200?Q80-26802.6 GHz175 W128x G48 x 3200?Q80-23802.3 GHz150 W128x G48 x 3200?Q72-30723.0 GHz195 W128x G48 x 3200?Q64-33643.3 GHz220 W128x G48 x 3200?Q64-30643.0 GHz180 W128x G48 x 3200?Q64-26642.6 GHz125 W128x G48 x 3200?Q64-24642.4 GHz95 W128x G48 x 3200?Q48-22482.2 GHz85 W128x G48 x 3200?Q32-17*321.7 GHz58 W128x G48 x 3200?Q32-17321.7 GHz45 W128x G48 x 3200?*With 4 TiB DRAM InstalledAmpere recently announced that the first cloud instances on Altra were starting to come online, startingwith Packet as part of the Early Access Program. Wider general availability is expected through the rest of the year. We’re already on the list for a review sample!Source:GIGABYTERelated ReadingNext Generation Arm Server: Ampere’s Altra 80-core N1 SoC for Hyperscalers against Rome and XeonAmpere’s Product List: 80 Cores, up to 3.3 GHz at 250 W; 128 Core in Q4Avantek's Arm Workstation: Ampere eMAG 8180 32-core Arm64 ReviewArm Development For The Office: Unboxing an Ampere eMag WorkstationAmazon's Arm-based Graviton2 Against AMD and Intel: Comparing Cloud Compute\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15949/ampere-altra-1p-server-pictured-gigabytes-2u-with-80-arm-n1-cores-pcie-40-and-ccix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: New #1 Supercomputer: Fugaku in Japan, with A64FX, take Arm to the Top with 415 PetaFLOPs\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-06-22T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/15869/new-1-supercomputer-fujitsus-fugaku-and-a64fx-take-arm-to-the-top-with-415-petaflops\n",
      "Content: High performance computing is now at a point in its existence where to be the number one, you need very powerful, very efficient hardware, lots of it, and lots of capability to deploy it. Deploying a single rack of servers to total a couple of thousand cores isn’t going to cut it. The former #1 supercomputer, Summit, is built from 22-core IBM Power9 CPUs paired with NVIDIA GV100 accelerators, totaling 2.4 million cores and consuming 10 MegaWatts of power. The new Fugaku supercomputer, built at Riken in partnership with Fujitsu, takes the top spot on the June 2020 #1 list, with 7.3 million cores and consuming 28 MegaWatts of power.The new Fugaku supercomputer is bigger than Summit in practically every way. It has 3.05x cores, it has 2.8x the score in the official LINPACK tests, and consumes 2.8x the power. It also marks the first time that an Arm based system sits at number one on the top 500 list.Due to the onset of the Coronavirus pandemic, Riken accelerated the deployment of Fugaku in recent months. On May 13th, Riken announced that more than 400 racks, each featuring multiple 48-core A64FX cards per server, were deployed. This was a process that had started back in December, but they were so keen on getting the supercomputer up and running to assist with the R&D as soon as possible – the server racks didn’t have their official front panels when they started working. There are still additional resources to add, with full operation scheduled to begin in Riken’s Fiscal 2021, suggesting that Fugaku’s compute values on the top 100 list are set to rise even higher.Alongside being #1 in the TOP500, Fugaku enters the Green500 List at #9, just behind Summit, and below the Fugaku Prototype installation which sits at #4.At the heart of Fugaku is the A64FX, a custom Arm v8-A CPU-based chip optimised for compute. The total configuration uses 158,976 of these 48+4-core cards, running at 2.2 GHz peak performance (48 cores for compute, 4 for assistance). This allows for some substantial Rpeaknumbers, such as 537 PetaFLOPs of FP64, the usual TOP500 metric. But A64FX also supports quantized models with lower precision, which is where we get into some fun numbers for Fugaku:FP64: 0.54 ExaFLOPsFP32: 1.07 ExaOPsFP16: 2.15 ExaOPsINT8: 4.30 ExaOPsDue to the design of the A64FX, it also allows for a total memory bandwith of 163 PetaBytes per second.To date, the A64FX compute card is the only implementation of Arm’s v8.2-A Scalable Vector Extensions (SVE). The goal of SVE is to allow Arm’s customers to build hardware with vector units ranging from 128-bit to 2048-bit, such that any software that is built to run on SVE will automatically scale regardless of the SVE execution unit size. A64FX uses two 512-bit wide pipes per core, with 48 compute cores per chip, and also adds in four 8 GiB HBM2 links per chip in order to feed the units for 1 TiB/s of total bandwidth into the chip.As listed above, the unit supports INT8 through FP64, and the chip has an on-board custom Tofu interconnect, supporting up to 560 Gbps of interconnect to other A64FX modules. The chip is built on TSMC’s N7 process, and comes in at 8.79 billion transistors. 90% execution efficiency is claimed for DGEMM type workloads, and additional mechanisms such as combined gather and unaligned SIMD loading are used to help keep throughput high. There is also additional tuning that can be done at the power level for optimization, and extensive internal RAS (over 128k error checkers in silicon) to ensure accuracy.Details on the A64FX chip weredisclosed at Hot Chips in 2018, andwe saw wafers and chipsat Supercomputing in 2019. This chip is expected to be the first in a series of chips from Fujitsu along a similar HPC theme.Work done on Fugaku to date includes simulations about Japan’s COVID-19 track and tracing app. According to Professor Satoshi Matsuoka,predictions calculated by Fugakusuggested a 60% distribution on the app development in order to be successful.Droplet simulationshave also been performed on virus activity. Deployment of A64FX is set to go beyond Riken, withSandia Labs to also have an A64FX systembased in the US.Source: TOP500Related ReadingA Success on Arm for HPC: We Found a Fujitsu A64FX WaferHot Chips 2018: Fujitsu's A64FX Arm Core Live Blog\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15869/new-1-supercomputer-fujitsus-fugaku-and-a64fx-take-arm-to-the-top-with-415-petaflops\n",
      "Title: Arm Announces Ethos-N78 NPU: Bigger And More Efficient\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-05-27T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/15817/arm-announces-ethosn78-npu-bigger-and-more-efficient\n",
      "Content: Yesterday Arm released the newCortex-A78, Cortex-X1 CPUsand the newMali-G78 GPU. Alongside the new “key” IPs from the company, we also saw the reveal of the newest Ethos-N78 NPU, announcing Arm’s new second-generation design.Over the last few years we’ve seen a literal explosion of machine learning accelerators in the industry, with a literal wild west of different IP solutions out there. On the mobile front particularly there’s been a huge amount of different custom solutions developed in-house by SoC vendors, this includes designs such as from Qualcomm, HiSilicon, MediaTek and Samsung LSI. For vendors who do not have the design ability to deploy their own IP, there’s the possibility of licensing something from an IP vendor such as Arm.Arm’s “Ethos” machine learning IP is aimed at client-side inferencing workloads, originallydescribed as “Project Trillium”and the first implementationseeing life in the form of the Ethos-N77. It’s been a year since the release of the first generation, and Arm has been working hard on the next iteration of the architecture. Today, we’re covering the “Scylla” architecture that’s being used in the new Ethos-N78.From a very high-level view, what the N78 promises is a quite large boost both in performance and efficiency. The new design scales up much higher than the biggest N77 configuration, now being able to offer 2x the peak performance at up to 10TOPs of raw computational throughput.Arm has revamped the design of the NPU for better power efficiency, enabled through various new compression techniques as well as an improvement in external memory bandwidth per inference of up to 40%.Strong points of the N78 are the IP’s ability to scale performance across different configuration options. The IP is available at 4 different performance points, or better said at four different distinct engine configurations, from the smallest config at 1TOPs, to 2, 5 and finally a maximum of 10TOPs. This corresponds to MAC configurations of 512, 1024, 2048 and 4096 units for the totality of the design.The interesting aspect of scaling bigger is that the area efficiency of the IP actually scales better the bigger the implementation, due to probably the fact that the unique fixed shared function blocks area percentage shrinks with the more computation engines the design has.Architecturally, the biggest improvements of the new N78 were in the way it handles data around in the engines, enabling new compression methods for data that not only goes outside the NPU (DRAM bandwidth improvement), but also data movement within the NPU itself, improving efficiency for both performance and power.The new compression and data handling can significantly reduce the bandwidth of the system with an average 40% reduction across workloads – which is an extremely impressive figure to showcase between IP generations.Generational performance uplifts, thanks to the higher performance density and power efficiency are on average 25%, which along with the doubled peak performance configuration means that it has the potential to represent a large boost in end devices.It’s quite hard to analyse NPUs on how they perform in the competitive landscape – particularly here in Arm’s case given that we haven’t yet seen the first generation NPU designs in silicon. One interesting remark that Arm has made, is that in this space, software matters more than anything else, and a bad software stack can possibly ruin what otherwise would be a good hardware design. Arm mentioned they’ve seen vendors adopt their own Ethos IP and dropping competitor designs because of this – Arm says they invest a very large amount of resources into software in order to facilitate customers to actually properly make use of their hardware designs.Arm’s new Ethos-N78 has already been licensed out to customers and they’re taping in their designs with it, with likely the first products seeing the light of day in 2021 at the earliest.Related Reading:Arm Announces New Ethos-N57 and N37 NPUs, Mali-G57 Valhall GPU and Mali-D37 DPUARM Details \"Project Trillium\" Machine Learning Processor ArchitectureImagination Goes Further Down the AI Rabbit Hole, Unveils PowerVR Series3NX Neural Network AcceleratorCEVA Announces NeuPro-S Second-Generation NN IPCadence Announces Tensilica Vision Q7 DS\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15817/arm-announces-ethosn78-npu-bigger-and-more-efficient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm's New Cortex-A78 and Cortex-X1 Microarchitectures: An Efficiency and Performance Divergence\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-05-26T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15813/arm-cortex-a78-cortex-x1-cpu-ip-diverging\n",
      "Content: 2019 was a great year for Arm. On the mobile side of things one could say it was business as usual, as the company continued to see successes with its Cortex cores, particularlythe new Cortex-A77which we’ve now seen employed in flagship chipsetssuch as the Snapdragon 865. The bigger news for the company over the past year however hasn’t been in the mobile space, but rather in the server space, where one can todayrent Neoverse-N1 CPUssuch asAmazon’s impressive Graviton2 chip, with more vendors such asAmpere expected to releasetheir server products soon.While the Arm server space is truly taking off as we speak, aiming to compete against AMD and Intel, Arm hasn't reached the pinnacle of the mobile market – at least, not yet. Arm’s mobile Cortex cores have lived in the shadow of Apple’s custom CPU microarchitectures over the past several years, as Apple has seemingly always managed to beat Cortex designs by significant amounts. While there’s certainly technical reasons to the differences – it was also a lot due to business rationale on Arm’s side.Today for Arm’s 2020 TechDay announcements, the company is not just releasing a single new CPU microarchitecture, but two. The long-expected Cortex-A78 is indeed finally making an appearance, but Arm is also introducing its new Cortex-X1 CPU as the company’s new flagship performance design. The move is not only surprising, but marks an extremely important divergence in Arm’s business model and design methodology, finally addressing some of the company’s years-long product line compromises.The New Cortex-A78: Doubling Down on EfficiencyThe new Cortex-A78 isn’t exactly a big surprise – Arm hadfirst publicly divulged the Hercules codenameover two years ago when they had presented the company’s performance roadmap through 2020. Two years later, and here we are, with the Cortex-A78 representing the third iteration of Arm’s new Austin-family CPU microarchitecture,which had started from scratch with the Cortex-A76.The new Cortex-A78 pretty much continues Arm’s traditional design philosophy, that being that it’s built with a stringent focus on a balance between performance, power, and area (PPA). PPA is the name of the game for the wider industry, and here Arm is pretty much the leading player on the scene, having been able to provide extremely competitive performance at with low power consumption and small die areas. These design targets are the bread & butter of Arm as the company has an incredible range of customers who aim for very different product use-cases – some favoring performance while some other have cost as their top priority.All in all (we’ll get into the details later), the Cortex-A78 promises a 20% improvement in sustained performance under an identical power envelope. This figure is meant to be a product performance projection, combining the microarchitecture’s improvements as well as the upcoming 5nm node advancements. The IP should represent a pretty straightforward successor to the already big jump that were the A76 and A77.The New Cortex-X1: Breaking the Design Constraint ChainsArm’s existing business model was aimed at trying to create a CPU IP that covers the widest range of customer needs. This creates the problem that you cannot hyper-focus on any one area of the PPA triangle without making compromises in the other two. I mentioned that Arm’s CPU cores have for years lived in the shadow of Apple’s CPU cores, and whilst for sure, the Apple's cores were technical superior, one very large contributing factor in Arm's disadvantage was that the business side of Arm just couldn’t justify building a bigger microarchitecture.As the company is gaining more customers, and is ramping up R&D resources for designing higher performance cores (with the server space being a big driver), it seems that Arm has finally managed to get to a cross-over point in their design abilities. The company is now able to build and deliver more than a single microarchitecture per year. In a sense, we sort of saw the start of this last year withthe introduction of the Neoverse-N1 CPU, already having some more notable microarchitectural changes over its Cortex-A76 mobile sibling.Taking a quick look at the new Cortex-X1, we find the X1 higher up in Arm’s Greek pantheon family tree of CPU microarchitectures. Codenamed Hera, the design at least is named similarly to its Hercules sibling, denominating their close design relationship. The X1 is much alike the A78 in its fundamental design – in fact both CPUs were created by the same Austin CPU design team in tandem, but with the big difference that the X1 breaks the chains on its power and area constraints, focusing to get the very best performance with very little regard to the other two metrics of the PPA triangle.The Cortex-X1 was designed within the frame of a new program at Arm, which the company calls the “Cortex-X Custom Program”. The program is an evolution of what the company had previously already done with the “Built on Arm Cortex Technology” program released a few years ago. As a reminder, that license allowed customers to collaborate early in the design phase of a new microarchitecture, and request customizations to the configurations, such as a larger re-order buffer (ROB), differently tuned prefetchers, or interface customizations for better integrations into the SoC designs. Qualcomm was the predominant benefactor of this license, fully taking advantage of the core re-branding options.The new Cortex-X program is an evolution of the BoACT license, this time around making much more significant microarchitectural changes to the “base” design that is listed on Arm’s product roadmap. Here, Arm proclaims that it allows customers to customize and differentiate their products more; but the real gist of it is that the company now has the resources to finally do what some of its lead customers have been requesting for years.One thing to note, is that while Arm names the program the “Cortex-X Custom Program”, it’s not to be confused with actual custom microarchitectures by vendors with an architectural license. The custom refers to Arm’s customization of their roadmap CPU cores – the design is still very much built by Arm themselves and they deliver the IP. For now, the X1 IP will also be identical between all licensees, but the company doesn’t rule out vendor-specific changes the future iterations – if there’s interest.This time around Arm also maintains the marketing and branding over the core, meaning we’ll not be seeing the CPU under different names. All in all, the whole marketing disclosure around the design program is maybe a bit confusing – the simple matter of fact is that the X1 is simply another separate CPU IP offering by Arm, aimed at its leading partners, who are likely willing to pay more for more performance.At the end of the day, what we're getting are two different microarchitectures – both designed by the same team, and both sharing the same fundamental design blocks – but with the A78 focusing on maximizing the PPA metric and having a big focus on efficiency, while the new Cortex-X1 is able to maximize performance, even if that means compromising on higher power usage or a larger die area.It’s an incredible design philosophy change for Arm, as the company is no longer handicapped in the ultra-high-tier performance ring with the big players such as Apple, AMD, or Intel – all whilst still retaining their bread & butter design advantages for the more cost-oriented vendors out there who deliver hundreds of millions of devices.Let’s start by dissecting the microarchitecture changes of the new CPUs, starting off with the Cortex-A78…\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15813/arm-cortex-a78-cortex-x1-cpu-ip-diverging\n",
      "Title: Arm Announces The Mali-G78 GPU: Evolution to 24 Cores\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-05-26T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15816/arm-announces-the-malig78-evolution-to-24-cores\n",
      "Content: Today as part of Arm’s 2020 TechDay announcements, alongside the release of the brand-new Cortex-A78 and Cortex-X1 CPUs, Arm is also revealing its brand-new Mali-G78 and Mali-G68 GPU IPs.Last year, Arm had unveiled the new Mali-G77 which was the company’s newest GPU design based on a brand-new compute architecture called Valhall. The design promised major improvements for the company’s GPU IP, shedding some of the disadvantages of past iterations and adapting the architectures to more modern workloads. It was a big change in the design, with implementations seen in chips such as the Samsung Exynos 990 or the MediaTek Dimensity 1000.The new Mali-G78 in comparison is more of an iterative update to the microarchitecture, making some key improvements in the matter of scalability of the configuration as well as balance of the design for workloads, up to some more radical changes such as a complete redesign of its FMA units.On the scalability side, the new Mali-G78 now goes up to 24 cores in an implementation, which is a 50% increase in core count compared to the maximum MP16 configuration of the Mali-G77. To date, the biggest configuration we’ve seen in the wild of the G77 was the M11 setup of the Exynos 990, with MediaTek employing an MP9 setup.In a projected end-device solution comparison between 2020 and 2021 devices, Arm is projecting the new Mali-G78 to achieve 25% better performance, which includes both microarchitectural as well as process node improvements. That’s generally the reasonable target that vendors are able to achieve on newer generation IPs, but it’s also going to be strongly depending on the exact process node improvements that are projected here – as GPUs generally scale better with improves process density rather than just frequency and power improvements of the silicon.At an ISO-process node under similar implementation area conditions, the Mali-G78 is claimed to improve performance density by 15%. This is referring to the either performing 15% better at the same area, or shaving off 15% area for the same performance, given that this can be done linearly by just adjusting the amount of GPU cores implemented.Power efficiency sees a more meagre 10% improvement, which honestly isn’t too fantastic and not that big of a leap to the Mali-G77. ML performance is also said to be improved by 15% thanks to some new microarchitectural tweaks.Seemingly, the Mali-G78 doesn’t look like too much of an upgrade compared to the vast new redesign we saw last year with the G77 – and in a sense, that does seem somewhat reasonable. Still, the G78 does some interesting changes to its microarchitecture, let’s dwell a bit deeper into what’s changed…\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15816/arm-announces-the-malig78-evolution-to-24-cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Avantek's Arm Workstation: Ampere eMAG 8180 32-core Arm64 Review\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-05-22T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/15733/ampere-emag-system-a-32core-arm64-workstation\n",
      "Content: Arm desktop systems are quite a rarity. In fact, it’s quite an issue for the general Arm software ecosystem in terms of having appropriate hardware for developers to actually start working in earnest on more optimised Arm software.To date, the solution to this has mostly been using cloud instances of various Arm server hardware – it can be a legitimate option andnew powerful cloud instances such as Amazon’s Graviton2certainly offer the flexibility and performance you’d need to get things rolling.However, if you actually wanted a private local and physical system, you’d mostly be relegated to small low-performing single-board computers which most of the time had patchy software support. It’s only been in the last year or two where Arm-based laptops with Qualcomm Snapdragon chips have suddenly become a viable developer platform thanks to WSL on Windows.For somebody who wants a bit more power and in particular is looking to make use of peripherals – actively using large amounts of storage or PCIe connectivity, then there’s options such as Avantek’s eMag Workstation system.The system is an interesting mish-mash of desktop and server hardware, and at the centre of it all enabling is Ampere’s “Raptor” motherboard containing the eMAG 8180 32-core chip. This is a server development board that doesn’t really adhere to any standard form-factor standard, but Avantek was able to make it fit into BeQuiet tower chassis with some modifications.Ian had published a more in-depth visual inspection of the machine a few weeks ago, so I recommend reading that in terms of the analysis of what’s physically present in the machine and its quirks.Read:Arm Development For The Office: Unboxing an Ampere eMag WorkstationThe notable characteristics of the system is that in fact it’s a setup that was designed by a vendor that’s usually server oriented – this is Avantek’s first foray into a desktop system.As noted, because the motherboard isn’t adhering to an ATX standard, the biggest incompatibility lies on the part of the PCIe slots which don’t match up with the slots of the chassis. Avantek here had to resort to using a riser card and a custom backplate in order to fit the graphics card.The graphics card provided in our sample was a Radeon Pro WX5100 – a lower-end unit meant for workstations.The biggest advantage of the system which we’ll address in more detail in a bit, is the fact that this is an SBSA (Server Base System Architecture) compliant system, which means it’ll be compatible with “most” PCIe hardware out there. For example, I had no issues replacing the graphics card with an older Radeon HD 7950 I had lying around and the system booted with display output without any issues. This might sound extremely boring, and it is – but for the Arm ecosystem it’s been a decade long journey to reach this point.In terms of general form-factor, Avantek’s choice here to go with a desktop chassis works well. It’s a big motherboard so it does require a bigger case, allowing it for plenty of additional hardware inside.I think one negative on the system from a practical hardware perspective is Avantek’s server pedigree. The CPU cooler in particular is the type you’d find in a server system, and the fan choice isn’t something you’d see in any traditional desktop system as it's a more robust 90mm fan. Although the company has said that it tried to minimise the noise of the system by adjusting the fan curves as well as opting for a low acoustics chassis – it’s still subjectively loud for a desktop system. I measured around 42dBA at idle which is still a bit much - but that also depends on your typical expectations of a silent system.I hope Avantek would change in the future is employ a more consumer grade CPU cooler system and reduce the acoustics of the machine.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15733/ampere-emag-system-a-32core-arm64-workstation\n",
      "Title: Arm Development For The Office: Unboxing an Ampere eMag Workstation\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-04-22T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15737/arm-development-for-the-office-unboxing-an-ampere-emag-workstation\n",
      "Content: One of the key elements I’ve always found frustrating with basic software development is that it can often be quite difficult to actually get the hardware in hand you want to optimize for, and get a physical interaction that isn’t delayed by networking or logging in or anything else. Having a development platform on the desk guarantees that direct access, and for the non-x86 vendors, I’ve been asking for these for some time. Thankfullywe’re now starting to see some appear, and Avantek, one of the Arm server retailers, have built an Ampere eMag workstation out of a server board, with some interesting trickery to get it to fit. They sent us one to have a look at.While Andrei is testing the system for our full review, I wanted to take some time to actually have a physical look at what one of the first Arm server workstations looks like. This system isn’t built by Ampere, but by Avantek, who takes one of the Ampere eMag motherboards and places into a consumer based PC chassis from Be Quiet, then modifies the chassis to fit the server-sized motherboard. This involves customization, given that the server motherboard does not have the standard E-ATX holes or PCIe spacings for the rear panel. This particular chassis has the option of a transparent side panel and LEDs – Avantek says that despite this market not being the typical recipient of these more consumer aesthetics, they had the demand!Inside the system is a 32-core Ampere eMag server, with 256 GB of eight-channel DDR-2666 memory, a 500GB WD Black SN750 NVMe SSD, a 960 GB Micron 5300 Pro SATA SSD in the rear, a Corsair VS 650W power supply, and an AMD Radeon Pro WX 5100 graphics accelerator, connected through a PCIe riser cable to be vertical. This is due to some awkward placement of the PCIe slots, as we’ll see in a bit.The power delivery for the 125 W processor is through a 5-phase design, using what looks like standard copper core chokes. Nonetheless, these are rated for a server environment.The CPU cooler looks very standard for a tower-style PC, with what looks like 5 double-sized heatpipes and a strong fan with extra baffles to direct airflow.Back to the memory, and we have eight 32 GB DDR4-2666 modules of Samsung’s RDIMMs.So a word on the cabling. Behind the GPU there is a USB 3.0 connector for the front panel, however due to the placement Avantek are using a right angled adaptor here, with the front panel cable eventually going behind the motherboard. The SATA ports to the left are out of the way, but there’s that big 24-pin power cable going right up through the front of the motherboard, rather than behind it. This is because the Corsair VS 650W power supply is a fully wired model, with fixed lengths. This sort of cabling would be standard for a server build, however a modular version might be a bit tidier and also offer the potential for custom cabling should lengths not fit. That being said, the 650W is an 80 PLUS ‘White’ model and easily enough for the 125 W processor and a 75W max graphics card. Should users decide to specify a more powerful GPU, then this power supply will handle it easily.For the PCIe slots, you will notice that we have two PCIe 3.0 x16 slots right next to each other, next to an OCP slot. Again, in a server chassis, this can be very common, given that add-in devices are typically given riser cables. In order to make this motherboard fit in the chassis, Avantek used a riser cable to the GPU mount with some modifications.Here we see the Phanteks riser cable with the AMD GPU. The customization of the chassis, as shown on the right, happens on the back panel, given that this chassis wasn’t designed for a vertical GPU. There are chassis in the market that have vertical setups, however Avantek couldn't find the right one that could also fit this motherboard well enough. They're more than happy to take on suggests for the next generation, but they also have to balance sound levels too.Avantek have cut away most of the horizontal GPU mounts here to put the graphics card in on a custom vertical mount. At the rear of the chassis here we also see the single 120mm fan, and the IO panel, consisting of an Intel I210-AT gigabit Ethernet port, two USB 2.0 ports, a D-Sub output from the BMC, and a COM port. There’s another Ethernet port, again for the BMC.Here’s conformation of that I210-AT controller.Here’s the BMC, a very common ASPEED AST2500 chip paired with some Micron memory. This enables the 2D interface over the D-Sub connector, or system monitoring and control through the Ethernet port.On the front of the chassis, there are no extra ODDs in the bays, but we get two front fans.Gallery:Ampere eMag Workstation by AvantekWe're going to take the CPU cooler off for a photo when our testing is complete, just in case (!). But for those interested, here's the lscpu:This Ampere eMag Arm-based system is unique to Avantek, and as we’ve covered before at AnandTech, andstarts at ~$2795 for the base model, with 8 GB of DRAM and a 240 GB SSD. The workstation is only offered with a single CPU SKU, the eMAG 8180. This isn’t to be confused with Intel’s 8180: this one has more cores! The eMAG 8180 is a 32-core design running at 2.8 GHz with a turbo up to 3.3 GHz, with a TDP of 125 W. This is a first generation eMAG, which uses the old AppliedMicro Skylark microarchitecture, a custom design of Arm v8 with 32 MB of L3, 42 PCIe lanes, and eight memory channels. Avantek offers the system with three optional graphics cards: AMD FirePro W2100, a Radeon Pro WX 5100, and the NVIDIA Quadro GV100. OS options are variants of Linux: Ubuntu, CentOS, SUSE SLES, and openSUSEAs mentioned, we’re planning a full review of the eMag processor shortly as a development system. Technically Ampere has already announced its next generation hardware,the 80-core Altra based on Arm N1 cores, for later this year, however the eMag has been around for a while and it is nice to get numbers to compare it to, especially given those that have deployed eMag will likely be retaining that hardware for several years.Avantek are looking to build an Altra based workstation model as well, should demand be sufficient. But I'd also like to see ThunderX2/X3 workstation systems, Phytium development systems, Graviton2 development systems, and when it comes around to it, Nuvia development systems.For those users that develop hardware-specific software, do you prefer local systems to work on, or cloud/non-local resources? What do you want to see in an upcoming workstation? Let us know in the comments.Related ReadingNext Generation Arm Server: Ampere’s Altra 80-core N1 SoC for Hyperscalers against Rome and Xeon80-Core N1 Next-Gen Ampere, ‘QuickSilver’: The Anti-Graviton2Arm Server CPUs: You Can Now Buy Ampere’s eMAG in a Workstationhttps://store.avantek.co.uk/ampere-emag-64bit-arm-workstation.htmlAmpere Computing: Arm is Now an Investor\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15737/arm-development-for-the-office-unboxing-an-ampere-emag-workstation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: European Processor Initiative Backed SiPearl Announces Licensing of Arm Zeus Neoverse CPU IP\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-04-21T16:00:00Z\n",
      "URL: https://www.anandtech.com/show/15738/epi-backed-sipearl-announces-licensing-of-arm-zeus-neoverse-cpu-ip\n",
      "Content: SiPearl, a new France-based company that is being backed and receiving grants from the European Comission’s European Processor Initiative project, has announced that is has licensed Arm’s next-generation Neoverse processor, codename Zeus.SiPearl is still in its infancy as it’s only been founded in January of this year, but the new company has lofty goals as it aims to be the design house for Europe’s HPC goals.Maisons-Laffitte, France, 21 April 2020– SiPearl, the company that is designing the high-performance, low-power microprocessor for the European exascale supercomputer, has signed a major technological licensing agreement with Arm, the global semiconductor IP provider. The agreement will enable SiPearl to benefit from the high-performance, secure, and scalable next-generation Arm® Neoverse™ platform, codenamed ‶Zeusʺ, as well as leverage the robust software and hardware Arm ecosystem.Taking advantage of the Arm “Zeus” platform, including Arm’s POP™ IP, on advanced FinFET2 technology enables SiPearl to accelerate its design and ensure outstanding reliability for a very highend offering, in terms of both computing power and energy efficiency, and be ready to launch its first generation of microprocessors in 2022.The announcement today more specifically covers the company’s licensing deal with Arm – pronouncing that they will be using the new “Zeus” core. Zeus follows up on the Neoverse N1 core “Ares”, and should be the infrastructure sibling to Arm’s Cortex-A77 mobile cores.On the EPI website, the EC also details a roadmap of the project, with the detail of “N6” alongside the Zeus core description, which likely means the chip will be designed on TSMC’s N6 process node – an improved and evolved variant of the manufacturers N7 node which retains design compatibility.The project is another boost to the Arm server ecosystem after the latest success ofAmazon’s Graviton2 chipas well asAmpere’s announcement of the Altraplatform which we’ll be seeing in a few months’ time.Related Reading:Arm Announces Neoverse N1 & E1 Platforms & CPUs: Enabling A Huge Jump In Infrastructure PerformanceArm Announces Neoverse Infrastructure IP Branding & Future RoadmapAmazon's Arm-based Graviton2 Against AMD and Intel: Comparing Cloud ComputeNext Generation Arm Server: Ampere’s Altra 80-core N1 SoC for Hyperscalers against Rome and Xeon80-Core N1 Next-Gen Ampere, ‘QuickSilver’: The Anti-Graviton2\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15738/epi-backed-sipearl-announces-licensing-of-arm-zeus-neoverse-cpu-ip\n",
      "Title: Marvell Announces ThunderX3: 96 Cores & 384 Thread 3rd Gen Arm Server Processor\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-03-16T12:30:00Z\n",
      "URL: https://www.anandtech.com/show/15621/marvell-announces-thunderx3-96-cores-384-thread-3rd-gen-arm-server-processor\n",
      "Content: The Arm server ecosystem is well alive and thriving, finally getting into serious motion after several years of false-start attempts. Among the original pioneers in this space was Cavium, which went on to be acquired by Marvell in 2018. Among the company’s server CPU products is the ThunderX line; while thefirst generation ThunderXleft quite a lot to be desired, theThunderX2 was the first Arm server silicon that we deemed viable and competitiveagainst Intel and AMD products. Since then, the ecosystem has accelerated quite a lot, and only last week we saw howimpressive the new Amazon Graviton2 with the N1 chips ended up. Marvell didn’t stop at the ThunderX2, and had big ambitions for its newly acquired CPU division, and today is announcing the new ThunderX3.The ThunderX3 is a continuation and successor to then-Cavium’s custom microarchitecture found in the TX2, adopting a lot of the key characteristics, most notably the capability of 4-way SMT. Adopting a new microarchitecture with higher IPC capabilities, the new TX3 also ups the clock frequencies, and now hosts up to a whopping 96 CPU cores, allowing the chip to scale up to 384 threads in a single socket.Marvell sees the ecosystem shifting in terms of workloads as more and more applications are shifting to the cloud, and applications are changing in their nature, with more customers employing their own custom software stacks and scaling out these applications. This means that workloads aren’t necessarily focused just on single-threaded performance, but rather on the total throughput available in the system, at which point power efficiency also comes into play.Like many other Arm vendors, Marvell sees a window of opportunity in the lack of execution of the x86 incumbents, very much calling out Intel’s stumbling in process leadership over the past few years, and in general x86 designs being higher power. Marvell describes that part of the problem is that the current systems by the x86 players were designed with a wide range of deployment targets ranging from consumer client devices to the actual server machines, never actually achieving the best results in either workloads. In contrast, the ThunderX line-up is reportedly designed specifically with server workloads in mind, being able to achieve higher power efficiency and thus also achieving higher total throughput in a system.We’ve known that ThunderX3 has been coming for quite a while now, admittedly expecting it towards the latter half of 2019. We don’t know the behind-the-scenes timeline, but now Marvell is finally ready to talk about the new chip. Marvell’s CPU roadmap is on a 2-year cadence, and the chip company here explains that this is a practical timeline, allowing customers time to actually adopt a generation and get good return on investment on the platform before possibly switching over to the next one. Of course, this also gives the design team more time to bring to market larger performance jumps once the new generations are ready.The ThunderX3 - 96 Cores and 384 Threads in Arm v8.3+So, what is the new ThunderX3? It’s a ambitious design hosting up to 96 Arm v8.3+ custom cores running at up to frequencies of up to 3GHz all-core, at TDPs ranging from 100 to 240W depending on the SKU.Marvell isn’t quite ready to go into much details of the new CPU microarchitecture just yet, saying that they’ll divulge a deeper disclosure of the TX3 cores later in the year (They’re aiming for Hotchips), but they do say that one key characteristic is that it now features 4 128-bit SIMD execution units, matching the vector execution throughput of AMD and Intel’s cores. When fully using these units, clock frequencies for all-core drop between 2.2 and 2.6GHz, limited by the thermal and power headroom available to the chip.Having SMT4, the 96-core SKU is able to scale up to 384 threads in a socket, which is by far the highest thread count of any current and upcoming server CPU in the market, a big differentiating factor for the ThunderX3.Marvell doesn’t go into details of the topology of the chip or its packaging technology, only alluding that it’ll have monolithic latencies between the CPU cores. The design comes in either 1 or 2 socket configurations, and the inter-socket communication uses CCPI (Cavium Cache Coherent Interconnect) in its 3rdgeneration, with 24 lanes at 28Gbit/s each, between the two sockets.External connectivity is handled by 64 lanes of PCIe 4.0 with 16 controllers per socket, meaning up to 16 4x devices, with the choice of multiplexing them for higher bandwidth connectivity for 8x or 16x devices.Memory capabilities of the chip is in line with current generation standards, featuring 8 DDR4-3200 memory controllers.Marvell plans several SKUs, scaling the core count and memory controllers, in TDP targets ranging from 100W to 240W. These will all be based on the same silicon design, and binning the chips.Large Generational Performance ImprovementsIn a comparison to the previous generation ThunderX2, the TX3 lists some impressive performance increases. IPC is said to have increased by a minimum of 25% in workloads, with total single-threaded performance going up to at least 60% when combined with the clock frequency increases. If we use the TX2 figures we have at hand, this would mean the new chip would land slightly ahead of Neoverse-N1 systemssuch as the Graviton2, and match more aggressively clocked designs such asthe Ampere Altra.Socket-level integer performance has at least increased by 3-fold, both thanks to the more capable cores as well as their vastly increased core number to up to 96 cores. Because the new CPU has now more SIMD execution units, floating point performance is even higher, increasing to up to 5x.Because the chip comes with SMT4 and it’s been designed with cloud workloads, it is able to extract more throughput out of the silicon compared to other non-SMT or SMT2 designs. Cloud workloads here essentially means data-plane bound workloads in which the CPU has to wait on data from a more distant source, and SMT helps in such designs in that the idle execution clocks between data accesses is simply filled by a different thread, doing long latency accesses itself.ThunderX3 Performance Claims Against the CompetitionUsing this advantage, the ThunderX3 is said to have significant throughput advantages compared to the incumbent x86 players, vastly exceeding the performance of anything that Intel has currently to offer, and also beating AMD’s Rome systems in extremely data-plane bound workloads thanks to the SMT4 and higher core counts.More execution and compute bound workloads will see the least advantages here, as the SMT4 advantages greatly diminishes.Yet for HPC and in particular floating-point workloads, the ThunderX3 is said to also be able to showcase its strengths thanks to the increased SIMD units as well as the overall power efficiency of the system, allowing for significant higher performance in such calculations. Memory bandwidth is also higher than a comparative AMD Rome based system because the lower latencies the TX3 is able to achieve. It’s to be noted that the ThunderX3 will be coming to market later in the year, by which time they’ll have to compete with AMD’s newer Milan server CPU.Marvell says that Arm in the cloud is gaining a lot of traction, and the company is already the market leader in terms of deployments of its ThunderX2 system among companies and hyperscalers (Microsoft Azure currently being the one publicly disclosed, but it’s said that there are more). I don’t really know if having a extremely high number of virtual machines being hosted on a single chip is actually an advantage (because of SMT4, per-VM performance might be quite bad), but Marvell does state that they’d be the leader in this metric with the ThunderX3, thanks to be able to host up to 384 threads.Finally, the company claims a 30% perf/W advantage over AMD’s Rome platform across an average of different workloads, thanks to the more targeted microarchitecture design. The more interesting comparison here would have been a showcase or estimate of how the ThunderX3 would fare against Neoverse-N1 systems such as the Graviton2 or the Altra, as undoubtedly the latter system would pose the closest competitor to the new Marvell offering. Given that the Altra isn’t available yet, we don’t know for sure how the systems will compete against each other, but I do suspect that the ThunderX3 to do better in at least FP workloads, and of course it has an indisputable advantage in data-plane workloads thanks to the SMT4 capability.More Information at Hotchips 2020Marvell hasn’t yet disclosed much about the cache configuration or any other specifics of the system, for example what kind of interconnect the cores will be using or what kind of CPU topology they will be arranged in. The ThunderX3’s success seemingly will depend on how it’s able to scale performance across all of its 96 cores and the 384 threads – but at least as an initial impression, it seems that it might do quite well.Today is just the initial announcement of the TX3, and Marvell will be revealing more details and information about the new CPU and the product line-up over the following months till the eventual availability later in the year.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15621/marvell-announces-thunderx3-96-cores-384-thread-3rd-gen-arm-server-processor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Amazon's Arm-based Graviton2 Against AMD and Intel: Comparing Cloud Compute\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-03-10T12:30:00Z\n",
      "URL: https://www.anandtech.com/show/15578/cloud-clash-amazon-graviton2-arm-against-intel-and-amd\n",
      "Content: It’s been a year and a half since Amazon released their first-generation Graviton Arm-based processor core, publicly available in AWS EC2 as the so-called 'A1' instances. While the processor didn’t impress all too much in terms of its performance, it was a signal and first step of what’s to come over the next few years.This year, Amazon is doubling down on its silicon efforts, havingannounced the new Graviton2 processorlast December, and planning public availability on EC2 in the next few months. The latest generation implements Arm’s new Neoverse N1 CPU microarchitecture and mesh interconnect, a combined infrastructure orientedplatform that we had detailed a little over a year ago. The platform is a massive jump over previous Arm-based server attempts, and Amazon is aiming for nothing less than a leading competitive position.Amazon’s endeavours in designing a custom SoC for its cloud services started back in 2015, when the company acquired Isarel-based Annapurna Labs. Annapurna had previously worked on networking-focused Arm SoCs, mostly usedin products such as NAS devices. Under Amazon, the team had been tasked with creating a custom Arm server-grade chip, and the new Graviton2 is the first serious attempt at disrupting the space.So, what is the Graviton2? It’s a 64-core monolithic server chip design, using Arm’s new Neoverse N1 cores (Microarchitectural derivatives of themobile Cortex-A76 cores) as well as Arm’sCMN-600 mesh interconnect. It’s a pretty straightforward design that is essentially almost identical to Arm’s 64-core reference N1 platform that the company had presented back a year ago. Amazon did diverge a little bit, for example the Graviton2’s CPU cores are clocked in at a bit lower 2.5GHz as well as including only 32MB instead of 64MB of L3 cache into the mesh interconnect. The system is backed by 8-channel DDR-3200 memory controllers, and the SoC supports 64 PCIe4 lanes for I/O. It’s a relatively textbook design implementation of the N1 platform, manufactured on TSMC’s 7nm process node.The Graviton2’s potential is of course enabled by the new N1 cores. We’ve already seen the Cortex-A76perform fantasticallyin last year’s mobile SoCs, and the N1 microarchitecture is expected to bring even better performance and server-grade features, all whilst retaining the power efficiency that’s made Arm so successful in the mobile space. The N1 cores remain very lean and efficient, at a projected ~1.4mm² for a 1MB L2 cache implementation such as on the Graviton2, and sporting excellent power efficiency at around ~1W per core at the 2.5GHz frequency at which Amazon’s new chip arrives at.Total power consumption of the SoC is something that Amazon wasn’t too willing to disclose in the context of our article – the company is still holding some aspects of the design close to its chest even though we were able to test the new chipset in the cloud. Given the chip’s more conservative clock rate, Arm’s projected figure of around 105W for a 64-core 2.6GHz implementation, and Ampere’s recent disclosure of their 80-core 3GHz N1 server chip coming in at 210W, we estimate that the Graviton2 must come in around anywhere between 80W as a low estimate to around 110W for a pessimistic projection.Testing In The Cloud With EC2Given that Amazon’s Graviton2 is a vertically integrated product specifically designed for Amazon’s needs, it makes sense that we test the new chipset in its intended environment (Besides the fact that it’s not available in any other way!). For the last couple of weeks, we’ve had preview access for Amazon Web Services (AWS) Elastic Compute Cloud (EC2) new Graviton2 based “m6g” instances.For readers unfamiliar with cloud computing, essentially this means we’ve been deploying virtual machines in Amazon’s datacentres, a service for which Amazon has become famous for and which now represents a major share of the company’s revenues, powering some of the biggest internet services on the market.An important metric determining the capabilities of such instances is their type (essentially dictating what CPU architecture and microarchitecture powers the underlying hardware) and possible subtype; in Amazon’s case this refers to variations of platforms that are designed for specialised use-cases, such as having better compute capabilities or having higher memory capacity capabilities.For today’s testing we had access to the “m6g” instances which are designed for general purpose workloads. The “6” in the nomenclature designates Amazon’s 6thgeneration hardware in EC2, with the Graviton2 currently being the only platform holding this designation.Instance Throughput Is Defined in vCPUsBeyond the instance type, the most important other metric that defined an instance’s capabilities is its vCPU count. “Virtual CPUs” essentially means your logical CPU cores that’s available to the virtual machine. Amazon offers instances ranging from 1 vCPU to up to 128, with the most common across the most popular platforms coming in sizes of 2, 4, 8, 16, 32, 48, 64, and 96.The Graviton2 being a single-socket 64-core platform without SMT means that the maximum available vCPU instance size is 64.However, what this also means, is that we’re quite in a bit of an apples-and-oranges conundrum of a comparison when talking about platforms which do come with SMT. When talking about 64 vCPU instances (“16xlarge” in EC2 lingo), this means that for a Graviton2 instance we’re getting 64 physical cores, while for an AMD or Intel system, we’d be only getting 32 physical cores with SMT. I’m sure there will be readers who will be considering such a comparison “unfair”, however it’s also the positioning that Amazon is out to make in terms of delivered throughput, and most importantly, the equivalent pricing between the different instance types.Today’s CompetitionToday’s article will focus around two main competitors to the Graviton2: AMD EPYC 7571 (Zen1) powered m5a instances, and Intel Xeon Platinum 8259CL (Cascade Lake) powered m5n instances. At the moment of writing, these are the most powerful instances available from the two x86 incumbents, and should provide the most interesting comparison data.It’s to be noted that we would have loved to be able to include AMD EPYC2 Rome based (c5a/c5ad) instances in this comparison; Amazon had announced they had been working on such deploymentslast November, but alas the company wasn’t willing to share with us preview access (One reason given was the Rome C-type instances weren’t a good comparison to the Graviton2’s M-type instance, although this really doesn’t make any technical sense). As these instances are getting closer to preview availability, we’ll be working on a separate article to add that important piece of the puzzle of the competitive landscape.Tested 16xlarge EC2 Instancesm6gm5am5nCPU PlatformGraviton2EPYC 7571Xeon Platinum 8259CLvCPUs64Cores Per Socket643224(16 instantiated)SMT-2-way2-wayCPU Sockets112Frequencies2.5GHz2.5-2.9GHz2.9-3.2GHzArchitectureArm v8.2x86-64 + AVX2x86-64 + AVX512µarchitectureNeoverse N1ZenCascade LakeL1I Cache64KB64KB32KBL1D Cache64KB32KB32KBL2 Cache1MB512KB1MBL3 Cache32MB shared8MB sharedper 4-core CCX35.75MB sharedper socketMemory Channels8x DDR4-32008x DDR-2666(2x per NUMA-node)6x DDR4-2933per socketNUMA Nodes142DRAM256GBTDPEstimated80-110W?180W210Wper socketPrice$2.464 / hour$2.752 / hour$3.808 / hourComparing the Graviton2 m6g instances against the AMD m5a and Intel m5n instances, we’re seeing a few differences in the hardware capabilities that power the VMs. Again, the most notorious difference is the fact that the Graviton2 comes with physical core counts matching the deployed vCPU number, whilst the competition counts SMT logical cores as vCPUs as well.Other aspects when talking about higher-vCPU count instances is the fact that you can receive a VM that spans across several sockets. AMD’s m5a.16xlarge here is still able to deploy the VM on a single socket thanks to the EPYC 7571’s 32 cores, however Intel’s Xeon system here employs two sockets as currently there’s no deployed Intel hardware in EC2 which can offer the required vCPU count in a single socket.Both the EPYC 7571 and the Xeon Platinum 8259CL are parts which aren’t publicly available or even listed on either company’s SKU list, so these are custom parts for the likes of Amazon for datacentre deployments.The AMD part is a 32-core Zen1 based single-socket solution (at least for the 16xlarge instances in our testing) clocking in at 2.5 GHz all-cores to up to 2.9GHz in lightly threaded scenarios. The peculiarity of this system is that it’s somewhat limited by AMD’s quad-chip MCM system which has four NUMA nodes (one per chip and 2-channel memory controller), a characteristic that’s been eliminated inthe newer EPYC2 Zen2based systems. We don’t have concrete confirmation on the data, but we suspect this is a 180W part based on the SKU number.Intel’s Xeon Platinum 8259CL is based on thenewer Cascade Lake generation CPU cores. This particular part is also specific to Amazon, and consists of 24 enabled cores per socket. To reach the 16xlarge 64 vCPU count, EC2 provides us a dual-socket system with 16 out of the 24 cores instantiated on each socket. Again, we have no confirmation on the matter, but these parts should be rated at 210W per socket, or 420W total. We do have to remind ourselves that we’re only ever using 66% of the system’s cores in our instance, although we do have access to the full memory bandwidth and caches of the system.The cache configuration in particular is interesting here as things differ quite a bit between platforms. The private caches of the actual CPUs themselves are relatively self-explanatory, and the Graviton2 here does provide the highest capacity of cache out of the trio, but is otherwise equal to the Xeon platform. If we were to divide the available cache on a per-thread basis, the Graviton2 leads the set at 1.5MB, ahead of the EPYC’s 1.25MB and the Xeon’s 1.05MB. The Graviton2 and Xeon systems have the distinct advantage that their last level caches are shared across the whole socket, while AMD’s L3 is shared only amongst 4-core CCX modules.The NUMA discrepancies between the systems aren’t that important in parallel processing workloads with actual multiple processes, but it will have an impact on multi-threaded as well as single-threaded performance, and the Graviton2’s unified memory architecture will have an important advantage in a few scenarios.Finally, there’s quite a difference in the pricing between the instances. At $2.46 per hour, the Graviton2 system edges out the AMD system in price, and is massively cheaper than the $3.80 per hour cost of the Xeon based instance. Although when talking about pricing, we do have to remember that the actual value delivered will also wildly depend on the performance and throughput of the systems, which we’ll be covering in more detail later in the article.We thank Amazon for providing us with preview access to the m6g Graviton2 instances. Aside from giving us access, Amazon nor any other of the mentioned companies have had influence in our testing methodology, and we paid for our EC2 instance testing time ourselves.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15578/cloud-clash-amazon-graviton2-arm-against-intel-and-amd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Next Generation Arm Server: Ampere’s Altra 80-core N1 SoC for Hyperscalers against Rome and Xeon\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-03-03T16:30:00Z\n",
      "URL: https://www.anandtech.com/show/15575/amperes-altra-80-core-n1-soc-for-hyperscalers-against-rome-and-xeon\n",
      "Content: Several years ago, at a local event detailing a new Arm microarchitecture core, I recall a conversation I had with a number of executives at the time: the goal was to get Arm into 25% of servers by 2020. A lofty goal, which hasn’t quite been reached, however after the initial run of Arm-based server designs the market is starting to hit its stride with Arm’s N1 core for data-centers getting its first outings. Out of those announcing an N1-based SoC, Ampere is leading the pack with a new 80-core design aimed at cloud providers and hyperscalers. The new Altra family of products is aiming to offer competitive performance, performance per watt, and scalability up to 210 W and tons of IO, for any enterprise positioning.The Arm Server Market: 2010-2019 (Abridged)We’ve seen companies such as Broadcom/Cavium/Marvell,Calxeda,Huawei,Fujitsu,Phytium,Annapurna/Amazon,AppliedMicro/Ampere, and even AMD put Arm-based IP into silicon and subsequently into the server market. Up until recently, most designs have been fairly lackluster – with companies either developing their own core on an Arm architecture license and not getting a performance lift, or using the standard Arm cores and not finding the right mix of performance, power, and software uptake needed to drive home the design. As a result, we’ve seen multiple companies fall by the wayside, be acquired, or limit their activities to specific customers and keep very hush-hush.First Generation Ampere eMAG, Built on Applied Micro designsA big example of the ‘be acquired’ type of company was Annapurna, whom Amazon acquired and eventually released itsGraviton2 processorin recent months. This chip has 64 cores based on Arm’s N1 design, which is the leading microarchitecture layout for Arm server chips at this point. To that end, Ampere (who originally purchased Applied Micro) is now set to release its second generation product, with 80 of the N1-based cores, and it now has a name: Altra.Ampere AltraAmpere has already given a number of details away about Altra inan announcement late last year, however this time around we have concrete details and the company has performance projections. On the back of its first generation eMAG product, Ampere is looking to offer better-than-Graviton2 performance to any cloud provider or hyperscaler who isn’t called Amazon, given that Graviton2 is built by Amazon and only available to Amazon. In that regard, Ampere has taking Arm’s full recommendations for its N1 design, building a chip with the most number of cores that N1 is designed to support.As with other N1-based products, Altra will be single threaded, ensuring that each thread has its own core, its own resources, and removing any potential core-sharing thread security issues that have occurred recently. The Altra SoC is built with containers in mind, ensuring high-levels of quality of service with multiple customers on the same chip, and additional RAS features to ensure consistent performance.The N1 core isby design what we’ve coveredwhen Arm detailed the microarchitecture design last year. There is a 4-cycle 64 KB L1I/L1D caches per core, along with a 9-11 cycle 1 MB of private L2 per core. This is partnered with 32 MB of system wide LLC distributed through the SoC mesh, and all these caches are ECC with SECDED operation. It’s worth noting that 32 MB across 80 cores is less per core than Amazon’s Graviton2, which has 32 MB for 64 cores. 32 MB is actually half of what Arm recommends, as in Arm’s presentation it stated that it would expect a 64-core design to have 64 MB.On top of the 80 cores, the SoC will also have eight DDR4-3200 memory channels with ECC support, up to 4 TB per socket. There are also 128 PCIe 4.0 lanes, with which the CPU can use 32 of them to hook up to another CPU for dual socket operation. The dual socket system can then have a total of 192 PCIe 4.0 lanes between it, as well as support for up to 8 TB of memory. We are told that it’s actually the CCIX protocol that runs over these PCIe lanes, which means 25 GB/s per x16 linkup. That’s good for 50 GB/s in each direction.Each of the PCIe lanes can be bifurcated down to x8/x4/x2, and every different variant of the Altra SoC will only be segmented on core count and frequency: all CPUs will have 4 TB support and 128 lanes of PCIe 4.0. Each CPU can also support up to four CCIX-based accelerators.Altra is built on TSMC’s 7nm, and while is technically an Arm v8.2 design, it does borrow a couple of features from 8.3 and 8.5, namely hardware based mitigations for side channel attacks and a couple of other small micro-architectural features.Each of the 80 cores is designed to run at 3.0 GHz all-core, and Ampere was consistent in its messaging in that the top SKU is designed to run at 3.0 GHz at all times, even when both 128-bit SIMD units per core are being used (thus an unlimited turbo at 3.0 GHz). The CPU range will vary from 45W to 210W, and vary in core count - we suspect these SKUs will be derived from the single silicon design, and it will depend on demand as well as binning as to what comes out of the fabs. Exact SKUs are going to be announced later this year.Also on security, Ampere was keen to point out that its new SoC will have two control processors: an SM Pro and a PM Pro. These allow for server manageability, up to SBSA Level 4, as well as Secure Boot, RAS error reporting, and advanced power management/temperature control.Ampere will be launching with two reference designs for Altra, one in single socket called Mt. Snow, and one in dual socket called Mt. Jade. Each design will be available in 1U and 2U form factors, with PCIe 4.0 and CCIX attach, and up to 16 memory modules per socket. We know that the partner for the single socket is the GIGABYTE Server team, however the dual socket partner has not be announced yet. We have been told that the CPUs are socketed, which makes mass scale production and testing (at least on our side) a little easier.Projected PerformanceAmpere has some performance numbers, which as always we take with a grain of salt. These include 2.23x the performance on SPEC2017_int rate over a single 28-core Intel Xeon Platinum 8280, and 1.04x over a single 64-core AMD EPYC 7742. This is obviously extended into a number of claims about improved TCO. Ampere didn’t provide similar numbers for SPEC2017_fp, because the company states that the SoC has been developed with INT workloads in mind. Exact power/performance numbers were not given, but based purely on TDP, which is somewhat of an unreliable metric at times. We’ll wait to run our own numbers in due course.Developing a Roadmap: 2021, 2022One of the key questions going into our briefings with Ampere is how closely they are working with Arm on the next generation enterprise server core designs for upcoming SoCs. They weren’t keen to position themselves as Arm’s key partner in this venture (which might be Amazon, given they were first), but did state that there is a lot of collaboration and feedback that goes into the future designs. As a result, Ampere is able to formally declare a long-term roadmap for its product portfolio.In this instance, Ampere is stating that today it has the 80-core Altra design on 7nm. In 2021, it will launch its Mystique product, which is currently in development (and when asked, Ampere told us will share the same socket as Altra). In 2022, Ampere will launch Siryn, and at this time the product has been defined and requires development.Having a sustained product cadence has been critical to a number of processor designs in the last couple of decades – it tells potential ODM partners and customers that the company is in for the long haul, and committed to future developments with targets to meet. Obviously with Ampere tying itself to Arm’s roadmap helps in those product definition stages. It’s a feature that has crippled previous Arm designs from coming to market – without a clear roadmap, customers are unwilling to invest in a one-generation wonder and provide long term support for it. There’s always the issue as to whether any investment funding might run out, so Ampere’s goal here with Altra is to be the obvious answer to Graviton2 for the other hyperscalers. With that large market on offer, the goal is to be profitable and self-sustaining as quickly as possible, which then in turn gives potential customers even more confidence.Next Stage for AltraAt this point, Ampere has stated to us that Altra is currently sampling with its key customers who are looking to deploy the hardware. From previous experience, the key customers who are involved early tend to get priority for deployment, and in that respect Ampere has stated that an official SKU list will come to market mid-year, along with pricing, and with official SPEC submissions. Hopefully at that time we will also get instance pricing from the companies intending to deploy the new chip.We’re currently in talks with Ampere in order to obtain Altra for in-house testing when they feel it is ready. We have a version of Ampere’sprevious generation eMAG workstationthat just arrived in the labs, which should help us provide a good base-line from the previous design to the new one. Stay tuned for our coverage of eMAG and Altra!Related Reading80-Core N1 Next-Gen Ampere, ‘QuickSilver’: The Anti-Graviton2Arm Server CPUs: You Can Now Buy Ampere’s eMAG in a WorkstationAmpere Computing: Arm is Now an InvestorAmpere eMAG in the Cloud: 32 Arm Core Instance for $1/hrGallery:Ampere Altra\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15575/amperes-altra-80-core-n1-soc-for-hyperscalers-against-rome-and-xeon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Announces Cortex-M55 Core And Ethos-U55 microNPU\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-02-10T17:00:00Z\n",
      "URL: https://www.anandtech.com/show/15494/arm-announces-cortexm55-core-and-ethosu55-micronpu\n",
      "Content: Today Arm announces its newest addition to the Cortex-M series, the new Cortex M55. In addition to the new CPU microarchitecture which brings several new improvements, we also see the introduction of the new Ethos-U55 NPU IP that is meant to be integrated with the new M55 core. Arm’s new IP is meant to advance the machine learning and inferencing capabilities of billions of low-power embedded devices over the next several years, and expand its product portfolio for new use-cases.We’ve seen Machine Learning become quite the buzzword over the past several years, but today the ecosystem has evolved to the point that it’s no longer just a brand-new novelty, but rather quickly becoming a useful feature to the point that it’s being increasingly deployed in various systems and use-cases in the industry. Arm sees the endpoint AI market particularly an area where we’ll be seeing explosive growth over the coming years, and this is the area that Arm wants to cover with the new IP releases.The new Cortex-M55 is a new generation IP more closely related to the M33, but brings a few new architectural advances with it that promise some large performance and flexibility improvements when it comes to machine learning as well as vector instructions.The Ethos-U55 is a dedicated “microNPU” dedicated inference accelerator that ties in with a Cortex-M class CPU and offers the performance and power efficiency of a dedicated NPU, or MAC-engine would usually bring to the table – all in within the similar small footprint of the M-class IPs.Cortex-M55: First Helium and Custom Instruction capable CPU coreThe new Cortex-M55 is important as it’s the first Arm CPU core that is announced with both Helium as well as Custom Instructions capabilities. Helium, whose technical name is actually MVE (for M-Profile Vector Extension), is the new vector extensions and dedicated vector execution units in the M-class processor line-up, making it the first CPU in this range that is capable of SIMD instructions. The addition gives the new core up to a 5x increase in DSP performance, and the optimised instructions for ML workloads in combination with MVE adds up to a 15x performance improvement compared to previous generation M-cores.In terms of overall microarchitecture, it’s a successor to the M33 and combined µarch as well as frequency improvements will see scalar workloads increase performance by roughly 20%, depending on the vendor’s configuration. The core had been designed with a focus on bandwidth and enabling the new MVE and new ML workloads that require it, so improvements have been made to the memory subsystem, such as having 4x 32-bit interfaces to the TCM (Tightly Coupled Memory).The Ethos-U55: Arm’s first microNPUArm was relatively late to the NPU scene as most vendors had employed their own first-party IP architectures in products, and most vendors today use such implementations. The embedded market however is a bit different and there’s a need for something that is a lot lower area and lower power than what you’re generally used to in “larger” implementation such as in mobile SoCs, which are covered by Arm’s Ethos-N NPU IP.The new U55 is a small-scale NPU that scales from 32 to 256 MACs, and requires coupling with a Cortex-M class NPU. Arm didn’t go into major specifics of the microarchitecture, but it’s a very lean design that focuses on area and power efficiency, as well as having small memory footprints, including some features that we see in the N-series such as weight decompression. We’re saying the U55 needs to be coupled with an M-class CPU to serve as the controller, but actually this isn’t all too different to what the N-series does as that IP already includes an M-class CPU. When it comes to the architecture of the NPU, it’s said to be different and not related to its bigger brethren, and was designed specifically for low-power use-cases.In terms of area size, the smallest 32 MAC implementation of the U55 is said to be around 2x the size of an M55. We don’t have absolute figures to present here, but we’re essentially talking about fractions of a mm².The performance improvements in such systems that use the M55 and U55 represent very major step-function increases over past generation solutions. Figures that Arm provides include up to a 50x performance uplift in a comparison to a Cortex-M7 based system, all while improving energy efficiency by 25x.As to where the new IPs will be employed, is a very wide variety of embedded systems. It’s important to understand here that the major volume of such systems will be actually subsystems of current existing chips. If we were to take mobile as an example, you’d see subsystems using the IP inside the fingerprint sensors of a phone, the always-listening audio chip for voice assistant features, or even uses inside the RF systems that would optimise workloads such as antenna tuning. There are hundreds of M-class processors in today’s mobile devices that would benefit from ML capabilities, most of them completely transparent to the user.Arm has currently licensed the M55 and U55 to its lead partners, and will open up wider range licensing to other customers in the coming months. As usual with IP, you should expect products using the new designs in around 2 years – if vendors ever publicly confirm whether they use the designs in their products.Related Reading:Arm Announces New Ethos-N57 and N37 NPUs, Mali-G57 Valhall GPU and Mali-D37 DPUArm TechCon 2019 Keynote Live Blog (Starts at 10am PT/17:00 UTC)Cortex-M7 Launches: Embedded, IoT and WearablesARM Details \"Project Trillium\" Machine Learning Processor ArchitectureArm's New Cortex-A77 CPU Micro-architecture: Evolving Performance\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15494/arm-announces-cortexm55-core-and-ethosu55-micronpu\n",
      "Title: LG Reveals 31.5-Inch UltraFine Ergo 4K Monitor with Ergonomic Arm\n",
      "Author: Anton Shilov\n",
      "Date Published: 2019-12-20T18:00:00Z\n",
      "URL: https://www.anandtech.com/show/15252/lg-reveals-315inch-ultrafine-ergo-4k-monitor-with-ergonomic-arm\n",
      "Content: LG has announced its new 31.5-inch UltraFine Ergo 4K display, the largest in the UltraFine family to date. Living up to its name, the Ergo monitor has an innovative ergonomic arm that provides far more flexibility than any other stand that comes with LCDs, affording some new opportunities to free up space on the desktop.The new LG UltraFine Ergo display model 32UN880 uses a 31.5-inch IPS panel with a 3840×2160 resolution, offering a maximum brightness of 350 nits, a 1000:1 contrast ratio, a 60 Hz refresh rate, a 5 ms response time, and the usual 178°/178° horizontal/vertical viewing angles.Traditionally for LG’s UltraFine LCDs — which are developed primarily for professional customers seeking for accurate colors — the new monitors can display 1.07 billion of colors and cover 95% of the DCI-P3 color gamut. Unfortunately, we have no idea whether the devices support any other color spaces. Typically, LG’s UltraFine monitors only support DCI-P3, which makes them a great fit for Macs, but a suboptimal choice for Windows-based PCs.Meanwhile, what is a bit unusual about the LG UltraFine Ergo is that it supports AMD’s FreeSync variable refresh rate technology, a feature that's mostly used for gaming. HDR10 is also supported here, conferring a basic level of HDR support. Though given the peak luminance of the LCD, it is hard to expect the monitor to provide a meaningful HDR experience.The key selling point of the LG UltraFine Ergo display is of course its full motion arm. The base of the arm uses a C-clamp, which allows it to be attached to almost any working surface. The arm itself can adjust not only the height, tilt, or swivel of the monitor, but also its distance to the viewer. In fact the arm is fairly long overall, which is quite different from what we normally see with standard displays.With regards to connectivity,, the new LG UltraFine Ergo is different than other monitors in the family. The upcoming unit does not have a Thunderbolt 3 port, but instead sports one DisplayPort input, two HDMI ports, and one USB Type-C input. It also comes with a dual-port USB hub, though not built-in speakers or a headphone jack.LG's 2019 UltraFine DisplaysLG UltraFine 4KLG UltraFine 5KLG UltraFine ErgoPanel23.7\" IPS27\" IPS31.5\" IPSNative Resolution3840 x 21605120 x 28803840 x 2160Refresh Rate60 HzVariable Refresh Rate-AMD FreeSyncBrightness500 cd/m²350 cd/m²Color GamutDisplay P3DCI-P3Color Depth8 bit (?)10 bit (?)?HDR-HDR10Response Time??5 msViewing Angles178°/178° horizontal/verticalInputsThunderbolt 3 or USB-C1 × DisplayPort2 × HDMI1 × USB-3USB Hub3 × 5Gbps USB-C2 × USBAudioStereo speakersStereo speakersMicrophone-Webcam-Integrated-StandAdjustable standAdjustable arm with C-clampExtend/RetractTiltSwivelPivotPower Delivery85 W94 W?Price$699.95$1,299.95?LG’s 31.5-inch UltraFine Ergo monitor will be available sometimes in 2020, but its price remains to be seen.Related Reading:LG Unveils New UltraFine 4K & 5K Monitors: Now with iPad Pro SupportLG Introduces New UltraFine 4K and 5K MonitorsLG Unveils 43UN700 Monitor: 42.5-Inch 4K w/ HDR10 for Work & GamingNeed for Speed: The LG UltraGear (27GN750) 240 Hz IPS Monitor with G-SyncSource:LG\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15252/lg-reveals-315inch-ultrafine-ergo-4k-monitor-with-ergonomic-arm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NVIDIA Details DRIVE AGX Orin: A Herculean Arm Automotive SoC For 2022\n",
      "Author: Ryan Smith\n",
      "Date Published: 2019-12-18T13:30:00Z\n",
      "URL: https://www.anandtech.com/show/15245/nvidia-details-drive-agx-orin-a-herculean-arm-automotive-soc-for-2022\n",
      "Content: While NVIDIA’s SoC efforts haven’t gone entirely to plan since the company first started on them over a decade ago, NVIDIA has been able to find a niche that works in the automotive field. Backing the company’s powerful DRIVE hardware, these SoCs have become increasingly specialized as the DRIVE platform itself evolves to meet the needs of the slowly maturing market for the brains behind self-driving cars. And now, NVIDIA’s family of automotive SoCs is growing once again, with the formal unveiling of the Orin SoC.First outlined as part of NVIDIA’s DRIVE roadmapat GTC 2018, NVIDIA CEO Jensen Huang took the stage at GTC China this morning to properly introduce the chip that will be powering the next generation of the DRIVE platform. Officially dubbed the NVIDIA DRIVE AGX Orin, the new chip will eventually succeed NVIDIA’s currently shipping Xavier SoC, which has been available for about the last year now. In fact, as has been the case with previous NVIDIA DRIVE unveils, NVIDIA is announcing the chip well in advance: the company isn't expecting the chip to be fully ready for automakers until 2022.What lies beneath Orin then is a lot of hardware, with NVIDIA going into some high-level details on certain parts, but skimming over others. Overall, Orin is a 17 billion transistor chip, almost double the transistor count of Xavier and continuing the trend of very large, very powerful automotive SoCs. NVIDIA is not disclosing the manufacturing process being used at this time, but given their timeframe, some sort of 7nm or 5nm process (or derivative) is pretty much a given. And NVIDIA will definitely need a smaller manufacturing process – to put things in comparison, the company’s top-end Turing GPU, TU102, takes up 754mm2 for 18.6B transistors, so Orin will pack in almost as many transistors as one of NVIDIA’s best GPUs today.NVIDIA ARM SoC Specification ComparisonOrinXavierParkerCPU Cores12x Arm \"Hercules\"8x NVIDIA Custom ARM \"Carmel\"2x NVIDIA Denver +4x Arm Cortex-A57GPU Cores\"Next-Generation\" NVIDIA iGPUXavier Volta iGPU(512 CUDA Cores)Parker Pascal iGPU(256 CUDA Cores)INT8 DL TOPS200 TOPS30 TOPSN/AFP32 TFLOPS?1.3 TFLOPs0.7 TFLOPsManufacturing Process7nm?TSMC 12nm FFNTSMC 16nm FinFETTDP~65-70W?30W15WThose transistors, in turn, will be driving several elements. Surprisingly, for today’s announcement NVIDIA has confirmed what CPU core they’ll be using. And even more surprisingly, it isn’t theirs. After flirting with both Arm and NVIDIA-designed CPU cores for several years now, NVIDIA has seemingly settled down with Arm. Orin will include a dozen of Arm’s upcomingHerculesCPU cores, which are from Arm’s client device line of CPU cores. Hercules, in turn, succeeds today’s Cortex-A77 CPU cores, with customers recently receiving the first IP for the core. For the moment we have very little information on Hercules itself, but Arm has previously disclosed that it will be a further refinement of the A76/A77 cores.I won’t spend too much time dwelling on NVIDIA’s decision to go with Arm’s Cortex-A cores after using their own CPU cores for their last couple of SoCs, but it’s consistent with the direction we’ve seen most of Arm’s other high-end customers take. Developing a fast, high-performance CPU core only gets harder and harder every generation. And with Arm taking a serious stab at the subject, there’s a lot of sense in backing Arm’s efforts by licensing their cores as opposed to investing even more money in further improving NVIDIA’s Project Denver-based designs. It does remove one area where NVIDIA could make a unique offering, but on the flip side it does mean they can focus more on their GPU and accelerator efforts.Speaking of GPUs, Jensen revealed very little about the GPU technology that Orin will integrate. Besides confirming that it’s a “next generation” architecture that offers all of the CUDA core and tensor functionality that NVIDIA has become known for, nothing else was stated. This isn’t wholly surprising since NVIDIA hasn’t disclosed anything about their forthcoming GPU architectures – we haven’t seen a roadmap there in a while – but it means the GPU side is a bit of a blank slate. Given the large gap between now and Orin’s launch, it’s not even clear if the architecture will be NVIDIA’s next immediate GPU architecture or the one after that, however given how Xavier’s development went and the extensive validation required for automotive, NVIDIA’s 2020(ish) GPU architecture seems like a safe bet.Meanwhile NVIDIA’s Deep Learning Accelerator (DLA) blocks will also be making a return. These blocks don’t get too much attention since they’re unique to NVIDIA’s DRIVE SoCs, but these are hardware blocks to further offload neural network inference, above and beyond what NVIDIA’s tensor cores already do. On the programmable/fixed-function scale they’re closer to the latter, with the task-specific hardware being a good fit for the power and energy-efficiency needs NVIDIA is shooting for.All told, NVIDIA expects Orin to deliver 7x the 30 INT8 TOPS performance of Xavier, with the combination of the GPU and DLA pushing 200 TOPS. It goes without saying that NVIDIA is still heavily invested in neural networks as the solution to self-driving systems, so they are similarly heavily investing in hardware to execute those neural nets.Rounding out the Orin package, NVIDIA’s announcement also confirms that the chip will offer plenty of hardware for supporting features. The chip will offer 4x 10 Gigabit Ethernet hosts for sensors and in-vehicle communication, and while the company hasn’t disclosed how many camera inputs the SoC can field, it will offer 4Kp60 video stream encoding and 8Kp30 decoding for H.264/HEVC/VP9. The company has also set a goal for 200GB/sec of memory bandwidth. Given the timeframe for Orin and what NVIDIA does for Xavier today, an 256-bit memory bus with LPDDR5 support sounds like a shoe-in, but of course this remains to be confirmed.Finally, while NVIDIA hasn’t disclosed any official figures for power consumption, it’s clear that overall power usage is going up relative to Xavier. While Orin is expected to be 7x faster than Xavier, NVIDIA is only claiming it’s 3x as power efficient. Assuming NVIDIA is basing all of this on INT8 TOPS as they usually do, then the 1 TOPS/Watt Xavier would be replaced by the 3 TOPS/Watt Orin, putting the 200 TOPS chip at around 65-70 Watts. Which is admittedly still fairly low for a single chip at a company that sells 400 Watt GPUs, but it could add up if NVIDIA builds another multi-processor board like theDRIVE Pegasus.Overall, NVIDIA certainly has some lofty expectations for Orin. Like Xavier before it, NVIDIA intends for various forms of Orin to power everything from level 2 autonomous cars right up to full self-driving level 5 systems. And, of course, it will do so while being able to provide the necessary ASIL-D level system integrity that will be expected for self-driving cars.But as always, NVIDIA is far from the only silicon vendor with such lofty goals. The company will be competing with a number of other companies all providing their own silicon for self-driving cars – ranging from start-ups to the likes of Intel – and while Orin will be a big step forward in single-chip performance for the company, it’s still very much the early days for the market as a whole. So NVIDIA has their work cut out for them across hardware, software, and customer relations.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15245/nvidia-details-drive-agx-orin-a-herculean-arm-automotive-soc-for-2022\n",
      "Title: Arm Server CPUs: You Can Now Buy Ampere’s eMAG in a Workstation\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-12-06T17:00:00Z\n",
      "URL: https://www.anandtech.com/show/15165/arm-server-cpus-you-can-now-buy-amperes-emag-in-a-workstation\n",
      "Content: One of the critical elements to all these new server-class Arm processors is availability. We are not yet at the point where these chips are freely sold on the open market: anyone who wants to use them needs to buy a server (or a rack of servers), or rent a cloud instance. One of the benefits of x86 in this space is that users can write code for x86 servers on other easily accessible hardware, then port it up to the big server iron. Well now it seems that one of the Arm licencees playing in the server space has a workstation based product in the hands of distributors ready for software developers to cut their teeth on the hardware.Over at Avantek, the Ampere eMAG 64-bit Arm Workstation is a single socket workstation design offered in an XL-ATX chassis with up to 512 GB of DDR4-2666 as well as an NVMe drive and some SATA ports. There are onboard video outputs from the IPMI interface, or a PCIe 3.0 x8 expansion slot could add in something else (assuming drivers are available).The workstation is only offered with a single CPU SKU, the eMAG 8180. This isn’t to be confused with Intel’s 8180: this one has more cores! The eMAG 8180 is a 32-core design running at 2.8 GHz with a turbo up to 3.0 GHz, with a TDP of 125 W. This is a first generation eMAG, which uses the old AppliedMicro Skylark microarchitecture, a custom design of Arm v8 with 32 MB of L3, 42 PCIe lanes, and eight memory channels.Official eMAG 8180 specifications - note the frequency here is higher.It looks like the workstation has decreased clocksAvantek offers the system with three optional graphics cards: AMD FirePro W2100, a Radeon Pro WX 5100, and the NVIDIA Quadro GV100. OS options are variants of Linux: Ubuntu, CentOS, SUSE SLES, and openSUSE.The base configuration requires the user to select at least a 240 GB SSD and 1x8GB of DRAM, which means a super low (!) price of $2,794.50 for the base model. Users who want a chassis with a window and LED lighting will need to shell out an extra $108, because it isn’t a proper workstation with LEDs, right?! A more sensible configuration with the W2100, 64 GB of DRAM, and 4x256GB of SSDs, comes to $4044.60. Validated purchasers can leave a review – so far none have been left. Perhaps we should ask for one for review.Source:AvantekRelated ReadingAmpere Computing: Arm is Now an InvestorAmpere eMAG in the Cloud: 32 Arm Core Instance for $1/hrArm Announces Neoverse N1 & E1 Platforms & CPUs: Enabling A Huge Jump In Infrastructure Performance\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15165/arm-server-cpus-you-can-now-buy-amperes-emag-in-a-workstation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Success on Arm for HPC: We Found a Fujitsu A64FX Wafer\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-12-05T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15169/a-success-on-arm-for-hpc-we-found-a-fujitsu-a64fx-wafer\n",
      "Content: When speaking about Arm in the enterprise space, the main angle for discussion is on the CPU side. Having a high-performance SoC at the heart of the server has been a key goal for many years, and we have had players such as Amazon, Ampere, Marvell, Qualcomm, Huawei, and others play for the server market. The other angle to attack is for co-processors and accelerators. Here we have one main participant: Fujitsu. We covered the A64FX when the design was disclosed at Hot Chips last year, with its super high cache bandwidth, and it will be available on a simple PCIe card. The main end-point for a lot of these cards will be the Fugaku / Post-K supercomputer in Japan, where we expect it to hit a one of the top numbers on the TOP500 supercomputer list next year.After the design disclosure last year at Hot Chips, at Supercomputing 2018 we saw an individual chip on display. This year at Supercomputing 2019, we found a wafer.I just wanted to post some photos. Enjoy.The A64FX is the main recipient of the Arm Scalable Vector Extensions, new to Arm v8.2, which in this instance gives 48 computing cores with a 512-bit wide SIMD powered by 32 GiB of HBM2. Inside the chip is a custom network, and externally the chip is connected via a Tofu interconnect (6D/Torus), and the chip provides 2.7 TFLOPs of DGEMM performance. The chip itself is built on TSMC 7nm and has 8.786 billion transistors, but only 594 pins. Peak memory bandwidth is 1 TB/s.The chip is built for both high performance, high throughput, and high performance per watt, supporting FP64 through to INT8. The L1 data cache is designed for sustained throughput, and power management is tightly controlled on chip. Either way you slice it, this chip is mightily impressive. We even saw HPE deploy two of these chips in a single half-width node.Related ReadingHot Chips 2018: Fujitsu's A64FX Arm Core Live BlogARM Research Summit 2016 Keynote Live Blog\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15169/a-success-on-arm-for-hpc-we-found-a-fujitsu-a64fx-wafer\n",
      "Title: Amazon Announces Graviton2 SoC Along With New AWS Instances: 64-Core Arm With Large Performance Uplifts\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2019-12-03T17:30:00Z\n",
      "URL: https://www.anandtech.com/show/15189/amazon-announces-graviton2-soc-along-with-new-aws-instances-64core-arm-with-large-performance-uplifts\n",
      "Content: We only recently reported on the story that Amazon are designing a custom server SoC based on Arm’s Neoverse N1 CPU platforms, only for Amazon to now officially announce the new Graviton2 processor as well as AWS instances based on the new hardware.AWS Re:Invent Event TwitterThe new Graviton2 SoC is a custom design by Amazon’s own in-house silicon design teams and is a successor to the first-generation Graviton chip. The new chip quadruples the core count from 16 cores to 64 cores and employs Arm’s newest Neoverse N1 cores. Amazon is using the highest performance configuration available, with 1MB L2 caches per core, with all 64 cores connected by a mesh fabric supporting 2TB/s aggregate bandwidth as well as integrating 32MB of L3 cache.Amazon claims the new Graviton2 chip is can deliver up to 7x higher performance than the first generation based A1 instances in total across all cores, up to 2x the performance per core, and delivers memory access speed of up to 5x compared to its predecessor. The chip comes in at a massive 30B transistors on a 7nm manufacturing node - if Amazon is using similar high density libraries to mobile chips (they have no reason to use HPC libraries), then I estimate the chip to fall around 300-350mm² if I was forced to put out a figure.The memory subsystem of the new chip is supported by 8 DDR4-3200 channels with support for hardware AES256 memory encryption. Peripherals of the system are supported by 64 PCIe4 lanes.Powered by the new generation processor, Amazon also detailed its new 6thgeneration instances M6g, R6g and C6g, offering various configuration up to the full 64 cores of the chip and up to 512GB of RAM for the memory optimised instance variants. 25Gbps “enhanced networking” connectivity, as well as 18Gbps bandwidth to EBS (Elastic Block Storage).Amazon is also making some very impressive benchmark comparisons against its fifth-generation instances, supporting Intel Xeon Platinum 8175 processor of up to 2.5GHz:All of these performance enhancements come together to give these new instances a significant performance benefit over the 5th generation (M5, C5, R5) of EC2 instances. Our initial benchmarks show the following per-vCPU performance improvements over the M5 instances:​SPECjvm®2008: +43% (estimated)SPEC CPU®2017 integer: +44% (estimated)SPEC CPU 2017 floating point: +24% (estimated)HTTPS load balancing with Nginx: +24%Memcached: +43% performance, at lower latencyX.264 video encoding: +26%EDA simulation with Cadence Xcellium: +54%Amazon is making M6g instances with the new Graviton2 processor available for CPU for non-production workloads, with expected wider rollout in 2020.The announcement is a big win for Amazon and especially for Arm’s endeavours in the server space as they try to surpass the value that the x86 incumbents are able to offer. Amazon describes that the new 6g instances are able to offer 40% higher performance/$ than the existing x86 5thgeneration platforms, which represents some drastic cost savings for the company and its customers.Related Reading:AWS Designing a 32-Core Arm Neoverse N1 CPU for Cloud ServersArm Announces Neoverse N1 & E1 Platforms & CPUs: Enabling A Huge Jump In Infrastructure PerformanceGIGABYTE's Cavium ThunderX2 Systems: 1U R181-T90 and 2U R281-T91Assessing Cavium's ThunderX2: The Arm Server Dream Realized At LastGIGABYTE's ThunderXStation with Dual Cavium ThunderX2 Arm SoCsInvestigating Cavium's ThunderX: The First Arm Server SoC With AmbitionMarvell Completes Acquisition of Cavium, Gets CPU, Networking & Security AssetsAmazon AWS Offers Another AMD EPYC-Powered Instance: T3aAmazon Offers More EPYC: M5ad & R5ad Instances\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15189/amazon-announces-graviton2-soc-along-with-new-aws-instances-64core-arm-with-large-performance-uplifts\n",
      "Title: AWS Designing a 32-Core Arm Neoverse N1 CPU for Cloud Servers\n",
      "Author: Anton Shilov\n",
      "Date Published: 2019-12-02T18:00:00Z\n",
      "URL: https://www.anandtech.com/show/15181/aws-designs-32core-arm-cpu-for-cloud-servers\n",
      "Content: Amazon Web Services’s CPU design unit is working on a new multi-core processor for AWS servers. The new CPU is said to use Arm’s new Neoverse N1 architecture and would feature a considerably higher core-count when compared to AWS’s first-generation Graviton processor, which should result in a significant performance increase.The yet-to-be-named AWS CPU will be based on Arm’sNeoverse N1microarchitecture and will integrate as many as 32 cores, according to Reuters, which cites two sources with knowledge of the matter. The chip will also be able to connect to various special-purpose accelerators using a ‘fabric’ interface to greatly speed up certain workloads.On a high level, the Neoverse N1 (aka Ares) to a large degree resembles Arm’s consumer-oriented Cortex-A76 microarchitecture: a 4-wide fetch/decode machine with a pipeline depth of only 11 stages that can reduce itself to 9 when needed. Meanwhile, the Neoverse N1 is designed to run at relatively high frequencies to provide maximum single-thread performance, it has a different cache architecture (coherent, with 1 MB L2 option, yet caches are technically not a part of the microarchitecture per se), and some other enhancements. Overall, with the Neoverse N1 Arm is looking at clocks of up to 3.1 GHz and a ~100 W TDP per SoC.Readers who are interested to find out more about Arm’s Neoverse N1 platform can readour coveragefrom earlier this year, but the key thing in the context of the newly released information is that AWS continues to believe in custom Arm-based processors for servers and would be among the first adopters of the Neoverse N1. As noted above, the microarchitecture and the platform were optimized for cloud server workloads from the ground up, so with with further customization from Amazon, the 32-core processor promises to offer rather serious performance in applications that it was designed for. Will these CPUs challenge AMD’s Rome or Intel’s Cascade Lake? Probably not, but the importance of custom chips is their ability to offer the right total cost of ownership and sufficient performance, not win all the benchmarks.Related Reading:Arm Announces Neoverse N1 & E1 Platforms & CPUs: Enabling A Huge Jump In Infrastructure PerformanceGIGABYTE's Cavium ThunderX2 Systems: 1U R181-T90 and 2U R281-T91Assessing Cavium's ThunderX2: The Arm Server Dream Realized At LastGIGABYTE's ThunderXStation with Dual Cavium ThunderX2 Arm SoCsInvestigating Cavium's ThunderX: The First Arm Server SoC With AmbitionMarvell Completes Acquisition of Cavium, Gets CPU, Networking & Security AssetsAmazon AWS Offers Another AMD EPYC-Powered Instance: T3aAmazon Offers More EPYC: M5ad & R5ad InstancesSource:Reuters\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15181/aws-designs-32core-arm-cpu-for-cloud-servers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Announces New Ethos-N57 and N37 NPUs, Mali-G57 Valhall GPU and Mali-D37 DPU\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2019-10-23T04:00:00Z\n",
      "URL: https://www.anandtech.com/show/15015/arm-announces-new-ethosn57-and-n37-npus-new-malig57-valhall-gpu-and-malid37-dpu\n",
      "Content: Today Arm is announcing four new products in its NPU, GPU and DPU portfolio. The company is branding its in-house machine learning processor IPs the Ethos line-up detailing more the existing N77 piece and also revealing the smaller N57 and N37 siblings in the family. To top things off, the company is also making ready its first mid-range GPU IP based on the brand-new Valhall architecture, the new Mali-G57. Finally, we’re seeing the release of a new mid-range DPU in the form of the Mali-D37.Introducing the Ethos NPU FamilyArm’s NPU IP offering was firstannounced early last year,detailing its architecture a few months later, and has been publicly been known until known just as “the Arm Machine Learning processor”. Arm at TechCon this year has officially branded the IP as the Ethos line-up, and the N77 has been the main product that’s been previously referred to as the Arm MLP codename.Microarchitecturally, the new branded Ethos-N77 now publicly changes its specs compared to what had been revealed last year by allowing for a configurable 1 to 4MB SRAM implementation, whilst last year it had been disclosed it would scale up to 1MB only. Arm explains that customers needed more memory bandwidth for processing these mesh networked NPUs, as DRAM bandwidth doesn’t scale up in the premium segment as fast as the core count does. The flagship IP offers up to 4TOPS processing power at 1GHz clock and has a respectable 5TOPS/W efficiency.Arm is able to use the same building blocks across the different IPs. The NPUs all share the same MAC computation engine (MCE) and programmable layer engines (PLE). The MCE consists out of 128 MAC units,as disclosed last year, and is paired alongside a PLE. An MCE and PLE, plus SRAM, make up a computation engine (CE), and this is the scaling block that differs between the N77, N57 and N37, coming in 16x, 8x and 4x configurations in terms of the CE count.The mid-range and low-end variants are being released a lot faster than other new IP technologies because Arm is seeing a lot more interest in doing ML in cost-constrained devices where every mm² of silicon is important. Particularly features like smartphone face unlocking or DTV resolution upscaling are becoming commodity features.The new NPUs have already been licensed and delivered to customers.Revealing the Mali-G57 - First Mid-range Valhall Based GPUEarlier this year, Arm had announcedthe new Valhall architecture in the new Mali-G77that we’re expecting to see in SoCs next year. The new GPU architecture is a more major departure from the Bifrost based GPUs we’ve seen over the last three years as Arm has completely revamped its graphics ISA and computation microarchitecture.Today, Arm reveals that the company is adopting the new Valhall architecture in the mid-range, starting off with the new Mali-G57. We currently don’t have too many details on exactly what the finer microarchitecture configurations of the new GPU looks like, but we’re very likely looking at something that will be very similar to the G77, scaled down similar to how the G52 looked like compared to the G72.Improvements compared to a G52 with three execution engines per core (3EE) promise 1.3x better performance in a similar core configuration, 30% better energy efficiency, and 30% better silicon density (due to the better performance).Mali-D37 DPU - Bringing High-End Features To the Mid-RangeFinally, to wrap things up, Arm is now bringing to market a new mid-range DPU in the form of the Mali-D37.The new IP is based on the “Komeda” architecture which wasfirst introduced in the Mali-D71and its follow-up,the Mali-D77 announced this year. Then new DPU targets resolutions of 2K and FHD and promises to take up only <1mm² on 16nm.Related ReadingARM Details \"Project Trillium\" Machine Learning Processor ArchitectureImagination Goes Further Down the AI Rabbit Hole, Unveils PowerVR Series3NX Neural Network AcceleratorCEVA Announces NeuPro-S Second-Generation NN IPCadence Announces Tensilica Vision Q7 DSArm's New Mali-G77 & Valhall GPU Architecture: A Major LeapArm Announces Mali D77 Display Processor: Facilitating AR & VRArm Announces New Mali-D71 Display Processor and IP Blocks\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15015/arm-announces-new-ethosn57-and-n37-npus-new-malig57-valhall-gpu-and-malid37-dpu\n",
      "Title: New Tools & IP Accelerate Development of 5nm Arm ‘Hercules’ SoCs\n",
      "Author: Anton Shilov\n",
      "Date Published: 2019-10-10T19:30:00Z\n",
      "URL: https://www.anandtech.com/show/14967/new-tools-accelerate-development-of-5nm-arm-hercules-socs\n",
      "Content: Arm, Synopsys, and Samsung Foundry have developed a set of optimized tools and IP that will enable chip designers to build next-generation SoCs based on Arm’s Hercules processor cores on Samsung’s 5LPE (5 nm, Low Power Early) node faster. The three companies expect the tools and IP to be used by designers of SoCs for a wide variety of applications.The set of Synopsys tools are certified by Samsung Foundry for its 5LPE manufacturing technology, and now includes the Fusion Design Platform as well as QuickStart Implementation Kit that are enabled to optimize power, performance, and area for 5LPE designs. Meanwhile, Arm will provide Artisan Physical IP and POP IP tailored for Samsung’s 5LPE process. The IP packages will enable Arm’s partners to quickly develop 5LPE-optimized SoCs based on the Arm Hercules general-purpose CPU cores.Samsung Foundry’s 5LPE fabrication process is the company’s 3rdGeneration refinement of its 7LPP node that uses more EUV layers and features other improvements. The new technology provides an up to 25% higher ‘logic efficiency’, it also allows chip designers to lower power consumption of their SoCs by 20% (at the same performance and complexity) or improve their performance by 10% (at the same power and complexity). While developers can reuse 7LPP IP on SoCs build for 5LPE while taking advantage of the benefits the latter provides, to extract the maximum value of the new technology, optimized tools and IP are needed.Considering that Arm’s Hercules are the company’s next-generation advanced CPU cores and 5LPE is a leading-edge process technology, Samsung expects the new tools and IP to be used for SoCs aimed at HPC, automotive, 5G, and AI applications.Samsung expected to tape out the first 5LPE chips in the second half of 2019 and plans to start volume production using the node in the first half of 2020.An official statement of Jaehong Park, executive vice president of Foundry Design Platform Development at Samsung Electronics, reads as follows.\"Synopsys' Fusion Design Platform and QuickStart Implementation Kits provide a design-ready solution for next-generation Arm-based processors. This is a great example of how Samsung 5LPE technology can be utilized to give designers a competitive advantage in their high-performance CPU designs. Through our close partnership with Arm and Synopsys, customers will now be able to extract maximum value out of our 5-nanometer processes for design applications targeted at high-performance and low-power markets.\"Related ReadingSamsung’s 5nm EUV Technology Gets Closer: Tools by Cadence & Synopsys CertifiedSamsung’s Aggressive EUV Plans: 6nm Production in H2, 5nm & 4nm On TrackSynopsys to Accelerate Samsung’s 7nm Ramp with Yield Explorer PlatformTSMC’s 5nm EUV Making Progress: PDK, DRM, EDA Tools, 3rd Party IP ReadySource:Synopsys\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/14967/new-tools-accelerate-development-of-5nm-arm-hercules-socs\n",
      "Title: Arm TechCon 2019 Keynote Live Blog (Starts at 10am PT/17:00 UTC)\n",
      "Author: Ryan Smith\n",
      "Date Published: 2019-10-08T16:45:00Z\n",
      "URL: https://www.anandtech.com/show/14959/arm-techcon-2019-keynote-live-blog\n",
      "Content: 01:00PM EDT- Kicking off today is Arm's annual technical conference, Arm TechCon. Now in its 15th year, the company is looking to continue their long, successful run in the IP market, with Arm designs powering everything from toasters to servers. To do so, the company not only needs to keep current customers happy, a bigger challenge in the face of the success of ISA and IP rival RISC-V, but also to continue to grow Arm into new & expanding markets.01:00PM EDT- For today's keynote, we've been told to expect specific announcements and discussions on IoT, autonomous vehicles, and what Arm is calling \"Total Compute\". So it sounds like Arm has an interesting show ahead.01:01PM EDT- And here we go01:01PM EDT- The bass is thumping. The room is shaking (or we're having an earthquake)01:02PM EDT- Now on stage, Simon Segears, Arm's CEO01:03PM EDT- Simon is quickly recapping the history of TechCon, and the history of Arm01:03PM EDT- Smart phones happened, and the Arm landscape changed forever01:04PM EDT- Arm is now on their fifth wave of computing01:05PM EDT- Arm is now up to 150 billion chips shipped based on their IP01:07PM EDT- One of Arm's current focuses is IoT. Arm wants to help its cusomters combine the physical and digital worlds by developing IoT devices to collect all sorts of data about the physical world01:07PM EDT- And once you have that data, you probably want some AI to analyze it01:08PM EDT- AI in the cloud, AI at the edge, AI in endpoint devices01:10PM EDT- One example of IoT: a sensor to identify a blocked pipe01:11PM EDT- Arm wants to move the data processing from the cloud to the local device. Slinging that data around is relatively expensive01:12PM EDT- And after that: ioT devices running on 5G, skipping the need for a local network01:14PM EDT- And with all of those devices in the world, Arm doesn't want security ignored either. Those devices must be secure - and Arm is developing technologies to do just that01:15PM EDT- \"There is no end to the ingenuity of bad people\"01:16PM EDT- One such technology being investigated by Arm is an architecture to isolate breaches01:17PM EDT- Now shifting to discussing business matters. Simon is recapping Arm's flexible access program, which they announced this summer01:18PM EDT- Flexible access allows extensive access to Arm's IP for a minimal upfront fee, and then sign a manufacturing license agreement once a company is ready to go into production01:19PM EDT- Flexible access has been 75% of Arm architecture licenses signed in the last two years01:19PM EDT- (CS grads represent! Today is Ada Lovelace day)01:20PM EDT- Arm is launching a new program today to allow clients to add custom instructions to their Arm architecture chips01:21PM EDT- This is for Armv8-M processors (so highly embedded parts)01:22PM EDT- This program is focused on adding a handful of instructions; new instructions that would greatly benefit the sopecific workload a chip has been tasked with01:23PM EDT- Think encryption and the like01:23PM EDT- The first chip to get custom instruction features will be M33, which is coming in 202001:24PM EDT- And all future M-series CPU cores after that01:24PM EDT- Custom instructions will be free to all customers01:25PM EDT- I'll dig into this more later, but this sounds like a reasonable extension, but also a response to RISC-V, which directly competes with Cortex-M01:25PM EDT- And that's Simon. Now on stage, SVP of Automotive and IoT, Dipti Vachani01:26PM EDT- \"What are the missing links taking us from prototypes to production?\"01:27PM EDT- Dipti (and Arm) want to get self-driving vehicles into the real world01:27PM EDT- To get there, there needs to be proven safety. Standards and certification for safety01:29PM EDT- As well as safety is proven software and compute infrastructure01:30PM EDT- Arm offers a series of Automotive Enhanced (AE) processors, which are designed with a safety-first mantra01:30PM EDT- Arm and its customers need to do security right. But they are also in a race to quickly get to market01:32PM EDT- In order to get there on the software side of matters, Arm has been working with both commercial and open source vendors to support their software01:33PM EDT- Arm has also been partnering with vendors like Swift Navigation, to develop a centimeter-accurate GNSS software stack01:34PM EDT- Now for compute. \"\"How do we solve the compute problems\" for automotive computing01:35PM EDT- CPUs, GPUs, ISPs, and machine learning processors01:35PM EDT- \"Heterogeneous compute is critical and necessary\" for autonomous compute01:36PM EDT- And this heterogeneous compute stack needs to appropriately scale in performance01:37PM EDT- Multi-core NPUs, meshes of NPUs, etc01:37PM EDT- \"The fourth missing link: collaboration\"01:37PM EDT- Arm has founded a new consortium, the Autonomous Vehicle Computing Consortium (AVCC)01:38PM EDT- Members include chip makers like NVIDIA as well as automotive companies like General Motors01:39PM EDT- Arm wants to bring together some of the \"greatest minds\" in the industry to help solve the problems in bringing autonomous driving to the market01:40PM EDT- In recap: safety, software, compute, and collaberation01:40PM EDT- Now on stage: Ian Smythe, VP of marketing for client products01:41PM EDT- \"The future for all mobile will move to digital immersion\"01:42PM EDT- \"Digital immersion will engage all of our senses\"01:43PM EDT- And there are already multi-billion dollar industries built around digital immersion01:44PM EDT- One challenge Arm wants to solve is getting video production and green screening entirely into a mobile device01:46PM EDT- Total Compute: making cores besides CPU cores easily accessible by developers01:48PM EDT- Arm will add a new instruction after Hercules, in Matterhorn, called MatMul01:48PM EDT- MatMuil will double CPU GEMM performance01:49PM EDT- Step 2 for digital immersion: security01:50PM EDT- Mobile devices hold immense personal data. Therefore they need to be trustworthy01:50PM EDT- Security will come in layers01:50PM EDT- Platform, in-process, and application01:51PM EDT- Matternhorn will introduce what Arm is calling \"Secure-EL2\"01:52PM EDT- Isolating individual processes within secure memory to avoid cross-contamination01:52PM EDT- There will also be stronger protections against return oriented programming exploits01:53PM EDT- Finally, detecting memory safety vulnerabilities01:53PM EDT- Step 3 for total compute: software & tools01:53PM EDT- There are over 23 million software developers01:54PM EDT- Arm wants Total Compute to be the platform of choice for all developers01:55PM EDT- Arm is announcing today that Arm and Unity are extending their partnership, integrating Arm support into Unity01:56PM EDT- Now recapping Unity and the importance of it01:57PM EDT- The partnership will allow Unity to improve performance against the CPU, GPU, and NPU02:00PM EDT- And that's a wrap on Total Compute02:01PM EDT- Now on stage, Sha Rabii, VP for Facebook02:02PM EDT- Dr. Rabii will be talking about augmented reality02:03PM EDT- Recapping all of the big use cases for AR: work, reminders, HUDs, and more02:04PM EDT- Even something simple like flashing a name will require an always-on contextual AI02:05PM EDT- Not to mention all of thw compute required for the computer vision orocessing02:05PM EDT- Facebook needs a dramatic improvement in silicon/efficiency02:08PM EDT- Compute can be expensive. Data transfer is even more expensive02:09PM EDT- Hitting their energy efficiency targets will also require intelligently picking optimal points on the voltage frequency curve02:10PM EDT- Finding that point is a multi-variable problem02:11PM EDT- Efficiency is also found in optimizing the AI workloads themselves02:11PM EDT- High accuracy often isn't required, or only required at certain points02:14PM EDT- Quantization-aware training remains a popular method to further reduce power requirements, by only using as much precision as is necessary02:17PM EDT- (This is actually a really interesting silicon and algorithm discussion. Though difficult to keep up with it while typing on a laptop)02:17PM EDT- \"The next years will usher in a revolution in computing\"02:18PM EDT- And that's a wrap for the day 1 keynote. Thanks for joining us.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/14959/arm-techcon-2019-keynote-live-blog\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm & TSMC Showcase 7nm Chiplet, Eight A72 at 4GHz on CoWoS Interposer\n",
      "Author: Anton Shilov\n",
      "Date Published: 2019-09-27T16:40:00Z\n",
      "URL: https://www.anandtech.com/show/14914/arm-tsmc-demo-7nm-chiplet-system-w-8-cortexa724ghz-on-cowos-interposer\n",
      "Content: Arm and TSMC this week unveiled their jointly developed proof-of-concept chip that combines two quad-core Cortex-72-based 7 nm chiplets on TSMC’s Chip-on-Wafer-on-Substrate (CoWoS) interposer. The two chips are connected using the company’s Low-voltage-IN-Package-INterCONnect (LIPINCON) interface. The chip is meant to showcase potential of Arm’s and TSMC’s technologies for high-performance computing applications.Large SoCs are hard and expensive to manufacture with decent yields using leading-edge process technologies these days. In fact, many elements of these SoCs do not need to be produced using the latest nodes at all. As a result, many chipmakers turn to the so-called chiplet design approach that relies on multiple smaller dies optimized for a particular function and produced using an appropriate process technology. Smaller dies afford better yields and better binning, allowing for a quicker return on investment. These smaller dies need to communicate with each other using a high-bandwidth low-latency and low-power inter-chiplet connections and the latter are the corner stone of any chiplet design.The proof-of-concept system contains two chiplets made using TSMC’s N7 process technology and placed on a CoWoS interposer. Each chiplet features four Arm Cortex-A72 cores running at a whopping 4 GHz (this core was designed to run at <2 GHz frequencies inside mobile SoCs) that are interconnected using an on-die network-on-chip (NoC) mesh bus operating at 4 GHz. The cores are equipped with a 2 MB L2 cache (512 KB per core) as well as a 6 MB unified L3 cache.The two chips are connected to each other using a LIPINCON die-to-die inter-chiplet connection that operates at 8 GT/s data transfer rate at 0.3 V and offers 320 GB/s bandwidth. When it comes to overall efficiency of LIPINCON on CoWoS, TSMC says that it features a 0.56 pJ/bit (pico-Joules per bit) power efficiency as well as a 1.6 Tb/s/mm2(terabits per second per square millimeter) bandwidth density.The proof-of-concept chiplet system was taped out in December 2018, and made in April 2019, so both Arm and TSMC had plenty of time to play with it. The chip will never be sold in volume, but it proves that technologies by the two companies can enable designers to build complex chiplet-based products with unique characteristics. THe companies are hoping that now this technology is proven that its customers will take advantage of it.Related Reading:Hot Chips 31 Keynote Day 2: Dr. Phillip Wong, VP Research at TSMC (1:45pm PT)TSMC Announces Performance-Enhanced 7nm & 5nm Process TechnologiesTSMC: First 7nm EUV Chips Taped Out, 5nm Risk Production in Q2 2019TSMC Kicks Off Volume Production of 7nm ChipsSource:TSMC\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/14914/arm-tsmc-demo-7nm-chiplet-system-w-8-cortexa724ghz-on-cowos-interposer\n",
      "Title: Arm Joins CXL Consortium\n",
      "Author: Anton Shilov\n",
      "Date Published: 2019-09-13T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/14865/arm-joins-cxl-consortium\n",
      "Content: Arm has officially joined theCompute Express Link (CXL) Consortiumin a bid to enable its customers to implement the new CPU-to-Device interconnect and contribute to the specification. Arm was among a few major technology companies that was yet to join the CXL consortium and given the number of chips that use Arm’s IP, its support is hard to overestimate.Arm is not completely new to CXL. The company has been participating in CXL workgroups and has provided technological and promotional resources to support development of the technology. The formal joining of the CXL consortium indicates the company’s commitment to provide its customers a full software framework to CXL, though the company does not say anything about plans to add appropriate logic to its upcoming AMBA PCIe Gen 5 PHY implementations.Arm is a board member in the PCI SIG and the Gen-Z Consortium. Besides, the company supports its own CCIX interface for inter-package chip-to-chip interface. By supporting CXL, Arm will enable its clients to build CPUs or accelerators that support low-latency cache coherency as well as memory semantics between processors and accelerators.Arm says that CCIX, which supports full cache coherency, will be used as an inter-package chip-to-chip interface for heterogeneous system-on-packages. Meanwhile, since this functionality is not in the scope of CXL at present, it will not compete against Arm’s version of CCIX.Related Reading:AMD Joins CXL Consortium: Playing in All The InterconnectsCompute Express Link (CXL): From Nine Members to Thirty ThreeCXL Specification 1.0 Released: New Industry High-Speed Interconnect From IntelGen-Z Interconnect Core Specification 1.0 PublishedHot Chips: Intel EMIB and 14nm Stratix 10 FPGA Live Blog (8:45am PT, 3:45pm UTC)Source: Arm\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/14865/arm-joins-cxl-consortium\n",
      "Title: Arm Flexible Access: Design the SoC Before Spending Money\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-07-16T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/14644/arm-flexible-access-design-the-soc-before-spending-money\n",
      "Content: One of the critical ways in which the Arm licensing model works relates to how its customers acquires Arm’s IP, the architecture licenses, or access, along with royalty payments. Every customer, especially the big ones on the leading edge, is different, and we’ve gone through how the Arm business model works in detail in a series of articlesback in 2013. Today, Arm is taking a different step in how vendors can approach most of its popular IP for the simple task of design - without having to open the wallet to buy a license.If a customer wants specific IP from Arm, say a Cortex M3 core, they have to first obtain a license to use it. Upon that license payment, the customer gets the required tools and support to implement that core in an SoC design, which is then fed to foundry for tapeout, and depending on the contract, each core or IP block that sells means that Arm gets a small royalty. This means that before a customer even looks into seeing if a specific core or IP block is suitable for the design, they have to buy the license for that design, even if it doesn’t ever end up in production. The per-core royalties, and licensing agreements, are typically done on a customer-by-customer basis depending on the size and scope of the company in question.Arm’s new Flexible Access platform is designed to turn this paradigm on its head.Under Arm Flexible Access, customers will pay a minor subscription fee to Arm’s toolset, and be able to design SoC cores that fall under the new schema without needing a license. Only at the point in which the SoC is pushed to the foundry for manufacturing and tapeout, or rapid prototyping in the silicon stage, will a license be needed.This diagram shows how Flexible Access will work. This top bar shows Arm’s traditional IP licensing model, which is still in place for Arm’s high powered Cortex cores, whereby partners have to select which IP they need and license it before designing the chip.With Flexible Access, the initial subscription fee is a fraction of the licensing cost, allowing companies that might not have considered Arm IP before to dive into SoC design without as much capital input as previously needed. Flexible Access partners will get access to either higher level models or direct RTL, allowing them to do accurate in-house simulation before that license fee is needed.Flexible Access will cover up to 75% of Arm’s most recent IP over the last two years, and will span from Arm’s mid-range Cortex A series through the R and M-series cores, to interconnects and security IP such as CryptoCell, and even physical IP built for TSMC 22ULL process nodes. Arm stated to us in a call that this is very much an ‘alive’ project, with IP being added to the offerings over time. The company said that when new IP is created, it will initially be run under the traditional model with key partners, and as it moves into maturity the idea is to enable it through Flexible Access where 100s of customers can access it.Worth noting on the list is the Arm Cortex-A34, which is a new core that has not been discussed previously. This is an AArch64 version of the A32.Overall, this gives Arm three direct ways of IP licensing:Architecture LicenseTraditional IP LicensingArm Flexible AccessWith Flexible Access, Arm is targeting the mass market items, such as IoT, drones, cameras, and low-to-mid range consumer devices.When asked about the higher performance markets, or even servers, Arm believes that the key customers in those segments are already comfortable with the licensing agreements in place as part of Arm's high-performance portfolio. The company states that the Flexible Access plan will enable Arm to ‘remain competitive and make the best of its competitive portfolio’ in the low-cost mass-market segments. The same argument was given when RISC-V was discussed as direct competition.We did probe Arm about how a concept like Flexible Access came about. Arm stated that it has been approaching its customers, both big and small, for feedback on its productization strategy, and the comments that came back were remarkably consistent. The Flexible Access model, Arm says, is the results of those discussions combined with a new Arm portal for its customers. One of the benefits of simplifying the way its customers can access its offerings is apparently in the ease of contractual agreements (such as simplification and commonality between customers) as well as its back-end cost structure.Ultimately Arm is giving out its RTL core designs to its customers at a fraction of the cost, before licensing. This can wave a red flag, however Arm believes that its customers are very honest with how they use its technology for when it comes down to creating silicon and generating license agreements.The outcome of all of this, to Arm, is to enable a broader ecosystem by lowering the cost to play with the IP. In the previous IP model, customers would be licensing cores that they might not necessarily end up using, but had to at least test to see if it was relevant / hit the right performance or power node. With Flexible Access, Arm believes that its customers will be able to identify better which IP is best suited to each design without having to perform guesswork, and then only license the bits that are actually needed, ultimately lowering cost. For Arm’s shareholders, or technically Softbank’s shareholders, the counter argument to this is that Flexible Access should encourage more companies to work with Arm, ultimately diversifying the consumer base.Arm Flexible Access has already been trialed with a handful of initial partners and from today is now open to the wider Arm audience.Related ReadingThe ARM Diaries, Part 1: How ARM’s Business Model WorksThe ARM Diaries, Part 2: Understanding the Cortex A12Arm's New Cortex-A77 CPU Micro-architecture: Evolving PerformanceArm's New Mali-G77 & Valhall GPU Architecture: A Major LeapArm Announces Mali D77 Display Processor: Facilitating AR & VRArm Announces Neoverse N1 & E1 Platforms & CPUs: Enabling A Huge Jump In Infrastructure Performance\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/14644/arm-flexible-access-design-the-soc-before-spending-money\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm's New Cortex-A77 CPU Micro-architecture: Evolving Performance\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2019-05-27T04:01:00Z\n",
      "URL: https://www.anandtech.com/show/14384/arm-announces-cortexa77-cpu-ip\n",
      "Content: 2018 was an exciting year for Arm’s own CPU designs. Last year in May we sawthe release of the Cortex-A76and the subsequent resulting siliconin the form of the Kirin 980as wellas Snapdragon 855 SoCs. We were very impressed by the IP, and Arm managed to deliver on all its performance, efficiency and area promises, resulting in some excellent SoCs and devices powering most of 2019’s flagship devices.This year we follow-up with another TechDay disclosure, and this time around we’re uncovering Arm’s follow-up to the Cortex-A76: the new Cortex-A77. The new generation is a direct evolution of last year’s major microarchitecture introduction, and represents the second instance of Arm’s brand-new Austin core family. Today we’ll analyse how Arm has pushed the IPC of its new microarchitecture and how this will translate into real performance for upcoming late-2019/early-2020 SoCs and devices.Deimos turns to Cortex-A77The announcement of the Cortex-A77 doesn’t come as a surprise as Arm continues on their traditional annual IP release cadence. In fact today is not the first time that Arm has talked about the A77: In August of last year Arm had teased the CPU core whenreleasing its performance roadmap through 2020:Codenamed as “Deimos”, the new Cortex-A77 picks up where the Cortex-A76 left off and follows Arm’s projected trajectory of delivering a continued solid 20-25% CAGR of performance uplift with each generation of Arm’s new Austin family of CPUs.Before we dwell into the new Cortex-A77, we should take a look back at how the performance of the A76 has evolved for Arm:The A76 has certainly been a hugely successful core for Arm and its licensees. The combination of the brand-new microarchitecture alongside the major improvements that the 7nm TSMC process node has brought some of the biggest performance and efficiency jumps we’ve ever seen in the industry.The results is that the Kirin 980 as well as the Snapdragon 855 both represented major jumps over their predecessors. Qualcomm has proclaimed a 45% leap in CPU performance compared to the previous generation Snapdragon 845 with Cortex-A75 cores, the biggest generational leap ever.While the performance increase was notable, the energy efficiency gains we saw this generation was even more impressive and directly resulted in improved battery life of devices powered by the new Kirin and Snapdragon SoCS.While the A76 performed well, we should remember that it does have competition. While Samsung’s own microarchitecture this year with the M4 has lessened the performance/efficiency gap, the Exynos CPU still largely lags behind by a generation, even though this difference is amplified by a process node difference this year (8nm vs 7nm). The real competition for Arm here lies with Apple’s CPU design teams: Currently the A11 and A12 still hold a large performance and efficiency lead that amounts to roughly two microarchitecture generations.Die shot credit:ChipRebel- Block labelling: AnandTechOne of Arm’s fortes however remains in delivering the best PPA in the industry. Even though the A76’s performance didn’t quite match Apple’s, it managed to achieve outstanding efficiency with incredibly small die area sizes. In fact, this is a conscious design decision by Arm as power efficiency and area efficiency are among the top priorities for Arm’s licensees.The Cortex-A77: A Top-Level OverviewThe Cortex-A77 being a direct microarchitectural successor to the A76 means the new core largely stays in line with the predecessor’s features. Arm states that the core was built in mind with vendors being able to simply upgrade the SoC IP without much effort.In practice what this means is that the A77 is architecturally aligned with its predecessor, still being an ARMv8.2 CPU core that is meant to be paired with a Cortex-A55 little CPU inside of a DynamIQ Shared Unit (DSU) cluster.Fundamental configuration features such as the cache sizes of the A77 also haven’t changed compared to its predecessor: We’re still seeing 64KB L1 instruction and data caches, along with a 256 or 512KB L2 cache. It’s interesting here that Arm did design the option for an 1MB L2 cache for the infrastructureNeoverse N1 CPU core(Which itself is derived from the A76 µarch), but chooses to stay with the smaller configuration options on the client (mobile) CPU IP.As an evolution of the A76, the A77 performance jump as expected won’t be quite as impressive, both from a microarchitecture perspective, but also from an absolute performance standpoint as we’re not expecting large process node improvements for the coming SoC generation.Here the A77 is projected to still be productised on 7nm process nodes for most customers, and Arm is proclaiming a similar 3GHz peak target frequency as its predecessor. Naturally since frequency isn’t projected to change much, this means that the core’s targeted +20% performance boost can be solely attributed to the IP’s microarchitectural changes.To achieve the IPC (Instructions per clock) gains, Arm has reworked the microarchitecture and introduced clever new features, generally beefing up the CPU IP to what results in a wider and more performant design.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/14384/arm-announces-cortexa77-cpu-ip\n",
      "Title: Arm's New Mali-G77 & Valhall GPU Architecture: A Major Leap\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2019-05-27T04:00:00Z\n",
      "URL: https://www.anandtech.com/show/14385/arm-announces-malig77-gpu\n",
      "Content: Along today’s announcement of the new Cortex-A77 CPU microarchitecture, the arguably bigger announcement is Arm’s unveiling of the new Valhall GPU architecture and the new Mali-G77 GPU. It’s been three years since theunveiling of the Bifrost architecture, and as the industry and workloads continue to evolve, so must the company’s GPUs.Valhall and the new Mali-G77 follow up on the last three generations of Mali GPUs with some significant improvements in performance, density and efficiency. While last year’s G76 introduced some large changes to the compute architecture of the execution engines, the G77 goes a lot further and departs from Arm’s relatively unusual compute core design.A look back at Bifrost – third time’s the charmIt’s not too big of a secret that the last few years haven’t been very kind to Arm’s GPU IP offerings. When the first Bifrost GPU - the Mali-G71 was announced back in 2016 and productised later that year in the Kirin 960 and Exynos 8895, we had expected good performance and efficiency gains.Bifrost was Arm’s first scalar GPU architecture, departing from the previous generation’s (Midgard: T-600, 700 & 800 series) vector instruction design. The change was fundamental and akin to what we saw desktop GPU vendors like AMD and Nvidia introduce with their new GCN and Tesla architectures last decade.Unfortunately the first two generations of Bifrost, the Mali-G71 and subsequent G72 weren’t very good GPUs. Arm’s two leading licensees, HiSilicon and Samsung, both came out with quite disappointing SoCs when it came to their GPUs these two generations. The Kirin 960 and 970 in particular were extremely bad in this regard and I’d argue it had quite alot of impact on Huawei and Honor’s product planning and marketing.GFXBench Manhattan 3.1 Offscreen Power Efficiency(System Active Power)Mfc. ProcessFPSAvg. Power(W)Perf/WEfficiencyiPhone XS (A12) Warm7FF76.513.7920.18 fps/WiPhone XS (A12) Cold / Peak7FF103.835.9817.36 fps/WGalaxy 10+ (Snapdragon 855)7FF70.674.8814.46 fps/WGalaxy 10+ (Exynos 9820)8LPP68.875.1013.48 fps/WGalaxy S9+ (Snapdragon 845)10LPP61.165.0111.99 fps/WHuawei Mate 20 Pro (Kirin 980)7FF54.544.5711.93 fps/WGalaxy S9 (Exynos 9810)10LPP46.044.0811.28 fps/WGalaxy S8 (Snapdragon 835)10LPE38.903.7910.26 fps/WLeEco Le Pro3 (Snapdragon 821)14LPP33.044.187.90 fps/WGalaxy S7 (Snapdragon 820)14LPP30.983.987.78 fps/WHuawei Mate 10 (Kirin 970)10FF37.666.335.94 fps/WGalaxy S8 (Exynos 8895)10LPE42.497.355.78 fps/WGalaxy S7 (Exynos 8890)14LPP29.415.954.94 fps/WMeizu PRO 5 (Exynos 7420)14LPE14.453.474.16 fps/WNexus 6P (Snapdragon 810 v2.1)20Soc21.945.444.03 fps/WHuawei Mate 8 (Kirin 950)16FF+10.372.753.77 fps/WHuawei Mate 9 (Kirin 960)16FFC32.498.633.77 fps/WHuawei P9 (Kirin 955)16FF+10.592.983.55 fps/WThe last iteration of the Bifrost architecture, the Mali-G76 was a more significant jump for Arm and the IP was largely able to resolve some of the critical issues of its predecessors, resulting in relatively good results for the Exynos 9820 and Kirin 980 chipsets.Unfortunately while Arm was catching up and fixing Bifrost’s issues, the competition didn’t merely hold still and was pushing the envelope. Qualcomm’s Adreno GPU architecture had been leading the mobile landscape for several years now, and even though the Adreno 640 didn’t post quite as impressive improvements this year, it’s still clearly leading Arm in terms of performance, efficiency and density. More worrisome is the fact that Apple’s GPU in the A12 was an absolutely major jump in terms of performance and efficiency, performing massively better than even Qualcomm’s best, not to speak of Arm’s own Mali GPUs.Introducing Valhall – A major revampToday we’ll be covering Arm’s brand-new GPU architecture: Valhall (anglicized version of the old Norse Valhöll, a.k.a. Valhalla). The new architecture brings a brand-new ISA and compute core design that tries to address the major shortcomings of the Bifrost architecture, and looks to be a lot more similar to the design approaches we saw adopted by other GPU vendors.The first iteration of the Valhall GPU is the new Mali-G77 which will implement all of the architectural and micro-architectural improvements we’ll be discussing today.What’s being promised is a 30% gain in energy efficiency as well as area density (at ISO-performance & process) and a 60% increase in performance of machine learning inferencing workloads on the GPU.More interestingly, upcoming end-of-2019 and 2020 SoCs are projected to see a 40% increase in performance over 2019 devices. Next-generation SoCs are projected to have only minor process node improvements, so most of the gains quoted here are due to the architectural and microarchitectural leaps made by the new Mali-G77 GPU.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/14385/arm-announces-malig77-gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Report: Arm Suspends Business with Huawei - Future Chip Development In Jeopardy\n",
      "Author: Ryan Smith\n",
      "Date Published: 2019-05-22T10:40:00Z\n",
      "URL: https://www.anandtech.com/show/14373/report-arm-suspends-business-with-huawei\n",
      "Content: In the latest event in the quickly moving saga that isHuawei’s technology export blacklisting by the United States Government, the BBC has published a report this morningclaiming that IP vendor Arm has “suspend business” with Huawei and its subsidiaries. If this is correct, then it would represent a massive setback for Huawei’s hardware development efforts, as the company and its HiSilicon chip design subsidiary rely heavily on Arm’s IP for its products.According to the BBC News report, Arm has almost entirely severed ties with Huawei, with the company instructing employees that they are not to “provide support, delivery technology (whether software, code, or other updates), engage in technical discussions, or otherwise discuss technical matters with Huawei, HiSilicon or any of the other named entities”.Huawei, for its part, is one of Arm’s top customers and a close ecosystem partner, shipping countless numbers of chips and devices with Arm IP in it every year. The company is a leading-edge implementer of new Arm CPU and GPU IP, and in the last few years has been the first vendor to ship chips using Arm’s latest Cortex-A series CPUs. Furthermore, via HiSilicon, Huawei is also an ARMv8 CPU architectural licensee. As a result of their close workings with Arm, Huawei has built up a significant amount of their product portfolio around Arm technologies, including their Kirin consumer SoCs andKunpeng server SoCs. So being cut off from Arm would touch virtually every aspect of Huawei’s hardware business, from smartphones to networking gear.Meanwhile Arm, for its part, is headquartered in the UK and not the US. However as a multi-national company, Arm develops its technology around the world, including its major design centers in San Jose and Austin. As a result, according to the report, Arm has deemed that its designs contain “US origin technology”, and as a result make it subject to the US technology blacklist.What’s less clear, however, is just how much Huawei will be impacted by Arm’s suspension and how soon. The BBC’s report indicates that Arm’s suspension only involves further technology transfers and development, and that the company can continue to manufacture chips based on technology they already have – including chips that have finished development and are coming on the market later this year. In which case Huawei wouldn’t immediately feel the impact of the suspension, as the long lead time on chip design means it would be a bit until that development pipeline runs dry. However it’s not as clear what this means for HiSilicon’s Arm architecture license as a whole, and if and how that could be rescinded.For now, the full ramifications for Huawei are going to depend heavily on whether they remain on the US technology blacklist, or if at some point they are removed or otherwise granted a waiver. If Huawei is reinstated, then the company can continue development of their current product pipeline – though the company would want to take a hard look at moving away from US-sourced IP anyhow to prevent a repeat of this event. Otherwise if they remain cut-off from Arm, then Huawei is without a doubt going to be left in a tough spot and will be forced to go it alone. This is where the nuances of their Arm architecture license come into play – if the company can legally develop their own hardware using the Arm ISA – but either way Huawei would need to increasingly develop its own IP and license other parts from non-US sources.Ultimately it’s been clear from the start that the US technology blacklisting would have severe repercussions for Huawei. However of all of Huawei’s US-bound technology partners, there is arguably none more important than Arm. So losing access to Arm’s IP could very well cripple the company.In the meantime, we’ve reached out to Huawei and Arm for further comment.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/14373/report-arm-suspends-business-with-huawei\n",
      "Title: Arm Announces Mali D77 Display Processor: Facilitating AR & VR\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2019-05-15T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/14340/arm-announces-mali-d77-display-processor-facilitating-ar-vr\n",
      "Content: Display processors usually aren’t really much a common topic in the press and only few companies actually do advertise the capabilities beyond a simple mention of the maximum resolution. As the ecosystem evolves, there’s however an increasing amount of new features added into the mix that adds more nuances to the discussion that go beyond resolution, colour depth or gamut.Two years ago, we saw the release of Arm’s newMali-D71 display processorwhich represented a branch new architecture and foundation for the company upcoming DP IP blocks. The D71 brought to market the brunt feature requirements to drive most of today’s higher resolution or higher framerate displays, along with providing robust and smart composition capabilities.Today’s announcement covers the new D77 which is an evolutionary upgrade to the D71. The new IP generation brings new features that go beyond one would normally expect of a display processor, expanding its capabilities, and in particular enables the new block to open up a slew of new possibilities for AR and VR use-cases.Currently display processors mostly act as the compositing engines inside of SoCs, meaning they take in the pixel data generated by GPU or other SoC blocks and composite them into a single surface, and handle all the required processing that is required to achieve this.Typically today’s display controllers lie towards the end of the display pipelines in an SoC, just before the actual physical inferface blocks which transform the data into signals for say HDMI or MIPI DSI, at which point we find ourselves outside of the SoC and connect to a display panel’s DDIC SoC. Here Arm promises to provide straightforward solutions and work closely with third-party vendors which provide IPs further down the chain.The new Mali-D77 being based on the D71 comes with all of its predecessors capabilities, with a large emphasis on AR and VR features that promise to vastly improve the experience in product employing the IP.Among the main features are “Asynchronous Timewarp”, “Lens Distortion Correction” and “Chromatic Aberration Correction”, which provide some new unique use-cases for display processors, along with continuing to provide further improvements in the baseline capabilities of the IP such as more layers as well as higher resolutions and framerates.Asynchronous timewarp is an interesting technique for AR and VR whose main goal is to reduce the motion to photon latency. In a normal GPU to display operation, the display always simply display the last GPU render frame. The problem with this approach is that the update interval of this render is limited by the actual rendering framerate which is a characteristic of the GPU’s performance capabilities. This causes a hard limitation for AR and VR workloads which require very high visual frame-rates in order to provide a better experience and most importantly avoid side-effects such as motion sickness caused by delayed images.Timewarp is able to disconnect the GPU render from what is actually scanned out to the display. Here, the D77 is able to integrate position data updates such as from motion sensors in HMDs into the most recent rendered GPU frame, post-process it with the motion data, and deliver to the display an updated image. In this new process, the user effectively will see two different frames displayed even though the GPU will have only rendered one.Effectively this massively reduces the motion to photon latency in AR and VR use-cases, even though the actual rendering framerate doesn’t actually change. Avoiding doing the work on the GPU also reduces the processing workload, which in turn opens up more performance to be dedicated to the actual rendering of content.In addition to ATW, the D77 is also able to correct for several optical characteristics of VR lenses such as pincushion effect. The IP is able to be programmed with the characteristics of a used HMD system and will be able to correct for distortions by applying an inverse effect (in this case a barrel distortion) to compensate for the distortion on the lenses.This optical compensation also applies to chromatic aberration correction. Similarly, the D77 needs to be aware of the optical characteristics of the lens in use, and will be able to post-process the outputting image in an inverse effect – eliminating the resulting experienced image artefacts when viewed through the lens. It’s to be noted that the spatial resolution of the correction achievable here is limited by the actual resolution of the display, as it can’t correct something that is smaller than a pixel in dimensions.The benefits of these new techniques on the DP is that it enables a significant amount of processing savings on the part of the GPU, which is much higher power.What this also opens up is a possible new generation of “dumber” HMDs in the future without a GPU, powered by some other external system, yet providing the same latency and optics advantages as described above in a cheaper integrated HMD SoC.Performance characteristics of the D77 are as follows: 4K60 with 8 layers or 4K120 with 4 layers. In smartphones with Android the higher layer number is a requirement so I don’t envision 4K120 to be a thing beyond special use-cases.For VR use-cases, the D77 is able to handle up to 4 VR layers in which (ATW, correction, etc) the maximum resolution if up to 1440p120 or 4K90 with respectively 4+4 or 2+2 layers.Overall, the new Mali-D77 is exciting news for AR and VR. While we’re expecting the IP to be used in smartphones in the next few years, the most exciting news for today in my opinion is that it enables higher quality standalone HMDs, expanding Arm’s market beyond the typical smartphone SoC. Arm unofficially described the VR ecosystem as currently being in the “trough of disillusionment” after the last few years of peaked expectations.The next few years however will see significant progress made in terms of improving the VR experience and bringing more consistent experiences to more people. Certainly the D77 is a first step towards such a future, and we’re excited to see where things will evolve.Related Reading:Arm Announces New Mali-D71 Display Processor and IP BlocksARM Announces Mali-Cetus, Their Next-Generation Display Processor\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/14340/arm-announces-mali-d77-display-processor-facilitating-ar-vr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Ampere Computing: Arm is Now an Investor\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-04-15T15:15:00Z\n",
      "URL: https://www.anandtech.com/show/14212/ampere-computing-arm-is-an-investor\n",
      "Content: The pipeline for Arm based server products has a few companies fighting for as much of the market as they can get – the server market is a big pie to consume, and not only do these companies have to tempt customers from x86, but they also have to compete with each other to get customers. Most of them do this by having highly focused and niche products, targeted to a few key select markets. Ampere is one of those companies, using the old AppliedMicro IP, and now the company has a solid presence in its investor lineup: Arm.Ampere, founded by former Intel president Renée James and funded by the Carlyle Group, currently offers its eMAG range of processors since October 2018. The company purchased AppliedMicro’s X-Gene Arm-based server assets back in early 2018, and offers a custom Arm v8.0-A chip design with 32 cores running at 3.3 GHz, built on TSMC’s 16FF+ process. The chip is based on the X-Gene II platform with optimizations, and the company has a roadmap planned for the next few years with updated generations of its eMAG product line. Recent product wins with eMAG includepublicly available cloud instancesfrom Packet, running at $1/hour.One of the aspects of these server companies using the Arm architecture is getting their elbows out into the market and selling at volume. At present, only Cavium has succeeded with its ThunderX2 platform, finding its way into servers, cloud instances, and even HPC deployments. These companies rely on rounds of investment early on in the design cycle, both in terms of money to develop products but also with respect to expertise that potential investors can bring. Ampere is part of The Carlyle Group, which has a large portfolio of investment, however the most recent round of investment into Ampere brings Arm under that list as well.With Arm as an investor, it will be interesting to see how much the companies will want to interact beyond the raw cash injection. Arm doesn’t invest in every Arm server chip company on the market, but the relationship has the potential to give Ampere some insight into Arm’s future design portfolio (perhaps more than previously) as well as opportunities to focus resources, drive down development costs, or enhance final products. Depending on the level of interaction, this could give Ampere an advantage, especially in front of customers, compared to Arm’s other licensees that also play in this space.This was the second round of investment for Ampere. It was not disclosed how much investment was made, nor the valuation, or how much each investor has contributed.“This significant investment from Arm and our initial investors signals their confidence in the future of Ampere and allows us to continue to accelerate and deliver our robust roadmap,” said Renee James, Ampere founder, CEO and Chair. The company announced its first product, eMAG™, in October 2018.“A robust ecosystem is critical to advance the Arm® architecture as the foundation for transforming the modern cloud to edge infrastructure,” said Rene Haas, president Arm IP Products Group. “Investing in Ampere underscores Arm’s commitment to accelerating both our ecosystem and availability of diverse Arm-based silicon solutions for an infrastructure market long deprived of choice and flexibility.”Related ReadingAmpere eMAG in the Cloud: 32 Arm Core Instance for $1/hrArm Announces Neoverse N1 & E1 Platforms & CPUs: Enabling A Huge Jump In Infrastructure PerformanceArm Announces Neoverse Infrastructure IP Branding & Future Roadmap\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/14212/ampere-computing-arm-is-an-investor\n",
      "Title: Ampere eMAG in the Cloud: 32 Arm Core Instance for $1/hr\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-03-29T12:40:00Z\n",
      "URL: https://www.anandtech.com/show/14141/ampere-emag-in-the-cloud-32-arm-core-instance-for-1hr\n",
      "Content: One of the companies working to put Arm processors into the cloud is Ampere, and the proliferation of Arm in the cloud is continuing to grow. Cloud company Packet is now offering a new instance which combines Ampere’s eMAG processor with 32 Skylark cores, a turbo up to 3.3 GHz, 128 GB of DRAM, a 480 GB SSD, and dual 10 Gbps networking.Ampere was formed by ex-Intel President Renée James and is funded by The Carlyle Group, with the company purchasing AppliedMicro’s X-Gene Arm server assets back in 2018, and introduced the first generation of eMAG processors in September of the same year, which was essentially the X-Gene II platform with optimizations. The custom Arm v8.0-A core design is built on TSMCs 16FF+ process and runs at 3.3 GHz in turbo mode. The full 32-core chip has a rated TDP of 125W, features support for eight memory channels and up to 1TB of memory.Thec2.large.arm instancefrom Packet is the first instance of the eMAG Skylark design being used in a publicly available cloud instance, allowing customers to both develop for the platform as well as develop cloud services. Depending on who you talk to, the $1/hr pricing has both been interpreted asvery reasonableorvery highdepending on which industry analyst you talk to, and based on our discussions with Ampere at Mobile World Congress this year, one of the key elements of the company is to get more users introduced to the platform for cloud and edge services. The company believes that the eMAG product offers a very competitive total cost of ownership (TCO) value for high performance compute, high memory capacity, and rich I/O. That being said, the instance offering from Packet is more on the development or HPC side, rather than probing the memory or IO.Ampere and Packet are also looking at ‘Edge’ deployments for companies that require a localised eMAG based infrastructure.We’re hoping to get a Skylark based platform in for review at some point, as it will provide a good comparison point against other non-x86 attempts to provide distinct cloud offerings.Related ReadingArm Announces Neoverse N1 & E1 Platforms & CPUs: Enabling A Huge Jump In Infrastructure PerformanceHuawei Server Efforts: Hi1620Just When You Thought It Was Dead: Qualcomm Centriq Arm Server Systems SpottedArm Announces Neoverse Infrastructure IP Branding & Future RoadmapAssessing Cavium's ThunderX2: The Arm Server Dream Realized At Last\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/14141/ampere-emag-in-the-cloud-32-arm-core-instance-for-1hr\n",
      "Title: Arm Announces Neoverse N1 & E1 Platforms & CPUs: Enabling A Huge Jump In Infrastructure Performance\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2019-02-20T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/13959/arm-announces-neoverse-n1-platform\n",
      "Content: Anybody following the industry over the last decade will have heard of Arm. We best know the company for being the enabler and providing the architecture as well as CPU designs that power essentially all of today’s mobile devices. The last 7-5 years in particular we’ve seen meteoric advances in silicon performance of the mobile SoCs found in our smartphones and tablets.However Arm's ambition goes widely beyond just mobile and embedded devices. The market for compute in general is a lot larger than that, and looking at things in a business sense, high-end devices like servers and related infrastructure carry far greater profit margins. So for a successful CPU designer like Arm who is still on the rise, it's a very lucrative market to aim for, as current leader Intel can profess.To that end, while Arm has been wildly successful in mobile and embedded, anything requiring more performance has to date been out of reach or has come with significant drawbacks. Over the last decade we’ve heard of numerous prophecies how products based on the architecture will take the server and infrastructure market by storm “any moment now”. In the last couple of years in particular we’ve seen various vendors attempt to bring this goal to fruition: Unfortunately,the resultsof the first generation of products were less than successful, and as such, even thoughsome did betterthanothers, the Arm server ecosystem has seen a quite a bit of hardship in its first years.A New Focus On PerformanceWhile Arm has been successful in mobile for quite some time, the overall performance of their designs has often left something to be desired. As a result, the company has been undertaking a new focus on performance that is spanning everything from mobile to servers. Working towards this goal, 2018 was an important year for Arm as the company hadintroduced its brand-new Cortex A76 microarchitecturedesign: Representing a clean-sheet endeavor, learning from the experience gained in previous generations, the company has put high hopes in the brand-new Austin-family of microarchitectures. In fact, Arm is so confident on its upcoming designs that the company has publiclyshared its client compute CPU roadmap through 2020 and proclaiming it will take Intel head on in PC laptop space.While we’ll have to wait a bit longer forproducts such as the Snapdragon 8CX to come to market, we’ve already hadour hands on the first mobile devices with the Cortex A76, and very muchindependently verified all of Arm’s performance and efficiency claims.And then of course, there's Neoverse, the star of today's Arm announcements. With Neoverse Arm is looking to do for servers and infrastructure what it's already doing for its mobile business, by greatly ramping up their performance and improving their competitiveness with a new generation of processor designs. We'll get into Neoverse in much deeper detail in a moment, but in context, it's one piece of a much larger effort for Arm.All of these new microarchitectures are important to Arm because they represent an inflection point in the market: Performance is now nearing that of the high-end players such as Intel and AMD, and Arm is confident in its ability to sustain significant annual improvements of 25-30% - vastly exceeding the rate at which the incumbent vendors are able to iterate.The Server Inflection Point: An Eventful Last Few Months IndeedThe last couple of months have been quite exciting for the Arm server ecosystem. At last year’s Hotchips we’ve covered Fujitsu’s session of their brand-new A64FX HPC (High performance compute) processor, representing not only the company shift from SPARC to ARMv8, but also delivering the first chip to implementthe new SVE(Scalable Vector Extensions) addition to the Arm architecture.Cavium’s ThunderX2 saw some very impressive performance leaps, making its new processor among the first to be able to compete with Intel and AMD – with partners such asGIGABYTE offering whole server systemssolutions based on the new SoC.Most recently, we sawHuawei unveiled their new Kunspeng 920 server chippromising to be the industry’s highest performing Arm server CPU.The big commonality between the above mentioned three products is the fact that each represents individual vendor’s efforts at implementing a custom microarchitecture based on an ARMv8 architectural license. This in fact begs the question: what are Arm’s own plans for the server and infrastructure market? Well for those following closely, today’s coverage of the new Neoverse line-up shouldn’t come as a complete surprise as the company had firstannounced the branding and road-map back in October.Introducing the Neoverse N1 & E1 platforms: Enabling the EcosystemToday’s announcement is all about enabling the ecosystem; we’ll be covering in more detail two new “platforms” that will be at the core of Arm’s infrastructure strategy for the next few years, the Neoverse N1 and E1 platforms:Particularly today’s announcement of the Neoverse N1 platform sheds light onto what Arm had teased back in the initial October release, detailing what exactly “Ares” is and how the server/infrastructure counter-part to the Cortex A76 µarchitecture will be bringing major performance boosts to the Arm infrastructure ecosystem.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/13959/arm-announces-neoverse-n1-platform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Announces Cortex-A65AE for Automotive: First SMT CPU Core\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2018-12-18T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/13727/arm-announces-cortex65ae-for-automotive-first-smt-cpu-core\n",
      "Content: Back in September, Arm had announced the new Cortex A76AE CPU with focus on automotive applications.Today, Arm adds to its portfolio by formally announcing a next gen processor with simultaneous multithreading, the new Cortex A65AE.The previously announced A76AE CPU was Arm’s first to feature the “split-lock” technology that allowed two CPU cores to operate in a configurable lock-step operation mode, allowing the units to process tasks in parallel to each other and compare results for discrepancies – achieving a required level of functional safety of the software that is run on the system.The Cortex A76AE was pretty much focused on heavy compute tasks and thus alsofully takes advantage of the new high performancethat is delivered by Arm’snew microarchitecture that came out of the Austin design centre. During the reveal of the A76AE, there were mentions of a “Helios” CPU core – to date it wasn’t too clear what this was meant to be, but it very much seemed like a new class of core that was meant to accompany the A76AE.The Arm Cortex A65AEToday, Arm finally formally announces the new Cortex A65AE, and although today’s material isn’t a full technical disclosure of the new CPU core, it does finally shed a little bit of light of what Helios is meant to be.Much like during the Cortex A76AE release, Arm talked quite a bit about the needs of the automotive market and how cars are becoming increasingly demanding in terms of their need for compute power. Every single part of a car is becoming increasingly computerised, and advances in ADAS and future autonomous applications will explode the amount of processing power needed.While the Cortex A76AE was focused on applications where high performance is needed, the Cortex A65AE is focused on high-throughput applications. The difference here would be in a sense the difference between demanding single-threaded workloads and demanding highly parallel and numerous multi-threaded workloads. Of the latter scenario, Arm emphasises the requirement of sensor processing in autonomous driving. Here the amount of sensors in a car is said to massively increase, and with it, also the need for higher throughput processing power.Arm’s First SMT CPU MicroarchitectureToday’s announcement is a bit of an odd one in that we're talking about something important like Arm's first SMT microarchitecture as part of a more mundane automotive IP announcement, and yet the use-case presented here is the perfect fit for it. The Cortex A65AE is Arm’s first multi-threaded CPU core, allowing two threads to be executed per core. At the moment Arm is being very tight-lipped about the details of the microarchitecture, but they were able to comment on a bit of background information of the core.As we’ve explained in the past, Arm usually has three main design centres which design the Cortex-A lineup of cores: The Cambridge team (A53, A55), the Sophia-Antipolis team (A73, A75), and the Austin team (A57, 72, and the new A76 family). The most interesting aspect of the Cortex A65AE is its heritage: although it was initially started by the Cambridge team, it then became a joint project and then finally finished to production quality by Arm’s newest team in their Chandler design centre in Arizona, making this effectively the first project coming out of this new team.The reason why I dug into where the core came from is that it gives us greater perspective into what the microarchitecture might look like. Arm was able to disclose that this is indeed an out-of-order CPU core with SMT, but that’s about it in terms of what they were willing to reveal. The fact that the design started in Cambridge very much hints that this is somehow related to previous little cores such asthe Cortex A53 and A55– but the addition of OoO and SMT does make it seem more of distant cousin rather than a successor.The only performance figure publicised during the presentation is the fact that the new CPU core is advertised as having a 3.5x higher throughput than the prior generation core in the same market segment – in this case a Cortex-A53. Arm usually makes performance projections based on the process node that an IP will typically be built on, again in this case that would be 7nm. Assuming a best-case scenario of 1.8-2x increased throughput through SMT, it still leaves quite a hefty difference that could be accounted for by frequency increases through the process node, or simply IPC improvements to the microarchitecture.Again, the main benefit of the inclusion of SMT comes from the fact that in the primary automotive use-case of the Cortex A65AE, we’ll be seeing a load of sensors all communicating simultaneously to the central control unit of a car.Arm’s SMT implementation also looks to be unique in terms of its functional safety features: Much alike “Split-Lock” mode on the Cortex A76AE where two physical cores can operate in lock-step with each other, the Cortex A65AE can also do this not only on a physical core level, but also on a thread level. Here a Cortex A65AE core can effectively have two threads operate in lock-stepon the same corewith two other threads on a physical shadow core. Here the instruction stream and each instruction output is checked for discrepancies at a hardware level, all transparent to the operating software (Obviously in the case of a failure, an exception would be generated).In a practical example of an envisioned system, we would see different clusters of Cortex cores dedicated to different workload tasks. In the above diagram, we would see multiple Cortex A65AE cores in a cluster operate independently in “Split” mode, maximising their throughput when working on sensor data collection.The data processing would then be passed on to different clusters for perception and decision tasks: Here the cores would require higher levels of functional safety, and thus the CPU cores would be operating in lock-step mode. Arm also emphasised its flexibility in terms of the configuration of the Split-Lock layout of the hardware; it’s something that would be determined on a firmware level, and vendors would be able to reconfigure with a software update if they so wished.The Cortex A65AE is Arm’s second dedicated core meant for the automotive market - beyond the key aspect that this is an ASIL D compliant microarchitecture, the most interesting aspect of today’s announcement is the fact that it is a new microarchitecture that we yet to see in Arm’s traditional mobile and embedded markets. It seems very much a derivative of Arm’s Cambridge line-up of small CPU cores, and today’s presentation does position the core as the more “traditional little core\" alongside the bigger Cortex A76AE.The Cortex A65AE also Arm’s first SMT core, which is undoubtedly going to generate some talk among our readers. My view on this still unchanged - SMT doesn’t make much sense in mobile workloads as the key focus in this market is energy efficiency. From an electrical engineering perspective, an SMT core will never be more efficient than simply spreading out workloads across more physical cores and clock gating functional blocks when they are being underutilised.Arm first let it slip that it was planning on introducing SMTduring its Neoverse infrastructure IP announcement: Here SMT makes significantly more sense as the workloads and throughput requirements would be very different. If there will be a traditional “Cortex-A65” non-AE variant of this core, it will be very interesting to see how Arm is going to position this, and what markets it will be targeted at. For now, we’ll have to remain patient until further disclosures of the microarchitecture.Arm envisions first silicon products with the Cortex A65AE in 2020.Related Reading:Arm Unveils Arm Safety Ready Initiative, Cortex-A76AE ProcessorArm Delivers on Cortex A76 Promises: What it Means for 2019 DevicesArm Announces Neoverse Infrastructure IP Branding & Future RoadmapArm's Cortex-A76 CPU Unveiled: Taking Aim at the Top for 7nmArm Unveils Client CPU Performance Roadmap Through 2020 - Taking Intel Head OnArm and Samsung Extend Artisan POP IP Collaboration to 7LPP and 5LPE NodesHot Chips 2018: Arm's Machine Learning Core Live Blog\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/13727/arm-announces-cortex65ae-for-automotive-first-smt-cpu-core\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Apacer Launches 32-Bit SODIMM for Arm & RISC-V Systems\n",
      "Author: Anton Shilov\n",
      "Date Published: 2018-11-26T22:00:00Z\n",
      "URL: https://www.anandtech.com/show/13635/apacer-launches-32bit-sodimm-for-arm-risc-v-systems\n",
      "Content: Apacer has announced a lineup of 32-bit SO-DIMMs designed for systems based on processors featuring Arm, RISC, or RISC-V architectures. The memory modules will enable SoC developers to take advantage of capacity and performance flexibility offered by modular memory solutions.Memory organization is a bit different between x86 and Arm/RISC-V based systems. The former typically feature one or more 64-bit memory interfaces to connect one or more 64-bit memory modules (or just a set of DRAM chips) in a bid to maximize raw memory bandwidth and capacity. By contrast, Arm or RISC-V powered SoCs use one or more 16-bit memory interfaces for granularity, power, and efficiency (to maximize channel utilization and effective memory bandwidth) reasons. Since the vast majority of Arm or RISC-V based systems are either mobile or special purpose, most of the memory subsystems are custom-designed with only a handful of SoCs featuring “wide” memory interfaces. As a result, most of them cannot use industry-standard 64-bit DIMMs and rely on soldered down memory.The situation is changing with release of Arm-powered server processors, but many of emerging applications still do not need 64 or 128-bit DRAM subsystems, but could take advantage of flexibility that memory modules provide in general. Apacer has offered 32-bit DDR2 and DDR3 modules for such devices for years and is now addressing this need with its new 32-bit DDR4 SO-DIMMs.By using modules instead of soldered-down memory subsystems, developers of applications running processors featuring Arm, RISC, or RISC-V architectures will be able to save PCB space, provide manufacturing flexibility, and ensure DRAM upgradeability of their products.The family of Apacer’s 32-bit memory modules includes 2 GB, 4 GB, and 8 GB SO-DIMMs operating at DDR4-2133, DDR4-2400, and DDR4-2667 speed bins at 1.2 Volts. The modules are based on various DRAM chips supporting industrial-grade wide-temperature ranges, so they can power a variety of applications. Apacer says that its 32-bit SO-DIMMs are compatible with Arm-based SoCs by NXP, Freescale, Marvell, Cavium, and Texas Instruments as well as “the latest RISC-V 32-bit processors.” Meanwhile, considering the fact that the modules are 32-bit wide, it is not guaranteed that they will work with all SoCs. Given the relatively limited market for such SO-DIMMs, expect them to be priced accordingly.Apacer did not say when to expect the modules to become available and how exactly it plans to market them (to enable end-user upgradeability), but the products are alreadylistedat the company’s website, so expect the first customers to get their 32-bit DDR4 SO-DIMMs shortly.Related ReadingCombo SDIMM: Apacer adds SATA M.2 Storage to DRAM ModulesTranscend Introduces Extreme Temperature DDR4 SO-DIMMsWestern Digital to Use RISC-V for Controllers, Processors, Purpose-Built PlatformsSource:Apacer(viaTechPowerUp)\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/13635/apacer-launches-32bit-sodimm-for-arm-risc-v-systems\n",
      "Title: Arm Delivers on Cortex A76 Promises: What it Means for 2019 Devices\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2018-11-20T18:10:00Z\n",
      "URL: https://www.anandtech.com/show/13614/arm-delivers-on-cortex-a76-promises\n",
      "Content: In the grand scheme of things, it hasn’t been all that long since we first coveredArm’s announcement of the new Cortex A76 CPU microarchitecture. The new CPU IP was publicly unveiled back on the first of June, and Arm had made big promises in regards to the performance and efficiency improvements of the new core. It’s been a little over 5 months since then, and as we originally predicted, we’ve seen vendors announce as well as ship silicon SoCs with the new CPU.Last week we publishedour review of the Huawei Mate 20 and Mate 20 Pro– both which containHiSilicon’s new Kirin 980chipset. Unfortunately for a lot of our readers which are based in the US, the review won’t be as interesting as the devices won’t be available to them. For this reason I’m writing up a standalone piece focusing more on the results of the new Cortex A76 inside the Kirin 980, and discuss more in detail how I think things will play out in the upcoming generation of competing SoCs.Verifying Arm’s performance projectionsNaturally one of the first things people will be interested in is seeing how the Cortex A76 actually manages to perform in practice.Arm had advertised the Cortex A76 to reach clocks of up to 3GHz, and correspondingly had all of its performance projections presented at this frequency. As I’ve written back in May, the 3GHz frequency was always an overly optimistic target that vendors would not be able to achieve; I said something along 2.5GHz would be a much more realistic figure. The Kirin 980 ended up being released with a final clock speed of 2.6GHz, which was more in line with what I expected.The Cortex A76 at 3GHz was projected to perform respectively 1.9x and 2.5x times better than a Cortex A73 at 2.45GHz – which is the configuration of Qualcomm’s Snapdragon 835. Translated to a clock speed of 2.6GHz, the improvements are adjusted to ~1.65x and ~2.15x.In practice, the Kirin 980 manages to reach an improvement of 1.77x in the integer score, as well as slightly exceeding the target improvement for the floating point score, achieving an increase of 2.21x. The reason the Kirin 980 here exceeds the targets is maybe linked to the fact that the chip is configured with a 4MB L3 while Arm’s simulations ran with a 2MB L3.Moving over to SPEC2006, we have a set of more complex and robust workloads that better represent the wider range of applications that you would come to expect.Here Arm’s performance projections were a bit more coloured, as we had been presented IPC comparisons as well as absolute score comparisons. In the absolute improvements at 3GHz,we saw claims of 2.1x “without thermal constraints” at 3.3GHz and figures of 1.9x “within 5W TDPs”. The latter figures was extremely confusing as Arm’s marketing was contradictory as to what this exactly means, which for a long time had me questioning if the CPU would somehow hit thermal limits in the single-threaded SPEC workloads, which would have been a pretty terrible result.The IPC comparisons are a lot more straightforward: Versus a Cortex A73, we would respectively see increases of 1.58x and 1.79x in the integer and FP suites.In practice, the Kirin 980 and the Cortex A76 more than delivers: we’re seeing 1.89x and 2.04x increases in the integer and FP scores. In terms of IPC, the increases over the Cortex A73 based Kirin 970 and Snapdragon 835 are even more significant: Here we’re seeing jumps of respectively 1.78x and 1.92x. In fact, because the Kirin 980 performed better than expected, it actually managed to reach my projected scores (based on Arm’s figures) I had estimated for a 3GHz Cortex A76, but actually achieving this at 2.6GHz.Memory subsystem performance matters enormouslyThere is one aspect of CPU performance that seems to be continuously misunderstood and misrepresented: Memory subsystem performance. A CPU can be incredibly wide as well as have any amount of execution resources, however no matter how big the microarchitecture is, it matters little if the memory subsystem (caches, memory controllers) are not able to keep the machine properly fed with data. The mobile space over the last few years has pretty much seen the same workload progression that we’ve seen in desktops over the past decades, just in a vastly more accelerated pace. Applications become bigger and more complex in terms of their program sizes, and the data they’re processing has also seen significant growth.The problem with this evolution is that the tools that we usually use to benchmark performance can become outdated if they can’t accurately reproduce the microarchitectural workload characteristics of modern every-day applications. Recently with the launch of the Kirin 980, I’ve seen some people get the wrong idea and come to the wrong conclusion in terms of the actual performance of the chipset, basing their opinion on results such as GeekBench 4 scores.To explain this, I wanted to showcase the evolution of recent generation SoCs, all relative to a fixed starting figure. I picked the Snapdragon 835 for this as it represented a well-balanced and popular SoC.In SPECint2006, the scores don’t seem to diverge all that much from what GeekBench4 is able to project, and this is valid for most SoCs. In this set, the only significant divergence comes from the Apple’s A11 and A12 chips. Here the A11 and A12 were able to show significantly larger increases in the SPEC workload performances than in GB4.Switching over to SPECfp2006, beyond the obvious fact that the benchmarks here are using more floating point datatypes in their programs, we see a much larger percentage of workloads that are characterised by putting a lot more demand on the memory subsystems. Here, we see a lot more discrepancy between the different SoCs.On one side again, the Apple A12 again was able to showcase much bigger generational improvementsin SPECfp than it was able to showcase in GB4’s FP workloads, again pointing out to the massive memory subsystem performance improvements Apple was able to introduce this generation. On the other hand, the Exynos 9810 sticks out in the opposite way: its performance in SPEC was much less than what we see in GeekBench4, againrepresenting the Achilles heel of the chipsetasthe CPU’s memory and cache subsystem largely lags behind the competition.The point I’m trying to make here is that the vast majority of real-world applications behave a lot more like SPEC than GeekBench4: Most notably Apple’s new A12 as well as Samsung’s Exynos 9810 contrast themselves in the two extremes as shown above. In more representative benchmarks such as browser JS framework performance tests (Speedometer 2.0), or on the Android side, PCMark 2.0, we see even greater instruction and data pressure than in SPEC – multiplying the differences exposed by SPECfp.There are also benchmarks who go in the opposite way of their workload characterisation: Dhrystone or Coremark have very small memory footprints. Here most of the benchmark will entirely fit into the lower cache hierarchies of a CPU, not putting any kind of pressure to the bigger caches or even DRAM. These are useful benchmarks still in their own regard, but shouldn’t be taken as a representation of overall performance in modern application. AnTuTu’s CPU test falls among these as its footprint is also tiny and not testing anything beyond the execution engines and the first level cache hierarchy.HiSilicon’s Kirin 980 along with Arm’s Cortex A76 here seem to strike a great balance in this regard: The performance between SPEC and GeekBench4 doesn’t diverge all too much. We’ll get back this just in a bit when looking at the efficiency results of the new Kirin chipset.Top-tier energy/power efficiency, absolute performance still quite behind AppleWhen it comes to power and energy efficiency, Arm made two claims: At the same power usage, the Cortex A76 would perform 40% better than the Cortex A75, and at the same performance point, the Cortex A76 would use only 50% of the energy of a Cortex A75. Of course these two figures are to be taken with quite a handful of salt as the comparison was made across process nodes.Looking at the SPEC efficiency results, they seem more than validate Arm’s claims. As I had mentioned before, I had made performance and power projections based on Arm’s figures back in May, and the actual results beat these figures. Because the Cortex A76 beat the IPC projections, it was able to achieve the target performance points at a more efficient frequency point than my 3GHz estimate back then.The results for the chip are just excellent: The Kirin 980 beats the Snapdragon 845 in performance by 45-48%, all whilst using 25-30% less energy to complete the workloads. If we were to clock down the Kirin 980 or actually measure the energy efficiency of the lower clocked 1.9GHz A76 pairs in order to match the performance point of the S845, I can very easily see the Kirin 980 using less than half the energy.The one metric that doesn’t quite pan out for Arm is the claim that at the same power, the Cortex A76 would perform 40% better. Here Arm chose an arbitrary 750mW point for the comparison – which may or may not make the claim accurate, however we don’t know where this intersection point lies, and it would require more exact measurements of the frequency-power curve of both chipsets. The matter of fact is, the Cortex A76 is a more power hungry CPU, and single core active platform power consumption has gone up by 14-21%.It’s here where we can make the interesting comparison to Apple’s latest: The energy efficiency for the Kirin 980 is ever so slightly ahead of the Apple A12, meaning the perf/W of both SoCs are nearly identical. The big difference here is that Apple is able to achieve a 61-74% performance advantage, at a linear cost of 60-70% increased power consumption.What it means for next Snapdragon and Exynos 9820The excellent showing of the Kirin 980 is a good omen for the upcoming Snapdragon flagship. I’m expecting Qualcomm to be a little more aggressive when it comes to the core clocks, aiming just a tad higher above the 2.6GHz of the Kirin 980. What this will actually mean in regards to the resulting power efficiency remains to be seen.Performance on paper should also fare well, but in practice Qualcomm does have an aspect that can complicate things: the SoC’s system cache. Here evidently Qualcomm is trying to mimic Apple in having a further system-wide cache hierarchy before going to DRAM; for the Snapdragon 845 this was a double-edged sword as memory latency saw a degradation over the Snapdragon 835. This degradation seemingly caused the Cortex A75 in the S845 to maybe not achieve its full potential. Hopefully the new generation SoC has less of an impact in this regard, and we can expect good performance figures.Samsung last weekofficially announced the Exynos 9820, and here the outlook is a bit more pessimistic.The Exynos 9810 did not fare well in benchmarks, but this wasnot only because of the scheduler issues, but also simply because the microarchitecture didn’t seem balanced. The Kirin 980 is able to beat the Exynos 9810’s top performance, all while consuming less than half the energy. At the more reasonable 2.3GHz frequency point of the chip, the performance gap widens to 23-30%, while still showcasing a 42-47% energy efficiency disadvantage over the Kirin 980.Samsung proclaims that the Exynos 9820 showcases 20% better performance, or 40% better efficiency. The keyword here being “or” – meaning the improvements are at an iso-comparison to the other axis. Taking the 2.7GHz figures as a base comparison, a 20% performance improvement could well compete with the Cortex A76, but the horrid energy efficiency of the chip would still remain. Similarly, taking the more efficient 2.3GHz result as the baseline performance, a 40% improvement in efficiency would match the Kirin 980 in efficiency, but still would have to endure the performance deficit.Samsung’s marketing figures just aren’t good enough, and mathematically I just don’t see any way the Exynos 9820 would be able to compete if the results do pan out like this. The only glimmer of hope here is that, much like Apple’s marketing department understated the performance improvements of the A12, S.LSI is understating the improvements of the Exynos 9820. Here the only scenario I could see as working out is that the claimed performance jump merely represents GeekBench4 scores, and actual improvements in SPEC and more realistic workloads see a much more significant jump, closing this ratio gap between the two benchmarks that we discussed just earlier. Let’s hope for this latter scenario.The Cortex A76 is a very solid CPU – Deimos & Hercules will follow upArm had already teased the successor to Enyo (Cortex A76)with the reveal of Deimos and Hercules. Here Arm promised 15-20% performance increases in the next generation. Arm’s strength here lies in actually delivering an overall excellent package of performance within great power envelopes. Also while this part of the PPA metric isn’t something consumer should inherently care about, Arm is able to also keep the CPUs extremely small.We’ve just recently seen Arm’s new server core in the wild– Ares should be the infrastructure counterpart to Enyo/A76 and part ofthe recently announced Neoverse family of CPU cores. It’s not hard to imagine 32 or 64 of cores of this calibre on a single chip. Overall, we’re looking forward to more exciting products in the next several months – both in the mobile and infrastructure spaces.Related ReadingThe Mate 20 & Mate 20 Pro Review: Kirin 980 Powering Two Contrasting DevicesHiSilicon Announces The Kirin 980: First A76, G76 on 7nmChipRebel Releases Kirin 980 Die Shot: Cortex A76's & Mali G76 in ViewArm's Cortex-A76 CPU Unveiled: Taking Aim at the Top for 7nmThe Samsung Galaxy S9 and S9+ Review: Exynos and Snapdragon at 960fpsSamsung Announces 8nm Exynos 9820 With Tri-Group CPU DesignHot Chips 2018: Samsung’s Exynos-M3 CPU Architecture Deep DiveHuawei Server Efforts: Hi1620 and Arm’s Big Server Core, AresArm Announces Neoverse Infrastructure IP Branding & Future RoadmapQualcomm Announces Snapdragon 675 - 11nm Mid-Range Cortex A76-BasedArm Unveils Client CPU Performance Roadmap Through 2020 - Taking Intel Head On\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/13614/arm-delivers-on-cortex-a76-promises\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Huawei Server Efforts: Hi1620 and Arm’s Big Server Core, Ares\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2018-11-20T16:00:00Z\n",
      "URL: https://www.anandtech.com/show/13620/huawei-server-efforts-hi1620-and-arms-big-server-core-ares\n",
      "Content: For at least four years now, Arm has been pushing its efforts to be a big part of the modern day server, the modern day data center, and in the cloud as a true enterprise player. Arm cores are found in plenty of places in the server world, with big deployments for its smartphone focused Cortex core family in big chips. However, for those same four years, we have been requesting a high-performance core, to compete in single threaded workloads with x86. That core is Ares, due out in 2019, and while Arm hasn’t officially lifted the lid on the details yet, Huawei has already announced it has hardware with Ares cores at its center.Huawei Is A BIG CompanyNormally at AnandTech when we discuss Huawei, it is in the context of smartphones and devices such as the Mate 20, or smartphone chips like the Kirin family. These both fall under Huawei’s ‘Consumer Business Group’, which accounts for just under half of the company’s revenue. One of Huawei’s other groups is its Enterprise wing, which is almost as big, and it creates a lot of custom hardware and silicon using its in-house design team, HiSilicon. HiSilicon’s remit goes all the way from smartphones to modems to SSD controllers to PCIe controllers and also high-performance enterprise compute processors....And It Makes Server CPUsLast month, Huawei’s Enterprise Group lifted the lid on its fourth generation data center processor. Part of the TaiShan family, the Hi1620 would follow hardware such as the Hi1616 in being built using Arm IP. The new Hi1620 was announced as the world’s first 7nm processor for the data center, with the Ares cores being what would drive high-performance for its deployments.While Huawei didn’t have any Hi1620 at the show, it was promoting the fact that it will be a cornerstone in its portfolio, and lifted the lid on a number of key parts of the chip.Huawei Hi16xx FamilyHi1620Hi1616Hi1612Hi1610Announced2018201720162015Cores24 to 64323216ArchitectureAresCortex-A72Cortex-A57Cortex-A57Frequency (GHz)2.4 to 3.02.4 GHz2.1 GHz2.1 GHzL164 KB L1-I64 KB L1-D48 KB L1-I32 KB L1-D48 KB L1-I32 KB L1-D48 KB L1-I32 KB L1-DL2512 KB Private1MB/4 cores1MB/4 cores1MB/4 coresL31MB/core Shared32MB CCN32MB CCN16MB CCNMemory8x DDR4-32004x DDR4-24004x DDR4-21332x DDR4-1866InterconnectUp to 4S240 Gbps/portUp to 2S96 Gbps/port??IO40 PCIe 4.02 x 100 GE46 PCIe 3.08 x 10GE16 PCIe 3.016 PCIe 3.0ProcessTSMC 7nmTSMC 16nmTSMC 16nmTSMC 16nmPower100 to 200 W85W??The new Hi1620 will feature 24-64 cores per socket, running from 2.4-3.0 GHz. Each of these cores will have a 64KB L1-Data cache and a 64 KB L1-Instruction cache, with 512KB of private L2 cache per core. L3 would run at 1MB/core of shared cache, up to 64MB. On a scale of a consumer Skylake core, that means more L2 cache per core, but less L3. No word on associativity, however. One of the key question marks is on performance: a lot of vendors are hoping for an Arm core with Skylake-levels of raw performance.Memory is set at 8 channels up to DDR4-3200, and the chip will support a multi-socket configuration up to 4S, with the coherent SMP interface capable of 240 GB/s for each chip-to-chip communication. The 4S layout would be a fully connected design.IO for the Hi1620 is set at 40 PCIe 4.0 lanes, which is less than the 46 lanes on the Hi1616, but those ones were rated for PCIe 3.0. The Hi1620 will also have CCIX support, as well as dual 100GbE MACs, some USB 3.0, and some SAS connectivity.The package listed is 60x75 mm BGA, which gives no real indication to the chip inside. But that’s a lot of balls on the back, and the package is larger than the 57.5x57.5 mm design from the last generation. Huawei states that the Hi1620 will be offered in TDP ranges from 100W to 200W, with the varying core count, but chips will be offered that can be fine-tuned for memory bound workloads.There are still plenty of unanswered questions, such as the interconnect, but we really want to get to grips with the microarchitecture of Ares to see what is under the hood. A number of journalists at the show were predicting that Arm should be having an event in the first half of 2019 to lift the lid on the design of the core.Related ReadingJust When You Thought It Was Dead: Qualcomm Centriq Arm Server Systems SpottedAssessing Cavium's ThunderX2: The Arm Server Dream Realized At LastAppliedMicro's X-Gene 3 SoC Begins Sampling: A Step in ARM's 2017 Server AmbitionsInvestigating Cavium's ThunderX: The First ARM Server SoC With Ambition\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/13620/huawei-server-efforts-hi1620-and-arms-big-server-core-ares\n",
      "Title: Just When You Thought It Was Dead: Qualcomm Centriq Arm Server Systems Spotted\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2018-11-13T20:00:00Z\n",
      "URL: https://www.anandtech.com/show/13597/just-when-you-thought-it-was-dead-qualcomm-centriq-arm-server-systems-spotted\n",
      "Content: The story of Qualcomm’s Arm server product line has been full of ups and downs. On the up, last year, we saw a large disclosure on how the new Falkor core design was put together, and Qualcomm even officially launched CPU pricing. It was going to be another push for Arm cores into the datacenter. Everything looked to be on an upwards path, until early 2018, when Qualcomm was looking to consolidate some of its divisions, and it was noted that a number of key members of the Centriq team had left. The official line is that Qualcomm was still going to support the product family, however any roadmap seemed to disappear.We weren’t expecting to see any Qualcomm servers at the Supercomputing show this week. However when casually walking by the GIGABYTE server booth, they had one of the early PVT samples ready to show for customers.On display was a H221-Q20 2-node 2U server using QDF2400 (Centriq) processors. Each processor is backed with 12-channel DDR4, which takes up a large portion of the motherboard. Other available parts include 4x1Gb/s, two management ports, twelve 3.5-inch hot swap bays, four M.2 slots for SATA drives, two PCIe low profile slots, four full PCIe slots, two OCP Mezzanine slots, a BMC, and 1200W Platinum redundant PSUs.The system is still in the early stage of development. We were told to come back in the middle of next year for systems that would be ready to go.Given that Qualcomm officially launched Centriq on November 10th2017, that would mark 18 months after launch when GIGABYTE will have systems ready. Interestingly we were told that only GIGABYTE is the only manufacturer who is allowed to sell these systems into the channel, with everyone else being an OEM.We’ve asked for one, when they’re ready to share.Related ReadingQualcomm Launches 48-core Centriq for $1995: Arm Servers for Cloud Native ApplicationsAnalyzing Falkor’s Microarchitecture: A Deep Dive into Qualcomm’s Centriq 2400 for Windows Server and LinuxQualcomm’s Server Lead, Anand Chandrasekher, Leaves Company\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/13597/just-when-you-thought-it-was-dead-qualcomm-centriq-arm-server-systems-spotted\n",
      "Title: Arm Announces Neoverse Infrastructure IP Branding & Future Roadmap\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2018-10-16T17:30:00Z\n",
      "URL: https://www.anandtech.com/show/13475/arm-announces-neoverse-infrastructure-ip-branding-future-roadmap\n",
      "Content: Among of the first announcements coming out of Arm’s TechCon convention in San Jose, is the unveiling of Arm’s new infrastructure branding and a sneak peek at the product roadmap for the next 3 years.Up till today, the Arm’s IP portfolio for the infrastructure market didn’t differentiate itself much from the regular consumer IP. Today this now changes, as Arm tries to convey its dedicated focus in this market. The new IP portfolio which will see broader announcements over the coming months and years, is now dubbed “Arm Neoverse”.The Neoverse branding is supposed to be live alongside the usual consumer device oriented Cortex IP branding, meaning that at some point we’ll see a new Neoverse CPU announced whose use-cases are meant to be in the infrastructure space.Arm showcases that over the last several years, they’ve been able to capture a significant amount of market share in infrastructure devices. What infrastructure devices actually means, is any kind of non-end user device, such as networking equipment, going from switches, base stations, gateways, router, and most importantly also servers. Here the various Arm vendors have reportedly gained up to a 28% market share.The biggest surprise today was that Arm again has publicly published the roadmap for the next several years, revealing the codenames of the infrastructure focused CPU and platform IPs. As such, we finally see the Ares, Zeus, and Poseidon codenames acknowledged. The CPU IPs based on these codenames should be the sever-oriented counterparts toEnyo/A76, Deimos and Hercules, the latter of which would remain under the Cortex branding. What the differences between the Cortex and Neoverse cores are is still unclear, but among the many possibilities, it’s not hard to imagine that we’ll be seeingSVEimplemented in the server IP first.Arm is also seemingly pushing the performance projections, and promises a 30% jump in performance for each generation over the coming years. This figure is more aggressive than the20 to 25% quoted CAGRfor the Cortex based CPUs in consumer devices, so we’ll likely see more differentiation in the infrastructure IPs.Arm went on to discuss more the various scaling possibilities that vendors can achieve by using Arm’s IP, varying from more simple configurations in edge devices up to very wide implementations for server CPUs, advertising the possibility of 48, 64 or 96 cores per socket in the maximum configuration.The one slide that really caught my eye and Arm notoriously went over quick was in regards to the offering for architecture partners. Here Arm wants to again communicate that it’s able to provide customised IP on demand, and alter things such as the interfaces to the cores, their memory hierarchy as well as the SVE units’ width and depth. Among these items is also “threading architecture”. While I’m not keen on guessing Arm’s intents here, I do wonder if this means we’ll be seeing SMT implemented in Arm’s Neoverse IPs?While today’s announcement didn’t have any major technical unveiling, it does put to rest the question how Arm is going to name its new server cores, as well as publicly acknowledging roadmap names and goals for the coming years. Arm promised more details and announcements at TechCon – and we’ll be sure to report on the happenings in the comings days.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/13475/arm-announces-neoverse-infrastructure-ip-branding-future-roadmap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm TechCon 2018 Keynote Live Blog (Starts at 1pm ET)\n",
      "Author: Ryan Smith\n",
      "Date Published: 2018-10-16T16:55:00Z\n",
      "URL: https://www.anandtech.com/show/13488/arm-techcon-2018-keynote-live-blog\n",
      "Content: 01:05PM EDT- We're here at the San Jose Convention Center for Arm's annual developer conference and tech showcase, TechCon. Arm of course needs no introduction, and while the company is an IP provider rather than a hardware manufacturer, the net result is that they have their finger in everything from servers to embedded devices. Which has presented the company with a lot of growth opportunities, but also no shortage of competition as everyone works to grab a piece of these markets.01:06PM EDT- The plan is for 3 mini-keynotes this morning. Starting with a keynote called The Fifth Wave of Computing01:07PM EDT- Presented of course by Arm CEO Simon Segars01:07PM EDT- Simon is starting us off with a history lesson of Charles Babbage and Ada Lovelace01:10PM EDT- Babbage & Lovelace never saw the results of their efforts, but it did kick off a computing industry01:10PM EDT- Mainframe to PCs to mobile & beyond. What comes next is the fifth wave01:11PM EDT- The data-driven computing era01:11PM EDT- Where there is a computer in every thing01:12PM EDT- It's also no longer about the performance of a single system, but about the collective performance of all systems01:14PM EDT- Now talking about a use case of micro-financing, and the kind of security required01:15PM EDT- Getting to the fifth wave will not be without challenges though. If it's about network computing, then you need a reliable network that can handle many devices01:15PM EDT- Enter 5G01:16PM EDT- 5G will be faster, of course. But it will also handle large numbers of devices a lot better01:17PM EDT- And what are the business opportunies that might arise from having such a capable network?01:18PM EDT- Citing what's already been done with 2/3/4G, from American Idol to Netflix to Uber01:18PM EDT- \"5G is going to unleash creativity and opportunity for a lot of people\"01:19PM EDT- Arm wants to get to 1 trillion connected devices01:20PM EDT- (This would be over 100 deivces/person)01:20PM EDT- And these connected devices will need to be intelligent01:24PM EDT- Now discussing a use case: pairing with a UK Children's Hospital to use smart edge devices to better control access and notify staff if someone is there who shouldn't be01:26PM EDT- Next use case: cars01:26PM EDT- Autonomous cars will be data centers on wheels. Generating many TBs per hour of data01:27PM EDT- This is too much to upload elsewhere. It has to be processed at the edge, in the car01:28PM EDT- (The slide transitions are at 60fps. nice!)01:29PM EDT- Meanwhile servers/the cloud have to evolve as well01:29PM EDT- Efficient. Low cost. Distributed.01:30PM EDT- We're still very early in the days of IoT. But Arm thinks it will evolve quickly01:30PM EDT- \"What about security?\"01:31PM EDT- The tech sector needs to be trusted01:32PM EDT- Talking about everyone's favorite exploits: Spectre and Meltdown01:33PM EDT- Arm sees it as part of their mission to get everyone involved in making security better01:34PM EDT- This is a challenge. How do you get skeptical devs on board? How do you communicate this to consumers and get them to pay for it?01:36PM EDT- It will take collaboration and a complete ecosystem to make the Fifth Wave happen01:38PM EDT- On to the next section, delivered by Marcelo Claure, COO of Softbank01:39PM EDT- Talking about the growth of the IoT market01:41PM EDT- 86% of IoT today is used to adhere to regulations or reduce costs01:41PM EDT- So only 14% is being used to grow revenues01:41PM EDT- But SoftBank expects IoT to start becoming disruptive01:43PM EDT- Now rolling a video of a self-driving car on the streets of San Francisco01:45PM EDT- Comparing and contrasting reactive vs. disruptive IoT01:45PM EDT- Smart power meters versus smart thermometers, etc01:46PM EDT- Home security systems with cameras and facial recognition01:48PM EDT- SoftBank of course is also merging T-Mobile with Sprint. So they also have a hand in making the underlying 5G network for IoT a reality01:48PM EDT- Planning to invest 40B dollars in the next few years on their 5G network01:49PM EDT- Now on to SoftBank's vision for AI01:51PM EDT- (This is all very high level/conceptual)01:51PM EDT- SoftBank expects an AI revolution, and that it will occur much faster than any prior revolution01:52PM EDT- And machines will have more processing capabilities than humans01:54PM EDT- Now going into a hypothetical future day about how various SoftBank companies are using AI, and how it's helping to meet consumer needs01:55PM EDT- Uber user load predictions, facial recognition to help with medical diagnosis, etc01:58PM EDT- IoT is going to make life better for dogs as well02:02PM EDT- (SoftBank's vision is very ambitious. Though this is starting to veer into an investment pitch)02:03PM EDT- Ultimately everything SoftBank wants to do will require the IoT hardware to power it, and that is where Arm comes in02:04PM EDT- And now on to part 3. Drew Henry, Arm's SVP & GM of their infrastructure business02:05PM EDT- \"Preparing the Cloud for the Fifth Wave of Computing\"02:05PM EDT- How will Arm evolve the cloud to support SoftBank's vision?02:06PM EDT- Arm believes they have a unique understanding of what all of this will require, due to their current businesses and customers02:07PM EDT- Starting with imaging sensors02:08PM EDT- 1B HD image sensors will generate an enormous amount of data02:09PM EDT- All of this would also require more than 40M servers. More than exist today02:11PM EDT- Comparing the number of wafers used for various processor architectures02:11PM EDT- \"Other\" in this case is x8602:12PM EDT- Arm, for obvious reasons, sees themselves as a leader02:13PM EDT- Arm is probably better known here for smartphones and other consumer devices. But they are also a major player in the infrastructure market. And proud of it02:16PM EDT- The compute and networking model is changing. It's no longer about sending content to client devices, but having data come in from the world and in to the servers02:19PM EDT- Arm believes that datacenters are spending too many CPU cycles - and thus money - on networking operations02:20PM EDT- Arm of course has a solution, with their Cosmos hardware02:21PM EDT- Arm doing the network processing. Arm doing the storage processing02:21PM EDT- Arm is announcing a new initiative: Neoverse02:22PM EDT- Cloud-to-edge infrastructure technologies02:24PM EDT- Cosmos, Ares, Zeus, Poseidon02:24PM EDT- All built on leading-edge nodes02:24PM EDT- 30% per generation perf improvement02:25PM EDT- And Arm being an IP provider, it will be up to partners to put this together and figure out what products the market needs02:27PM EDT- Already putting together an ecosystem of silicon and service providers02:28PM EDT- And of course, software support from all the usual suspects02:30PM EDT- Neoverse for infrastructure, Cortex for edge devices02:32PM EDT- Now talking about conference sessions over the next few days and how participants can get involved in Neoverse02:32PM EDT- And that's a wrap. Thanks for joining us.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/13488/arm-techcon-2018-keynote-live-blog\n",
      "Title: Arm Unveils Arm Safety Ready Initiative, Cortex-A76AE Processor\n",
      "Author: Anton Shilov\n",
      "Date Published: 2018-09-26T20:00:00Z\n",
      "URL: https://www.anandtech.com/show/13398/arm-unveils-arm-safety-ready-initiative-cortexa76ae-processor\n",
      "Content: The market of automobiles is changing. Modern cars use more electronics than ever and adoption of electronic components in general and processors in particular is not going to slow down. All major automakers are working on self-driving vehicles, which means that cars of the future will need even more sophisticated SoCs.As demand for components needed for autonomous cars is about to explode in the coming years, it is not surprising that more companies start to develop solutions specifically designed for such vehicles. Arm on Wednesday launched its new Arm Safety Ready Program that is aimed to develop solutions for self-driving cars. In addition, the company launched its Cortex-A76AE, its first processor IP designed specifically for autonomous vehicles.Arm Safety Ready ProgramArm is clearly not a newcomer to the automotive market. The company’s general-purpose and real-time cores have been used by makers of various vehicles since 1996. Nowadays Arm’s IP is used for ADAS (collision avoidance, cruise control, etc.), connectivity, infotainment, powertrain control, and other components of the cars.Meanwhile, up until lately Arm supplied automakers its IP originally developed for various devices in general. By contrast, the Arm Safety Ready Program is a multi-year program under which the company will develop Automotive Enhanced (AE) custom and semi-custom solutions for autonomous cars. Initially, Arm will start with solutions for Level 3 self-driving vehicles, but over time it will offer products built for Level 4 and Level 5 autonomous cars sometimes in 2020 and beyond.The Arm Safety Ready Program spans the company’s entire portfolio of products and will include certified physical IP blocks to be made using specific process technologies, ISO 26262-certified software tools and components, safety documentation, and so on.Right now, Arm only talks about its Cortex-A76AE processor for self-driving cars, but the grand plan includes Automotive Enhanced processors based on the Helios and the Hercules microarchitectures. Besides, Arm intends to offer AE versions of its future Cortex-R cores sometimes in 2020 and beyond. While Arm is developing AE flavors of its future IP, it will keep offering its existing cores (e.g., Cortex-A72, Cortex-R5, Cortex-R52, Cortex A53, Cortex-M4, Cortex-M7, Cortex-M44, etc.) to developers of SoCs for automotive market.The implementation of the ASRP will enable makers of cars to obtain IP that will make their systems for autonomous driving significantly more energy efficient and cheaper, which will make self-driving vehicles more affordable in general. Besides making the said systems cheaper, Arm Safety Ready Program also promises to speed up their development.Arm’s Cortex-A76AE: The First Member of the AE FamilyThe first product that is a part of Arm’s Safety Initiative is the company’s Cortex-A76AE processor with integrated redundancy. As the model number suggests, a Cortex-A76AE compute complex relies on up to 16 Cortex-A76 cores that support all the RAS (reliability, availability, serviceability) capabilities featured by the Arm v8.2 microarchitecture, and work in work in Split-Lock mode to ensure reliability.Actual SoCs based on the Cortex-A76AE can scale to up to 64 cores. Besides general-purpose compute cores, Arm’s autonomous-class compute complexes also integrate Mali-G76 graphics cores, ARM’s ML cores, and other necessary IP. Besides, the complexes are set to support Arm’s memory virtualization and protection technologies required for flawless operation of ML and NN accelerators.According to Arm, a 30-Watt 16-core Cortex-A76AE SoC implementation made using TSMC’s 7nm process technology has performance of over 250 KDMIPS, which is enough for today’s applications. If a customer wants a higher performance, it may build into more cores, or even use more than one SoC.Raw performance is what actually matters for self-driving vehicles. Modern Level 3 autonomous cars run multiple programs at once and that is not going to change anytime soon. According to Arm, software for a Level 5 self-driving auto will contain 1 billion lines of code. By contrast, software used to run a Boeing 787 Dreamliner contains 14 million lines of code.Arm’s Split-Lock: Redundant Computing in HardwareNow, time to talk about the key feature of Arm’s Cortex-A76AE — the Split-Lock technology. The Split-Lock feature enables SoC developers to use the cores in two modes: the Split Mode to runs the cores independently and achieves higher performance, and in the Lock Mode a core is run in lockstep with a paired core, running the same code and monitoring for any kind of divergences that would be then reported as an error, and failure recovery mechanisms would take over (or at least a driver will be notified).The Lock Mode somehow resembles how HP’s NonStop works for mission-critical applications, but the key difference is that Arm’s solution relies completely on hardware and is therefore compatible with any software (think AutoWare, Deepscale, Linaro, Linux, QNX, etc.).Arm proposes to use the locked clusters for ASIL-D application that are critically important for safety. By contrast split clusters are intended for ASIL-B apps like infotainment. Given the flexibility of Arm’s hardware-only approach, the Split-Lock can be used by any automaker to run almost any software while ensuring either performance and error-free computing.Related Reading:Arm's Cortex-A76 CPU Unveiled: Taking Aim at the Top for 7nmArm Unveils Client CPU Performance Roadmap Through 2020 - Taking Intel Head OnArm and Samsung Extend Artisan POP IP Collaboration to 7LPP and 5LPE NodesHot Chips 2018: Arm's Machine Learning Core Live BlogGallery:Arm Unveils Arm Safety Ready Initiative, Cortex-A76AE Processor\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/13398/arm-unveils-arm-safety-ready-initiative-cortexa76ae-processor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Hot Chips 2018: Fujitsu's A64FX Arm Core Live Blog\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2018-08-22T00:25:00Z\n",
      "URL: https://www.anandtech.com/show/13258/hot-chips-2018-fujitsu-afx64-arm-core-live-blog\n",
      "Content: 08:30PM EDT- Remember back when Arm announced Scalable Vector Extensions? Well Fujitsu has made an Arm CPU that uses it with a 512-bit width. The presentation looks super interesting, so follow along with our live blog. The talk is set to start at 5:30pm PT / 12:30am UTC.08:32PM EDT- Last time we were here, had a 3-min presentation about Post-K08:32PM EDT- Called A64FX08:32PM EDT- First chip to use Arm SVE08:32PM EDT- Scalable Vector Extensions08:33PM EDT- New microarch maximises SVE perf08:33PM EDT- Fujitsu has been making processors for 60 years08:34PM EDT- SPARC? Remember that?08:34PM EDT- UNIX, HPC, Mainframe, now HPC + AI08:34PM EDT- New CPU inherits DNA from Fujitsu08:34PM EDT- Reliability, speed, flexibility, high perf/watt08:34PM EDT- end up with CPU w/ extremely high throughput08:35PM EDT- low power08:35PM EDT- (A64FX doesn't mean Athlon 64, FX)08:35PM EDT- Optimized for massively parallel08:35PM EDT- Four features08:35PM EDT- Perf: FP64 through to INT808:36PM EDT- Throughput: 512-bit SIMD x 2 pipes/core, HBM2, 48-cores, Tofu interconnect08:36PM EDT- Efficiency: GEMM and Triad perf08:37PM EDT- Standards: Arm v8.2 + SVE + SBSA level 3 (Server Base System Architecture)08:37PM EDT- AArch64 only, no 3208:37PM EDT- 48 computing cores and 4 identical assistant cores08:37PM EDT- 32GiB HBM208:38PM EDT- 6D Mesh - 28 Gbps x 2 lanes x 10 ports08:38PM EDT- PCIe 3.0 x1608:38PM EDT- Built on 7nm FinFET08:38PM EDT- 8.786B transistors, but only 594 pin08:38PM EDT- 2.7 TFLOPS08:38PM EDT- 1TB/s memory bandwidth08:39PM EDT- ISA feature support08:39PM EDT- Optimized SVE for wide range of applications08:39PM EDT- INT8 Dot Product08:39PM EDT- Enhanced compression08:39PM EDT- AI applications08:40PM EDT- HW Barrier and Sector cache - implementation defined system registers from AArch6408:40PM EDT- Enahnced blocks in chip08:40PM EDT- Predicated operations dedicated pipe08:41PM EDT- SVE has limitation on operands - FMA equivalent requires destructive 3-operand FMA308:41PM EDT- MOVPRFX instruction08:41PM EDT- hides overhead of main pipelin08:42PM EDT- 21.6 TOPS for INT8 dot product08:42PM EDT- 90% execution efficiency08:42PM EDT- Still 2x in 64-bit DGEMM over SPARC64 PrimeHPC FX10008:43PM EDT- Almost 20x the K comp in DGEMM08:43PM EDT- L1 cache is key to design for 512-bit SIMD08:43PM EDT- Combined Gather mechanism to increase throughput08:44PM EDT- Combined Gather enables return up to two consecutive elements in a 128-byte aligned block08:44PM EDT- Throughput per core is 32 bytes/cycle08:44PM EDT- Full chip is Divided into four memory groups08:45PM EDT- One CMG is 13 cores, an L2 cache, and a memory controller08:45PM EDT- One core handles Daemon/IO08:45PM EDT- Cache coherency by ccNUMA with on-chip directory08:45PM EDT- X-bar connection for L2 cache efficiency08:45PM EDT- Process binding ensures scaling08:45PM EDT- Wide Ring Bus for IO across whole chip08:46PM EDT- Bandwidth in cache and memory is key08:46PM EDT- Out-of-order mechanisms in cores, caches, and IMCs08:46PM EDT- L1 cache at 11.0 TB/s08:46PM EDT- L2 cache is 3.6 TB/s08:47PM EDT- Normalized compared to previous processor, perf is 2x across wide range of workloads08:48PM EDT- For AI, convolution low precision is 9.4x using INT8 dot product08:48PM EDT- Each chip has energy monitor in msec08:49PM EDT- Each core has energy analyzer in nanosec08:49PM EDT- Fine grained power analysis of a core, an L2 cache and memory08:49PM EDT- Power Knob for optimization08:49PM EDT- Can change hardware config for power08:50PM EDT- Change decode width, floating point pipeline, and general frequency reduction08:50PM EDT- Extensive RAS08:50PM EDT- ECC on all caches08:50PM EDT- Parity cehc on execution units08:50PM EDT- 128400 error checkers08:50PM EDT- Parity Check* on execution units08:51PM EDT- Hardware instruction retry08:52PM EDT- Software stacks developed by RIKEN and Fujitsu08:52PM EDT- Will continue to use Arm in the future08:52PM EDT- Work with partners08:53PM EDT- Q&A time08:54PM EDT- Q: When can you reach exascale? A: The Post-K system will be available in 2021. 100x perf from K-comp. But exa-scale not answerable08:55PM EDT- Q: nanosecond level power monitoring - what techniques do you use? A: Activity based on coefficient based on operations08:56PM EDT- Q: Support 64-bit FP, not 128-bit? A: No.08:59PM EDT- That's a wrap. Next talk is on the NEC Vector processor:https://www.anandtech.com/show/13259/hot-chips-2018-nec-vector-processor-live-blog\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/13258/hot-chips-2018-fujitsu-afx64-arm-core-live-blog\n",
      "Title: Hot Chips 2018: Arm's Machine Learning Core Live Blog\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2018-08-21T20:55:00Z\n",
      "URL: https://www.anandtech.com/show/13253/hot-chips-2018-arm-machine-learning-core-live-blog\n",
      "Content: 04:57PM EDT- Arm officially announced Project Trillium earlier this year, as a way to bring machine learning to the Arm ecosystem. As part of the trade show today, we have a presentation where Arm is going to talk about its first machine learning core. The talk is set to start at 2pm PT / 9pm UTC.05:01PM EDT- Combined effort of large team05:01PM EDT- 150+ people05:02PM EDT- Arm works with partners across many segments. Without fail, all those markets want machine learning acceleration05:02PM EDT- Need to come up with a fundamental tech that can be used for many different segments05:03PM EDT- ML processor focused for Neural Networks05:03PM EDT- Optimized ground up architecture05:03PM EDT- open source stack for easy deployment05:03PM EDT- First design targets mobile with derivatives for additional segments05:05PM EDT- Main component is the Machine Learning Processor. Biggest block is SRAM05:05PM EDT- 16 compute engines per ML Proc05:05PM EDT- 4 TOP/s of convolution at 1 GHz05:05PM EDT- Targeting 3 TOP/W in 7nm and 2.5mm205:06PM EDT- INT8 support, support for NNAPI05:06PM EDT- Optimized for CNN/RNN05:06PM EDT- Support 16-bit, in case pixels are coming05:06PM EDT- 8-bit is 4x perf over 16-bit05:07PM EDT- Need to get four things right05:08PM EDT- Static Scheduling, Efficient Convolutions05:08PM EDT- CNNs are static analyzable. So you can optimize for deterministic data with stuff laid out in memory ahead of time05:08PM EDT- Creates command stream for different parts in the processor05:09PM EDT- Control Unit takes command stream05:09PM EDT- No caches, simplified flow control. Simplified hardware (co-design with compiler). Predictable performance05:10PM EDT- Compiler needs to know the details of the core, e.g. SRAM size05:11PM EDT- Underlying requirement of high-throughput dot products are worth optimizing for05:12PM EDT- Output feature maps are interleaved across compute engines05:12PM EDT- So 16 engines = 16 output feature maps05:12PM EDT- MAC Engine: 8 x 16-bit wide dot products per cycle05:13PM EDT- Each engine is 2*8*16 = 256 ops/cycle05:13PM EDT- 32b accumulators, 4.1 TOPs @ 1 GHz05:13PM EDT- Gate for zeros to save power - 50% power reductions05:13PM EDT- Keep inputs to datapaths stable to save energy05:14PM EDT- Also have broadcast network05:14PM EDT- Creates tensor block of activations that is broadcast to all the MAC engines05:15PM EDT- when final result the 32b values are scaled back to 8b and sent to the programmable layer engine05:15PM EDT- POP IP for Mac Engine for 16nm and 7nm05:16PM EDT- Custom physical design with 40% area reduction and 10-20% power improvements05:16PM EDT- DRAM power can be up to 50% without bandwidth saving05:16PM EDT- use weight compression, activation compression, and tiling, to save DRAM power05:17PM EDT- Spend dedicated silicon on compression is essential05:18PM EDT- Histogram of 8x8 blocks in Inception V3 shows key ML matrixes05:18PM EDT- Develop lossless compression for the popular 8x8 blocks05:19PM EDT- 3.3x compression over standard implementation05:19PM EDT- When training, many training weights taper down to zero05:19PM EDT- Can force the weights to zero with no change in accuracy but offers better compression05:20PM EDT- This allows for weight compression and 'pruning'05:20PM EDT- Support a compression format to get the zeroes out05:20PM EDT- Take the remaining non-zero can be clamped / normalized05:20PM EDT- All happens in hardware05:21PM EDT- Weights stay compressed until read from internal SRAM05:21PM EDT- Very model dependent05:21PM EDT- Tiling05:22PM EDT- Tricks that the compiler can use to reduce bandwidth by using the SRAM05:22PM EDT- Possible due to static scheduling05:23PM EDT- State of the art in tiling is still evolving05:23PM EDT- Programmable Layer Engine helps with future-proofing05:24PM EDT- Use Arm M-class CPU and extended it with vector extensions for neural networks05:24PM EDT- Engine can manipulate SRAM data05:25PM EDT- Designed to scale in engines05:25PM EDT- Designed to scale with MAC engine throughput05:25PM EDT- Designed to scale with Machine Learning Processors for multiple workloads05:26PM EDT- Q&A time05:27PM EDT- Q: If there's one M-series core in the engine, is there 16 per MLP? A: Yes05:28PM EDT- Q: What about lower than 8-bit? A: Industry momentum is around 8-bit. We're looking into lower bit levels, but that could be in the future05:28PM EDT- Q: When will it be available for licencing ? A: Later this year05:29PM EDT- That's a wrap. Come back at 3pm for a talk on Tachyum\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/13253/hot-chips-2018-arm-machine-learning-core-live-blog\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Unveils Client CPU Performance Roadmap Through 2020 - Taking Intel Head On\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2018-08-16T13:05:00Z\n",
      "URL: https://www.anandtech.com/show/13226/arm-unveils-client-cpu-performance-roadmap\n",
      "Content: Today’s announcement is an oddball one for Arm as we see the first-ever public forward looking CPU IP roadmap detailing performance and power projections for the next two generations through to 2020.Back in May we extensively coveredArm’s next generation Cortex A76 CPUIP and how it’s meant to be a game-changer in terms of providing one of the biggest generational performance jumps in the company’s recent history. The narrative in particular focused on how the A76 now brought real competition and viable alternatives to the x86 market and in particular how it would be able to offer performance equivalent to Intel’s best mobile offerings, at much lower power.Arm sees always-connected devices with 5G connectivity as a prime opportunity for a shift in the laptop market. Qualcomm’s recent Snapdragon 835 andSnapdragon 850platforms were the first attempts in trying to establish this new slice for Arm-based PCs.Today’s roadmap now publicly discloses the codenames of the next two generations of CPU cores following the A76 – Deimos and Hercules. Both future cores are based on the new A76 micro-architecture and will introduce respective evolutionary refinements and incremental updates for the Austin cores.The A76 being a 2018 product – and we should be hearing more on the first commercial devices on 7nm towards the end of the year and coming months, Deimos is its 2019 successor aiming at more wide-spread 7nm adoption. Hercules is said to be the next iteration of the microarchitecture for 2020 products and the first 5nm implementations. This is as far as Arm is willing to project in the future for today’s disclosure, as the Sophia team is working on the next big microarchitecture push, which I suspect will be the successor to Hercules in 2021.Part of today’s announcement is Arm’s reiteration of the performance and power goals of the A76 against competing platforms from Intel. The measurement metric today was the performance of a SPECint2006 Speed run under Linux while complied under GCC7. The power metrics represent the whole SoC “TDP”, meaning CPU, interconnect and memory controllers – essentially the active platform power much in a similar way we’ve been representing smartphone mobile power in recent mobile deep-dive articles.Here a Cortex A76 based system running at up to 3GHz is said to match the single-thread performance of an Intel Core i5-7300U running at its maximum 3.5GHz turbo operating speed, all while doing it within a TDP of less than 5W, versus “15W” for the Intel system. I’m not too happy with the power presentation done here by Arm as we kind of have an apples-and-oranges comparison; the Arm estimates here are meant to represent actual power consumption under the single-threaded SPEC workload while the Intel figures are the official TDP figures of the SKU – which obviously don’t directly apply to this scenario.We didn’t have internal data to verify Arm’s claims as of publishing of the article, but the 15W Intel figure is naturally on the high side, given that this just the official TDP representing multi-threaded workloads – a very quick test of CB15 ST power as reported by MSR registers on an 7200U at 3.1GHz measured 9.3W package+DRAM power while an 8250U at 3.35GHz came in at 11W. I haven’t correlated SPEC power on x86 to date, but I’m expecting it on average to be less than CB15. Even if the 15W figure for the 7300U is correct, and I’m expecting something more in the range of 9-11W, Arm might be using one of Intel’s notably less efficient performance points when doing the comparison for these SKUs. Of course this doesn’t invalidate the data as efficiency for the A76 at those frequencies would also not be optimal, it’s just something to keep in mind.It’s also interesting to see Arm scale back on the performance comparison as they’re using a 3GHz A76 as the comparison data-point – this is in contrastto the 3.3GHz maximum 5W performance pointpresented during TechDay. I had tried to estimate the A76’s power in mobile form-factors based on the different metrics Arm disclosed and came atan estimated 2.3W at 3GHz. Naturally Arm says “less than 5W” and they could be erring on the safe side of not over-promising – but if it had been *that* much lower, as in my estimate, we would have maybe seen even more aggressive marketing figures. In the end, until we get the first A76 devices in our hands, we won’t know for sure what the exact figures will be and at which point on the efficiency curve Arm’s projected 3GHz performance figures will end up at.The last slide that is notable to talk about is the performance projections for Deimos and Hercules. Here Arm’s taking a direct stab at Intel’s lack of significant progress over the last few years and reiterating its confidence in the company’s ability in sustaining high CAGR (compound annual growth rate) performance figures for the next generations.Again at TechDay we quoted figures of 20-25% while today’s announcement contained a more conservative figures of “>=15%” – likely better representing a seemingly larger 20% projected boost for Deimos as well as what seems to be a 10% gain for the 5nm follow-up Hercules. Taking into account the relative positioning of the data-points in this chart, I did some quick correlation and it matchesmy initial estimated performance figuresfor a 3GHz A76 at around ~26 SPECint2006. Deimos and Hercules would come in at figures of ~31 and ~34 points.Finally today’s announcement is a marketing exercise attempting to emphasise Arm’s performance and power commitments over the next few generations, trying to showcase it has the strategy and technology in place to make the Arm laptop market a real growth opportunity. If and how this pans out is something that we won’t find out at least until later on in the year, with the first actual A76 based large form-factor designs not being a thing until at least sometime in 2019. We’re eagerly awaiting the first A76 based mobile designs in the months to come and to have a first hand-on evaluation of the new microarchitecture family.Related ReadingExploring DynamIQ and ARM’s New CPUs: Cortex-A75, Cortex-A55Arm's Cortex-A76 CPU Unveiled: Taking Aim at the Top for 7nmThe Samsung Exynos M3 - 6-wide Decode With 50%+ IPC IncreaseThe Samsung Galaxy S9 and S9+ Review: Exynos and Snapdragon at 960fps\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/13226/arm-unveils-client-cpu-performance-roadmap\n",
      "Title: Arm and Samsung Extend Artisan POP IP Collaboration to 7LPP and 5LPE Nodes\n",
      "Author: Anton Shilov\n",
      "Date Published: 2018-07-06T21:15:00Z\n",
      "URL: https://www.anandtech.com/show/13052/arm-and-samsung-extend-artisan-pop-ip-collaboration-to-7lpp-and-5lpe-nodes\n",
      "Content: Arm and Samsung Foundry this week announced plans to extend their collaboration to 7LPP and 5LPE process technologies. Under the terms of the agreement, Arm will offer Samsung Foundry customers pre-designedArtisan POPIP solutions ready to be integrated into various SoCs.One of the first Artisan POP physical IP building blocks that Arm will offer for Samsung’s 7LPP and 5LPE platforms will be the Arm Cortex-A76 high-performance processor core. The two companies expect the core to run at 3 GHz or more, which is potentially higher than the target clock rate of 3 GHzannouncedby Arm earlier this year. The developers do not specify how high the final frequencies will be for the 7LPP and the 5LPE manufacturing processes, but keeping in mind that the latter does not explicitlypromiseto enable higher clocks when compared to the former, it is likely that the Cortex-A76 will run at about the same frequency in both cases.In addition to the high-performance Cortex-A76, Arm plans to offer physical implementations of its “latest processor cores featuring Arm DynamIQ technology” for the 7LPP and the 5LPE fabrication processes. The two companies do not name the processor cores, but considering the fact that we are talking about DynamIQ-capable cores, the likely candidates are the low-power Cortex-A55 as well as the high-performance Cortex-A75 (the A75 may not necessarily be popular in the 7LPP/5LPE era).The Arm Artisan physical IP offerings for Samsung’s 7LPP and 5LPE will also include high-density logic architecture, 1.8 V and 3.3 V GPIO libraries, as well as a suite of memory compilers. Meanwhile Samsung reiterated plans to offer a comprehensive suite of IP blocks for its 7LPP technology “by the first half of 2019.”Arm’s Artisan Physical IP blocks for Samsung’s 7LPP and 5LPE manufacturing technologies will be used by designers of various SoCs who do not need custom processing cores from Arm. Expect the first fruits of today’s agreement to reach the market sometimes in 2020.Related Reading:Arm's Cortex-A76 CPU Unveiled: Taking Aim at the Top for 7nmSamsung Foundry Roadmap: EUV-Based 7LPP for 2018, 3 nm IncomingSamsung’s 8LPP Process Technology Qualified, Ready for ProductionSamsung and TSMC Roadmaps: 8 and 6 nm Added, Looking at 22ULP and 12FFCTSMC Details 5 nm Process Tech: Aggressive Scaling, But Thin Power and Performance GainsIntel Delays Mass Production of 10 nm CPUs to 2019TSMC Kicks Off Volume Production of 7nm ChipsTSMC Starts to Build Fab 18: 5 nm, Volume Production in Early 2020\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/13052/arm-and-samsung-extend-artisan-pop-ip-collaboration-to-7lpp-and-5lpe-nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NGD Launches Catalina 2 Programmable SSDs: 16 TB - 32 TB, ARM A-53 Cores\n",
      "Author: Anton Shilov\n",
      "Date Published: 2018-07-05T22:00:00Z\n",
      "URL: https://www.anandtech.com/show/13047/ngd-launches-catalina-2-programmable-ssds\n",
      "Content: NGD Systems has announced its second-generation SSDs that feature compute capabilities. The Catalina 2 drives come in both 2.5-inch/U.2 and add-in-card form-factors, and are aimed at high-density servers for artificial intelligence (AI), machine learning (ML), and computer vision (CV) server applications. The drives feature 16 TB – 32 TB capacities depending on the form-factor and use 3D TLC NAND memory.On a high level, NGD’s Catalina 2 drives are NVMe 1.3-compliant SSDs featuring a PCIe 3.0 x4 interface and based on Toshiba’s BICS3 3D TLC NAND memory. It is noteworthy that NGD’s architecture supports any 3D TLC flash and is 3D QLC ready, so the manufacturer can adopt any type of flash and is not dependent on its supplier. In fact, the AIC versions of the drives rely on M.2 modules that can be switched at any time. U.2/2.5-inch Catalina SSDs will be available in 4 TB, 8 TB, and 16 TB configurations, whereas add-on-cards will support maximum capacity of 32 TB. Peak read/write performance of the Catalina 2 drives can be as high as 3.9 GB/s, but this is not the main selling point of these SSDs. As for power consumption, the U.2 is speced for 12 W (which is in-line with requirements of enterprise-grade 2.5-inch storage devices), while AIC consumes up to 13 W.Just like in case of the original Catalina drives launched in early 2017, the key features of the Catalina 2 are in-storage processing capabilities (In-Situ Processing), patented Elastic FTL (Flash Translation Layer) algorithm, and a strong LDPC-based ECC (since it is QLC ready, it must feature a strong ECC). The NGD Catalina 2 are based on a recent Xilinx FPGA featuring several general-purpose ARM Cortex-A53 cores along with programmable transistors, and flexible I/O capabilities. The FPGA runs a micro-OS based on Linux to perform in-storage processing along with flash management, and ECC tasks.Processing data on an SSD enables to perform relatively simple tasks (e.g., search, analyze, etc.) without moving massive amounts of data to host CPU or over a network, which greatly reduces loads on data buses and networks, therefore speeding up processing while reducing overall power consumption of a data center. Because workloads vary greatly, NGD does not like to talk about raw performance numbers of its drives because their value for the end clients does not necessarily depend on it.NGD Catalina 2 SSDsPCIe AICU.2 2.5-InchRaw Capacity4 TB8 TB16 TB32 TB4 TB8 TB16 TBInterfacePCIe 3.0 x4ProtocolNVMe 1.3ECCProprietary LDPCActive Power13 W12 WTemperature0 - 60 degrees CelsiusDimensions111.28 × 254 × 18.05 mm69.85 × 100.45 × 15mmWeight485 grams285 gramsWarranty3 YearsNGD’s Catalina 2 SSDs are already available to select customers who can tailor their apps to the drives.Related Reading:NGD Launches Catalina: a 24 TB PCIe 3.0 x4 SSD with 3D TLC NAND\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/13047/ngd-launches-catalina-2-programmable-ssds\n",
      "Title: ARM Announces Mali-V76 Video Processor: Planning For the 8K Video Future\n",
      "Author: Ryan Smith\n",
      "Date Published: 2018-05-31T19:05:00Z\n",
      "URL: https://www.anandtech.com/show/12835/arm-announces-maliv76-video-processor-planning-for-the-8k-video-future\n",
      "Content: Alongside today’s 76-seriesCPUandGPUannouncements from Arm, the company also has one last product announcement for the day. Joining Arm’s video processor family is a new encode/decode IP block, the aptly-named Mali-V76.Arm’s Mali video decoders aren’t as flashy as their GPUs and don’t get the same degree of attention accordingly. But for an IP vendor like Arm, they’re an important part of their graphics portfolio and a very necessary counterpart to their Mali GPU designs. So along with the new Mali-G76 GPU, Arm has put together a new video block to go with it.The new V76 is essentially the successor to Arm’s previous high-end video block, theMali-V61, which was announced back in 2016. Understandably the world of video encoding and decoding doesn’t evolve at quite as brisk a pace as GPUs, so Arm generally only revises their video blocks at about half the frequency.Overall the V76 brings a slew of changes to Arm’s video processor ecosystem. On the performance front the new video block offers twice the decode performance of the V61, and on the encode side Arm says that encoding quality has been improved by about 25%. Meanwhile on the features front, the latest block adds support for 10-bit H.264 encoding and decoding, the one major codec/format that wasn’t already present on the V61.From a hardware perspective, Arm has retained their scalable core design, and the V76 is intended for designs ranging from 2 to 8 cores. Arm’s ambitions are very forward-looking given the longer timeframe between generation, and as a result at a nominal frequency of 600MHz, an 8 core design is slated to be able to decode up to decode videos up to 8Kp60, and encode up to 8Kp30. Or for a smaller 4 core design, that becomes 4Kp120 decoding and 4Kp60 encoding. As previously stated, this is twice the throughput per-core of the V61, meaning that at least at nominal frequencies, this is the first Arm video processor block suitable for 8K video (as well as high frame rate 4K).Under the hood things haven’t changed too much, as Arm has stuck with their now traditional mix of fixed function and programmable hardware to split up the various steps of video encoding/decoding and issue it to purpose-built hardware as much as possible. And while this wasn’t said by Arm, looking at the performance and feature specifications of the new processor, it looks like the V76 is derived from the same general architecture as the also recently announced low-endMali-V52 video processor, which offers the same per-core performance and features, but not the total scalability.Along with the greater resolution support, the other most notable addition to the new processor is support for 10-bit H.264 video. This format was oddly absent in the V61 – the processor supported 10-bit HEVC, but not H.264 – and at the time the company didn’t think it would be needed. The slow adoption of HEVC relative to the faster adoption of HDR has changed that however, so for the V76 both encode and decode support for the format is being included.On that note, however, this processor willnotinclude any support for the upcoming AV1 codec. While the bitstream specification for the eagerly anticipated codec was released a couple of months back, the timing was unfortunately after Arm had already completed the V76 RTL (never mind the fact that the specification isn’t closed yet). So it’s going to have to be the next video block after the V76 before Arm can include AV1 decode support.Arm Mali-V Supported Video Codecs (Encode & Decode)CodecV61V52V76H.264 8-BitYesYesYesH.264 10-BitNoYesYesHEVC 8-BitYesYesYesHEVC 10-BitYesYesYesVP9 8-BitYesYesYesVP9 10-BitYesYesYesAV1NoNoNoMeanwhile on the encode front, Arm’s latest processor includes a couple different improvements to improve their encode quality. Overall the encode quality is up roughly 25% over the V61 based on PSNR metrics, however this improvement isn’t entirely attributable to software. Arm is basing their comparison against the initial release firmware of the V61, which continued to receive updated firmware over its lifetime that also improved its encoding quality (though that firmware was often not distributed to existing phones). In practice the quality improvements are a 40/60 split between software and hardware, so just over half of the improvement is on the hardware side, or around 15% of the cumulative quality improvement.Still, with phone manufacturers finally embracing HEVC encoding for video and images, the improvement comes as an important time. The very high encoding requirements of AV1 also mean that even after a decoder ships in a phone, we’re unlikely to see a full-featured encoder in a phone any time soon. So it will be necessary to maximize HEVC encoding quality going forward.It all goes without saying, of course, that Arm is looking to scoop (and keep) competition from other vendors of video processors. Which means that they need to not only improve over their past designs, but keep ahead of their competition as well. Arm customers already get other benefits from remaining in-ecosystem, as it were, by being able to use Arm’s frame buffer compression technology throughout their display pipeline, but at the end of the day Mali GPUs can be used with other video blocks, and for that matter V76 could be used with other GPUs.Finally, while not a focus of their presentation, Arm also briefly commented on HDR support in our briefing. In concert with their display processor, the new video processor is currently able to handle HDR10 and HLG formatted HDR video. Meanwhile support for HDR10+ – which is HDR10 with support for dynamic metadata – is set to arrive in the future. This is an important distinction, as Arm’s display controller can’t support Dolby Vision, meaning that HDR10+ would be the only dynamic HDR format that Arm can support.Gallery:Arm Mali-V76 Slide Deck\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/12835/arm-announces-maliv76-video-processor-planning-for-the-8k-video-future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm's Cortex-A76 CPU Unveiled: Taking Aim at the Top for 7nm\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2018-05-31T19:01:00Z\n",
      "URL: https://www.anandtech.com/show/12785/arm-cortex-a76-cpu-unveiled-7nm-powerhouse\n",
      "Content: Another year, another TechDay from Arm. Over the last several years Arm’s event has come as clockwork in the May timeframe and has every time unveiled the newest flagship CPU and GPU IPs. This year is no exception as the event is back on the American side of the Atlantic in Austin Texas where Arm has one of its major design centres.Two years ago during theunveiling of the Cortex A73I had talked a bit more about Arm’s CPU design teams and how they’re spread across locations and product lines. The main design centres for Cortex-A series of CPUs are found in Austin, Texas; Cambridge, the United Kingdom, and Sophia-Antipolis in the south of France near Nice. For the last two years the Cortex A73 and Cortex A75 were designs that mainly came out of the Sophia team while the Cortex A53 and more recently the A55 were designs coming out of Cambridge. This means that we haven’t seen any recent designs coming out of Austin and the last of the “Austin family” of CPUs were the A57 and A72.The project being worked on in Austin had been hyped up for several years – I remember even as early as the A73 release back in 2016 the company had pulled forward some elements from an advanced future microarchitecture on the back-end pipelines, especially on the FP/SIMD side. The Cortex A75 was further remarked as pulling more elements from this new mysterious project.Today we can finally unveil what the Austin team has been working on – and it’s a big one. The new Cortex A76 is a brand new microarchitecture which has been built from scratch and lays the foundation for at least two more generations for what I’ll call “the second generation of Austin family” of CPUs.The Cortex A76 is important for Arm for a design perspective as it represents a new start from a clean sheet. It’s rare for IP claim to be able to do this as it represents a great resource and time investment and if it weren’t for the Sophia design team taking over the steering wheel for the last two generations of products it wouldn’t have been reasonable to execute. The execution of the CPU design teams should be emphasised in particular as Arm claims this is the 5thgeneration “annual beat” product where the company delivers a new microarchitecture every new year. Think of it as an analogue to Intel’s past Tick-Tock strategy, but rather Tock-Tock-Tock for Arm with steady CAGR (compound annual growth rate) of 20-25% every generation coming from µarch improvements.So what is the Cortex A76? In Arm’s words, it’s a “laptop-class” performance processor with mobile efficiency. The vision of the A76 as a laptop-class processor had been emphasised throughout the TechDay presentation so it seems Arm is really taking advantage of the large performance boost of the IP to cater to new market segments such as the emerging “Always connected PCs” which Qualcomm is spearheading with their SoC platforms.The Cortex A76 microarchitecture has been designed with high performance while maintaining power efficiency in mind. Starting from a clean sheet allowed the designers to remove bottlenecks throughout the design and to break previous microarchitectural limitations. The focus here was again maximum performance while remaining within energy efficiency that is fit for smartphones.In broad metrics, what we’re promised in actual products using the A76 is the follows: a 35% performance increase alongside 40% improved power efficiency. We’ll also see a 4x improvements in machine learning workloads thanks to new optimisations in the ASIMD pipelines and how dot products are handled. These figures are baselined on A75 configurations running at 2.8GHz on 10nm processes while the A76 is projected by Arm to come in at 3GHz on 7nm TSMC based products.The new CPU is naturally still compatible with DynamIQ’s common cluster topology and Arm envisions designs to be paired with Cortex A55s as the little more power efficient CPUs. The configuration scalability of the DynamIQ IP again was reiterated and we were presented with example configurations such as 1+7 or 2+6 with either Cortex A75 or A76 CPU IP. This presentation slide was one of the rare ones where Arm referred to the area size of the A76, pointing out that the A75 still had better PPA and thus might still be a valid design choice for companies, depending on their needs. One comparison that was made during the event is that in terms of area, three A76’s with larger caches would fit inside the size of a Skylake core – all while within 10% of the IPC of the Intel CPU, but obviously there’s also process node scaling considerations to take into account.A standout claim is that Arm aims to outperform the competition at half the area and half the power. Arm was slightly beating around the bush here in what it considers the competition, but generally the answer was that it was considering everybody the competition. Taking into account Intel, AMD or Samsung it’s actually not that hard to imagine Arm beating them in PPA as historically the company always had the smallest CPU designs and that directly translates into more efficient microarchitectures.Before we get into more detailed breakdowns of the performance and power improvements and what I’m expecting to happen into products, let’s see the microarchitectural improvements on the core and how Arm managed to extract this much performance while maintaining power efficiency.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/12785/arm-cortex-a76-cpu-unveiled-7nm-powerhouse\n",
      "Title: Arm Announces Mali-G76 GPU: Scaling up Bifrost\n",
      "Author: Ryan Smith\n",
      "Date Published: 2018-05-31T19:00:00Z\n",
      "URL: https://www.anandtech.com/show/12834/arm-announces-the-mali-g76-scaling-up-bifrost\n",
      "Content: Section by Andrei FrumusanuIt’s been two years since Arm announced their first “next-generation”GPU architecture based on Bifrostand alongside with it its first implementation the G71. Following its release in the first products the GPU was off to a very shaky start as the G71 was quitea disaster in the Kirin 960and Exynos 8895 as both GPU implementations blew past their power budgets in severe manners.This year’s G72 was a much more reasonable product as it offered up to a100% improvement in efficiency in the Kirin 970andExynos 9810, putting the G72 a lot nearer to the performance and efficiency targets that the Bifrost architecture was promised to achieve.Today’s Arm announces the follow-up to the G72 and the latest offspring in the Bifrost family: The Mali G76. The targets of the GPU IP should be pretty clear: Improve performance, efficiency and area and try to catch up with the competition as much as possible.Overall what Arm promises for the next generation of SoCs using the G76 on a new TSMC 7nm process is a 50% increase in performance versus current generation devices.In terms of apples-to-apples comparisons, we see three key metrics that are improved: A 30% improvement in performance density is the first one. What this means is that either for the same area, the new GPU will perform 30% better, or for the same performance, the vendor can shrink the GPU space on the SoC.The new GPU promises a 30% microarchitectural efficiency improvement thanks to a consolidation of the functional blocks of the unit. Efficiency is particularly something Arm needs to focus on in regards to Mali as we’ve seen a few missteps over the last year or two and the competition from Qualcomm in the GPU and 3D gaming space is particularly fierce.Finally, there’s a quoted 2.7x improvement for machine learning inferencing applications thanks to the inclusion of new dedicated 8-bit dot product instructions.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/12834/arm-announces-the-mali-g76-scaling-up-bifrost\n",
      "Title: Assessing Cavium's ThunderX2: The Arm Server Dream Realized At Last\n",
      "Author: Johan De Gelas\n",
      "Date Published: 2018-05-23T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/12694/assessing-cavium-thunderx2-arm-server-reality\n",
      "Content: A little less than 2 years ago,we investigatedthe first Arm server SoC that had a chance to compete with midrange Xeon E5s: the Cavium ThunderX. The SoC showed promise, however the low single-threaded performance and some power management issues relegated the 48-core SoC to more niche markets such as CDN and Web caching. In the end, Cavium's first server SoC was not a real threat to Intel's Xeon.But Cavium did not give up, and rightfully so: the server market is more attractive than ever. Intel's datacenter group is good for about 20 Billion USD (!) in revenue per year. And even better, profit margins are in 50% range. When you want to profits and cash flow, the server market far outpaces any other hardware market. So following the launch of the ThunderX, Cavium promised to bring out a second iteration: better power management, better single thread performance and even more cores (54).The trick, of course, is actually getting to a point where you can take on the well-oiled machine that is Intel. Arm, Calxeda, Broadcom, AppliedMicro and many others have made many bold promises over thepast 5 yearsthat have never materialized, so there is a great deal of skepticism – and rightfully so – towards new Arm Server SoCs.However, the new creation of underdog Cavium deserves the benefit of the doubt. Much has changed – much more than the name alone lets on – as Cavium has bought the \"Vulcan\" design from Avago. Vulcan is a rather ambitious CPU design which was originally designed by the Arm server SoC team of Broadcom, and as a result has a much different heritage than the original ThunderX. At the same time however, based on its experience from the ThunderX, Cavium was able to take what they've learned thus far and have introduced some microarchitectural improvements to the Vulcan design to improve its performance and power.As a result, ThunderX2 is a much more \"brainiac\" core than the previous generation. While the ThunderX core had a very short pipeline and could hardly sustain 2 instructions per clock, the Vulcan core was designed to fetch 8 and execute up to 4 instructions per clock. It gets better: 4 simultaneous threads can be active (SMT4), ensuring that the wide back-end is busy most of the time. 32 of those cores at clockspeeds up to 2.5 GHz find a home in the new ThunderX2 SoC.With up to 128 threads running and no less than eight DDR4 controllers, this CPU should be able to perform well in all server situations. In other words, while the ThunderX (1) was relegated to niche roles, the ThunderX2 is the first Arm server CPU that has a chance to break the server market open.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/12694/assessing-cavium-thunderx2-arm-server-reality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Details \"Project Trillium\" Machine Learning Processor Architecture\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2018-05-22T15:30:00Z\n",
      "URL: https://www.anandtech.com/show/12791/arm-details-project-trillium-mlp-architecture\n",
      "Content: Arm first announcedProject Trillium machine learning IPsback in February and we were promised we’d be hearing more about the product in a few months’ time. Project Trillium is unusual for Arm to talk about because the IP hasn’t been finalised yet and won’t be finished until this summer, yet Arm made sure not to miss out on the machine learning and AI “hype train” that has happened over the last 8 months in both the semiconductor industry and as well as particularly in the mobile industry.Today Arm details more of the architecture of what Arm now seems to more consistently call their “machine learning processor” or MLP from here on now. The MLP IP started off a blank sheet in terms of architecture implementation and the team consists of engineers pulled off from the CPU and GPU teams.With the MLP Arm set out to provide three key aspects that are demanded in machine learning IPs: Efficiency of convolutional computations, efficient data movement, and sufficient programmability. From a high level perspective the MLP seems no different than many other neural network accelerator IPs out there. It still has a set of MAC engines for the raw computational power, while offering some sort of programmable control flow block alongside a sufficiently robust memory subsystem.Starting off at a more detailed view of the IP’s block diagram, the MLP consists of common functional blocks such as the memory interconnect interfaces as well as a DMA engine. The above graphic we see portrayal of the data flow (green arrows) and control flow (red arrows) throughout the functional blocks of the processor. The SRAM is a common block to the MLP sized at 1MB which serves as the local buffer for computations done by the compute engines. The compute engines each contained fixed function blocks which operate on the various layers of the neural network model, such as input feature map read blocks which pass onto control information to a weight decoder.At the heart the actual convolution engine which does the computations is a 128-wide MAC unit operating on 8-bit integer data storing the quantised weights of the NN model.Arm recognises that the future of neural network models is relatively uncertain and we haven’t really seen any sort of “standard” model emerge. It is most certain that in the coming years we’ll see different kind of model architectures improve on existing neural networks, and this means that any kind of hardware implementation today will need to provide sufficient flexibility that it will be able to be compatible with eventual future models. The MLP thus offers a “programmable layer engine” which is a specialised processing unit with vector and NN specific instructions that are able to operate on different pooling or other layer operations with enhanced flexibility and programmability compared to a fixed function block.Important for efficient convolutions is how data is handled through the IP block as data-flow and bandwidth requirements easily overshadow any computational unit power. Again, the MLP operates on 8-bit quantized data and Arm states that the internal precision is varied, however without further detailing more on this aspect of the architecture.Another important feature unique to the MLP is the inclusion of compression capability on the part of the data set / feature map.Arm’s compression algorithm is lossless and promises an up to 3x compression ratio which reduces the bandwidth to external DRAM and thus having a very large impact on overall system power.While the compression of weights and feature maps happen on the MLP losslessly at runtime, another important optimisation for memory bandwidth is the actual training and optimisation of the neural network model. This happens offline through Arm’s software tools which are then able to optimise the data for better compression ratio as well as prune irrelevant connections to save on the computation requirements on the MACs – further improving both power efficiency as well as performance.I’ll admit that Arm’s MLP architecture disclosure might seem interesting on paper – how it will actually perform in terms of actual performance as well as efficiency versus other architectures from other vendors is still something we won’t know until we actually have silicon back and can run some real world workloads. This seems to be an issue that other vendors also have recognised and why many of them have actually decided against divulging more in-depth architectural characteristics such as presented by Arm today.Arm proclaims target metrics such as >3TOPs/W on 7nm implementations with absolute throughputs of 4.6TOPs (deriving target power of ~1.5W). Again these metrics are relatively ephemeral in their meaning as actual performance of inferencing work will also strongly depend on additional characteristics such as overall MAC utilisation or performance of the IP fully-connected layers.In terms of scalability the MLP is meant to come with configurable compute engine setups from 1 CE up to 16 CEs and a scalable SRAM buffer up to 1MB. The current active designs however are the 16CE and 1MB configurations and smaller scaled down variants will happen later on in the product lifecycle. Arm wasn’t ready to note exact area figures for the MLP but generally we’ll be looking at around larger than an A55.As I’ve noted in past releases from competitor neural network IPs – we’re still very early on in the ecosystem so to give any actual judgment on the competitiveness or how good a certain IP is against the other is something that’s essentially impossible to predict. The more obvious aspect we can talk about is timelines – ARM’s MLP and project Trillium does lag behind in terms of availability as we won’t be seeing finalisation of the RTL until sometime this summer, which means we won’t be seeing silicon until around mid- to late 2019 at the earliest meaning the MLP will likely have to face off competitor IP such as the third-generation NPU from HiSilicon.On the high-end there’s seemingly also not that many prospective customers for the MLP as many are opting for their own solutions – it’s on the low- and mid-range where we might see more success for Project Trillium but also that will be a tough uphill battle against the quite varied IP offerings from competitors and Arm’s best chance for success here is to really emphasize on its CPU-GPU-MLP software ecosystem advantage.Related ReadingHiSilicon Kirin 970 - Android SoC Power & Performance OverviewImagination Joins the AI Party, Announces PowerVR Series 2NX Neural Network AcceleratorCEVA Launches Fifth-Generation Machine Learning Image and Vision DSP Solution: CEVA-XM6CEVA Announces NeuPro Neural Network IP\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/12791/arm-details-project-trillium-mlp-architecture\n",
      "Title: NVIDIA ARM SoC Roadmap Updated: After Xavier Comes Orin\n",
      "Author: Ryan Smith\n",
      "Date Published: 2018-03-29T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/12598/nvidia-arm-soc-roadmap-updated-after-xavier-comes-orin\n",
      "Content: As part of this week’s GTC 2018 keynote address, NVIDIA CEO Jen-Hsun Huang quickly touched upon the future of NVIDIA’s ARM SoC lineup. While the company no longer publicly advertises or sells new ARM-based SoCs – the last SoC branded Tegra was the Tegra X1 – they have continued development for private uses. Chief among these of course being their DRIVE systems, where the Xavier SoC is at the heart of both the single-SoC Xavier module, as well as the larger and more powerful muti-processor Pegasus module for level 5 vehicles.While Xavier itself is just now sampling to partners, NVIDIA already has their eye on what’s next. And that is Orin.Unlike even the Xavier tease in 2016, NVIDIA is saying very little about Orin other than the fact that it’s the next generation of NVIDIA SoCs. Like Xavier, it’s a single-chip solution. But otherwise we don’t know anything about the planned architecture or features.NVIDIA ARM SoC Specification ComparisonOrinXavierParkerCPU Cores?8x NVIDIA Custom ARM \"Carmel\"2x NVIDIA Denver +4x ARM Cortex-A57GPU Cores?Xavier Volta iGPU(512 CUDA Cores)Parker Pascal iGPU(256 CUDA Cores)DL TOPS[A Ton]30 TOPSN/AFP32 TFLOPS?1.3 TFLOPs0.7 TFLOPsManufacturing Process7nm?TSMC 12nm FFNTSMC 16nm FinFETTDP?30W15WWith respect to performance, NVIDIA isn’t giving hard figures there either, but they are saying that they want to replace a Pegasus module with a couple of Orins. Pegasus, as a reminder, is a pair of Xaviers each with an unnamed, post-Volta discrete GPU attached, with a total power consumption of 500W. So to replace that with a couple of single-chip SoCs would be a significant accomplishment – and presumably a massive bump in energy efficiency.But finally, let’s talk about thereal questionon everyone’s mind: which superhero is this new SoC named after? After a run in the Marvel universe, it looks like NVIDIA is back to favoring DC. A brief search shows thatOrinis another name for Aquaman. Which certainly isn’t as high-profile as the likes of Kal-El, Wayne, or Xavier, but perhaps Jen-Hsun Huang is a big fan of Jason Momoa? (ed: and indeed, whodoesn’tfind Aquaman outrageous?)\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/12598/nvidia-arm-soc-roadmap-updated-after-xavier-comes-orin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: GIGABYTE's ThunderXStation with Dual Cavium ThunderX2 Arm SoCs\n",
      "Author: Anton Shilov\n",
      "Date Published: 2018-03-27T11:30:00Z\n",
      "URL: https://www.anandtech.com/show/12571/gigabyte-thunderxstation-cavium-thunderx2-socs\n",
      "Content: GIGABYTE this month introduced its ThunderXStation workstation based on two Cavium ThunderX2 processors featuring Armv8 architecture. The machine is primarily aimed at software developers porting or developing various applications to Armv8 platforms. The ThunderXStation is already available in the US.GIGABYTE's ThunderXStation (W281-T90)comes in 4U tower and is based on a dual-socket motherboard supporting Cavium’s ThunderX2 SoCs featuring up to 64 custom Armv8 cores with four-way SMT as well as 16 DDR4 memory channels (1DPC, 8 channels per SoC) when two CPUs are installed. A dual-processor ThunderXStation comes with two PCIe Gen3/OCP x16 slots, four PCIe Gen3 slots (two x16 and two x8), four M.2 (NVMe/PCIe 3.0 x4) slots for SSDs, two U.2/SATA 2.5” bays for SSDs/HDDs, two 10 GbE/GbE QLogic NICs, NVIDIA’s GeForce GT 710 GPU, an 800 W redundant PSU and so on.Image fromServeTheHomefrom OCP Summit 2018From hardware standpoint, the ThunderXStation looks rather versatile: it supports various expansion slots, enabling developers to use various add-on cards, accelerators, and storage devices required by their applications. The system also has an Aspeed AST2500 server management chip to bring it even closer to target machines that will run the ThunderX2.The ThunderXStationshipswith Ubuntu 17.10 OS, but can come with CentOS 7.4 or OpenSUSE, if required. It also comes with preinstalled software development tools, including gcc 7.2, LLVM, gdb, Golang, OpenJDK 9.0, HHVM, Python, PHP, Ruby etc. The OS supports KVM and Docket to enable developers to test their apps in hypervisor-based or containerized environments. The machine ships with open sources graphics drivers, primarily because it comes with a GeForce GT 710 graphics card (so, open source drivers should be reasonably good for such an old architecture).GIGABYTE's ThunderXStationPreliminary SpecificationsCPUDual or single Cavium ThunderX2 2.2 GHz CPU with 32 coresGPUNVIDIA GeForce GT 710Memory8 × DDR4-2666 DIMM slots per CPUStorage2 × M.2 slots for PCIe/NVMe SSDs2 × 2.5\" SATA/U.2 baysWirelessunknown/noneEthernet1 × Gigabit Ethernet, can be outfitted with 10 GbE cardsPCIe1 × PCIe Gen 3/OCP x16 per CPU2 × x16/x8/x4/x1 and x8 per CPUDisplay Outputs1 × D-Sub for managementOthers via discrete graphicsAudiounknownUSB4 × USB 3.0 Type-APSU800 W redundantOSUbuntu 17.10. CentOS 7.4 or OpenSUSEOne of the reasons why contemporary Armv8 SoCs are rarely used in datacenters is lack of software support. Developers need to recompile their programs for Arm, but since very few have access to hardware and appropriate tools, the software porting process is proceeding very slowly. GIGABYTE’s ThunderXStation will enable more parties to work on server programs for Arm platforms. This is similar to how Intel approached the Xeon Phi ecosystem, by launching a tower equivalent with aKnights Landing machine with the Xeon Phi chipas the host for development work.The Armv8-based workstation for software developers is now available from PhoenicsElectronics in the U.S. GIGABYTE does not publish pricing of the ThunderXStation, but we have reached out to PhoenicsElectronics and will update the story once we get more information on the matter.Related ReadingMarvell to Acquire Cavium for $5.5 Billion, Augmenting Marvell's CPU, Networking, & Security AssetsInvestigating Cavium's ThunderX: The First Arm Server SoC With AmbitionAppliedMicro's X-Gene 3 SoC Begins Sampling: A Step in Arm's 2017 Server AmbitionsMACOM Sells AppliedMicro’s X-Gene CPU BusinessArm Challenging Intel in the Server Market: An OverviewSupermicro Releases Intel Xeon Phi x200 (KNL) Systems: Servers and a Developer Mid-TowerSources:Cavium,ThunderXForums,Liliputing\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/12571/gigabyte-thunderxstation-cavium-thunderx2-socs\n",
      "Title: Arm Launches New Mali G52, G31 GPUs, New Display And Video IP\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2018-03-06T06:00:00Z\n",
      "URL: https://www.anandtech.com/show/12501/arm-launches-new-mali-g52-g31-gpus-new-display-and-video-ip\n",
      "Content: Today Arm announces a new Mali Multimedia Suite of products consisting of GPU, display processor and video processor IPs targeting the mainstream and low-end. The announcement comes at a time where the industry’s biggest growth comes from China where “premium” experience devices hold >30% of global smartphone market share. Indeed we’ve seen thesame narrative reproduced by MediaTekas it was a core factor in the strategy shift and re-focus on the P-series.The coverage starts with the announcement of the Mali-G52 mid-range GPU IP which follows the G51announced in October 2016. The G51 was a rather odd GPU design in the sense that we haven’t seen any consumer SoCs adopt it, as vendors seemingly have preferred to use low core count G71 and G72s. Arm states that the DTV market is also a large volume market where mainstream GPUs are in demand, however we have less visibility into the silicon of those markets.The G52 promises great gains as Arm posts up to a 30% improvement in performance density, meaning fps/mm². The efficiency improvements are more conservative with a posted 15% improvement over the G51.With what was surprising to me is to see that Arm has divulged that one of the core changes of the G52 over the G51 is also a characteristic that I'm expecting to find in Arm’s next generation high-end GPUs. The big change is the doubling of the ALU lanes within an execution engine of a core. As a refresher, a single ALU lane for the Bifrost architecture (G71, G72, G51) included a FMA and an ADD/SF unit. An execution engine was comprised of four of these lanes, making up a wavefront which in Arm’s terminology was called a Quad. The G52 is the first to double the lanes up from four to eight, effectively doubling the ALU throughput within an execution engine.Arm says that this is where most of the gains in performance and density come from as the doubling of the ALU lanes only increases the core area by ~1.22x. The 3.6x increase in machine learning workloads is attributed to the fact that the new ALUs can now handle 8-bit dot product operations.The G52 continues to use the G51’s “dual-pixel” texture units which are able to process 2 pixels and 2 texels per cycle. A confusing matter for some G51 configurations was the fact that the GPU consisted of either cores with “uni-pixel” setups or “dual-pixel” setups and configurations such as MP3, which consists of a pairing a uni-pixel core with a dual-pixel core to make an “G51MP3”. And indeed there’s even more confusion when we realise that in the past Arm’s MP denotation for GPUs meant multi-pixel and actually counted the amount of pixel throughput of a GPU. The G52 now fixes this confusion and future MP denotations will actually refer to multi-processor configuration, so a G52MP4 will mean there are 4 GPU cores whereas a G51MP4 officially described a two-core configuration.Arm Mali G52 vs G51Mali-G52Mali-G51Core Configurations1-41-3ALU Lanes Per Core (Default)16 (2 EU)24 (3 EU)12Texture Units Per Core21-2Pixel Units Per Core21-2FLOPS:Pixel Ratio16:1 (2 EU)24:1 (3 EU)12:1 (Dual-pixel)24:1 (Uni-pixel)APIsOpenGLES 3.2OpenCL 2.0VulkanOpenGLES 3.2OpenCL 2.0VulkanTo give customers more choice between compute and fill-rate focused configurations, Arm allows the G52 to be used with core setups containing either two or three execution engines, meaning the FLOPS/core will come in at either 32 or 48 counting only the FMA’s to 48 or 72 if you count in the additional ADD/SF unit. The FLOPS:pixel ratio naturally also changes as that is the point of the configuration flexibility, able to use a 16:1 or a 24:1 ratio. This ratio is a lot more compute balanced compared to the G51’s 12:1 ratio and now is the same as the higher-end GPUs.The Mali-400 is Arm’s most successful GPU, and one could probably say it’s the most successful GPU ever from any vendor as the IP is now nearing its 10 year anniversary and it’s still shipping in new products today. Having received generational updates over the years, it’s only now that we finally see the need for a new ultra-low end GPU as operating systems and workloads make OpenGLES >3.0 and Vulkan a hard requirement, something that the good old Mali400 can’t do.The new Mali-G31 is meant to finally replace the Mali-400 in super low end designs. The G31 is not related to the G52 in architecture as it still employs the traditional quad-layout (4 ALU lanes). While the G52 helped clear the confusion in configuration, the G31 remains confusing as it comes with either a one execution engine (4 lanes) with a 1 pixel per clock texture unit, or with two execution engines (2x4 lanes) with a 2 pixel per clock TMU. In a single-core configuration the G31 promises up to a 20% area reduction over the G51MP2 and up to 12% better UI performance, a metric likely tied to the fillrate efficiency of the core.Wrapping up today’s announcement is an update on the display processor and video processors.The Mali-V52 is a follow-up to theV61 also announced back at the end of 2016along with the G51. The V52 scales down the V61 and targets the mid-range with more limited capabilities with up to 4K60 encoding and decoding (as opposed to 4K120 for the V61). The improvements allowed a 2x decode performance increase which in turn enabled a 38% smaller silicon area, which is a significant figure. Arm also says that for HEVC encoding the new architecture has improved its heuristics and achieves up to a 20% better quality when handling the variable block sizes of the codec.Finally the Mali-D51 is a follow-up to the DP650 and is derived from the higher-endMali-D71whose architecturewas disclosed under the Mali-Cetus codenamehere at AnandTech. The new IP allows for a 2x increase in area efficiency and supports up to 8 composition layers much like the D71. Arm’s display processors are quite unique as they allow for offloading UI rendering completely to the display processor from the GPU and in doing this achieve very good power efficiency compared to GPU-only approaches.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/12501/arm-launches-new-mali-g52-g31-gpus-new-display-and-video-ip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Announces Project Trillium Machine Learning IPs\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2018-02-13T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/12427/arm-announces-trillium-machine-learning-ip\n",
      "Content: Today’s Arm announcement is a bit out of the norm for the company, as it’s the first in a series of staggered releases of information. For this first announcement Arm is publicly unveiling “Project Trillium” – a group of software solutions as well IP for object detection and machine learning.Machine learning is indeedthehot new topic in the semiconductor business and has particularly seen a large focus in the mobile world over the last couple of months, with announcements from various IP companies as well as consumer solutions from the likes of Huawei. We’ve most recently had a more in-depth look and exploration of the topic of machine learning and neural network processing in adedicated sectionof ourreview of the Kirin 970.Whilst we had a great amount of noise from many industry players on the topic of machine learning IPs. Arm was conspicuously absent from the news and until now the focus has been on the CPU ISA extensions of Armv8.2, which introduce specialised instructions which simplify and accelerate implementations of neural networks with the help of half-precision floating point and integer dot products.Alongside the CPU improvements we've also seen GPU improvements for machine learning in the G72. While both of these improvements help, they are insufficient in use-cases where maximum performance and efficiency are required. For example, as we’ve seen in the our test of the Kirin 970’s NPU and Qualcomm’s DSP – the efficiency of running inferencing on specialized IPs is above an order of magnitude higher than running it on a CPU.As Arm explains it, the Armv8.2 and GPU improvements were only the first results towards establishing solutions for machine learning, while in parallel they’ve examined the need for dedicated solutions. Industry pressure from partners made it clear that the performance and efficiency requirements made dedicated solutions inevitable and started work on its machine learning (ML) processors.Today’s announcement covers the new ML processors as well as object detection processors (OD). The latter IP is a result of Arm’sApical acquirementin 2016 which saw the company add solutions for the display and camera pipelines to their IP portfolio.Starting with the ML processor – what we’re talking about here is a dedicated IP for neural network model inferencing acceleration. As we’ve emphasised in our NN related announcements of late, Arm also emphasises that having an architecture which is specifically designed for such workloads can have significant advantages over traditional CPU and GPU architectures. Arm also made a great focus on the need to design an architecture which is able to do optimised memory management of the data that flows through a processor when executing ML workloads. These workloads have high data reusability and minimising the in- and out-bound data through the processor is a key aspect of reaching high performance and high efficiency.Arm’s ML processor promises to reach theoretical throughput of over 4.6TOPs (8-bit integer) at target power envelopes of around 1.5W, advertising up to 3TOPs/W. The power and efficiency estimates are based on a 7nm implementation of the IP.In regards to the performance figures, Arm agrees with me that the TOPs figure alone might not be the best figure to represent performance of an IP; however it’s still useful until the industry can work towards some sort of standardisation for benchmarking on popular neural network models. The ML processor can act as a fully dedicated and standalone IP block with its own ACE-Lite interface for incorporation into a SoC, or it can be integrated within DynamiQ cluster, which is a lot more novel in terms of implementation. Arm wasn’t ready to disclose more architectural information of the processor and reserves that for future announcements.An aspect that seemed confusing is Arm’s naming of the new IP. Indeed Arm doesn’t see that the term “accelerator” is appropriate here as traditionally accelerators for Arm meant things such as packet handling accelerators in the networking space. Instead Arm sees the new ML processor as a more fully-fledged processor and therefore deserving of that naming.The OD processor is a more traditional vision processor and is optimised for the task of object detection. There is still a need for such IP as while the ML processor could do the same task via neural networks, the OD processor can do it faster and more efficiently. This showcases just how far the industry is going to make dedicated IP for extremely specialised tasks to be able to extract the maximum amount of efficiency.Arm envisions use-cases where the OD and ML processors are integrated together, where the OD processor would isolate areas of interest within an image and forward them to the ML processor where more fine-grained processing is executed on. Arm had a slew of fun examples as ideas, but frankly we still don’t know for sure how use-cases in the mobile space will evolve. The same can’t be said about camera and surveillance systems where we see the opportunity and need for continuous use of OD and ML processing.Arm’s first generation of ML processors is targeted at mobile use while variants for other spaces will follow on in the future. The architecture of the IP is said to be scalable both upwards and downwards from the initial mobile release.As part of Project Trillium, Arm also makes available a large amount of software that will help developers implement their neural network models into different NN frameworks. These are going to be available starting today on Arm’s developer website as well as Github.The OD processor is targeted for release to partners in Q1 while the ML processor is said to be ready mid 2018. Again this is highly unusual for Arm as usually public announcements happen far after IP availability to customers. Due to the nature of SoC development we should thus not expect silicon based on the new IP until mid to late 2019 at the earliest, making Arm one of the slow-adopters among the semiconductor IP vendors who offer ML IP.Related ReadingHiSilicon Kirin 970 - Android SoC Power & Performance OverviewImagination Joins the AI Party, Announces PowerVR Series 2NX Neural Network AcceleratorCEVA Launches Fifth-Generation Machine Learning Image and Vision DSP Solution: CEVA-XM6CEVA Announces NeuPro Neural Network IP\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/12427/arm-announces-trillium-machine-learning-ip\n",
      "Title: Microsoft Launches Windows 10 On ARM: Always Connected PCs\n",
      "Author: Brett Howse\n",
      "Date Published: 2017-12-05T19:30:00Z\n",
      "URL: https://www.anandtech.com/show/12119/microsoft-launches-windows-10-on-arm-always-connected-pcs\n",
      "Content: This morning at the Qualcomm Snapdragon Summit in Hawaii, Microsoft’s EVP of Windows and Devices, Terry Myerson, is announcing the introduction of the first Windows 10 PCs to be powered by Qualcomm Snapdragon processors. It was almost a year ago to the day that the companyfirst announcedtheir partnership with Qualcomm, which would bring ARM support to Windows 10, but with x86 emulation, allowing all existing applications to work as well. The first PCs will be the ASUS NovaGo, which is a convertible laptop, and the HP ENVY x2 convertible tablet.The ASUS NovaGo LaptopThis is exciting news on a couple of fronts. The Qualcomm Snapdragon 835, which was the processor first announced for Windows 10 on ARM, offers reasonable performance, but with lower power consumption than what we’ve been used to in the PC space, and especially in low-power states. Without having the devices in-hand, we still don’t know how the SD835 compares in performance to the competition. We should finally be able to answer that soon though.The HP ENVY x2 TabletOne of the key pieces of using a mobile SoC in a PC is the extra integration. Smartphones don’t have room for large circuit boards, while still providing room for all of the other equipment and batteries required, and mobile SoCs offer a lot more features integrated into the SoC than what a typical PC would, which allows for substantial board space savings over the competition. Back at Computex, Qualcomm was showing off the SD835 PC board compared to a competing 14nm Intel board, and the space savings were up to 30%. This allows smaller, thinner, and lighter devices, but with more battery capacity.One of the major integrations with the SD835 compared to PC SoCs is the integrated cellular connectivity, which is one of the features that Microsoft is championing the most with this new partnership. There have been PCs with cellular cards added on for some time, but Qualcomm’s cellular tech is aimed at mobile, where always-on connectivity, and low-power usage, is a requirement. That same connectivity will be available on the PC as well, with an always-connected network connection providing a better user experience than what we’ve become accustomed to in the laptop world.Battery life should also be a big win, and while we don’t have our own tests done yet, Microsoft’s information is claiming up to 30 days of standby and up to 22 hours of active use, while the detachable tablet-style HP ENVY x2 is claiming up to 20 hours of active use. That’s impressive, and blows past the all-day battery life that we’ve come to expect in a laptop, and should free a device up for a couple of days of use before charging. Terry Myerson has stated that he’s been getting up to a week of use out of a device before he needs to charge it.The always-on nature of mobile brings other advantages too. The PCs will wake up instantly, just like you’d expect on your phone, or mobile tablet.The ARM equipped devices will be running full Windows 10, so no desktop apps are left behind, unlike the previous time that Microsoft attempted this. The ARM chips at the time offered much less performance as well, so this time around, it should be a much better experience. Universal Windows Apps will be available compiled for ARM directly, but x86 apps will run in emulation, which is still a cause for concern for both performance and battery life, so we’ll have to see how that pans out. Microsoft has an “optimized” version of Office 365 for the new ARM powered PCs, which likely means it’s been recompiled for native performance.Perhaps the most exciting part of the announcement is what added competition in this space should bring to the end-user. We have Qualcomm coming to the PC from the mobile space, where low-power has always been key, but the performance has been improving steadily, and we have Intel and AMD on the other side, coming from a high-performance but higher-power world, where integration of components into the SoC hasn’t been as high of a priority. It’ll be interesting to see where the convergence happens in the coming months.Source: Microsoft\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/12119/microsoft-launches-windows-10-on-arm-always-connected-pcs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qualcomm Launches 48-core Centriq for $1995: Arm Servers for Cloud Native Applications\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2017-11-10T11:30:00Z\n",
      "URL: https://www.anandtech.com/show/12025/qualcomm-launches-48core-centriq-for-1995-arm-servers-for-cloud-native-applications\n",
      "Content: Following on from the SoC disclosure at Hot Chips, Qualcomm has this week announced the formal launch of its new Centriq 2400 family of Arm-based SoCs for cloud applications. The top processor is a 48-core, Arm v8-compliant design made using Samsung’s 10LPE FinFET process, with 18 billion transistors in a 398mm2 design. The cores are 64-bit only, and are grouped into duplexes – pairs of cores with a shared 512KB of L2 cache, and the top end design will also have 60 MB of L3 cache. The full design has 6 channels of DDR4 (Supporting up to 768 GB) with 32 PCIe Gen 3.0 lanes, support for Arm Trustzone, and all within a TDP of 120W and for $1995.Wecovered the design of Centriq extensivelyin our Hot Chips overview, including the microarchitecture, security and new power features. What we didn’t know were the exact configurations, L3 cache sizes, and a few other minor details. One key metric that semiconductor professionals are interested in is the confirmation of using Samsung’s 10LPE process, which Qualcomm states gave them 18 billion transistors in a 398mm2 die (45.2MTr/mm2). This was compared to Intel’s Skylake XCC chip on 14nm (37.5MTr/mm2, from an Intel talk), but we should also add in Huawei’s Kirin 970 on TSMC 10nm (55MTr/mm2). Today Qualcomm is releasing all this information, along with a more detailed block diagram of the chip.The chip has 24 duplexes, essentially grouped into sets of four. Connecting them all is a bi-directional segmented ring bus, with a mid-silicon bypass to speed up cross-core transfers. This ring bus is set with 250 GBps of aggregate bandwidth. Shown in the diagram are 12 segments of L3 cache, which means these are shipped with 5 MB each (although there may be more than 5 MB in a block for yield redundancy). This gives a metric of 1.25 MB of L3 cache per core, and for the SKUs below 48 cores the cache is scaled accordingly. Qualcomm also integrates its inline memory bandwidth compression to enhance the workflow, and provides a cache quality of service model (as explained in our initial coverage). Each of the six memory controllers supports a channel of DDR4-2667, with support up to 768GB of memory and a peak aggregate bandwidth of 128 GB/s.Qualcomm Centriq 2400 SeriesAnandTech.comCentriq 2460Centriq 2452Centriq 2434Cores484640Base Frequency2.2 GHz2.2 GHz2.3 GHzTurbo Frequency2.6 GHz2.6 GHz2.5 GHzL3 Cache60.0 MB57.5 MB50 MBDDR46-Channel, DDR4-2667PCIe32 PCIe 3.0TDP120 W120 W110 WPrice$1995$1373$888Starting with the chips on offer, Qualcomm will initially provide three different configurations, starting with 40 cores at 2.3 GHz (2.5 GHz turbo), up to 46 and 48 cores both at 2.2 GHz (2.6 GHz turbo). All three chips are somewhat equal, binned depending on active duplexes and cache, with $1995 set for the top SKU. Qualcomm is aiming to attack current x86 cloud server markets on three metrics: performance per watt, overall performance, and cost. In that regard it offered three distinct comparisons, one for each chip:Centriq 2460 (48-core, 2.2-2.6 GHz, 120W) vs Xeon Platinum 8180 (28-core, 2.5-3.8 GHz, 205W)Centriq 2452 (46-core, 2.2-2.6 GHz, 120W) vs Xeon Gold 6152 (22-core, 2.1-3.7 GHz, 140W)Centriq 2434 (40-core, 2.3-2.5 GHz, 110W) vs Xeon Silver 4116 (12-core, 2.1-3.0 GHz, 85W)Qualcomm provided some SPECint_rate2006 comparisons between the chips, showing Centriq either matching or winning in performance per thread, beating in performance per watt, and up to 4x in performance per dollar. It should be noted that the data for the Intel chips were interpolated from other Xeon chips, except the 8180. Those numbers can be found in our gallery below.One interesting bit of data from the launch was the power consumption results provided. As a server or cloud CPU scales to more cores, there will undoubtedly be situations where not all the cores are always drawing power, either due to how the algorithm works or the system is waiting on data. Normally the TDP values are given as a measure of power consumption, despite the actual definition of thermal dissipation requirements – a 120W chip does not always draw 120W, in other words. To this end, Qualcomm provided the average power consumption of the 120W Centriq 2460 while running SPECint_rate2006.It shows a median power consumption of 65W, peaking just below 100W for hmmer and h264ref. The other interesting point is the 8W idle power, which is indicated as for only when C1 is enabled. With all idle states enabled, Qualcomm claims under 4W for the full SoC. Qualcomm was keen to point out that this includes the IO on the SoC, which requires a separate chipset on an Intel platform.Any time an Arm chip comes into the enterprise space, thoughts immediately turn to high-performance, and Qualcomm is keen here to point out that while performant, their main goal is to cloud services and hyper-scale, such as scale-out situations, micro-services, containers, and instance-based implementations. At the launch in San Diego, they rolled out quotes from Alibaba, Google, HPE, and Microsoft, all of whom are working closely with Qualcomm for deployment. Demonstrations at the launch event included NoSQL, cloud automation, data analytics with Apache Spark, deep learning, network virtualization, video and image processing, compute-based bioinformatics, OpenStack, and neural networks.On the software side, Qualcomm is working with a variety of partners to enable and optimize their software stacks for the Falkor design.At Hot Chips, Qualcomm also stated that there are plans in the works to support Windows Server, based on work done with their Snapdragon on Arm initiative, although this seemed to be missing from the presentation.Also as a teaser, Qualcomm gave the name of its next-generation enterprise processor. The next design will be called the Qualcomm Firetail, using Saphira cores. (Qualcomm has already trademarked both of those names).Qualcomm Centriq is now shipping (for revenue) to key customers. We should be on the list for review samples when they become available.Related ReadingAnalyzing Falkor’s Microarchitecture: A Deep Dive into Qualcomm’s Centriq 2400 for Windows Server and LinuxGallery:Qualcomm Launches 48-core Centriq for $2000: Arm Servers for Cloud Native Applications\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/12025/qualcomm-launches-48core-centriq-for-1995-arm-servers-for-cloud-native-applications\n",
      "Title: Arm Announces New Mali-D71 Display Processor and IP Blocks\n",
      "Author: Nate Oh\n",
      "Date Published: 2017-11-01T02:00:00Z\n",
      "URL: https://www.anandtech.com/show/11983/arm-announces-new-malid71-display-processor-and-ip-blocks\n",
      "Content: Today, Arm is announcing their Mali-D71 display processor and two closely-related IP blocks, CoreLink MMU-600 and Assertive Display 5, angling for 4K VR and HDR implementations on mobile displays. Previewed earlier in May under the codenameMali-Cetus, the D71 introduces the new Komeda display architecture, and combined with the new memory management unit (MMU) offers up to 4K120fps real-time performance. Meanwhile, Assertive Display 5 brings HDR capabilities and improved color and gamut management, adding onto the sunlight compensation and power-saver featureset of previous Assertive Display technologies.All three blocks were developed together, and in turn they possess mutual optimizations in order to achieve VR-capable performance and HDR functionality. The D71 and MMU-600 are tightly coupled to target high resolution and frame rates, while Assertive Display 5 enables HDR capabilities and adaptation to panel variety. Whether alone or together, premium smartphones and tablets are the most straightforward application, but previous display processors and IP blocks have been used for VR headsets and TVs/STBs.As mentioned in the Mali-Cetus overview, the D71 represents a clean architectural break from the Mali-DP650 and older display processors, which were more targeted for efficient sub-4K performance. In terms of generational uplift, Arm cites double the area efficiency and four times the latency tolerance, with a new side-by-side mode able double DP-650’s pixel throughput. The new fixed function hardware, covered in May, offloads work from the GPU and in turn saves power. Of those units, the new composition unit can handle and scale more display layers, functionality that works well in supporting multi-window on Android. For that purpose, the D71 has been optimized for the Android Hardware Composer HAL (HWC) and more for Android multi-window.Where the Mali-Cetus was described more in terms of 4K90, Arm commented that due to the improvements of the specialized D71-specific MMU-600, particularly the latency reduction, they were able to “guarantee” 4K120 for the D71 + MMU-600. Area-wise, that combination is 55% smaller than a DP650 + MMU-500. The MMU-600 also includes TrustZone Media Protection (TZMP).Assertive Display 5 represents the latest iteration of Assertive Display, and the first to be under the Arm umbrella since the acquisition of Apical, the original developers. Previously, Assertive Display revolved around a combination of tone-mapping, pixel processing, and adaptive screen brightness to save power or compensate for outdoor lighting, typically sunlight. These capabilities are extended into the HDR content that Assertive Display 5 supports.Assertive Display 5 brings HDR10 and Hybrid Log-Gamma (HLG) support, as well as HDR-to-SDR and HDR-to-HDR capabilities. Powered by an iridix8-HDR local tone-mapping engine, Assertive Display 5 can map content for the particular specifications of a display. Assertive Display also includes improved color and gamut management, particularly complementing HDR content.Additionally, Assertive Display 5 has blue light filtering. With all these features, Arm is aiming at keeping uniform color and lighting quality across HDR and SDR panels, and ideally compounding the advantages of HDR.The D71, MMU-600, and Assertive Display 5 are available now to partners, with the technology expected to appear in devices around early 2019.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/11983/arm-announces-new-malid71-display-processor-and-ip-blocks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: TSMC Teams Up with ARM and Cadence to Build 7nm Data Center Test Chips in Q1 2018\n",
      "Author: Anton Shilov\n",
      "Date Published: 2017-09-14T15:45:00Z\n",
      "URL: https://www.anandtech.com/show/11832/tsmc-teams-up-with-arm-and-cadence-to-build-7-nm-chip-in-q1-2018\n",
      "Content: TSMC has announced plans to build its first test chips for data center applications using its 7 nm fabrication technology. The chip will use compute cores from ARM, a Cache Coherent Interconnect for Accelerators (CCIX), and IP from Cadence (a DDR4 memory controller, PCIe 3.0/4.0 links). Given the presence of the CCIX bus and PCIe 4.0 interconnects, the chip will be used to show the benefits of TSMC’s 7 nm process primarily for high-performance compute (HPC) applications. The IC will be taped out in early Q1 2018.The 7 nm test chips from TSMC will be built mainly to demonstrate capabilities of the semiconductor manufacturing technology for performance-demanding applications and find out more about peculiarities of the process in general. The chip will be based on ARMv8.2 compute cores featuring DynamIQ, as well as a CMN-600 interconnect bus for heterogeneous multi-core CPUs. ARM and TSMC do not disclose which cores they are going to use for the device - the Cortex A55 and A75 are natural suspects, but that’s a speculation at this point. The new chip will also have a DDR4 memory controller as well as PCI Express 3.0/4.0 links, CCIX bus and peripheral IP buses developed by Cadence. The CCIX bus will be used to connect the chip to Xilinx’s Virtex UltraScale+ FPGAs (made using a 16 nm manufacturing technology), so in addition to implementation of its cores using TSMC’s 7 nm fabrication process, ARM will also be able to test Cadence’s physical implementation of the CCIX bus for accelerators, which is important for future data center products.TSMC's 7 nm Test Chip at GlanceLogicPHYCompute CoresARM v8.2 with DynamIQInternal Interconnect BusARM CMN-600CCIXCadenceDDR4 DRAM Controller?CadencePCI Express 3.0/4.0CadencePeripheral BusesI2C, SPI and QSPI by CadenceVerification and Implementation ToolsCadenceAs reported multiple times, TSMC’s 7 nm manufacturing process will be a “long” node and the foundry expects the majority of its large customers to use it. By contrast, the current 10 nm technology is aimed primarily at developers of smartphone SoCs. TSMC projects that its first-generation CLN 7FF fabrication technology, compared to its CLN16FF+, will enable its customers to reduce power consumption of their chip by 60% (at the same frequency and complexity), increase their clock rate by 30% (at the same power and transistor count) and shrink their die sizes by 70% at the same complexity. Sometime in 2019, TSMC plans to start making chips using its CLN7FF+ process technology with EUV for critical layers. TSMC claims that the CLN7FF+ will enable the company’s customers to further increase transistor density while improving other areas, such as yields and power consumption.TSMC does not disclose which of its 7 nm process technologies announced so far it is going to use for the test chip, but the use of EUV for test chips is something that cannot be excluded. For example, GlobalFoundries claims that they use EUV to accelerate production of test chips. On the other hand, since design rules for CLN7FF and CLN7FF+ are different, it is highly likely that TSMC conservatively uses the former for the test chip.TSMC’s CLN7FF process tech passed qualification in April and was expected to enter risk production in Q2 2017, according to TSMC’s management. The foundry expected 13 CLN7FF tape outs this year and it is projected that the fabrication technology would be used commercially starting from Q2 2018. Therefore, taping out the test vehicle using the first-gen DUV-only 7 nm process in Q1 2018 seems a bit late for early adopters who intend to ship their 7 nm SoCs in the second half of next year. Meanwhile, early adopters (read: Apple, Qualcomm, and some others) get access to new process technologies long before their development is completed and final PDKs (process development kits) are ready. Keeping in mind that the test chips feature a CCIX and PCIe 4.0 buses, it is clearly designed to show advantages of TSMC’s 7 nm process technologies for HPC applications. In fact, this is what TSMC says itself:“Artificial intelligence and deep learning will significantly impact industries including media, consumer electronics and healthcare,” said Dr. Cliff Hou, TSMC vice president, Research & Development/Design and Technology Platform. “TSMC’s most advanced 7nm FinFET process technology provides high performance and low power benefits that satisfy distinct product requirements for High-Performance Computing (HPC) applications targeting these markets.”Related ReadingGlobalFoundries Details 7 nm Plans: Three Generations, 700 mm², HVM in 2018Intel to Equip Fab 42 for 7 nmSamsung and TSMC Roadmaps: 8 and 6 nm Added, Looking at 22ULP and 12FFCARM Launches DynamIQ: big.Little to Eight Cores Per ClusterARM Announces Mali-G72: Bifrost Refined for the High-End SoCARM Announces Mali-G72: Bifrost Refined for the High-End SoC\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/11832/tsmc-teams-up-with-arm-and-cadence-to-build-7-nm-chip-in-q1-2018\n",
      "Title: ARM Announces Mali-G72: Bifrost Refined for the High-End SoC\n",
      "Author: Ryan Smith\n",
      "Date Published: 2017-05-29T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/11459/arm-announces-malig72-bifrost-refined-for-the-highend-soc\n",
      "Content: While the bulk of the focus in today’s ARM announcements is on major launch of the first CPU cores to support ARM’s DynamIQ topology –the Cortex-A55 and Cortex-A75– ARM’s GPU division isn’t sitting by idly. Rather, today the company is giving their GPU IP a timely refresh for the year with the announcement of the Mali-G72. ARM’s new high-end, high-performance GPU design, the Mali-G72 supplants the Mali-G71, undergoing a design revision and optimization against Mali-G71 to further improve performance and power efficiency for high-performance SoCs.Coming off of last year’s launch of the Mali-G71 and its underlying Bifrost GPU architecture, ARM isn’t doing anything quite as wild this year. The company is now invested into Bifrost for the long haul, so like the Midgard architecture before it, the company will be continuing to optimize, tweak, revise, and otherwise refresh the architecture to meet the needs of their customers a year or two down the line. Mali-G72 in turn is the first such revision of the architecture, taking advantage of what ARM learned when designing the G71 in order to improve on their high-performance GPU design.At the architectural level then, Mali-G72 doesn’t make any radical alterations to the Bifrost architecture. The SIMT quad based execution model stands firm, and the ratios of the various functional blocks have not changed. So clock-for-clock, Mali-G72’s fundamental, throughput on-paper is unchanged from Mali-G71.That said, the devil is in the details. And the details on Mali-G72 are all about optimizing. While ARM hasn’t made any high-level changes, the company has made a number of smaller, low-level changes that add up to a more significant impact for elevating Mali-G72 over Mali-G71. As a result the company is promoting the newer GPU design as offering 25% greater energy efficiency and 20% better performance density than Mali-G71, leading to a 40% performance improvement. Area and power efficiency are of course the lifeblood for mobile GPUs, and while the high-performance designs like the Mali-G71/G72 aren’t designed to push the envelope on area efficiency quite as much – favoring high performance instead – SoC vendors are all for trimming precious mm to reduce costs.ARM isn’t offering a great deal of information on where all of these optimizations come from – it’s very much the sum of a large number of small changes – but at have provided us some key pieces of information. In particular, ARM has actually removed some complex instructions from their architecture, instead executing them over multiple clocks as other, simpler instructions. Excising instructions one big way to save on die space, allowing ARM to throw out the transistors that would be needed to execute those instructions. Obviously this is a double-edged sword – the emulated instructions are slower – but instructions that aren’t used very often likely aren’t worth the silicon. In this case I suspect we’d be looking at some especially esoteric things, such as atomic floating points.ARM has also been doing some tinkering under the hood to improve the throughput of other desirable complex operations. This includes things such as the reciprocal square root and other reciprocal functions, which can now complete faster, but only for graphics (an interesting distinction, since the IEEE 754-compliant operation for compute remains unchanged). This goes hand-in-hand with a more broad set of tweaks to the internal datapaths for the ALUs, though besides further optimizing how data moves between the FMA and ADD/SF units, ARM hasn’t said much more.However when it comes to overall performance efficiency, the big changes on Mali-G72 aren’t at the instruction level, but rather the cache level. All-told, ARM has tweaked buffers and caches at pretty much every step of the way. This includes making the L1 cache, the writeback cache, and the tiler buffer all larger. Meanwhile the instruction cache is unchanged in size, but ARM has tweaked the logic of it (presumably the algorithm used) to improve utilization by reducing misses.All of this cache-related tweaks are geared towards the common goal of reducing memory bandwidth usage. Not only is this important for scaling performance with larger GPUs – GPUs get more powerful faster than memory bandwidth increases – but it improves power efficiency as well, as memory operations are relatively expensive. The overall performance improvement from the larger caches reducing misses certainly doesn’t hurt, either.ARM in turn is particularly pitching the benefits of the cache changes for both graphics and machine learning tasks. In the case of graphics, their case study of choice found a 42% reduction in how much off-chip memory bandwidth was used in G-buffer writes, primarily due to the larger write buffer. Bear in mind this is likely a case of cherry-picking, but ARM isn’t off-base in that more complex scenes push the limits of smaller buffers (finally justifying the area cost of larger buffers). Meanwhile on the machine learning front, ARM is reporting a 13% improvement in SGEMM benchmark energy efficiency (and 17% for HGEMM thanks to the combination of cache changes and the earlier mentioned instruction changes. One of ARM’s big pushes for their entire lineup of SoC IP is for inference at the edge, so even small improvements help their standings overall.Wrapping things up, we should see ARM’s new Mali-G72 design show up in devices in another year or so. While ARM isn’t responsible for the actual silicon their IP goes into – and as such, this is ultimately in the hands of SoC vendors – the Mali-G71 did show up on theHiSilicon Kirin 960only about 8 months after its launch. So if a partner wants to push it, they could do Mali-G72 in a similar amount of time. Though something closer toSamsung’s roughly 1 year cadenceis likely to be more par for the course.Gallery:Mali-G72 Press Deck\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/11459/arm-announces-malig72-bifrost-refined-for-the-highend-soc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Exploring DynamIQ and ARM’s New CPUs: Cortex-A75, Cortex-A55\n",
      "Author: Matt Humrick\n",
      "Date Published: 2017-05-29T04:00:00Z\n",
      "URL: https://www.anandtech.com/show/11441/dynamiq-and-arms-new-cpus-cortex-a75-a55\n",
      "Content: ARM moves at an aggressive pace, pushing out new processor IP on a yearly cadence. It needs to move fast partly because it has so many partners across so many industries to keep happy and partly because it needs to keep up with the technology its IP comes into contact with, everything from new process nodes to higher quality displays to artificial intelligence. To keep pace, ARM keeps multiple design teams in several different locations all working in parallel.At its annual TechDay event last year, held at one such facility in Austin, Texas, ARM introduced the Mali-G71 GPU—the first to use itsnew Bifrost GPU architecture—andthe Cortex-A73 CPU—a new big core to replace the A72 in mobile. Notably absent, however, was a new little core.Another year, another TechDay, and another ARM facility (this time in Cambridge, UK)—can only mean new ARM IP. Over the span of several days, we got an in-depth look at its latest technologies, including DynamIQ, theMali-G72 GPU, the Cortex-A75, and (yes, finally) the successor to the A53: Cortex-A55.The A53 was announced alongside the A57 and has been in use for several years, both on its own or as the little core in a big.LITTLE configuration. It’s been hugely successful, with more than 40 licensees and 1.7 billion units shipped in just 3 years. But during this time ARM introduced new big cores on a yearly cadence, moving from A57 to A72 to A73. The A53 remained unchanged, however, even as the performance gap between the big and little cores continued to grow.Predictably then, the focus for A55 was on improving performance. The A53’s dual-issue, in-order core, which serves as the starting point for A55, already delivers good throughput, so ARM focused on improving the memory system. A new data prefetcher, an integrated L2 cache that reduces latency by 50%, and an extra level of L3 cache (among other changes) give the A55 significantly better memory performance—quantified by a nearly 2x improvement in the LMBench memory copy test. The numbers provided by ARM also show an 18% performance gain in SPECint 2006 and an even bigger 38% gain in SPECfp 2006 relative to the A53. These numbers, as well as the others shown in the chart, comparing the A55 and A53 are at the same frequency, same L1/L2 cache sizes, same compiler, etc. and are meant to be a fair comparison. The actual gains should actually be a little higher, because partner SoCs will benefit from adding the L3 cache, which these numbers do not include.The additional performance does not come for free, however. Power consumption is up 3% relative to the A53 (iso-process, iso-frequency), but power efficiency still improves by 15% when running SPECint 2000 because of its higher performance.The A55 includes several new features too that will help it expand into new markets. Virtual Host Extensions (VHE) are very important for the automotive market and the advanced safety and reliability features, including architectural RAS support and ECC/parity for all levels of cache are critical for many applications, including automotive and industrial. There’s new features for infrastructure applications too, including a new Int8 dot product instruction (useful for accelerating neural networks). Because A55 is compatible with DynamIQ, it also gets cache stashing and access to a 256-bit AMBA 5 CHI port.When ARM announced the A73 last year, it talked a lot about improving sustained performance and working within a tight thermal envelope. In other words, the A73 was all about improving power efficiency. The A75 goes in a different direction: Taking advantage of the A73’s thermal headroom, ARM focused on improving performance while maintaining the same efficiency as the A73.Our previous performance testing revealed mixed results when comparing the A73 to the A72—not too surprising given the significant differences in microarchitecture—with the A73 generally outpacing the A72 by a small margin for integer tasks but falling behind the older CPU in floating point workloads. Things look better for the A75, at least based on ARM’s numbers, which show noticeable gains over the A73 in both integer and floating-point workloads as well as memory streaming.The graph above shows that the A75 operating at 3GHz on a 10nm node achieves better performance and the same efficiency as an A73 operating at 2.8GHz on a 10nm node, which means the A75 consumes more power. How much more is difficult to tell based on this one simple graph. We know that the A73 is thermally limited when using 4 cores (albeit less so than the A72), so the A75 definitely will be as well. This is not a common scenario, however. Most mobile workloads only fire up 1-2 cores at a time and usually only in short bursts. ARM obviously felt comfortable enough using the A73’s extra thermal headroom to boost performance without negatively impacting sustained performance.ARM wants to push the A75 into larger form-factor devices with power budgets beyond mobile’s 750mW/core too by pushing frequency higher. Something like a Chromebook or a 2-in-1 ultraportable come to mind. At 1W/core the A75 delivers 25% higher performance than the A73 and at 2W/core the A75’s advantage bumps up to 30% when running SPECint 2006. If anything, these numbers highlight why it’s not a good idea to push performance with frequency alone, as dynamic power scales exponentially.ARM targeted the A73 specifically at mobile by focusing on power efficiency and removing some features useful for other applications to simplify the design, including no ECC on the L1 cache and no option for a 256-bit AMBA 5 CHI port. With A75, there’s now a clear upgrade path from A72. For the server and infrastructure markets, A75 supports ECC/parity for all levels of cache and AMBA 5 CHI for connecting to larger CCI, CCN, or CMN fabrics, and for automotive and other safety critical applications there’s architectural RAS support, protection against data poisoning, and improved error management.On the next few pages, we’ll dive deeper into the technical details and features of ARM’s new IP, including DynamIQ (the next iteration of big.LITTLE), Cortex-A75, and Cortex-A55.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/11441/dynamiq-and-arms-new-cpus-cortex-a75-a55\n",
      "Title: AT20 Giveaway Day 15: ARM Connects You with Chromebooks\n",
      "Author: Ryan Smith\n",
      "Date Published: 2017-05-16T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/11396/at20-giveaway-day-15-arm-samsung-chromebook-plus\n",
      "Content: With a warm welcome to Tuesday, we’re now on day 15 of our20 day giveaway celebration of AnandTech’s 20thanniversary.Today’s prizes come courtesy of ARM. While not a product manufacturer in and of themselves – ARM’s business is all about IP and architecture licensing – the 800lb gorilla of the mobile world has their hands in a multitude of products thanks to the ubiquity and performance of the ARM CPU architecture and their own Cortex-A CPU designs. So for our giveaway, the company has sent over a pair of Samsung’s Chromebook Pluses, which are among the most popular of the ARM-powered Chromebooks on the market right now.Samsung Chromebook Plus (#1)Samsung Chromebook Plus (#2)The AnandTech 20th Anniversary Celebration – ARM architectures GiveawaySamsung Chromebook PlusTheSamsung Chromebook Plusis Samsung’s high-end ARM Chromebook offering. The 12.3” Chromebook features a 3:2 aspect ratio IPS touchscreen display, which offers a 2400x1600 pixel resolution. The laptop is 2.2lbs and 13.9mm thick, making it thin and light. This in turn is critical to its 2-in-1 design, allowing it to also be used as a tablet thanks to its 360 degree hinge and included stylus. Samsung’s Chromebook Plus is also one of what’s still only a handful of Chromebooks with shipping Android app support, allowing the aforementioned tablet functionality to mesh well with tablet-optimized Android applications.Samsung Chromebook PlusSoCOP12x ARM Cortex-A72 @ 2GHz4x ARM Cortex-A53 @ 1.5GHzARM Mali-T860MP4RAM4GB LPDDR3NAND32GB eMMCDisplay12.3\" 2400x1600Touchscreen IPS LCDDimensions280.8 x 221.6 x 12.9-13.9 mm,1.08 kgCamera720p FFCBattery39 WHrOSChromeOSConnectivity2x2 802.11ac, BT4.0,2x USB-C, microSDStylusWacom EMRAC Adapter30W USB-CKeyboard1.5mm stroke, backlit, curved capsSpeakers2x 1.5WUnder the hood, the Chromebook Plus is built around the ARM-powered OP1 SoC, which is a Rockchip-built and Google-approved SoC that packs a pair of ARM Cortex-A72 CPU cores for performance with a quartet of ARM Cortex-A53 cores for efficiency. On the GPU side, the SoC includes another ARM design, their Mali-T860MP4 GPU. Rounding out the package is 4GB of RAM, 32GB of eMMC storage, and 2x2 802.11ac WiFi. Meanwhile the 39Whr battery gives the Chromebook Plus a 9 hour battery life, while charging it is accomplished by using either of the device’s USB Type-C ports.Finally, as with our other giveaways, today’s giveaway is only open for 48 hours, so be sure to enter soon. However please note that for legal reasons, we’re only able to open these giveaways to residents of the United States.Good luck to everyone! And be sure to check in this afternoon for our next giveaway.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/11396/at20-giveaway-day-15-arm-samsung-chromebook-plus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Announces Mali-Cetus, Their Next-Generation Display Processor\n",
      "Author: Ryan Smith\n",
      "Date Published: 2017-05-01T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/11317/arm-announces-mali-cetus-display-processor\n",
      "Content: This morning ARM is taking the wraps off another new product in their Mali graphics portfolio: the Mali-Cetus. The oddly named product (ed: this is a codename, not the final product name) is the company’s latest generation display processor, designed for use in conjunction with ARM GPUs and other bits of Mali IP, and succeeding earlier products like theMali-DP550and Mali-DP650.ARM’s display processors don’t tend to attract much attention, as they’re a fairly slow-moving, low-key part of the larger display pipeline. The flip side of that is that on those occasions where ARM does make a significant technological change, it’s definitely worth noting. And in the case of the Mali-Cetus, that is very much the case as the company is rolling out a brand-new display processor architecture.The previous Mali-DP500/550/650 display processors were all based on a common architecture, with ARM iterating on it over the years. Mali-Cetus, on the other hand, is making a clean break, something we don’t see very often in this space. ARM is doing this in order to assemble a more modern design that can incorporate features such as HDR output and high throughput modes for VR, which were not a part of the earlier display processor family. Similarly,while previous generation processors could handle 4K, they weren’t especially efficient at it relative to lower resolution modes, as ARM was aiming a bit lower.The big feature additions enabled by Cetus can be broken down to improved layering and scaling, higher performance for VR, further power optimizations, and the ability to support newer transport standards such as HDR10 and Hybrid Log-Gamma. Each of these in turn is meant to address a specific market, such as smartphones/tablets, VR headsets, and TVs/STBs.As part of the new architecture, ARM is offering a bit more detail on the technical underpinnings of the architecture. At the same time, they’ve also changed how their processors are logically designed/broken down so that they are broken up into 5 different units: the ARM FrameBuffer Compression (AFBC) Unit, the Global Control Unit, the Layer Processing Unit, the Composition Unit, and the Display Output Unit. Each one, in turn, handles a specific type of display processing task.The Layer Processing Unit is arguably the head of the display pipeline – especially since it contains the memory interface and buffers. It and the AFBC unit do most of the heavy lifting when it comes to memory-intensive operations, such as simple, orthogonal rotations of display layers.However from a practical perspective, the composition unit is likely where most users will see or feel the real differences. Responsible for compositing and scaling the different layers within a UI, ARM has consistently been buffing up their composition capabilities over the years to handle Android’s increasingly complex UI and features. The Cetus display processor can now composite 8 layers – up from 7 on the previous-generation processors – and with 4 scaling engines, can scale up to 4 of those layers.This is especially significant for Android 7, which introduced multiple/side-by-side window modes, and each of which requires its own layer. Combined with any special layering effects on a given window (e.g. window animations), and a multi-window setup can easily eat up layers, not to mention pixels as well in the case of a high resolution tablet.Meanwhile Cetus as a whole has learned a few tricks to further improve power efficiency. One especially interesting trick is spitting up large (4K) layers into multiple smaller layers. Since Cetus contains multiple sub-units for processing multiple layers, this allows one large layer to be distributed over more hardware. The net result is that rather than having to clock up the chip and burn more power to process a large layer, the processor can go wide instead and stay lower clocked by spreading the work out, keeping down total power consumption.Finally, new to Cetus is an interface ARM is calling the coprocessor interface. ARM is primarily pitching it as a means for its customers to differentiate from one-another by using their choice of coprocessor. Over time the interface can potentially support a number of coprocessor blocks (including 3rdparty blocks), but for now ARM is launching with a block of their own: the Assertive Display block. Another product of last year’s Apical acquisition, the Assertive Display block is the company’s dedicated unit for tone-mapping, which is a critical part of mapping HDR image data to today’s (and tomorrow’s) SDR displays. In turn, the combination of Cetus and the Assertive Display block makes the combined product ARM’s first HDR-capable display processor solution.Wrapping things up, while it will take some amount of time for ARM’s new display processor to trickle down into SoCs and then finally consumer products, it should help them remain competitive in the display processor market. Besides improving their overall performance and energy efficiency, the specific features of the Mali-Cetus will enable ARM to finally supply display processor designs for VR headsets and 4K HDR TVs. The former is always an interesting market – one that is poised for rapid growth if it can find its killer app – while the addition of 4K HDR support will help ARM maintain and expand their footing in the very productive (and profitable) smart TV market, one that they have had a majority presence in for a number of years, and one where that trend is expected to continue.Gallery:ARM Mali-Cetus Presentation Deck\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/11317/arm-announces-mali-cetus-display-processor\n",
      "Title: ARM Announces Mali-C71: Their First Automotive-Grade Image Signal Processor\n",
      "Author: Ryan Smith\n",
      "Date Published: 2017-04-25T10:30:00Z\n",
      "URL: https://www.anandtech.com/show/11293/arm-announces-mali-c71-automotive-isp\n",
      "Content: ARM’s success in the CPU IP field has and remains the cornerstone of the company, but it has not been a company that sits idle. Over the years – and especially in the smartphone boom of the last decade – ARM has been investing their profits into expanding their business into other fields, both software and hardware. Now this morning the company is making their big move in theImage Signal Processing(ISP) market, taking the wraps off of their first in-house ISP design: the Mali-C71.ARM’s foray into the ISP market has been in the works for a bit now, and comes as a product of one of their 2016 acquisitions: imaging technology developer Apical. Picked up by ARM just under a year ago, Apical specialized in ISPs and computer vision technology, two growing markets for processor IP and two areas that meshed well with ARM’s own product plans, especially in the case of computer vision. With ARM further investing in their new imaging division to make Apical’s previous roadmap a reality, today the company is announcing the first imaging product developed and released as part of ARM.While there are a significant number of ISPs in the market – it is a growing field, but a highly competitive one – what makes the Mali-C71 particularly notable is how aggressively ARM is going after the high-end of the market. Immediately swinging for the fences, ARM’s first in-house ISP is designed specifically for Advanced Driver Assistance Systems (ADAS) market. ADAS a lucrative market that the company is expecting to grow quickly, but also one with extensive reliability and safety requirements.ARM expectation – and indeed much of the industry’s – is that cars are going to continue to have larger and larger numbers of processors and other electronics integrated into them. And while self-driving cars are obviously the end goal of this process, consumer products are still many years off. So ARM’s more immediate focus is on processor designs that would improve Level 1 and Level 2 driver assistance features, which brings us back to ADAS. The company is expecting the number of cameras in a given car to double (or more), and they want to be the vendor developing the processor IP that consumes all of that camera information.The Mali-C71 then is the first of the company’s Mali-C series of ISPs, designed specifically for this task. From a feature standpoint it’s a high-end ISP, supporting 4 real-time (streaming) camera inputs and another 16 non-streaming inputs. Altogether the ISP can stream/process 1.2GPixels/second, about twice as many inputs and twice as much data as predecessor Apcial’s contemporary smartphone ISPs. And if that’s not enough, then multiple ISPs can be setup to work in tandem to handle more camera inputs and more overall pixels. The ultimate goal of the Mali-C71 being to efficiently and most appropriately processing imaging data so that it can be passed on to the driver and to other processors for true computer vision processing.Raw specifications aside, however, a bigger part of ARM’s pitch to SoC developers and automakers is Mali-C71’s feature set, and this is where Mali-C71 is distinctly an ADAS-focused ISP. ARM is heavily focusing on both dynamic range and reliability features. The former being extremely useful to ADAS applications, and the latter being critical to getting anything on the road.In the case of dynamic range, the Mali-C71 can handle a rather massive 24 stops of camera range via multiple exposures. Which from a more practical perspective means that it can process 16.8 million (2^24) light intensity levels. This is over twice as many stops as most smartphone ISPs and is even better than DSLRs, but more importantly it’s critical to ARM’s ADAS goals. Because cars need to see exceptionally well in the dark as well as in the light – and because the ultimate goal for many in the industry is to do all-visual (no LIDAR/RADAR) self-driving – C71 needs to support enough stops to capture and map a very large dynamic range.The goal, in some ways, is to exceed human vision. Humans can see a rather wide range of intensities, but as anyone who’s driven at night can attest, we can’t see bright and dark at the same time. Supporting a large number of stops means that not only can ADAS systems see dark and light details at the same time – allowing computer vision processes to better understand what they’re seeing – but it can also quickly be tone mapped to something that can be displayed on a non-HDR (or at least not quite as HDR) automotive display for the drive.The back-half of the feature set for the Mali-C71 then is focused on the reliability side of the equation, so that CV systems and ultimately automakers can trust what the C71 is reporting. A big part of this is ASIL D compliance, which is the most risk-averse level under the ASIL scheme. This means that as a processor the Mali-C71 needs numerous safeguards such as self-testing and numerous fault-detection circuits, but it also needs safeguards in place with respect to imaging. The latter comes in the form of an additional data layer the ISP provides, the consistency plane. A form of sorts of image metadata, the consistency plane is responsible for several tasks, including informing consuming processors about motion and whether any unusual data (e.g. a bright pixel) is accurate or if it’s an erroneous hot pixel. In the case of hot pixels it’s important to detect them and pass that information on to computer vision systems so that they can determine if they need to act on the bright pixel they’re seeing or if it’s something that can be ignored.Moving on, while the focus of ARM’s announcement is on the ISP IP itself, the company is offering more than just hardware. Mali-C71 is intended to be a combined hardware + software product, with ARM also including a full reference software suite to go with the ISP. Customers will still want to tune the system to their specific needs – and ARM provides those tools as well – but it means that customers can hit the ground running with ASIL-compliant software as well.Finally, although ARM isn’t naming specific partners, they have stated that the lead partner for Mali-C71 already has working sample silicon. Other partners are farther back at the design or licensing stage. Automotive electronics have a significant lag period in development – the hardware needs to be certified, and then it needs to continue to ship for a number of years – so unlike consumer gear, Mali-C71 is a long-term product for ARM’s chip partners and the eventual automaker customers. Even after chip designers adopt it and get it into shipping chips, it will likely be years until it finally shows up in retail cars, which highlights just how long of a game ARM is playing here by getting into ISPs for ADAS.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/11293/arm-announces-mali-c71-automotive-isp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Launches DynamIQ: big.Little to Eight Cores Per Cluster\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2017-03-21T05:00:00Z\n",
      "URL: https://www.anandtech.com/show/11213/arm-launches-dynamiq-biglittle-to-eight-cores-per-cluster\n",
      "Content: Most users delving into SoCs know about ARM core designs over the years. Initially we had single CPUs, then paired CPUs and then quad-core processors, using early ARM cores to help drive performance. In October 2011, ARM introduced big.Little – the ability to use two different ARM cores in the same design by typically pairing a two or four core high-performance cluster with a two or four core high-efficiency cluster design. From this we have offshoots, like MediaTek’s tri-cluster design, or just wide core mesh designs such as Cavium’s ThunderX. As the tide of progress washes against the shore, ARM is today announcing the next step on the sandy beach with DynamIQ.The underlying theme with DynamIQ is heterogeneous scalability. Those two words hide a lot of ecosystem jargon, but as ARM predicts that another 100 billion ARM chips will be sold in the next five years, they pin key areas such as automotive, artificial intelligence and machine learning at the interesting end of that growth. As a result, performance, efficiency, scalability, and latency are all going to be key metrics moving forward that DynamIQ aims to facilitate.The first stage of DynamIQ is a larger cluster paradigm - which means up to eight cores per cluster. But in a twist, there can be a variable core design within a cluster. Those eight cores could be different cores entirely, from different ARM Cortex-A families in different configurations.Many questions come up here, such as how the cache hierarchy will allow threads to migrate between cores within a cluster (perhaps similar to how threads migrate between clusters on big.Little today), even when cores have different cache arrangements. ARM did not yet go into that level of detail,howeverwe were told that more information will be provided in the coming months.Each variable core-configuration cluster will be a part of a new fabric, with uses additional power saving modes and aims to provide much lower latency. The underlying design also allows each core to be controlled independently for voltage and frequency, as well as sleep states. Based on the slide diagrams, various other IP blocks, such as accelerators, should be able to be plugged into this fabric and benefit from that low latency. ARM quoted elements such as safety critical automotive decisions can benefit from this.One of the focus areas from ARM’s presentation was one of redundancy. The new fabric will allow a seemingly unlimited number of clusters to be used, such that if one cluster fails the others might take its place (or if an accelerator fails). That being said, the sort of redundancy that some of the customers of ARM chips might require is fail-over in the event of physical damage, such as automotive car control is retained if there are >2 ‘brains’ in the vehicle and there is an impact which disables one. It will be interesting to see if ARM’s vision for DynamIQ extends to that level of redundancy at the SoC level, or if it will be up to ARM’s partners to develop on the top of DynamIQ.Along with the new fabric, ARM stated that a new memory sub-system design is in place to assist with the compute capabilities, however nothing specific was mentioned. Along the lines of additional compute, ARM did state that new dedicated processor instructions (such as limited precision math) for artificial intelligence and machine learning will be integrated into a variant of the ARMv8 architecture. We’re unsure if this is an extension of ARMv8.2-A, which introduced half-precision for data processing, or a new version. ARMv8.2-A also adds in RAS features and memory model enhancements, which coincides with the ‘new memory sub-system design’ mentioned earlier. When asked about which cores can use DynamIQ, ARM stated that new cores would be required. Future cores will be ARMv8.2-A compliant and will be able to be part of DynamIQ.ARM’s presentation focused mainly on DynamIQ for new and upcoming technologies, such as AI, automotive and mixed reality, although it was clear that DynamIQ can be used with other existing edge-case use models, such as tablets and smartphones. This will depend on how ARM supports current core designs in the market (such as updates to A53, A72 and A73) or whether DynamIQ requires separate ARM licenses. We fully expect any new cores announced from this point on will support the technology, in the same way that current ARM cores support big.Little.So here’s some conjecture. A future tablet SoC uses DynamIQ, which consists of two high-powered cores, four mid-range cores, and two low-power cores, without a dual cluster / big.Little design. Either that or all three types of cores are on different clusters altogether using the new topology. Actually, the latter sounds more feasible from a silicon design standpoint, as well as software management. That being said, the spec sheet of any future design using DynamIQ will now have to list the cores in each cluster. ARM did state that it should be fairly easy to control which cores are processing which instruction streams in order to get either the best power or the best efficiency as needed.ARM states that more information is to come over the next few months.Gallery:ARM DynamIQ Slides\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/11213/arm-launches-dynamiq-biglittle-to-eight-cores-per-cluster\n",
      "Title: AppliedMicro's X-Gene 3 SoC Begins Sampling: A Step in ARM's 2017 Server Ambitions\n",
      "Author: Johan De Gelas\n",
      "Date Published: 2017-03-15T12:15:00Z\n",
      "URL: https://www.anandtech.com/show/11189/appliedmicro-x-gene-3-soc-starts-sampling\n",
      "Content: There has been a lot of recent movement in the ARM Server SoC space, with three major players.The third player, AppliedMicro, has been acquired by MACOM.MACOM has announced that the third generation 16-nanometer FinFET Server-on-a-Chip (SoC) solution, X-Gene 3, is sampling to \"lead customers\". Despite all the products so far on ARMv8, the server world continues to mature and to move forward.The AppliedMicro X-Gene 3Back in 2015, we reviewed the 40 nm 8-coreX-Gene 1(2.4 GHz, 45W), which found a home in HP's Moonshot processors. Performance wise the SoC was on par with the Atom C2750 (8 cores @ 2 GHz), but consumedtwice as much power, which led in our review to an overall negative conclusion. The power consumption issue was understandable: it was baked on a very old 40 nm process. But the performance was rather underwhelming, as we expected more from a 4-issue superscalar processor at 2.4 GHz. The Atom core, by comparison, was only a dual-issue design and offered similar performance at a lower frequency.Moving forward, we got the X-Gene 2. This was a refresh of the first design, but built on 28 nm. It was still at 2.4 GHz, but with a lower power consumption (35 W TDP) and a smaller die size of around 100 mm². Despite the relatively lackluster CPU performance, the overall efficiency increase meant that the X-Gene 2 did find a home in several appliances where CPU performance was not the top priority, such as switches and storage devices.MACOM, the new owners of the X-Gene IP, claim that the new X-Gene 3 is a totally different beast. The main performance claim is that it should be >6 times faster in SPECintRate than X-Gene 1 or 2. That performance increase is mostly because the new SoC has 4 times as many cores: 32 rather than 8. Besides the 32 ARMv8-A 64-bit cores in X-Gene 3, it will also include eight ECC capable DDR4-2667 memory channels, supporting up to 16 DIMMs (max. 1 TB), and 42 PCIe Gen 3.0 lanes.MACOM's reference X-Gene 3 platform has everything working at near full speed: all 32 cores are functional and run as fast as 3.3 GHz. The SoC design gives 32 MB of L3 cache through a coherent network, and we are told is 'at full speed'. PCIe, USB and integrated SATA ports all work at full speed also. Memory is initially limited to 2400 MT/s instead of 2667 MT/s, but considering that the current memory market only offers buffered DDR4 DIMMs at 2400, that is not an immediate issue.That set of specifications is impressive, but if the X-Gene 3 really wants to be a \"Cloud SoC\", performance has to be competitive. We look forward to testing.The ARM CompetitionThe other two players are Cavium and Qualcomm.Cavium has been on a buying spree as of late, acquiring Broadcom Vulcan IP and also Qlogic, a network/storage vendor. If Cavium can inject all that IP in it'sThunder-Xserver SoC line, its next generation could be a very powerful contender.Qualcomm will have its 48-coreCentriq-2400 SoCready by the second half of this year, and it will run Windows Server.Predicted Performance Analysis: Xeon-D AlternativeThe only performance figures for X-Gene 3 we have seen so far are the ones found in a Linley Group white paper that can be accessedhere:Based on testing of the current configuration of 3.0GHz CPU frequency and DDR4-2400, the company expects the chip to deliver a SPECint_rate2006 (peak) score of at least 500 when running at its peak speed of 3.3GHz and DDR4-2667 and with some additional hardware and compiler tuning.That benchmark value is the basis for the claim of \"6x more powerful than the predecessor\". We can somewhat predict how this can be possible, since SPECInt_Rate2006 scales almost perfectly: 32 cores instead of 8 already give us a 4 times increase. In order to get an overall 6x bump in performance then, each core must be (overall, including frequency) about 50% faster.Most of the performance boost will come from the frequency: as the SoC can boost to 3.3 GHz on X-Gene 3 over the 2.4 GHz X-Gene 2, this translates to a 37.5% increase. The rest of the gains are most likely related to IPC improvement, in branch prediction and TLB architecture. All in all, 6 times higher performance is not an outrageous claim, but there are few snakes in the grass to consider.Firstly, MACOM extrapolates from numbers at 3 GHz to 3.3 GHz. Thus the final frequency for the parts is still at the whim of tweaking and optimization, and may result in an increase in TDP over 125W. Also to note is that \"additional hardware and compiler tuning is necessary\", which is a general term for expected software improvements. While that might turn out to be true, other companies have promised similar and been unable to deliver, so until there's some proof it might be hard to determine at this point.Last year APM estimated that the new X-Gene 3 would achieve 550 SPECInt_Rate2006 at 3 GHz. That claim has been revised to 500 at 3.3 GHz.The graph above also seems to show SPEC scores run with GCC, as mostpublished scoresplace the Xeon E5-2580v4 at 669. While we favor resultsobtained with GCCtoo as they more realistic, based on experience we are wary that the graph above could paint a rosy picture of X-Gene's performance.The Linley Group states:“X-Gene 3 can handle a broad range of cloud workloads, including scale-up and scale-out applications. The processor excels on big data, particularly in-memory databases, because of its high memory bandwidth.\"The 8-channel 32 core X-Gene 3 achieves 67 GB/s. It is weird that the paper, written in March 2017, still mentions the use of DDR4-2133. If we compare the results to the typical Xeon scores we have measured in previous reviews, we get the following:Our testing methodology isdescribed here.For those of you who are not familiar with Stream: the CPU does not matter much. When there are enough cores/threads to generate sufficient demand on the memory subsystem, the peak bandwidth numbers are observed regardless of additional cores (seetesting doneby Dell). In some circumstances adding more cores actually gets a net decrease. So despite including Intel's top model in the graph above, there is no performance benefit.The 8-channel X-Gene 3 achieves, with 32 cores, somewhere between 24% (compared to the best result of the Xeon in ICC) and 63% better bandwidth than a similar single Xeon system with DDR4 at the same speed. But an Intel system with the same amount of memory channels would still be better. For comparison, but not listed on the chart, in our test of a single CPU Power8 system, itachieved 91 GB/sdue to its memory subsystem (using Centaur chips), despite our relatively simple GCC settings and the use of DDR3-1333. The X-Gene 3 bandwidth numbers are vastly superior to those of the X-Gene 1 (19 GB/s,see here), but it is worth noting that X-Gene 1 had only 4 channels using DDR3-1600.The X-Gene 3 results are more than respectable, but the official quotes from the Linley paper that 'the processor excels on big data' would seem to come across as an exaggeration without any direct benchmark data to back it up. As always at AnandTech, we like to make conclusions on hard data, and look forward to being able to verify the claims.ConclusionFrom the announcement and released data, the new X-Gene 3 core would appear to be the fastest ARM-v8a server SoC announced in the market so far. The engineers behind X-Gene deserve some applause for their tenacity, and for gradually improving their product to the point where it is a serious threat to the lower to mid-end of the Xeon E5 range. But those numbers need to be externally verified.There are still a number of uncertainties, for sure. The bandwidth numbers are good, but not impressive. The power usage has not been tested, and only publishing SPECInt_rate2006 estimates (that already have been revised downwards) does not by itself as a guarantee of good overall server performance for the platform.One thing is interesting: the arrival of the X-Gene 3 puts a lot of pressure on Intel's decision to artificially curtail the Xeon D* platform. Intel's fastest Xeon D (D-1587) offers lot of performance with 16 cores and 32 threads as 2.3 GHz, all inside a low 65W TDP - but the Xeon D has only 2 memory channels, can support only 128 GB of memory, and costs $1754 list price.*We say curtail, based on Xeon-D being based on Broadwell and rather than updating to the latest microarchitecture. Intel's recent release of new networking focused Broadwell-based Xeon-D parts suggests that an update to the platform might be far off in the distance.From what we can tell, the X-Gene 3 is rumored to cost less than $1200. At that price, it offers much more memory bandwidth and capacity, given its 8-channels and support for up to 1 TB. So although we have some reservations, we welcome the X-Gene 3 to be the cat among the Xeon D pigeons.Additional ImagesWhile at MWC, Anton was able to score some images of an X-Gene 3 system being demonstrated at the show. Despite it being a mobile show, given the size of ARMs presence, perhaps it might not be unexpected to see some of them on display. The unit was at the Kontron booth, and the date code on the heatspreader puts the manufacturing timing at 2016, week 53.Gallery:X-Gene 3 at MWCRelated ReadingApplied Micro's X-Gene: The First ARMv8 SoCARM Challenging Intel in the Server Market: An OverviewX-Gene 1, Atom C2000 and Xeon E3: Exploring the Scale-Out Server World\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/11189/appliedmicro-x-gene-3-soc-starts-sampling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Announces Mali-G51 Mainstream GPU, Mali-V-61 Video Processing Block\n",
      "Author: Ryan Smith\n",
      "Date Published: 2016-11-01T01:00:00Z\n",
      "URL: https://www.anandtech.com/show/10805/arm-announces-mali-g51-mali-v61\n",
      "Content: These days ARM and its customers are in the midst of a major evolution in GPU design. Back in May the company announced their newBifrost GPU architecture, a new and modern architecture for future GPUs. With Bifrost ARM would be taking a leap that we’ve seen many other GPU vendors follow over the years, replacing an Instruction Level Parallelism (ILP)-centric GPU design with a modern, scalar, thread level parallelism (TLP)-centric design that’s a better fit modern workloads.The first of these new Bifrost GPUs was introduced at the same time, and that was Mali-G71. However as our regular readers likely know, ARM doesn’t stop with just a single GPU design; rather they have multiple designs for their partners to use, running the gamut from high performance cores to area efficient cores. Mali-G71 was the former, and now this week ARM is introducing the latter with the release of the Mali-G51 design.If Mali-G71 was the successor to the Mali-T880, then Mali-G51 is the successor to theMali-T820 & T830. That is to say, it’s a mainstream part that has been optimized for performance within a given area – when SoC space and/or cost is at a premium – as opposed to G71’s greater total throughput. Broadly speaking, mainstream parts like Mali-G51 end up in equally mainstream SoCs like the Exynos 7870 (Galaxy A-series), as opposed to flagship-level SoCs like the Exynos 8890 (Galaxy S7). And along those lines, somewhat surprisingly, ARM is rather keen on talking about the VR market in conjunction with G51, even though it’s not their high-performance GPU design. Even G51, they’re confident, can offer good VR performance for the kinds of admittedly simpler workloads they have in mind.Meanwhile at a technical level, rather than just being a cut-down version of Mali-G71, Mali-G51 is an interesting GPU design in its own right. ARM has opted to go with a continuous development cycle for the Mali-G series, which means that each GPU is in essence branched off of the ongoing Mali design process when a new design is needed. That means besides market-specific optimizations, successive GPUs can contain features not found in earlier GPUs under the same brand, and that’s definitely the case for G51.So what sets G51 apart from G71? From the area efficiency perspective, the big change here is that ARM has reworked the shader cores to offer what they call a “dual pixel” design, as opposed to G71’s “single pixel’ design. In brief, per a G71 shader core could process 24 FLOPS (12 FMAs) over its three execution engines, while its texture and blending units could process 1 texel and 1 pixel respective. G51, by contrast, has adjusted the throughput ratio to more heavily favor pixel/texel throughput; a G51 shader core has the same 24 FLOPS throughput, but couples that with 2 texels and 2 pixels per clock. ARM did something similar in previous Mali Midgard generations – varying the number of ALUs – and the reason to do so is fairly straightforward, as advanced graphical effects are traditionally more shader-heavy than pixel-heavy. The end result being that for simpler workloads such as application UIs, the need for the shader throughput tends to scale down more rapidly in the mobile space.ARM Mali G SeriesMali-G71Mali-G51RoleHigh PerformanceArea EfficientCore Configurations4-32N/AALU Lanes Per Core (Default)1212Texture Units Per Core12Pixel Units Per Core12FLOPS:Pixel Ratio24:112:1APIsOpenGLES 3.2OpenCL 2.0VulkanOpenGLES 3.2OpenCL 2.0VulkanAnd while the dual pixel core is the biggest change for G51, it’s not the only change. By being based on a newer iteration of Bifrost, it includes a few notable, low-level tweaks to improve performance. Transcendental performance has been significantly improved; it turns out those operations are still used more often than ARM expected, G51 bakes in better support to maintain higher performance. There are also some outright new instructions on G51, and ARM’s framebuffer compression technology has been improved as well. Version 1.2 of AFBC implements some optimizations for better memory traffic shaping and burst lengths, as well as an improvement for constant color blocks.Overall, ARM is touting that G51 offers significant improvements to performance, density, and energy efficiency relative to the Mali-T830. On equal processes, G51 a mix of 30% smaller than T830, 60% better performance per mm2, and 60% higher performance per watt. I’m told area efficiency was the primary design in the goal, making the latter a pleasant surprise of sorts.Finally, like ARM’s other GPU IP announcements, this week’s announcement is about making the technology available to the company’s partners for implementation, rather than being a consumer-oriented announcement. ARM’s partners are already looking at early versions of the G51 design, and based on typical product development cycles, G51 should be showing up in devices in 2018.Mali-V61Meanwhile on a quick note, alongside the Mali-G51 GPU, ARM is also announcing the Mali-V61 video processor. This is the product formerly known asEgil, which ARM unveiled back in June while it was still under development. Now, along with G51, V61 is being released to ARM’s partners as well.V61/Egil has not significantly changed since we’ve last seen it. ARM’s fully modernized video encode and decode block follows a who’s who list of codecs and features, supporting 10-bit HEVC encode/decode and 10-bit VP9 encode/decode. Relative to the VP550 before it, ARM’s latest video processor supports a wider range of codecs, and now, having a full-feature HEVC encoder implementation, offers much better HEVC compression as well.Ultimately ARM is looking to sell Mali-V61 alongside Mali-G51 and their DP650 display process as a complete graphics solution to partners, which they call the Mali Multimedia Suite (though it can be used stand-along as well). And like Mali-G51, expect to see Mali-V61 start showing up in devices around a year from now.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10805/arm-announces-mali-g51-mali-v61\n",
      "Title: ARM TechCon 2016 Keynote Live Blog\n",
      "Author: Ryan Smith\n",
      "Date Published: 2016-10-25T16:25:00Z\n",
      "URL: https://www.anandtech.com/show/10786/arm-techcon-2016-keynote-live-blog\n",
      "Content: 12:27PM EDT- I'm here at the Santa Clara Convention Center for ARM's annual TechCon conference12:27PM EDT- This year's focus is expected to be on IoT devices12:28PM EDT- Of particular interest, Masayoshi Son of ARM's new owner Softbank will be speaking12:29PM EDT- Softbank's acquisiton of ARM still has an air of mystery to it. So hopefully we get a bit more insight here12:30PM EDT- And here we go12:31PM EDT- First up is Simon Segars, CEO of ARM12:33PM EDT- ARM partners shipped 15B chips last year. About $50B in sales12:34PM EDT- Big change for ARM this year is of course their buyout12:34PM EDT- A deal that was closed in 7 weeks12:35PM EDT- What's it going to do for ARM? What's it going to do for ARM's partners?12:37PM EDT- \"But why is it that Softbank bought ARM?\"12:39PM EDT- Masayoshi Son12:40PM EDT- He wastes no time in his opening statement, reiterating that ARM's direction isn't changing12:43PM EDT- Son is opening up with a somewhat unconventional analogy between evolution, the first species with eyes, and sensors12:45PM EDT- Biological sensing made a massive difference12:46PM EDT- Just like the Cambrian explosion, there will be an IoT explosion12:47PM EDT- So why buy ARM? Because this is the explosion of IoT, and Softbank wants to be there for it12:49PM EDT- Billions and billions of IoT devices over the next 20 years12:49PM EDT- Adding up to a trillion devices12:50PM EDT- \"Deep learning will make us super smart\"12:51PM EDT- But to have this IoT future, IoT devices need to be secure12:51PM EDT- At this point it's more important than performance12:52PM EDT- Hundreds of chips in a modern car, with very little security. There is no encryption12:55PM EDT- AI is already exceeding humans in some areas12:56PM EDT- The Singularity12:56PM EDT- (ed: Kurzweil's dream)12:57PM EDT- \"Whether we like it or not, technology will evolve\"12:59PM EDT- We want to make life better for everyone. And this revolutioln cannot happen by just one company (e.g. ARM needs its partners)12:59PM EDT- So no concrete details on tech, but a vision for why Softbank needs ARM01:00PM EDT- Now Q&A time01:01PM EDT- ARM partners already sells billions of microcontrollers, but what happens when they're all connected?01:04PM EDT- Son: \"I do not intend to micromanage\"01:05PM EDT- Son is reiterating that he's in this for the long haul. Decades, not quarters01:10PM EDT- And that's a wrap on the Q&A01:11PM EDT- Now for a bit more of a tech focus01:11PM EDT- ARM's Future Technology Research Group will be presenting on the future of scaling01:13PM EDT- Some people say 28nm was the best node ever. ARM agrees01:13PM EDT- However right now we need multiple patterning, and that gets expensive01:14PM EDT- Where do we go from here?01:14PM EDT- What can we do besides finer pitches?01:16PM EDT- Steppers are getting faster01:17PM EDT- If we don't get a good EUV tech for 5nm, there will be trouble01:19PM EDT- Going below 5nm will be about materials engineering. A replacement for silicon01:21PM EDT- Beyond transistors and wires. Find something better than SRAM01:24PM EDT- A lot of these boosters are one-time tricks01:24PM EDT- So are they worth it if it only helps once?01:26PM EDT- ARM joined the IMEC consortium this year01:28PM EDT- More discussion of what groups ARM is working with as of late on continuing Moore's Law01:29PM EDT- In summary, Moore's Law will struggle with pitches. But scaling boosters can help keep things going01:29PM EDT- (Chip design is about to get a lot less traditional)01:32PM EDT- But on the plus side, scaling up until now is what has made IoT possible01:32PM EDT- Final speaker for today is Mike Muller, ARM's CTO01:35PM EDT- Mike is focusing on wearables, and what they can be used for01:36PM EDT- An interesting discussion given the recent news that smartwatch sales have taken a large hit this year01:37PM EDT- Mike is reflecting on how things have changed for him in the last 16 years01:39PM EDT- The medical field is a key change01:39PM EDT- Now, how can modern IoT-type tech be used to improve this furter?01:42PM EDT- Can you use recent research to detect cancer early and cheaply?01:43PM EDT- More present day: using tiny sensors to measure inter-cranial pressure01:47PM EDT- \"There are 1001 vertical segments for IoT\"01:47PM EDT- \"We are currently in the feature phone era\"01:48PM EDT- \"The product works well, but it's a vertically integrated solution\"01:48PM EDT- What's coming next: the smartphone revolution for ioT01:49PM EDT- Being announced today on the product side:01:50PM EDT- Cortex-M23 and M3301:50PM EDT- CrytoCell-31201:50PM EDT- CoreLink SIE-20001:51PM EDT- And Cordio, the link layer RTL for wireless communications01:52PM EDT- nved Cloud: ARM's software-as-a-service cloud infrastructure for IoT devices01:52PM EDT- All with a focus on security and encryption01:54PM EDT- Where does securitty meet privacy in IoT?01:57PM EDT- Moving on, machine learning will change how data is correlated. Discovering correlations humans couldn't find before01:58PM EDT- \"innovation is not always about technology\"01:59PM EDT- We need to provide seucirty. We need to work on privacy so that users trust us with the data01:59PM EDT- And that's a wrap. Off to some more Q&As02:00PM EDT- :)\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10786/arm-techcon-2016-keynote-live-blog\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NVIDIA Teases Xavier, a High-Performance ARM SoC for Drive PX & AI\n",
      "Author: Ryan Smith\n",
      "Date Published: 2016-09-28T11:45:00Z\n",
      "URL: https://www.anandtech.com/show/10714/nvidia-teases-xavier-a-highperformance-arm-soc\n",
      "Content: Ever since NVIDIA bowed out of the highly competitive (and high pressure) market for mobile ARM SoCs, there has been quite a bit of speculation over what would happen with NVIDIA’s SoC business. With the company enjoying a good degree of success with projects like the Drive system and Jetson, signs have pointed towards NVIDIA continuing their SoC efforts. But in what direction they would go remained a mystery, as the public roadmap ended with the current-generation Parker SoC. However we finally have an answer to that, and the answer is Xavier.At NVIDIA’s GTC Europe 2016 conference this morning, the company has teased just a bit of information on the next generation Tegra SoC, which the company is calling Xavier (ed: in keeping with comic book codenames, this is Professor Xavier of the X-Men). Details on the chip are light – the chip won’t even sample until over a year from now – but NVIDIA has laid out just enough information to make it clear that the Tegra group has left mobile behind for good, and now the company is focused on high performance SoCs for cars and other devices further up the power/performance spectrum.NVIDIA ARM SoCsXavierParkerErista (Tegra X1)CPU8x NVIDIA Custom ARM2x NVIDIA Denver +4x ARM Cortex-A574x ARM Cortex-A57 +4x ARM Cortex-A53GPUVolta, 512 CUDA CoresPascal, 256 CUDA CoresMaxwell, 256 CUDA CoresMemory?LPDDR4, 128-bit BusLPDDR3, 64-bit BusVideo Processing7680x4320 Encode & Decode3840x2160p60 Decode3840x2160p60 Encode3840x2160p60 Decode3840x2160p30 EncodeTransistors7B??Manufacturing ProcessTSMC 16nm FinFET+TSMC 16nm FinFET+TSMC 20nm PlanarSo what’s Xavier? In a nutshell, it’s the next generation of Tegra, done bigger and badder. NVIDIA is essentially aiming to capture much of the completeDrive PX 2system’s computational power (2x SoC + 2x dGPU) on a single SoC. This SoC will have 7 billion transistors – about as many as a GP104 GPU – and will be built on TSMC’s 16nm FinFET+ process. (To put this in perspective, at GP104-like transistor density, we'd be looking at an SoC nearly 300mm2big)Under the hood NVIDIA has revealed just a bit of information of what to expect. The CPU will be composed of 8 custom ARM cores. The name “Denver” wasn’t used in this presentation, so at this point it’s anyone’s guess whether this is Denver 3 or another new design altogether. Meanwhile on the GPU side, we’ll be looking at a Volta-generation design with 512 CUDA Cores. Unfortunately we don’t know anything substantial about Volta at this time; the architecture was bumped further down NVIDIA’s previous roadmaps for Pascal, and as Pascal just launched in the last few months, NVIDIA hasn’t said anything further about it.Meanwhile NVIDIA’s performance expectations for Xavier are significant. As mentioned before, the company wants to condense much of Drive PX 2 into a single chip. With Xavier, NVIDIA wants to get to 20 Deep Learning Tera-Ops (DL TOPS), which is a metric for measuring 8-bit Integer operations. 20 DL TOPS happens to be what Drive PX 2 can hit, and about 43% of what NVIDIA’s flagship Tesla P40 can offer in a 250W card. And perhaps more surprising still, NVIDIA wants to do this all at 20W, or 1 DL TOPS-per-watt, which is one-quarter of the power consumption of Drive PX 2, a lofty goal given that this is based on the same 16nm process as Pascal and all of the Drive PX 2’s various processors.NVIDIA’s envisioned application for Xavier, as you might expect, is focused on further ramping up their automotive business. They are pitching Xavier as an “AI Supercomputer” in relation to its planned high INT8 performance, which in turn is a key component of fast neural network inferencing. What NVIDIA is essentially proposing then is a beast of an inference processor, one that unlike their Tesla discrete GPUs can function on a stand-alone basis. Coupled with this will be some new computer vision hardware to feed Xavier, including a pair of 8K video processors and what NVIDIA is calling a “new computer vision accelerator.”Wrapping things up, as we mentioned before, Xavier is a far future product for NVIDIA. While the company is teasing it today, the SoC won’t begin sampling until Q4 of 2017, and that in turn implies that volume shipments won’t even be until 2018. But with that said, with their new focus on the automotive market, NVIDIA has shifted from an industry of agile competitors and cut-throat competition, to one where their customers would like as much of a heads up as possible. So these kinds of early announcements are likely going to become par for the course for NVIDIA.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10714/nvidia-teases-xavier-a-highperformance-arm-soc\n",
      "Title: New ARM IP Launched: CMN-600 Interconnect for 128 Cores and DMC-620, an 8Ch DDR4 IMC\n",
      "Author: Johan De Gelas\n",
      "Date Published: 2016-09-27T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/10711/arm-cmn-600-dmc-620-128-cores-8-channel-ddr4\n",
      "Content: You need much more than a good CPU core to conquer the server world. As more cores are added, the way data moves from one part of the silicon to another gets more important. ARM has announced today a new and faster member to their SoC interconnect IP offerings in the form of CMN-600 (CMN stands for 'coherent mesh network', as opposed to cache coherent network of CCN). This is a direct update to CCN-500 series, which we've discussed at AnandTech before.The idea behind a coherent mesh between cores as it stands in the ARM Server SoC space is that you can put a number of CPU clusters (e.g. four lots of 4xA53) and accelerators (custom or other IP) into one piece of silicon. Each part of the SoC has to work with everything else, and for that ARM offers a variety of interconnect licences for users who want to choose from ARM's IP range. For ARM licensees who pick multiple ARM parts, this makes it easier for to combine high core counts and accelerators in one large SoC.The previous generation interconnect, the CCN-512, could support 12 clusters of 4 cores and maintain coherency, allowing for large 48-core chips. The new CMN-600 can support up to 128 cores (32 clusters of 4). As part of the announcement, There is also an agile system cache which a way for I/O devices to allocate memory and cache lines directly into the L3, reducing the latency of I/O without having to touch the core.Also in the announcement is a new memory controller. The old DMC-520, which was limited to four channels of DDR3, is being superseded by the DMC-620 controller which supports eight channels of DDR4. Each DMC-620 channel can contain up to 1 TB DDR4, giving a potential SoC support of 8TB.According to ARM through simulations, the improved memory controller offers 50% lower latency and up to 5 times more bandwidth. Also, the new DMC is being advertised as supporting DDR4-3200. 3200 MT/s offers twice as much bandwidth than 1600 MT/s, and doubling the channels offers twice the amount of bandwidth - so we can explain 4 times more bandwidth, so it is interesting that ARM claims 5x more, which would suggest efficiency improvements as well.If you double the number of cores and memory controllers, you expect twice as much performance in the almost perfectly scaling SPEC int2006_rate. ARM claims that their simulations show that 64 A72s will run 2.5 times faster than 32 A72 cores, courtesy of the improved memory controller. If true, that is quite impressive. By comparison, we did not see such a jump in performance in the Xeon world when DDR3 was replaced by DDR4. Even more impressive is the claim that the maximum compute performance of a 64x A72 SoC can go up by a factor six compared to 16x A57 variant. But we must note that the A57 was not exactly a success in the server world: so far only AMD has cooked up a server SoC with it and it wasslower and more power hungry than the much older Atom C2000.We have little doubt we will find the new CMN-600 and/or DMC-620 in many server solutions.The big question will be one of application: who will use this interconnect technology in their server SoCs?As most licensees do not disclose this information, it is hard to find out. As far as we know, Cavium uses its own interconnect technology, which would suggest Qualcomm or Avago/Broadcom are the most likely candidates.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10711/arm-cmn-600-dmc-620-128-cores-8-channel-ddr4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Announces the Cortex-R52 CPU: Deterministic & Safe, For ADAS & More\n",
      "Author: Ryan Smith\n",
      "Date Published: 2016-09-19T23:30:00Z\n",
      "URL: https://www.anandtech.com/show/10690/arm-announces-the-cortexr52-cpu-deterministic-safe-for-adas-more\n",
      "Content: Though it didn’t attract a ton of attention at the time, back in 2013 ARM announced the ARMv8-R architecture. An update for ARM’s architecture for real-time CPUs, ARMv8-R was developed to further the real-time platform by adding support for newer features such as virtualization and memory protection. At the time the company didn’t announce any specific CPU designs for the architecture, but rather just announced the architecture on its own.Now just under 3 years later, ARM is announcing their first ARMv8-R CPU design this evening with the Cortex-R52. An upgrade of sorts to ARM’s existing Cortex-R5, the R52 is the company’s first implementation of ARMv8-R. R52 makes specific use of many of the new features enabled by the architecture, while improving performance at the same time. ARM is pitching the new CPU core at markets that need a safety-critical CPU – a market that the Cortex-R series has been in for a while – where the deterministic nature of the CPU’s execution model is critical to ensuring quick and accurate execution.While the focus on today’s CPU design announcement is on functionality and utility over microarchitecture, ARM has revealed a bit about how the Cortex-R52 is organized under the hood. The microarchitecture is a direct evolution of the previous Cortex-R5. This means we’re looking at a dual-issue in-order execution pipeline, with a pipeline length of 8 stages. Broadly speaking, this description is very similar to that of the better-known Cortex-A7/A53 cores, which implies that this is a real-time optimized version of the basic elements in that design.As the Cortex-R series is focused on determinism and real-time responsiveness over total performance, ARM doesn’t heavily promote these cores on the basis of performance. But at least within the Cortex-R family, they are talking about a performance increase of upwards of 35% in common CPU benchmarks. More important for this market than throughput however is responsiveness: for the R52, ARM has done some specific work to improve interrupt entry and context switching performance, doubling the former and achieving a staggering 14-fold increase on the latter.The big deal here of course is the deterministic nature of the CPU. The entire microarchitecture is optimized to avoid variable time, non-deterministic operations, which is why it’s an in-order processor to begin with. This design extends to how memory is managed as well, with ARM avoiding a virtual memory system and its associated TLB translation-misses in favor of a model they call the Protected System Memory Architecture (PSMA), which is used in conjunction with an MPU to handle memory operations without the translation.On the safety side of matters, the R52 has a few different error-resiliency features to ensure accuracy. Multi-core lock step returns for this design, allowing two R52 cores to execute the same task in parallel for redundancy. And on the memory side of matters, ECC is offered across both the memory busses and the memory itself, in order to avoid random bitflips.Meanwhile in terms of new functionality for hardware developers, as part of ARMv8-R, Cortex-R52 implements support for hardware virtualization. Like virtually everything else in R52, this is deterministic as well, with the hypervisor working with the MPU to offer each guest OS its own section of the physical memory space. According to ARM this is a particularly important advancement, as previous means of separating tasks on real-time CPUs were non-deterministic, which is an obvious problem for the target market.The significance of virtualization in a real-time processor is that it allows for multiple tasks to be executed on the R52 without interfering with each other. In large, complex devices (e.g. cars), this allows for fewer processors within the device, as these tasks can be consolidated onto a smaller number of processors. At the same time, the rigid separation between the tasks means that it’s possible to run both safety-critical and non-critical (but still real-time) tasks on an R52 together, knowing that the latter will not interrupt the safety-critical tasks. For cars and other devices where there is stringent safety certification, this is especially useful as it means that other tasks can be added (via their own guest OS) without invalidating the certifications of the safety-critical tasks.This is also why ARM’s earlier context switching and interrupt entry improvements are so important. With a hypervisor now in play and multiple tasks executing on a single processor, the vastly improved ability to switch between tasks is critical for allowing multi-tasking without a major performance hit from context switching overhead.Finally, for the potential market for the Cortex-R52, ARM is pushing the big three traditional markets for real-time and safety-critical processors; automotive, industrial, and medical. All three of these make significant use of real-time functionality, and there’s also a great deal of overlap on safety as well.ARM is particular interested in the Advanced Driver Assistance Systems (ADAS) market, where the Cortex-R is part of a full system of ARM IP. A full ADAS setup from start to end would utilize all three processor types – M, R, and A – with the Cortex-R handling the real-time decision making and executing on those decisions, while Cortex-A would be used to handle sensor perception/interpretation, and Cortex-M would be in many of the individual sensors.Wrapping things up, as with most other ARM IP announcements, the announcement of the Cortex-R52 is setting the stage for future products. ARM isn’t talking about specific customers at this time, but they already have a number of companies who have licensed ARMv8-R and will be in need of a CPU design to go with it. To that end, we should be seeing Cortex-R52 start appearing under the hood of various devices in the coming years.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10690/arm-announces-the-cortexr52-cpu-deterministic-safe-for-adas-more\n",
      "Title: ARM Research Summit: Research Roadmap Keynote Live Blog\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2016-09-15T11:59:00Z\n",
      "URL: https://www.anandtech.com/show/10684/arm-research-summit-research-roadmap-keynote-live-blog\n",
      "Content: 12:33PM EDT- We're still at the ARM Research Summit, and the keynote talk on the ARM roadmap is about to start!12:33PM EDT- Eric Hennenhoefer, VP Research for ARM, on stage12:34PM EDT- Has a background in verification, moved to ARM when the company was acquired12:35PM EDT- ARM Licencing model: IP Provider, licence fees and royalties12:35PM EDT- 'Everyone who participates, wins'12:35PM EDT- 'Design things once, and reuse it across multiple partners'12:35PM EDT- 1250 licences, increases 100+ per year12:36PM EDT- 350+ partners for potential royalties12:36PM EDT- 86 billion ARM chips (not cores) shipped to date, 25% compound annual growth rate over last 5 years12:37PM EDT- Ongoing royalties are typically based on a percentage of chip price12:37PM EDT- Upfront licence fees cover development cost12:38PM EDT- 'There's no point making something, signing up partners, and they don't ship anything'12:40PM EDT- ARM Artisan IP is popular apparently12:40PM EDT- Acquired Apical in the last year12:41PM EDT- Price ranges from 50c / chip to over $25 (depends on partners, ARM doesn't make the chips remember)12:42PM EDT- ARM has silicon partners, software/training/consortia partners, design support partners12:43PM EDT- ARM Research as a unit are 3-7 years ahead of the product teams. That's important12:43PM EDT- ARM Research are based from n+212:43PM EDT- A combination of targeted and blue-sky research12:44PM EDT- Combination of internal research teams and academic/research partnerships12:44PM EDT- 'We work on disruptive technology roadmaps'12:45PM EDT- 'One of the jobs is to look at potential disruptive technologies, what they mean to ARM, and what it takes to make them real'12:45PM EDT- This includes attending conferences and knowing lots of people with potential12:47PM EDT- As the number of devices increases, research like energy harvesting becomes important12:47PM EDT- especially for 'edge' devices (ones at the end of the communication chain)12:48PM EDT- 'The compute is there, you can see the future coming'12:49PM EDT- ARM Research is split into skill areas12:49PM EDT- Some technologies die before it reaches ARM, but ARM still needs to keep abreast of partner development12:50PM EDT- Memory/Interconnect team deals with 3D Memory, non-volatile memory etc12:51PM EDT- The architecture group works with the product teams, but is working on long term projects12:51PM EDT- e.g. Scaleable Vector Extensions was in the works for 7 years12:51PM EDT- Also, 'More without Moore' is discussed in this group12:52PM EDT- Such as arch/uArch, or new methodology, or new materials12:53PM EDT- 'Let's not leave security to software, too many unsecure video teddy bears are being hacked - we need to bake it into hardware'12:53PM EDT- 'We also need to do it in cost effective ways'12:53PM EDT- 'The more protocols, the more ways to screw it up'12:55PM EDT- The Applied Silicon group goes through integration, sensor nodes, or things like printed electronics12:55PM EDT- Also future silicon technology, what's after MOS12:56PM EDT- 'Aggressively looking for new and interesting things'12:56PM EDT- 'Please invent something. Disruptive. In this area.'12:58PM EDT- Research into verification, HPC, efficiency, compute near memory12:59PM EDT- Machine learning, Graphics, Computer vision and mobile systems have strong research teams01:00PM EDT- Special projects such as motors and low power radio01:01PM EDT- ARM has doubled in since in a few short years01:01PM EDT- Didn't used to be interesting to Academia, was considered the keyboard chip people01:02PM EDT- ARM sees value in creating a Research Ecosystem, with discussions and events like this01:03PM EDT- Academia and ARM can work together - it depends on what you want to do01:04PM EDT- Cooperations involve publishing, open source, non-exclusive IP rights, pooled funding01:04PM EDT- Most academics are on the pre-competitive side01:04PM EDT- Some work in the Competitive side, which has more rules01:05PM EDT- NDAs and Patents/IP are issues to cover on a case-by-case basis01:06PM EDT- ARM encourages university enablement via free materials01:06PM EDT- starter kits, tools, some IP are free01:06PM EDT- More access can be had, have to ask. There may be free toys which can help depending on what we have01:08PM EDT- Accelerators are a common topic of discussion01:09PM EDT- IP enablement is a win-win when it accelerates research, especially when it gets disruptive01:09PM EDT- ARM Research has strong IP and legal teams to help smooth IP sponsorship with academia as well01:10PM EDT- ARM developing the Research ecosystem. The current Summit is an example01:11PM EDT- Participation in many EU academic collaborative projects01:11PM EDT- Lots of positions available at ARM, doubled in size in four years01:12PM EDT- Graduate positions, internships as well01:12PM EDT- Shanghai site is being ramped up as well01:14PM EDT- That's a wrap!\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10684/arm-research-summit-research-roadmap-keynote-live-blog\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Research Summit 2016 Keynote Live Blog\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2016-09-15T07:28:00Z\n",
      "URL: https://www.anandtech.com/show/10680/arm-research-summit-2016-keynote-live-blog\n",
      "Content: 03:41AM EDT- ARM's first Research Summit is happening today at Churchill College, Cambridge. We have near-front row seats and are expecting some details on future HPC plans today.03:42AM EDT- Initial comments from the pulpit: the SoftBank acquisition means business as usual03:42AM EDT- Also, 'Brexit means Brexit', but ARM is multinational and is still being awarded EU funding for projects.03:44AM EDT- Eric van Hensbergen first up03:45AM EDT- He was first involved with the 2002 Earth Simulatior, 35.6 TF, 640 nodes03:45AM EDT- Also involved in Roadrunner, first PF machine in 2008, then joined ARM in 2012 to lead the Exascale program03:46AM EDT- He's the Director of HPC03:47AM EDT- Back in 2012, 64-core ARM was a 'monster project', from Phytium. Realised 2015 with Mars using 64 'Xiaomi' cores03:49AM EDT- Mont-Blanc project, seeing what could be done with current ARM cores with the Barcelona Supercomputing Centre. Based on Exynos (A15) with Mali-T60403:51AM EDT- Discussing big.Little with HPC03:51AM EDT- In some instances, lots of small cores are better for SoC rather than big chunky ones.03:53AM EDT- Simulated energy department workload via PCA for HPC, with microarchitecture analysis (PCA)03:53AM EDT- Some workloads are core, L1, L2, L3 or DRAM dependent - vital to see what core designs make sense03:54AM EDT- Lots of workloads were cache sensitive, over core sensitive, and the graph shows this03:55AM EDT- Scaleable Vector Extensions (the new announcement) was a result of input from this testing and partners like Cray and Fujitsu03:56AM EDT- Neon wasn't enough, it was more DSP focused. Hence SVE was created03:56AM EDT- We saw a lot of workloads were pushing out vector lengths, so rather than redesign the uArch every 2 years, make an agnostic design03:57AM EDT- SVE is an optional part of the licencable ARMv8-A architecture03:58AM EDT- Aside from SVE, bottlenecks in memory and cache are frustrating. Working with Sandia National Labs and DoE to address this issue through new technologies03:59AM EDT- Lenovo servers with Cavium deployed in the UK were the first HPC ARM cores being shipped. 1152 64-bit ARM cores in 6U04:00AM EDT- Design centre in Manchester (UK) to focus on tools/libraries and runtimes for ARM HPC support in commercial04:04AM EDT- Porting OpenHPC packages for ARM04:05AM EDT- ARM is a silver member of OpenHPC04:06AM EDT- Currently at 131/166 packages ported04:06AM EDT- Member of many international standards - HSA, JEDEC, OpenSHMEM, CCIX, OpenCompute, OpenMP, HMC Consortium04:06AM EDT- 'Some of the competition consolidate the aspects of computing into their portfolio - ARM is about encouraging diversity and competition'04:07AM EDT- 'We want to test real world workloads so we can tune our general purpose architectures towards what people are facing'04:08AM EDT- 'You want Exascale to be widely used and widely applicable to the widest range of applications'04:08AM EDT- 'Data analysics, for design and application, is an important area we focus on'04:08AM EDT- A number of China partners are focusing a lot on HPC, there were six strains being followed and now down to three to be developed in 201704:08AM EDT- Also embedded HPC, next gen processors for space04:09AM EDT- Dedicated HPC Tools for ARM are all online. Building the community over time, user group meetings etc04:14AM EDT- Steve Furber, ICL Professor of Computer Engineering, University of Manchester04:14AM EDT- SpiNNaker project has been discussed for 20 years, developed over the last 1004:14AM EDT- The issue is the ability to simulate a brain04:14AM EDT- Part of the EU Big Brain Project04:15AM EDT- 'Simulating a brain can help a lot of common problems in many areas'04:16AM EDT- Current estimates to run a real-time human brain model require a post Exascale system04:17AM EDT- Obviously that takes tens of megawatts, and the human brain uses 20W or so04:17AM EDT- Conversely, a brain can't simulate a computer either.04:18AM EDT- Brains are comparatively slow, doing things on the order of a millisecond, not nanoseconds04:18AM EDT- The most efficient cores tend to be the ones that do the least work04:18AM EDT- Brain is very good for fault tolerance04:21AM EDT- The Human Brain project is a headline EU Flagship project, headline 1b euro budget over 10 years. or 100m/year for 120 institutions04:21AM EDT- Questions about Brexit are unknown, for now04:22AM EDT- If you're wondering what brains has to do with ARM HPC, it's like a big roadmap goal in computing in general, so ARM want to play a role04:22AM EDT- Hence why it's a big part of this Keynote04:23AM EDT- 'Turn the high performance computer into something you can interact with because it has a 'brain''04:24AM EDT- An issue with a million ARM SoCs, network topology is complex and scalability is an issue04:25AM EDT- A mouse is 1/1000 of a human brain, so that's an intermediate target04:26AM EDT- Apply computer-based mouse brain to a mouse robot, and if the result likes cheese, it's a ticked box04:27AM EDT- 'You decouple the topology of the network from the topology of what you need.'04:28AM EDT- 'You decouple the topology of the network from the topology of what you need.'04:28AM EDT- 'Time models itself and it all runs async. Processors have to cope'04:28AM EDT- Spinnaker chip uses a core and LPDDR memory from Micron in one 2.5D package04:29AM EDT- This package method saves 15% power04:30AM EDT- 18 ARM968 cores in a chip, at 130nm04:30AM EDT- ARM968, because it's cheap. DRAM is 1mm from the processor04:31AM EDT- The memory is scratchpad, not caches04:32AM EDT- The packet switch router is the key data transfer point04:32AM EDT- The router is the key innovation in spinnaker for realtime neural networks04:34AM EDT- The router shows processor spikes, and where spikes have to travel to. 3-state content addressable associative table04:34AM EDT- 1 incoming packet could become 24 outgoing packets, depending on table configuration04:34AM EDT- 'You decouple the topology of the network from the topology of what you need.'04:35AM EDT- dispatch the packet ASAP, which the hardware achieves04:36AM EDT- SpiNNaker will be used by non-ARM gurus, so SDKs and coding hierarchy is provided04:38AM EDT- Obviously this is a university project, so budgets are low. Hence ARM968, rather than say A5304:39AM EDT- 20k core machine requires 2kW04:39AM EDT- 100k cores at 10kW, managed to 5kW, idle much lower04:41AM EDT- Human Brain Project platform uses 500,000 cores, 6 cabinets, and any academic can use it.04:44AM EDT- Spinnaker uses 10nanojoule per spiked connection. Human brain is 10 femtojoule, BlueGene is about 1 Joule04:45AM EDT- Prof Furber developed a Sudoku solver in 36400 neurons, solves any sudoku in 10 seconds. Example project04:47AM EDT- Third part of the Keynote, dealing with cache and memory design for HPC for the 2020-203004:50AM EDT- Need to rethink memory, lots of chip space is dedicated to memory04:51AM EDT- THis talk is fast paced with lots of slide detail. I may resort to a lot of images here05:07AM EDT- Basically, Rowhammer is an issue. How to solve05:29AM EDT- Keynote is over, upload is 10KB/s so just waiting for the last pictures to upload...06:10AM EDT- Uploaded! There's two days of talks to go to for the summit. You can follow at #armsummit\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10680/arm-research-summit-2016-keynote-live-blog\n",
      "Title: ARM Announces ARM v8-A with Scalable Vector Extensions: Aiming for HPC and Data Center\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2016-08-22T08:37:00Z\n",
      "URL: https://www.anandtech.com/show/10586/arm-announces-arm-v8a-with-scalable-vector-extensions-aiming-for-hpc-and-data-center\n",
      "Content: Today ARM is announcing an update to their line of architecture license products. With the goal of moving ARM more into the server, the data center, and high-performance computing, the new license add-on tackles a fundamental data center and HPC issue: vector compute. ARM v8-A with Scalable Vector Extensions won’t be part of any ARM microarchitecture license today, but for the semiconductor companies that build their own cores with the instruction set, this could see ARM move up into the HPC markets. Fujitsu is the first public licensee on board, with plans to include ARM v8-A cores with SVE in the Post-K RIKEN supercomputer in 2020.Scalable Vector Extensions (SVE) will be a flexible addition to the ISA, and support from 128-bit to 2048-bit. ARM has included the extensions in a way that if included in the hardware, the hardware is scalable: it doesn’t matter if the code being run calls for 128-bit, 512-bit or 2048-bit, the scheduler will arrange the calculations to compensate for the hardware that is available. Thus a 2048-bit code run on a 128-bit SVE core will manage the instructions in such a way to complete the calculation, or a 128-bit code on a 2048-bit core will attempt to improve IPC by bundling 128-bit calculations together. ARM’s purpose here is to move the vector calculation problem away from software and into hardware.This is different to NEON, which works on 64-bit and 128-bit vectors. ARM is soon submitting patches to GCC and LLVM to support the auto-vectorization for VSE, either by directives or detecting applicable command sequences.Performance metrics performed in ARMs labs show significant speed up for certain data sets already and expect that over time more code paths will be able to take advantage of SVE. ARM is encouraging semiconductor architecture licensees that need fine-grained HPC control to adopt SVE in both hardware and code such that as the nature of the platform adapts over time both sides will see a benefit as the instructions are scalable.Gallery:ARM Announces ARM v8-A with Scalable Vector Extensions: Aiming for HPC and Data Center\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10586/arm-announces-arm-v8a-with-scalable-vector-extensions-aiming-for-hpc-and-data-center\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Softbank Acquires ARM in 24B GBP Deal\n",
      "Author: Joshua Ho\n",
      "Date Published: 2016-07-18T04:17:00Z\n",
      "URL: https://www.anandtech.com/show/10495/softbank-acquires-arm-in-24b-gbp-deal\n",
      "Content: While we normally focus on the technology, to have a holistic understanding of the industry it’s necessary to see the business side as well. While ARM has been an independent company for some time now, today Softbank has leveraged the fall in the value of the British Pound to buy ARM Holdings plc, the company responsible for the ARM ISA, ARM Cortex CPUs and MCUs, Mali GPUs, and numerous other IP blocks that are often used in SoCs today. If you’ve followed our coverage, this isn’t really a surprise, but Softbank is primarily a telecom and internet-centric company with relatively little focus on CMOS chip design.ARM is valued at 11.89 GBP per share, so to be bought out at 17 GBP per share is a fairly significant premium above the market rate. Looking at past investments it seems that Softbank is no stranger to these kinds of plays but it’s not clear whether Softbank will take a hands-on approach to managing the company or mostly leave ARM to its devices. One option is that they favor a more hands-off approach and ARM is primarily a financial investment for Softbank. It remains to be seen what kind of effect this will have, but the financial backing of a major Japanese conglomerate would likely allow for ARM to devote additional time and resources to designing new architectures to better target high performance mobile and server applications which could impact the industry significantly to ARM’s benefit.The other option is if Softbank takes a much more hands-on approach to managing ARM, which could be interesting if it helps root out whatever normalized deviance exists in management or engineering at ARM, but could also completely destroy the company if a poor approach is taken. Of course, there’s no way to know what path Softbank will take here, but regardless it’ll be an interesting few years in the semiconductor space.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10495/softbank-acquires-arm-in-24b-gbp-deal\n",
      "Title: ARM Announces Mali Egil Video Processor: VP9 Encode & Decode For Mobile\n",
      "Author: Ryan Smith\n",
      "Date Published: 2016-06-16T12:45:00Z\n",
      "URL: https://www.anandtech.com/show/10428/arm-announces-mali-egil-video-processor\n",
      "Content: Earlier this month we took a look atARM’s new Mali-G71 GPU. Based on the company’s equally new Bifrost architecture, Mali-G71 marks a significant architectural change for the Mali family, incorporating a modern thread level parallelism (TLP) centric execution design. The Mali GPU is in turn the heart of ARM’s graphics product stack – what ARM calls their Mali Multimedia Suite – but in practice it is not acompletegraphics and display solution on its own.As part of their IP development process and to allow SoC integrators to mix and match different blocks, the Mali GPU is only the compute/rendering portion of the graphics stack; the display controller and video encode/decode processor are separate. Splitting up these blocks in this fashion gives ARM’s customers some additional flexibility, allowing something like Mali-G71 to be mixed with other existing controllers (be it ARM or otherwise), but at the same time these parts aren’t wholly divorced within ARM. Even though they’re separate products, ARM likes to update all of the parts of their graphics stack in relative lockstep. To that end, with the Mali GPU core update behind them, this week ARM is announcing an updated video processor, codenamed Egil, to replace the current Mali-V550 processor.TheMali-V550 video processorwas launched back in October 2014, and these days is typically paired with Mali-T8xx series GPUs. V550 introduced basic HEVC support, and the forthcoming Egil processor will in turn build off of that. From a codec standpoint, the two principle additions to Egil are further fleshed out encode support for HEVC, and full encode/decode support for VP9.On the encode side, V550 implemented a form of HEVC that ARM politely calls HEVC Lite, which is not too far removed from doing limited HEVC encoding with an H.264 encoder. HEVC Lite lacked support for HEVC B-frames, and overall while it generated HEVC compliant streams, it couldn’t achieve the same quality/compression levels as a full HEVC encoder. To that end, Egil updates the Mali video processor to support full HEVC encoding, offering not just Main profile (8bpc) support, but Main10 profile (10bpc) support as well. This, interestingly enough, is something V550 could already decode, but now ARM finally has a video processor that can encode it as well.As for VP9, Egil adds both encode and decode support for the codec. VP9 has not seen quite the same level of usage as say H.264, but Google in particular favors it for their streaming services when it’s available, as it’s a royalty-free codec. This latter point is especially important as the patent holders behind HEVC have continued to struggle to develop a simple licensing scheme, which has further pushed ARM and other vendors towards VP9 and other royalty-free codecs. Egil in turn offers support for both Profile 0 (8bpc) and Profile 2 (10bpc) video, encoding and decoding. I suspect the decoding abilities will be the more useful feature there, but VP9 hardware encoding is still relatively new, so it’s hard to say how much we’re going to see it used.On a technical side note, after talking with ARM’s engineers, they seem especially proud of their encoding implementation for VP9’s reference frame scaling capability. Reference frame scaling allows the resolution of a stream to be altered on the fly – by scaling the resolution of the inter frames – with a goal of allowing for quick bitrate scaling of a stream. According to ARM’s engineers, doing reference frame scaling in real time in a low power hardware encoder was a Very Hard™ task that they’re quite proud they were able to get working.Moving on, at an implementation level, Egil retains the same overall architecture as V550 and other ARM video processors. Which is to say that ARM uses a multi-core design from 1 to 8 cores, to allow SoC integrators to scale Egil’s size and performance to meet their needs. The individual cores are essentially small processors on their own, implementing a very rigid DSP to handle the necessary non-fixed functionality.ARM says that a single core built on TSMC’s 16nm FinFET process and clocked at 800MHz will be enough for 1080p80 encode/decode, with a 6 core design scaling up to 4Kp120. For that reason ARM likes to tout that Egil is capable of exceeding what any mobile device needs – and indeed, I expect any large configurations would be used in something more along the lines of a TV – but there is still a need to balance Egil’s size and power requirements with mobile SoC design needs. Though it’s also worth noting that since Egil supports simultaneous video encoding and decoding (something used in video conferencing, for example), the total video processing requirements for a device can be a bit higher than they would initially seem.Wrapping things up, Egil is expected to be released later this year. And while it is not necessarily tied to the hip with Mali-G71 (especially since these designs aren’t being released simultaneously) I’d certainly expect Egil to eventually show up paired with a G71 in a future SoC.Gallery:ARM Egil Video Processor\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10428/arm-announces-mali-egil-video-processor\n",
      "Title: Investigating Cavium's ThunderX: The First ARM Server SoC With Ambition\n",
      "Author: Johan De Gelas\n",
      "Date Published: 2016-06-15T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/10353/investigating-cavium-thunderx-48-arm-cores\n",
      "Content: When is a worthy alternative to Intel's Xeon finally going to appear? That is the burning question in the server world. If PowerPoint presentations from various ARM-based SoCs designers released earlier this decade were to be believed, Intel would now be fighting desperately to keep a foothold in the low end server market. But the ARM SoCs so far have always disappointed: the Opteron A1100 was too late, the X-Gene 1performed poorly,consumed too much power, and Broadcomm's Vulcan project is most likely dead. This Ubuntu page is an excellent illustration of the current state of the ARM server market:Discontinued products, many announced products which do not even appear on this page (we are in the middle of 2016, after all), and despite the fact that there is anARM Server Base System Architecture(SBSA) specification, every vendor has its own installation procedure. It is still a somewhat chaotic scene.Meanwhile, Intel listened to their \"hyperscaler customers\" (Facebook, Google...) and delivered the Xeon D. We reviewedIntel's Broadwell SoCand we had to conclude that this was one of the best products that Intel made in years. It is set a new performance per watt standard and integrated a lot of I/O. The market agreed: Facebook's new web farms were built upon this new platform, ARM servers SoCs were only successful in the (low end) storage server world. To make matter worse, Intel expanded the Xeon D line with even higher performing 12 and 16 core models.But losing a battle does not mean you lose the war. Finally, we have a working and available ARM server SoC which has more ambition than beating the old Atom C2000 SoC. In fact, Cavium's ThunderX SoC has been shipping since last year, but you need more than silicon to get a fully working server. Firmware and kernel need to get along, and most libraries need to be compiled with platform-specific optimizations. So the quality assurance teams had a lot of work to do before Cavium could ship a server that could actually run some server software in a production environment. But that work has finally been finished. Cavium send us the Gigabyte R120-T30 running Ubuntu 14.04 server with a ThunderX ready Linux kernel (4.2.0) and ThunderX optimized tools (gcc 5.2.0 etc.).Cavium?Who is Cavium anyway? Even for those working in the enterprise IT, it is not a well known semiconductor company. Still, Cavium has proven itself as fabless network/security/storage and video SoC designing company. The company based in San José counts IBM, Juniper, Qualcomm, Netgear, Cisco among its customers.With a net revenue of about $400 million, Cavium is about one-tenth the size of AMD. But then again, Cavium either reports small losses or smal profits despite heavy investments in the new ARMv8 project ThunderX. In other words, the company's financials look healthy. And Cavium did already design a 16-core MIPS64 Octeon Network Service Processor (NSP) back in 2006. So Cavium does have a lot of experience with high core count SoCs: the network processor Octeon III CN78xx has 48 of them.Handling server applications is of course very different from network processing. A large amount of independent network packets creates a lot of parallelism, and more complex computation can be offloaded to co-processors. Still, Cavium is the first vendor that delivers an ARMv8 server chip with an impressive core count: no less than 48 cores can be found inside the ThunderX die.To keep the design effort reasonable, Cavium based their first ARMv8 server processor, ThunderX, on the Octeon III CN78xx. We described this in more detailhere, but the main trade-off is that Cavium used a relatively simple dual issue core. As a result, single threaded performance is expected to be relatively low. On the opposite side of the coin however, it is the first ARM SoC that has claimed throughput numbers in the realm of the best Xeon D and even midrange Xeon E5, instead of competing with the Atom C2000. It is the most ambitious ARMv8 SoC that has made it into production.All of this gives us plenty of reasons to put the Cavium ThunderX through paces. And throwing in the latest Supermicro boards with the latest 12 and 16 core Xeon-Ds made it a lot more interesting ...\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10353/investigating-cavium-thunderx-48-arm-cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Unveils Next Generation Bifrost GPU Architecture & Mali-G71: The New High-End Mali\n",
      "Author: Ryan Smith\n",
      "Date Published: 2016-05-30T11:00:00Z\n",
      "URL: https://www.anandtech.com/show/10375/arm-unveils-bifrost-and-mali-g71\n",
      "Content: Over the last few years the SoC GPU space has taken an interesting path, and one I admittedly wasn’t expecting. At the start of this decade the playing field for SoC-class GPUs was rather diverse, with everyone from NVIDIA to Broadcom (and everything in between) participating in it. Consolidation in the GPU space would be inevitable – something we’ve already seen with SoC vendors dropping out – however I am surprised by just how quickly it has happened. In just six years, the number of GPU vendors with a major presence in high-end Android phones has been whittled down to only two: the vertically integrated Qualcomm, and the IP-licensing ARM.That ARM has managed to secure most of the licensed GPU market for themselves is a testament to both their engineering and their IP licensing efforts. ARM’s path into this market has been non-traditional, having acquired an essentially unknown GPU vendor a decade ago, and growing it into the 800lb gorilla it has now become. ARM’s expertise in IP licensing, coupled with a somewhat unusual GPU architecture, has proven to be a powerful combination for the company as they have secured a number of significant wins from the high end to the low end.Much of this growth was built on the back of the company’s GPU architecture of the last few years, Midgard. Initially launched in 2012, Midgard has been the cornerstone of ARM’s Mali 600, 700, and 800 series designs. As ARM’s first unified shader design for GPUs, Midgard has been extended over the years to support newer features such as geometry tessellation and 10bpc color, along with newer APIs such as OpenGL ES 3.1/3.2 and Vulkan.However as Midgard approaches its fourth birthday and the SoC GPU landscape evolves, Midgard’s time at the top will soon be coming to an end. Amidst the backdrop of Computex 2016 and alongside their new Cortex-A73 CPU, ARM is announcing their next generation GPU architecture, Bifrost. A significant update to ARM’s GPU architecture, Bifrost will first be deployed in ARM’s Mali-G71 GPU.Recap: Mali & VLIWOne of the interesting aspects of SoC GPU development over the years is that it has been a very distinct echo of larger discrete GPU development. Many innovations and changes that first show up with dGPUs will show up in SoC GPUs a few years later, as newer manufacturing processes allow for those developments to fit within the extreme space and power requirements of an SoC-class GPU. At the same time mobile games/graphics development follows a similar path, with mobile application developers picking up rendering techniques first used elsewhere.ARM’s architectural development, in turn, has been a good example of this process. The non-unified Utgard architecture gave way to the unified Midgard architecture in 2012, about 6 years after dGPUs first made the transition. And as we learned when weexamined the Midgard architecture in depth, Midgard was an architecture well suited for the rendering paradigms of the time.Midgard’s shader core, in short, was an Instruction Level Parallelism-centric design, employing a Very Long Instruction Word (VLIW) instruction format. To achieve maximum utilization out of Midgard’s shader cores, you needed to be able to extract a significant amount of ILP – 4 concurrent instructions – in order to fill all of the slots in a shader core. This sort of design maps well to basic graphics workloads, as 4 color component RGBA is a natural fit for the 4 lanes of ARM’s VLIW-4 design. Furthermore VLIW designs are traditionally very space efficient, as there’s relatively little overhead logic, which is always a boon for the tight constraints of the SoC space.However getting back to what we said earlier about SoC GPUs being an echo of discrete GPUs, as we’ve seen there, VLIW does have a limited shelf life. Newer rendering paradigms often work with just 1 or 2 components at once, which leaves open lanes that need to be filled to achieve full GPU utilization. A good shader compiler can help here, but it does become an escalating technology war over time, as getting good performance becomes increasingly compiler-centric, and writing a compiler that can extract the necessary ILP is a challenge in and of itself. What history has shown us – and what is going to happen again in the mobile market – is that rendering workloads will continue to shift away from a style that is suitable for VLIW.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10375/arm-unveils-bifrost-and-mali-g71\n",
      "Title: The ARM Cortex A73 - Artemis Unveiled\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2016-05-30T03:00:00Z\n",
      "URL: https://www.anandtech.com/show/10347/arm-cortex-a73-artemis-unveiled\n",
      "Content: It’s only been a little over a year since we had a good look into ARM’s Cortex A72 presented at ARM’s TechDay event in London. Over the span of this year we’ve seen various vendors not only announce SoCs with the new core IP but actually deliver devices in high volume. HiSilicon’s Kirin 950/955 definitely left a long-standing impression on the industry by providing incredible power efficiency gains while continuing to improve performance.Fast-forward to 2016. Another year, another TechDay, this time from ARM’s brand-new offices in Austin Texas. So for today we have the pleasure to have a very deep look at ARM’s new Artemis CPU microarchitecture:A Look Back At Recent HistoryThe Cortex A73 is ARM’s new premium mobile CPU micro-architecture meant to succeed the Cortex A72 in consumer segments. Before we dive into details of the new CPU, it’s important to try understand and recap ARM’s CPU recent microarchitectures released the last few years.The Cortex A9 was an incredibly important design for ARM as, in my view, it provided the corner-stone for SoC and device vendors to create some of the designs that powered some of the most successful devices that brought with them a turning-point in smartphone performance and experience. Apple’s A5, Samsung’s Exynos 4210/4412, and TI OMAP4430/4460 were all SoCs which made the A9 a very successful CPU micro-architecture.Following the Cortex A9 we saw the introduction of the Cortex A15. The core was a substantial jump in terms of performance as it provided the single largest IPC improvement in ARM’s Cortex A-profile of application processors. While the A15 represented a large performance boost, it came at significant cost in terms of power efficiency and overall power usage. It took some time for the Cortex A15 to establish itself in the mobile space as the first designs such as the Exynos 5250 and 5410 failed to impress due to bad power efficiency due to various issues.It’s at this point where ARM introduced big.LITTLE with the argument that one can have the best of both worlds, a high-power performant core together with a low-power high-efficiency core. It was not until late 2014 and 2015 did we finally see some acceptable implementations of A15 big.LITTLE solutions such as the Kirin 920 or Exynos 5422.The Cortex A57 succeeded the Cortex A15 and was ARM’s first “big” core to employ ARMv8 64-bit ISA. Accompanied by the high-efficiency Cortex A53 cores this represented an important shift not unlike the x86-64 introduction in the desktop PC space well over a decade before. The cores came at a moment where the industry was still at shock of Apple’s introduction of the A7 SoC and Cyclone CPU micro-architecture, beating ARM in terms delivering the first 64-bit ARMv8 silicon. Suddenly everybody in the industry was playing catch-up in trying to bring their own 64-bit products as it was seen as an absolutely required feature-check to remain competitive.This pressured shift to 64-bit was in my view a crippling blow to many 2015’s SoCs as it forced vendors into employing sub-optimal Cortex A57 and A53 designs. HiSilicon and MediaTek saw an actual regression in performance as flagship SoCs such as the Kirin 930 and Helio X10 had to make due with only A53 cores for performance as they decided against employing A57 cores due to power consumption concerns. The Kirin 930 or the X10 were in effect slower chipsets than their predecessors. Only Samsung was fairly successful in releasing reasonable designs such as the Exynos 5433 and Exynos 7420 – yet these had respectively regressed or barely improved in terms of power efficiency when compared to mature Cortex A15 implementations such as the Exynos 5430. Then of course we had sort of a lost generation of devices due to Qualcomm’s unsuccessful Snapdragon 810 and 808 SoCs, a topic we’ll eventually revisit in our deep dive of the Snapdragon 820 and Exynos 8890.Some readers will notice I left out the Cortex A12 and A17 – and I did that on purpose in trying to get to my point. The Cortex A12 was unveiled in July 2013 and presented as a successor to the Cortex A9. The core had a relatively short lifetime as it was quickly replaced within 6 months with the Cortex A17 in February 2014 which improved performance and also made the core big.LITTLE compatible with the Cortex A7. The Cortex A17 saw limited adoption in the mobile space. In fact, among the few SoCs such as Rockchip’s RK3288 and some little known chips such as HiSilicon’s Hi3536 multimedia SoC, it was only MediaTek’s MT6595 that saw moderate success in design wins such as Meizu’s MX4.The MT6595 was actually an outstanding performer for its time delivering among the highest power efficiency while still providing excellent performance thanks to its 2.2GHz clock, all while competing with SoCs which had manufacturing node advantages. Unfortunately the Meizu MX4 suffered from some questionable hardware component choice and software decisions which ended up handicapping the device when it came to real-world battery life and performance.The overall impression of the MT6595 left me asking myself how the device ecosystem would have evolved if vendors hadn’t insisted on moving onto the 64-bit architectures but rather had chosen to adopt A17-based designs. It looked like the A17 wouldn’t have had much issues in scaling in clock and power and would have represented the better alternative for flagship devices until the new A72 and FinFET manufacturing processes became available.Introducing the Cortex A73Having finished my tangent on my view of ARM’s microarchitecture history, it’s time to get to the main story today, and that’s the new Artemis microarchitecture used in the Cortex A73. As ARM releases more and more microarchitectures with product names that often don't represent the evolution of a given microarchitecutre, it can get complicated when talking about successors to certain designs. I reached out to ARM to see if there's a more fitting terminology when refering to designs that a clearly related to one-another, and in fact it seems there is. For example, the A15, A57, A72 all belong to the Austin family of microarchitectures, and as one would have guessed from the name, this is because they originated from ARM's Austin CPU design centre.The A5, A7 and A53 belong to the Cambridge family while the Cortex A12, A17 and today's new A73 belong to the Sophia family, owning its name to the small city of Sophia-Antipolis which houses one of Europe's largest technology parks as well as ARM's French CPU design centre. Refering to their design location is however not enough to disambiguate microprocessor families, as we'll see completely new designs come out from each R&D center. In fact, this has already happened as the A12/17/73 can be seen as a new generation over the preceding the A9 microarchitecture and as such can be referred to as a \"second generation Sophia family\". This is an important notion to consider as in the future we'll be seeing completely new microarchitectures come out of ARM's various design teams.The Cortex A73 being still in the same Sophia family effectively means the design is very much a 64-bit successor to the Cortex A17. The new core effectively inherits some of the main characteristics of its predecessor such as overall µarch philosophy as well as higher-level pipeline elements and machine width. And herein lies the biggest surprise of the Cortex A73 as a A72 successor: Instead of choosing to maintain A72’s 3-wide, or increase the microarchitecture’s decoder width, ARM opted to instead go back to a 2-wide decoder such as found on the current Sophia family. Yet the A73 positions itself a higher-performance and lower-power design compared to the larger A72.ARM has recognized that power efficiency is a top priority for today’s smartphones. Two words that kept being repeated during the TechDay presentation was “sustained performance”. Performance of today’s mobile devices, and especially smartphones, is limited by their thermal envelope. The thermal envelope is the amount of heat that the body of the device is able to absorb and dissipate until it reaches an equilibrium state. We’ve covered this extensively inrecent reviewswhen testing the amount of time that a device is able to sustain its peak performance before it has to throttle its speed in order to maintain safe and comfortable operating temperatures.In the past this has been mainly a problem during heavy 3D workloads when SoCs dissipate power at the limits and often exceeding the thermal capabilities of the devices they’re employed in. With some exception the vast majority of today’s flagships are not able to maintain their peak performance for more than a few minutes, and this can be felt in everyday use-cases and can hinder a device’s experience. Over the last 2 generations this has been of especially grave concern as we’ve seen CPUs with peak power regularly exceeding 5W and some problematic SoCs reaching well into 10W+ figures.It’s imperative that the industry resolves these power and power efficiency concerns as device vendors continue to strive for thinner and lighter smartphone designs. SoCs such as the Kirin 950 which display excellent thermal characteristics must become the norm for the ecosystem to evolve and advance.ARM understands this and that is why the Cortex A73’s main focus is power efficiency and sustained peak performance. ARM felt that it could achieve these goals with a 2-wide design by improving various aspects of the micro-architecture while still maintaining the same IPC of the Cortex A72.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10347/arm-cortex-a73-artemis-unveiled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Details Built on ARM Cortex Technology License\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2016-05-30T03:00:00Z\n",
      "URL: https://www.anandtech.com/show/10366/arm-built-on-cortex-license\n",
      "Content: As part of today's announcements, we're able to provide more information on ARM's new \"Built on ARM Cortex Technology\" license. The license was first officially revealed in ARM's quartely financial call back in February, however at the time the company wasn't ready to talk about the exact details of this new license.We covered ARM's business and licensing modelsback a few years agoin a dedicated article which goes into more depth what kind of options vendors have when deciding to license an ARM IP. ARM likes to represent the licensing model in a pyramid shape with increasing cost and involvement the higher you get on the pyramid. Until now vendors had two main choices: Use one of the various available Cortex licenses, or get an architectural license and develop one's own microarchitecture based on ARM's ISA.The former licensing options varied depending on what kind of engagement and deployment a vendor is looking for. Lead licensees for example get early access to new microarchitectures but also have to pay more for this access and it's possible that they will have to deal with still immature toolkits and documentation, both which would then require more invovement and investment on their part. Vendors who are willing to wait a bit more or who aren't looking in an as deep engagement are able to use some of the cheaper licenses and more mature tools and documentation.The common limitation of all current Cortex licenses however is that a vendor is not able to change any aspect of the microarchitecture. If a customer needed a feature that ARM's cores didn't provide, they had to go with an architectural license and develop their own microarchitecture from scratch. Currently examples of such licensees with shipping custom microarchitectures include Apple, Qualcomm and Samsung.The new license being detailed today is the \"Built on ARM Cortex Technology\" license, which is quite a mouthfull and will unofficially refer to as \"Built on Cortex\"/BoC from here on. The new BoC license represents a new \"tip of the pyramid\" for Cortex licenses with even greater engagement than that of lead licensees.The new license allows vendors to request changes of an ARM microarchitecture and use this customized IP in their products. The way this works is that basically ARM provides its engineering and design services to the vendor who wants a certain aspect of an \"off-the-shelf\" Cortex design customized. Under the license's terms, ARM still owns and controls the IP, however the changes requested for that particular vendor's design is not shared or made available to other vendors.An example of a customization that a vendor would be able to request is the instruction window size. An increase in the instruction window size would increase the IPC of a microarchitecture, however this can cause higher area and power which would need to be compensated by more implementation work by the vendor.While ARM didn't want to go into details of what other customization options a vendor would have, they say that it will have a rather limited scope and things such as altering decoder width or changing the execution resources of a microarchitecture are beyond the scope of the license. In general, it seems more that the license is meant to allow vendors to tweak and configure the knobs on some aspects of a microarchitecture rather than do significant changes to the way the µarch works.What is in my view the most important and controversial aspect of the new license is that it allows vendors full branding freedom on this customized CPU design. This means that a Built on Cortex licensee is free to give the resulting new core any name it sees fit. We'll however still be able to differentiate the core from a full custom microarchitecture as ARM still requires a disclaimer / footnote / subtitle with the \"Built on ARM Cortex Technology\" phrase.In February ARM disclosed that Qualcomm is the first costumer signed up for this license, and what this means for the Snapdragon SoC lineup is currently still unclear. If this new licensing model will be able to allow vendors to truly differentiate their products beyond just the marketing aspect is something we won't know until the first designs come out and will be tested, and until then, the verdict on ARM's new license is still open.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10366/arm-built-on-cortex-license\n",
      "Title: Computex 2016: ARM Press Conference Live Blog\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2016-05-30T02:57:00Z\n",
      "URL: https://www.anandtech.com/show/10371/computex-2016-arm-press-conference-live-blog\n",
      "Content: 11:54PM EDT- OK, no more questions. That's a wrap!11:52PM EDT- CCI-550 is fully coherent so CPU/GPU can snoop each other11:51PM EDT- Question about Qualcomm not mentioned as a partner: 'we can only talk about companies that have agreed to go public at this time'11:50PM EDT- A73 is two wide, compared to A72 which was three wide, but improved branch prediction and other things11:50PM EDT- Apical acquisition is a step towards mobile user experience11:48PM EDT- ARM has been working with TSMC on 10nm for two years11:47PM EDT- Spec 2k numbers shown were done via simulation11:43PM EDT- Q&A session now11:42PM EDT- Yes, it's as cheesy as last year with the watering plants and thumbs up11:41PM EDT- Now a photo call11:41PM EDT- A small wrap up now. A73 + G71: the new premium ARM IP11:40PM EDT- 9 licencees on A73 already, 7 on Mali G7111:39PM EDT- Leading partners are HiSilicon/Huawei, Marvell, Mediatek, Samsung, TSMC11:39PM EDT- This slide shows all the ARM IP together11:38PM EDT- (but based on 10nm)11:38PM EDT- A73 + G71 + CCI 550 = 1.5x perf of 2016 devices11:38PM EDT- Now for SoC and Ecosystem11:38PM EDT- But power consumption is also down11:37PM EDT- That's a 13-14% increase in perf for A73 over A7211:37PM EDT- A72 to A73 on this graph looks like a 4.9x to 5.5x (compared to A7).11:35PM EDT- Spec 2k numbers11:35PM EDT- Compared A72 on 16nm vs A73 on 10nm11:35PM EDT- Under 0.65mm2 per core (inc NEON and L1 cache)11:34PM EDT- Up to 2.8 GHz, up to 30% higher perf in the same power envelope11:34PM EDT- Now introducing A7311:33PM EDT- More smartphone evolution in getting thin11:32PM EDT- G71 Will be seen up to 120 Hz, up to 4K, 4ms graphics pipeline latency and 4x MSAA11:31PM EDT- Mali G71 MP16 matches 2015 Discrete GPU Laptop in GFXBench 4.0 1080p Manhattan offscreen11:30PM EDT- Compared to T88011:30PM EDT- But G71 is not process fixed. Could do it in 14/16nm if a customer wanted due to improved energy efficiency and 40% better performance density11:29PM EDT- We'll see BiFrost in 10nm chips11:29PM EDT- First BiFrost GPU is Mali G71, a 16-shader config but scalable up to 3211:28PM EDT- But the Coherent Interconnect (is that CCI ?) works with both clusters and the GPU in a multi-cluster environment11:28PM EDT- ARM hasn't mentioned HSA regarding heterogeneous compute yet11:27PM EDT- BiFrost reduces 'wirelets' between shaders, decreasing die size and increasing performance11:27PM EDT- BiFrost focuses on efficiency, development and heterogeneous computing11:26PM EDT- We've covered Midgard extensively before:http://www.anandtech.com/show/8234/arms-mali-midgard-architecture-explored11:26PM EDT- following Utgard and Midgard11:25PM EDT- Announcing BiFrost: ARM's new GPU architecture11:25PM EDT- 'Approaching immersive experiences requires a new step'11:24PM EDT- Now Mail: 750m GPUs in 2015, 27 new licencees in FY1511:24PM EDT- ARM inquired Enlighten a couple of years back, it's used by a few major games developers and firms inc. Square Enix11:23PM EDT- Video for ARM's Enlighten. Global Illumination11:22PM EDT- Nandan Nayampally now on stage, VP Marketing and Strategy11:21PM EDT- 'Look at another 5 years in the future: we're innovating for performance, power and new use cases'11:21PM EDT- Compared to 2009, smartphone has: 300x GPU, 20x Connectivity, 24x Screen Resolution, 100x Compute, 5x sensors11:20PM EDT- '>90% home UHD devices are ARM based'11:20PM EDT- Things like CV and CNN typically requires immediate response. At the edge is important, rather than cycling via the cloud, but again, can be power hungry and might need dedicated ASIC11:19PM EDT- 'Merging ARM and Augmented Reality: GPU for image recognition and at-the-edge computer vision'11:17PM EDT- ARM is talking about people wanting to play console/PC titles on smartphones, being able to play a game no matter the platform. It's a lofty goal, but difficult under power restraints and gaming fidelity. Thinks like Vulkan help, but it also drives Console/PC gaming further too11:16PM EDT- 'AAA Gaming requires each pixel to look more realistic, which requires power'11:15PM EDT- for VR content consumption (video etc), battery will last longer, but same requirements11:15PM EDT- Slide says 4K 120 Hz: it might be a while before we see that in a smartphone with a battery life more than an hour when VR gaming11:14PM EDT- 'VR on smartphone needs high frame rate, high resolution, low latency'11:13PM EDT- 'VR is a hot market'11:13PM EDT- 'ARM seems well suited for new use cases not expected 2/3 years ago'11:13PM EDT- 'Smartphone: the portal for machine learning, digital assistants, consumer IOT, VR, AR'11:12PM EDT- 100M units for $30b = average $300 smartphone price? Seems high if the next 1b users are in developing areas11:11PM EDT- Smartphone growth for 2016 should be +100M units over 2015, source: Gartner and ARM Internal Data11:10PM EDT- A couple of years ago, ARM celebrated 50b chips shipped. In the last two years, another 25b shipped11:10PM EDT- Mali is still #1 GPU in terms of units shipped11:09PM EDT- Today's talk is focusing on mobile11:09PM EDT- 'Smartphones were still in their infancy'11:08PM EDT- Recalling Computex 2006, talking about laptops over desktops becoming more dominant11:08PM EDT- Rene Haas, EVP and CCO on stage11:07PM EDT- Andrei just posted his analysis of the new A73:http://www.anandtech.com/show/10347/arm-cortex-a73-artemis-unveiled11:07PM EDT- Part of the event is in English, some in Mandarin11:05PM EDT- ARM is having a press conference at Computex, and Billy and I are here for it. Almost ready to start!\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10371/computex-2016-arm-press-conference-live-blog\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Announces 10FF \"Artemis\" Test Chip\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2016-05-18T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/10329/arm-announces-10ff-artemis-test-chip\n",
      "Content: Today in collaboration with TSMC, ARM's physical IP division is announcing the tapeout of a 10nm test chip demonstrating the company's readiness for the new manufacturing process. The new test chip is particularly interesting as it contains ARM's yet-to-be-announced \"Artemis\" CPU core. ARM discloses that tapeout actually took place back in December 2015 and is expecting silicon to come back from the foundry in the following weeks.The test chip serves as a learning platform for both ARM and TSMC in tuning their tools and manufacturing process to achieve the best results in terms of performance, power, and area. ARM actually implemented a full 4-core Artemis cluster on the test chip which should serve as a representative implementation of what vendors are expected to use in their production designs. The test chip also harbours a current generation Mali GPU implementation with 1 shader core that serves as a demonstration of what vendors should expect when choosing ARM's POP IP in conjunction with its GPU IP. Besides the CPU and GPU we find also a range of other IP blocks and I/O interfaces that are used for validation of the new manufacturing process.TSMC's 10FF manufacturing process primarily promises a large improvement in density with scalings of up to 2.1x compared to the previous 16nm manufacturing node. At the same time, the new process is able to achive 11-12% higher performance at each process' respective nominal voltage, or a 30% reduction in power at the same frequency.In terms of a direct comparison between a current Cortex A72 design on 16FF+ and an Artemis core on 10FF on the preliminary test chip with an early physical design kit (PDK) we see that the new CPU and process are able to roughly halve the dynamic power consumption. Currently clock frequencies on the new design still don't reach what is achievable on the older more mature process and IP, but ARM expects this to change in the future as it continues to optimise its POP and the process stabilises.As manufacturing processes increasingly rise in their complexity, physical design implementation becomes an increasingly important part of CPU and SoC designs. As such, tools such as ARM's POP IP become increasingly important for vendors to be able to achieve a competitive result both in terms of PPA and time-to-market of an SoC. Today's announcement serves as demonstration of ARM commitment to stay ahead of the curve in terms of enabling its partners to make the best out of the IP that they license.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10329/arm-announces-10ff-artemis-test-chip\n",
      "Title: ARM Announces Cortex-A32 IoT and Embedded Processor\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2016-02-23T00:01:00Z\n",
      "URL: https://www.anandtech.com/show/10061/arm-announces-cortex-a32\n",
      "Content: Today ARM announces the new Cortex A32 ultra-low power/high efficiency processor IP. For some readers this might come as a surprise as it's only been a few months since we saw the announcement of the Cortex A35 which was presented as a replacement for the Cortex A7 and A5, so this leaves us with the question of where the Cortex A32 positions itself against both past IPs such as the A7 and A5, but also how it compares against the A35.The answer is rather simple: It's still a replacement for the A7 and A5, but targets even lower power use-cases than what the A35 was designed for. While ARM sees the A35 as the core for the next billion low-end smartphones, the A32 seems to be more targeted at the embedded market. In particular it's the \"Rich Embedded\" market that ARM seems to be excited about. The differentiation lies between use-cases which require a full-fledged MMU and thus able to run full operating systems based on Linux, and those who don't and could make due with a simpler micro-controller based on one of ARM's Cortex-M profile IPs. It's also worth to mention that although last time we claimed that the A35 would servce the IoT market, ARM seems to see wearables and similar devices as part of the \"Rich Embedded\" umbrella-term and thus now it seems more likely that it's the A32 that will be the core that will power such designs.This leads us to the mystery of what exactly is the A32? During the briefing the only logical question that seemed to come to mind is: \"Is this an A35 with 64-bit 'slashed off'?\" While ARM chuckled at my oversimplification, they agreed that from a very high-level perspective that it could be considered as an accurate description of the A32.In more technical terms, the A32 is an 32-bit ARMv8-A processor with largely the same microarchitectural characteristics of the Cortex A35. As a reminder to our readers out there: The ARMv8 ISA is not only an 64-bit instruction set but also contains many improvements and additions to the 32-bit profile commonly named as AArch32. Among the larger differences between the A35 and A32 is that the latter's microarchitecture has been tuned and optimized to achieve the best performance and efficiency for 32-bit.Indeed, performance wise, the A32 is advertised as being able to match the Cortex A35. The improvements lie in power efficiency: as a result of dropping its 64-bit capabilities, the new core is now able to achieve up to 10% better efficiency than the Cortex A35. Similarly to the A35, the A32 promises to achieve vastly superior performance per clock versus the Cortex A5 and A7, achieving anywhere from a 31% increase in integer workloads to a massive factor of 13x in crypto workloads, which the A32 is still capable of as they're included in the AArch32 ARMv8 profile.While only a few months ago the Cortex A35 was advertised as ARM's smallest Cortex-A core, this title has now been passed on to the A32. ARM claims the core is around 30% smaller than the A35; The decrease in size, mostly due to the slimming down of the micro-architecture due the removal of 64-bit capability, allows the Cortex A32 to scale down to <0.25mm² in its smallest configuration, a significant decrease compared to the A35's disclosed <0.4mm². The core remains as configurable as the Cortex A35, able to run as either as single core or any as a cluster up to four cores. Optionally vendors can also configure cache sizes, with L1 ranging from 8KB to 32KB and L2 either being completely absent to up to 1MB in size.ARM's philosophy of \"having the right design for the job\" now seems more apparent than ever as we see an steadily increasing portfolio of processor IPs specialized for different use-cases. The A32 seems to fit right in with this strategy and we'll more than certainly see a large array of devices powered by the core in the future to come.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10061/arm-announces-cortex-a32\n",
      "Title: ARM Announces New Cortex-R8 Real-Time Processor\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2016-02-18T00:01:00Z\n",
      "URL: https://www.anandtech.com/show/10049/arm-announces-cortex-r8\n",
      "Content: ARM’s Cortex-R range of processor IP is something we haven’t talked about too much in the past, yet it’s a crucial part of ARM’s business and is integrated in a lot of devices. ARM divides its CPU offerings into three categories – At the high-end performance end we find the Cortex-A profile of application processors which most of us should be familiar with as cores such as the Cortex A53 and Cortex A72 are ubiquitous in today’s smartphone media coverage and get the most attention. The low-end should also be pretty familiar as the Cortex-M microcontrollers are found in virtually any conceivable gadget out there and also has seen increased exposure in the mobile segment due to their usage as the brain inside of both discrete as well as SoC-integrated sensor-hubs.The Cortex-R profile of real-time processors on the other hand has seen relatively small coverage due to the fact that its use-cases are more specialized. Today with the announcement of the new Cortex-R8 we’ll be covering one well-established segment as well as an increasingly growing application of the real-time processors from ARM.In storage devices such as disk drive microcontrollers the Cortex R processors are well established as such systems require response-times in the microsecond range. These systems use increasingly complex algorithms for things such as error correction and the control software. SSDs in particular require increasingly higher performance controllers as data-rates increase with each generation. ARM discloses that currently all major hard-drive and SSD manufacturers use controllers based on Cortex R processors, which is least to say an interesting market position.Today’s announcement of the Cortex R8 was particularly centred on the use of R-profile processors in the modem space with a focus on the increasing performance requirements required to run future cellular standards such as LTE Advanced Pro and 5G. Here the processors are used for scheduling the data-flows through the signal processing for reception and transmission and as well run the protocol stack’s software tasks. These are so-called hard real-time tasks in which the processor must respond to events in the communication channel with a microsecond granularity. New standards such as 5G will vastly increase the transmission speeds to gigabits with complex carrier frequency and MIMO configurations which will also increase the feature-set requirements and workloads for the modem processors.ARM also discloses that modem designers are looking more and more to modems that manage Layer-1 scheduling activities to be done by software on the processor to provide more flexibility among different standards, something which requires a lot of investment and R&D to do in hardware.The Cortex-R8 is similar in architecture to the R7 - we still see usage of an 11-stage OoO (Out-of-Order) execution pipeline and clocks of up to 1.5GHz on a 28nm HPM process. The differences are found in the configuration options: The new core can now be deployed as a quad-core, versus the limited dual-core configuration of the R7, doubling the theoretical processing power over its predecesssor. The cores can also be run asymetrically and also each have their own power-plane, meaning they can be turned off for power savings and increased battery life. While concrete performance figures were a bit scarce, ARM talks about an example quad-core configuration on a 28nm or 16nm FinFET process being able to reach up to 15000 Dhrystone MIPS at 1.5GHz frequency.Cortex-R processors are able to employ a low-latency on-CPU memory called Tightly-Coupled Memory (TCM) which is able to be used as a predictable and guaranteed memory subsystem that is able to service interrupts as quickly as possible with code and data, avoiding longer and less deterministic latency cycles when fetching data out of the cache memory system. The Cortex R8 now is able to significantly increase the size of the TCM and now provides up to 2MB (1MB instruction, 1MB data, up from 128KB instruction/data on the R7) of TCM per core for a maximum of 8MB for a quad-core configuration.ARM disclosed one of the licensees being Huawei:“The ARM architecture is the trusted standard for real-time high-performance processing in modems,” said Daniel Diao, deputy general manager, Turing Processor Business Unit, Huawei. “As a leader in cellular technology, Huawei is already working on 5G solutions and we welcome the significant performance uplift the Cortex-R8 will deliver. We expect it to be widely deployed in any device where low latency and high performance communication is a critical success factor.”Among other licensees we'll also definitely see vendors such as Samsung who also currently deploy Cortex-R inside of their modems, such as the Shannon 333 found in last year's Galaxy devices.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10049/arm-announces-cortex-r8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Announces New 28nm POP IP For UMC Foundry\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2016-02-04T18:05:00Z\n",
      "URL: https://www.anandtech.com/show/10012/arm-announces-new-28nm-pop-ip-for-umc-foundry\n",
      "Content: Today ARM announces a new POP IP offering directed at UMC's new 28HPCUmanufacturing process. To date we haven't had the opportunity to properly explain what ARM's POP IP actually is and how it enables vendors to achieve better implementation of ARM's IP offerings. While for today's pipeline announcement we'll be just explaining the basics, we're looking forward to a more in-depth article in the following months as to how vendors take various IPs through the different stages of development.When we talk about a vendor licensing an ARM IP (CPU for example), this generally means that they are taking the RTL (Register Transfer Level) design of an IP. The RTL is just a logical representation of the functioning of a block, and to get to from this form to one that can be implemented into actual silicon requires different development phases which is generally referred to as the physical implementation part of semiconductor development.It's here where ARM's POP IP (Which by the way is not an acronym) comes into play: Roughly speaking, POP IP is a set of tools and resources that are created by ARM to accelerate and facilitate the implementation part of SoC development. This includes standard cell libraries, memory compilers, timing benchmarks, process optimized design changes and in general implementation knowledge that ARM is able to amass during the IP block development phase.The main goal is to relieve the vendor from re-doing work that ARM has already done and thus enable a much better time-to-market compared to vendors which have their in-house implementation methodology (Samsung and Qualcomm, among others, for example). ARM explains this can give an up to 5-8 month time to market advantage which is critical in the fast-moving mobile SoC space.One aspect that seemed to be misunderstood, and even myself had some unclear notions about, is that POP IP is not a hard-macro offering but rather all the resources that enable a vendor to achieve that hard-macro (GDSII implementation).This is where we come back to today's announcement. ARM's new POP IP targets UMC's new 28nm process called28HPCUfor ARM's Cortex A7 and Cortex A53 cores. The acronym has a dual meaning standing for 28nm High Performance Compact \"UMC\" or \"Ultra-Low IDDQ\" with IDDQbeing the leakage current which is being describes as being considerably lower than UMC's first-generation 28nm HKMG process and able to give significant battery life improvements to devices.While ARM isn't able to disclose which vendors use POP IP, they state that the main target is low-cost Asian market, which most likely means various Chinese vendors. According toS.C. Chien, vice president, corporate marketing, UMC:\"Multiple customers from a variety of applications have engaged with UMC to design their products on 28HPCU. Our collaboration with long-time partner ARM enables UMC to offer a comprehensive design platform with POP IP for two of the most efficient ARM processor cores.\"\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/10012/arm-announces-new-28nm-pop-ip-for-umc-foundry\n",
      "Title: ARM on AMD: The A1100 Seattle Silicon at SuperComputing 15\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2015-11-24T12:30:00Z\n",
      "URL: https://www.anandtech.com/show/9807/arm-on-amd-the-a1100-seattle-silicon-at-supercomputing-15\n",
      "Content: Anyone looselyfollowing AMD’s efforts with ARM intellectual property would have had on their periphery the noise of the A1100 productaimed at servers, codenamed Seattle. The idea was to use AMD’s back-end expertise to produce a multi-core ARM chip based on eight A57 cores for server and professional embedded systems, supporting up to 128GB of RDIMM memory and two 10GBase-KR Ethernet ports. The secret sauce of the processor is in the co-processors – a cryptographic one to offload dedicated acceleration of encryption/decryption or compression/decompression, and a system control co-processor that focuses on security and acts like a ‘processor within a processor’ with its own Ethernet connection, RAM, ROM and IO connectivity for remote management and sensing.The AMD A1100 –Steven’s piece last yeargoes into a lot more detail.The chip was designed to be the ‘scale-out’ ARM part that Calxeda attempted to produce with Cortex A9’s, but only marginally edged out a dual-Xeon running virtual machines – the step up to a more powerful core was vital to attack this element of the industry. Despite Calxeda’s attempts at dense web-server traffic requests, AMD is focusing more on the data center where it feels this chip is more suited for. So even though there have been many delays with the hardware, missing its original time-to-market window by at least nine months, I finally got to see the silicon for my own eyes.Starting with the platform it is in: this is the SoftIron Overdrive 3000. The silicon has access to eight SATA ports and eight PCIe lanes by default, along with a dual-channel memory controller, but what was interesting in this device was that there are six other SATA ports but no extra controllers. I quizzed the ARM personnel around the product and they said that a future chip might support more SATA ports, so this was almost a long-term drop-in PCB so for the second generation it doesn’t need a redesign. But this mini-server design is meant for that simple integration into a rack as an ARM development platform or connecting ARM to an accelerator/storage subsystem.We also had the HuskyBoard from 96Boards on display, aimed at the embedded development market with the same A1100 silicon in the center but with access to a slot of memory on each side (one on the rear), various power options (seems like DC-In and Molex) and a full PCIe 3.0 x8 slot. This almost looks like one of the maker boards we commonly see with other ARM based solutions.There we go, it exists! Speaking to other people in ARM, they are actually quite pleased that it is now in production, and they are also impressed with the internal metrics they are seeing from it. The march on ARM for server and embedded has been fraught with peril if you go too big too fast, but I wonder how many resources AMD is still putting into this project rather than their core business units.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/9807/arm-on-amd-the-a1100-seattle-silicon-at-supercomputing-15\n",
      "Title: ARM Announces New Cortex-A35 CPU - Ultra-High Efficiency For Wearables & More\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2015-11-10T17:31:00Z\n",
      "URL: https://www.anandtech.com/show/9769/arm-announces-cortex-a35\n",
      "Content: Today as part of the volley of announcements at ARM's TechCon conference we discover ARM's new low-power application-tier CPU architecture, the Cortex-A35. ARM follows an interesting product model: The company chooses to segment its IP offerings into different use-cases depending on market needs, designing different highly optimized architectures depending on the target performance and power requirements. As such, we see the Cortex-A lineup of application processors categorized in three groups: High performance, high efficiency, and ultra-high efficiency designs. In the first group we of course find ARM's big cores such as the Cortex A57 or A72, followed by the A53 in more efficiency targeted use-cases or in tandem with big cores in big.LITTLE designs.What seems to be counter-intuitive is that ARM sees the A35 not as a successor to the A53, but rather a replacement for the A7 and A5. During our in-depth analysis of the Cortex A53 in ourExynos 5433 reviewearlier this year I claimed that the A53 seemed to be more like an extension to the perf/W curve of the Cortex A7 instead of it being a part within the same power levels, and now with the A35 ARM seems to have validated this notion.As such, the A35 is targeted at power targets below ~125mW where the Cortex A7 and A5 are still very commonly used. To give us an idea of what to expect from actual silicon,ARM shared with us a figure of 90mW at 1GHz on a 28nm manufacturing process. Of course the A35 will see a wide range of implementations on different process nodes such as for example 14/16nm or at much higher clock rates above 2GHz, similar to how we've come to see a wide range of process and frequency targets for the A53 today.Most importantly, the A35 now completes ARM'sARMv8 processor portfolio with designs covering the full range of power and efficiency targets. The A35 can also be used in conjunction with A72/A57/A53 cores in big.LITTLE systems, enabling for some very exotic configurations (A true tri-cluster comes to mind) depending if vendors see justification in implementing such SoCs.At heart, the A35 is still an in-order limited dual-issue architecture much like the A7 or A53. The 8-stage pipeline depth also hasn't changed so from this high-level perspective we don't see much difference in comparison to preceding designs. What ARM has done though is to improve the individual blocks for better performance and efficiency byhaving bits and pieces of architectural enhancements that are even newer than what big cores such as the A72 currently employ.Areas where the A35 had focused attention on are front-end efficiency improvements, such as a redesigned instruction fetch unit that improves branch prediction. The instruction fetch bandwidth was balanced for power efficiency while the instruction queue is now smaller and also tuned for efficiency.It's especially on memory benchmarks where the A35 will shine compared to the A7: The A35 adopts a lot of the Cortex A53's memory architecture.On the L1 memory system of which A35 can have configurable 8 to 64KB of instruction and data caches we now see use of multi-stream automatic data prefetching and automatic write stream detection. The L2 memory system (configurable from 128KB to 1MB) has seen increased buffering capacity and resource sharing while improving write stream efficiency and introducting coherency optimizations to reduce contention.The NEON/FP pipeline has seen the biggest advancements, besides improved store performance the new units now add fully pipelined double precision multiply capability. The pipeline has also seen improvements in terms of area efficiency, part of the reason enabling the A35 to be smaller than the A53.In terms of power management, the A35 much like the A53 now implements hardware retention states for both the main CPU core and NEON pipeline (separate power domains). What seems to be interesting here is that there is now a hardware governor within the CPU cluster able to arbitrate automatic entry and exit for retention states. Until now we've seen very little to no use of retention states by vendors, the only SoC that I've confirmed to use it was the Snapdragon 810 and that was subsequently disabled in later software updates in favour of just using the core power collapse CPU idle state.At the same frequency and process, the A35 architecture (codenamed Mercury), promises to be 10% lower power than the A7 while giving an 6-40% performance uplift depending on use-case. In integer workloads (SPECint2006) the A35 gives about 6% higher throughput than the A7, while floating point (SPECfp2000) is supposed to give a more substantial 36% increase.What is probably more interesting are apples-to-apples performance and power comparisons to the A53. Here the A35 actually is extremely intriguing as it is able to match the A53's performance from 80% to up to 100% depending on use-case. Browser workloads are where the A35 will trail behind the most and only be able to provide around 80% of the A53's performance. Integer workloads are quoted at coming in at 84-85% of the Apollo core, while as mentioned earlier, memory-heavy workloads are supposed to be on par with the larger bretheren.What puts things in perspective though is that the A35 is able to achieve all of this at 75% the core size and 68% the power of the A53. ARM claims that the A35 and A53 may still be used side-by-side and even envisions big.LITTLE A53.A35 designs, but I have a hard time justifying continued usage of the A53 because of the cost incentive for vendors to migrate over to the A35. Even in big.LITTLE with A72 big cores I find it somewhat hard to see why a vendor would choose to continue to use an A53 little cluster while they could theoretically just use a higher clocked A35 to compensate for the performance deficit. Even in the worst-case scenario where the power advantage would be eliminated by running a higher frequency, vendors would still be able to gain from the switch due to the smaller core and subsequent reduced die size.The A35 is touted as ARM's most configurable processorwith vendors able to alter their designs far beyond simple choices such the core-count within a cluster. Designers will now also be able to choose whether they want NEON, Crypto, ACP or even the L2 blocks included in their implementations. The company envisions this to be processor for the next billion smartphone users and we'll likely see it in a very large variety of SoCs powering IoT devices such as wearables and embedded platforms, to budget smartphones and even high-end ones in big.LITTLE configurations.ARM expects first devices with the A35 to ship by the end of 2016.Due to the sheer number of possible applications and expected volume, the Cortex A35 will undoubtedly be a very important CPU core for ARM that will be with us for quite some time to come.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/9769/arm-announces-cortex-a35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Announces ARMv8-M Instruction Set For Microcontrollers – TrustZone Comes to Cortex-M\n",
      "Author: Ryan Smith\n",
      "Date Published: 2015-11-10T17:30:00Z\n",
      "URL: https://www.anandtech.com/show/9775/arm-announces-armv8m-instruction-set\n",
      "Content: Kicking off today in Santa Clara, California is ARM’s annual developer conference and expo, TechCon. Although ARM announces products year-round, they always have a couple of announcements reserved for TechCon and this year is no exception. Being unveiled at 2015’s show is theARM Cortex-A35 CPUand the ARMv8-M instruction set architecture, the latter being the focus of this article.As a brief bit of background since we don’t extensively cover ARM’s microcontroller efforts, in recognition of the unique power and performance requirements for microcontrollers, ARM produces a separate instruction set architecture and lineup of CPU cores specifically for these kinds of products. These are the ARM-M ISAs and the Cortex-M series of CPUs respectively. The ARM-M ISAs can be thought of as a cut-down version of ARM’s full ISAs, paring down the features to allow for simpler CPUs as needed in microcontrollers.At this year’s TechCon, ARM is announcing the latest iteration of the ARM-M ISA, the ARMv8-M ISA. Unlike the full ARMv8 (i.e. ARMv8-A) ISA that we’re accustomed to seeing implemented in products like ARM’s Cortex-A57 CPU, Apple’s Twister CPU, and other products, ARM’s focus on their microcontroller ISA is a bit narrower. Here the focus isn’t on performance or memory space – factors that led to the expansion to 64-bit CPUs with ARMv8-A AArch64 – but rather on continuing with microcontroller-suitable 32-bit CPUs while investing in the new features ARM sees as important over the next half decade or so.To that end, ARM’s big focus with ARMv8-M is on security. Key to that is that ARM’s TrustZone technology is coming to microcontrollers for the first time.Previously only available to ARM-A architecture CPUs, TrustZone is now being extended to ARM based microcontrollers. And like their bigger siblings, ARM’s aim here with TrustZone is to lay the groundwork for their customers to build highly secure devices, for all the benefits and drawbacks such a device entails. This includes protecting cryptography engines and certain stored assets (e.g. the secure enclave) against attack, locking down systems to prevent userland applications from breaking into the operating system itself, and various degrees of DRM (one example, as ARM gives is, is firmware IP protection).ARM over the last few years has been betting increasingly heavy on wearables and ioT, so the announcement of ARMv8-M and their focus on TrustZone is consistent with those bets. ARM microcontrollers are used in a number of devices as the sole processor, and in more devices still as a specialized processor working alongside a full ARMv8-A application processor. So as ARM microcontroller use increasingly expands from industrial devices and simple black boxes to complex devices that end-users interact with, there is a need for better security to follow into these products.With that said, as microcontrollers are the lowest of the low power devices in the ARM ecosystem, ARM had needed to take some care in implementing that security within the constraints of a microprocessor. Seeking to avoid compromising response time or efficiency, the ARMv8-M TrustZone retains the deterministic properties developers need on such devices, so a TruzeZone interrupt has a low and deterministic latency to the operation. Similarly, the core of the implementation is based on switching states rather than hypervisors, avoiding the overhead and higher resource requirements of the latter.Of course like the ARMv8-M ISA itself, TrustZone is an ISA and a model for just the CPU. To flesh out the full technology ARM is also making a couple of other ARMv8-M announcements. The first is that the company is announcing the ARM Advanced Microcontroller Bus Architecture 5 (AMBA 5) Advanced High-performance Bus 5 (AHB5) specification. The main system bus for ARM’s microcontrollers, AHB5 goes hand-in-hand with TrustZone to extend the security model to the rest of the SoC. Through AHB5, TrustZone microcontroller CPUs can interact with both trusted and non-trusted devices, including trusted segments of SRAM and flash memory as required for implementing separated storage.Also being announced today is TrustZone CryptoCell, ARM’s implementation of a TrustZone crypto block, which provides the fixed function hardware necessary for a full TrustZone implementation. The TrustZone CryptoCell includes a secure enclave, key generation/provisioning/management, and the actual fixed function hardware crypto engines.Ultimately with today’s ARMv8-M and associated security announcements, ARM is looking to further flesh out the ARM ecosystem to support full security at every level and every device from end to end. ARM believes that developers now need an easier and more standardized way to implement security on their microcontroller-equipped devices, and this is what ARMv8-M will provide.Finally, and not all that surprising, today’s announcement of the ARMv8-M ISA is just for the ISA itself, and not for any specific CPUs. ARM has traditionally announced new Cortex CPU designs separately from the ISA, and in this case it’s no different. To that end ARM isn’t specifically talking about when we’ll see ARMv8-M Cortex-M designs announced, but after today’s announcement it’s safe to say that it’s only a matter of time.Gallery:ARMv8-M Announcement Slide Deck\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/9775/arm-announces-armv8m-instruction-set\n",
      "Title: ARM Announces New CCI-550 and DMC-500 System IPs\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2015-10-27T08:00:00Z\n",
      "URL: https://www.anandtech.com/show/9743/arm-announces-new-cci550-and-dmc500\n",
      "Content: Today ARM announces two new additions to its CoreLink system IP design portfolio, the CCI-550 interconnect and DMC-500 memory controller. Starting off with the CCI announcement, we find the third iteration of the Cache Coherent Interconnect. The CCI is the cornerstone of ARM’s big.LITTLE strategy as it provides the required cache-coherent system interconnect between CPU clusters and other SoC blocks such as the main memory controllers and thus enabling heterogeneous multiprocessing between all the IP blocks.The CCI-550 is an improvement to the CCI-500 which ARMannounced back in Februaryamong other IPs such as the new Cortex A72 core design. Both the CCI-500 and the new CCI-550 are generational successors to the CCI-400 that is found in all currently released big.LITTLE SoCs such as Samsung’s Exynos, MediaTek’s Helio or Qualcomm’s Snapdragon designs. Back in February I was pretty excited to see ARM improve this part of their IP portfolio as it seemed that there was a lot of optimization that could be done in terms of performance and power.As a reminder, the primary characteristics of the new CCI-5X0 designs is the addition of a snoop filter within the interconnect that is able to maintain a directory of all cache contents among its coherent agents. On previous IP such as the CCI-400, all coherency messages needed to be broadcasted among all agents, causing them to have to wake up and respond. This not only impacted performance due to the increased latency but also had a power impact caused by the processing overhead. For the new CCI family, ARM explains that in heavy use-cases the new snoop filter can save up to “100’s” of milliwatts of power which is a quite significant figure.Due the broadcast nature of how the CCI-400 was operated, it meant that adding another coherent agent would have incurred a quadratical increase in the amount of messages such as snoop lookups. The CCI-500 on the other hand is able to take advantage of the new filter to increase the number of ACE (AXI Coherency Extension) master ports from 2 to 4 without increased overhead. This for example enabled the implementation of up to 4 CPU clusters if a vendor wished to do so. The new CCI-550 again improves this configuration option by raising the maximum number of ACE master ports to up to 6.In the example SoC layout diagram that ARM provides, we see the CCI-550 configured with two CPU clusters such as the Cortex A53 and a Cortex A72. The remaining four ACE master ports could be then dedicated to a fully coherent GPU.ARM explains that its still to-be-announced next-generation Mali IP codenamed \"Mimir\" will be fully cache-coherent and would be a perfect fit to take advantage of such a configuration (Current generation Midgard-based GPUs such as the T6-/7-/800 series are only I/O coherent). Fully coherent GPUs will be able to take advantage of shared virtual memory and new simplified programmers models provided by APIs such as OpenCL 2.0 and HSA.While the amount of ACE master ports increases from 4 to 6, the amount of possible memory interfaces has also gone up from a maximum of 4 to up to 6. This allows an increase of up to 60% in the total peak interconnect bandwidth (total aggregate bandwidth). This improvement not only comes from the two additional memory interfaces, but also an additional increase which can be credited to micro-architectural improvements on the interconnect itself. For example, we're told the CCI-550 is able to reduce CPU-to-memory latency by 20% when compared to the CCI-500.ARM explains that its CCI IP is highly customizable and thus each vendor can configure it to their needs. The IP will be able to scale in terms of physical implementation based on the number of desired interfaces and ports.As an IP vendor, ARM is aiming to provide highly optimized integrated solutions, and memory controllers are consequently part of such designs. ARM previously offered the DMC-520 with DDR4 support but this memory controller was aimed at more complex enterprise designs employing AMBA 5 system IP such as ARM’s CCN (Cache Coherent Network). The DMC-500 announced today on the other hand is ARM’s first mobile-targeted memory controller with support for the new LPDDR4 memory standard. Aimed for AMBA 4 system IPs such as the CCI family, this is the memory controller IP we’ll most likely see adopted by vendors in consumer devices such as smartphones.The DMC-500 promises support for LPDDR4 up to 2133MHz while still maintaining LPDDR3 compatibility. This is an important differentiation factor as in doing so ARM is able to offer maximum flexibility in terms of choice of implementation for vendors. Performance wise, ARM promises up to 27% increase in memory bandwidth utilization in a low power design.All in all today’s announcements provide some solid improvements in ARM’s IP portfolio. On the memory controller side I’m not certain what the rate of adoption ARM’s DMC’s is; as far as I know the main \"heavyweight\" SoC vendors currently chose to employ their own memory controller IP. Those who don’t have their own IP and instead use ARM's designs are often hard to single out as many times the choice of memory controller is completely invisible to the system.On the interconnect side I predict that we’ll be seeing a lot more discussions and developments from third-party vendors. Even among today’s higher-profile big.LITTLE SoCs I’m only aware of LG’s Odin to use ARM’s CCI as a “center-piece” in their SoC fabric while other vendors such as Samsung chose to implement it alongside their own interconnect fabric. Vendors who have the resources and design talent may also chose to implement cache coherency into their own interconnect IP. They would thus be able deploy big.LITTLE systems or other similar fully coherent SoCs without ARM’s CCI IP. For example, MediaTek is among the first to do exactly this inthe Helio X20with help of the in-house designed MCSI. Next year we should be seeing new big.LITTLE SoCs equipped with both ARM’s IP such as the CCI-500 or 550 alongside third-party IP, creating a new differentiation point for SoC vendors that will undoubtedly make competitive landscape much more interesting.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/9743/arm-announces-new-cci550-and-dmc500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Announces Mali-470 GPU: Low Power For Wearables & More\n",
      "Author: Ryan Smith\n",
      "Date Published: 2015-10-20T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/9733/arm-announces-mali470-gpu-low-power-for-wearables-more\n",
      "Content: One of the more surprising facts we learned from ARM when wesat down with them last year to discuss their Mali GPU architecturewas just how successful the Mali-400 family was and is; not just in the past tense but in the present tense as well. Although ARM has been selling Mali T600/700/800 designs based on their Midgard architecture for a few years now, the Utgard based Mali-400/450 have continued to sell well due Utgard’s “no-frills” OpenGL ES 2.0 design. This has made the Mali-400/450 popular parts in low cost and low power environments, where a complete Mali design takes up a minimal amount of die space and similarly small amounts of power.Though chasing the low-end of the market doesn’t come with the glamour of the high-end (and for that there’s Midgard), for ARM it has in many ways the textbook low-price/high-volume success story. The company’s estimates put them in plenty of low-end tablets and smartphones, but more unexpectedly they have a lion’s share of the smart TV market, with 75% of TVs implementing a Mali design. With TVs only requiring enough graphics power for basic UI functions and video decoding at 1080p – few smart TVs play games or do fancy transparency/animation effects – even after having been eclipsed by more powerful GPUs at the high-end, the Mali-400 family has found a second home in these sort of simple devices that need little more than a no-frills GPU, be it for cost reasons, power reasons, or both.Given the continued success of the Mali-400 family it should come as little surprise then that ARM has continued to develop the family, as the need for OpenGL ES 2.0 won’t be coming to an end any time soon. To that end, today ARM is announcing the release of the next iteration of the Mali-400 family IP, the Mali-470. Mali-470 serves as something of a decedent of both Mali-400 and Mali-450, with ARM intending to make it a direct successor to Mali-400 while also integrating some of the improvements that original went in to Mali-450 as well.With the Mali-470 ARM is specifically targeting the IoT and embedded market, even more so than they were already going after it with the Mali-400 and Mali-450. In recent times the Mali-450 has been ARM’s more powerful Mali-400 family GPU, specifically going after low-end devices and the aforementioned TV market. Meanwhile thanks in part to the introduction of smaller processes since its introduction in 2008, the Mali-400 has already been something of a darling for low-power applications, appearing in wearable devices such asSamsung’s Gear S2. ARM and their licensees continue to be bullish on wearables and IoT in general, and this has lead to ARM setting out to develop a more direct update to the Mali-400 to further improve on its energy efficiency.Overall ARM is aiming to double Mali-400’s energy efficiency with Mali-470, in accordance with their goals to reach further into the wearables and IoT market. To do so, ARM has taken some of Mali-450’s energy optimizations and further built on them; chief among these changes are far more extensive use of power gating to shut off transistors when they’re not needed, and improved use of clock gating when power gating isn’t an option. In order to accomplish this ARM has implemented separate power domains on Mali-470 – each pixel/fragment block can be on its own domain – and working with these domains is an on-GPU power controller, which works with the system power controller to better manage the GPU.Improving the use of power and clock gating can greatly impact overall energy efficiency by cutting down on the amount of energy wasted by the GPU waiting for its next job, but it’s especially potent for bringing down idle power consumption. In turn, given that idle power consumption continues to be one of the chief issues dogging wearables vis-a-vis battery life, the Mali-470 should help out battery life to a degree thanks to these improvements.Along with these immediate power optimizations, ARM has also made some smaller optimizations to the Mali-400 family architecture to cut down on wasted power at a workload level. Mali-470’s vertex processor has been optimized to better handle fixed-point arithmetic, a use case that is again very common in simple UIs and can be even more power efficient than floating-point arithmetic. The pixel/fragment processor on the other hand has not gone untouched, with ARM working to cut down on the amount of time spent by the fragment processors changing between states.ARM Mali-400 Family400450470Fragment Shader Blocks (MP#)1-41-81-4Vertex Shader Blocks111ArchitectureUtgardUtgardUtgardOpenGL ESES 2.0ES 2.0ES 2.0Year Released (IP)200820122015Moving on, in terms of scalability Mali-470 retains the multi-processor capabilities of the previous Mali designs, allowing Mali-470 to be scaled up from 1 fragment processor (MP1) to as many as 4 (MP4). Mali-470MP1 will in turn be the intended design for wearables given its spot as the lowest-power design, while the more powerful designs will allow Mali-470 to be used in other environments where more performance is needed. That ARM would retain their scalability for Mali-470 is a very intentional choice; as we mentioned earlier Mali-470 is intended to be the successor to Mali-400, with ARM going so far as to keep Mali-400’s MMU and bus interfaces and holding Mali-470’s die area closer to Mali-400’s. This in turn means that Mali-470 needs to be able to carry on from where Mali-400 has left off, not only in the wearables/IoT market but in the embedded market as well.Wrapping things up, as with past ARM IP launches today’s announcement coincides with the general release of new IP to ARM’s customers. ARM tells us that they expect chips integrating Mali-470 designs to start leaving the fabs in the second-half of 2016, which would in turn have Mali-470 start appearing in consumer devices in the first-half of 2017. Ultiamtely ARM doesn’t control which products use which IP, but given their intentions with Mali-470, there’s a good chance that those consumer devices will include Android/Tizen wearables launching early that year.Gallery:ARM Mali-470\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/9733/arm-announces-mali470-gpu-low-power-for-wearables-more\n",
      "Title: ARM Announcements: New IoT Subsystem for TSMC 55ULP and ARM Cordio Radio IP\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2015-06-01T03:40:00Z\n",
      "URL: https://www.anandtech.com/show/9310/arm-announcements-new-iot-subsystem-for-tsmc-55ulp\n",
      "Content: Today at Computex ARM is announcing a new 'IoT Subsystem'for developers: ARM Cortex-M with ARM Cordio radio IP (see below) with full integration with mBed OS. The principle behind this licencable IP block is to allow developers an end-point to integrated sensors and other peripherals on the route to design a complete SoC. The ARM Artisan physical IP, a flash technology for low power operation, is produced in conjunction with TSMC's 55nm Ultra-Low-Power process for embedded flash memory. 55nm sounds like a large process, but for a lot of IoT type devices it provides a mix of power, price and performance, particularly when an ultra-low-power process node is placed in the mix.ARM's goal here is to provide a solution to its customers. ARM's business model revolves around licencing either their instruction set or the blueprints for partners to design their own SoCs. However, if we considered a saturated top tier market for processor IP, the way to grow with mid and low range partners is to provide more complete solutions. This is ARM's goal here with the IoT Subsystem, especially when IoT and 'billions of units' are spoken often in the same sentence more often. ARM also likes to note that by investing in a more complete subsystem, developers can reduce elements of risk with SoC design.Because companies like ARM are more often the start point when it comes to developing devices around the internet of things, it can be difficult for them to drive the end products. Personally, I feel that while IoT is spoken about often at events, we don't always see the end product in ours hands in the volumes that are quoted. Part of that is the ecosystem, which needs to evolve. Offering more complete start points is one element of that process.With reference to ARM Cordio, this is a new IP portfolio from ARM as a direct result of acquiring two companies - Wicentric and Sunrise Micro Devices, as formally announced a couple of weeks ago. This includes Bluetooth smart software solutions for low power wireless products as well as radio IP including pre-qualified, self-contained radio block with related firmware. The Cordio IP with TSMC 55ULP is rated to get over 2x more life over 1.2 volt and up when Lithium batteries are used. Dr Dipesh Patel, the VP of Technical Operations at ARM, showed off a radio beacon device built on the basis of Cordio at ARM's press event today.Source: ARM\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/9310/arm-announcements-new-iot-subsystem-for-tsmc-55ulp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD’s 2016-2017 Datacenter Roadmap: x86, ARM, and GPGPU\n",
      "Author: Ryan Smith\n",
      "Date Published: 2015-05-06T19:12:00Z\n",
      "URL: https://www.anandtech.com/show/9234/amds-20162017-datacenter-roadmap-x86-arm-and-gpgpu\n",
      "Content: As part ofAMD’s business unit reorganization in 2014, many of AMD’s high-growth businesses were organized into a new group at the company, the Enterprise, Embedded, and Semi-Custom Business (EESC). Now through their first few quarters, Forrest Norrod, the Senior VR and General Manager of the EESC was on-hand at FAD to present AMD’s specific plans for that business for the next 2 years.Forrest’s comments on the embedded and semi-custom businesses generally reflected AMD’s earlier comments on theirthree growth opportunities, but I wanted to call specific attention to AMD’s datacenter plans, which Forrest we into in more detail.AMD’s datacenter plans for the next couple of years will see AMD taking a three-pronged approach to the market. On the CPU side, AMD will of course be leveraging their forthcoming x86 Zen and ARM K12 CPU designs in various fashions. Zen, as previously discussed, should offer a significant increase in IPC and improve AMD’s competitive positioning in the x86 space. And since AMD is launching their high-performance desktop Zen CPU as the first Zen product, the implication is that the server version of that product should not be too far behind.Meanwhile over the next couple of years AMD will be leveraging the Opteron A1100 “Seattle”, followed by the K12 in 2017. AMD is positioning their ARM datacenter products as being primed for efficiency, whereas their x86 products are primed for high native I/O capacity and high overall performance. Opteron A1100 will ship later this year, though I’m still unsure just how much adoption it is going to see as opposed to the K12 in 2017.Last but certainly not least however, I wanted to call attention to AMD’s GPU/APU plans in the datacenter space. AMD already has a presence with GPU accelerator cards in the form of the FirePro S-series, with these cards being the basis of their high performance computing (HPC) and virtual desktop infrastructure (VDI) initiatives. AMD is expecting these markets to continue to grow as customers increasingly turn towards GPUs for better compute throughput, and VDI for remote client hosting and the server-side use cases it was designed for.But the most interesting thing about this roadmap is AMD’s “high-performance server APU”. To date, the closest AMD has come to a server APU is are theirFirePro APUs, which are versions of AMD’s standard desktop APUs with the ability to use their FirePro driver set and are intended for workstations. Consequently the creation of a high-performance server APU represents a new product within AMD’s portfolio, as they have never done something quite like this before.AMD isn’t saying much about this APU, but it will be a multi-teraflops chip for both HPC and workstations, which implies something significantly more powerful than today’s Kaveri APUs, and much closer to the performance of some of AMD’s discrete GPUs. The end-game here of course is to leverage HSA and the close proximity of CPU and GPU in a way no other vendor currently can, to deliver high performance in workloads that benefit from close CPU/GPU interaction. The fact that AMD is targeting this at HPC makes me curious just what they’re planning for FP64 performance, but they could just as well be going after markets such as oil & gas and machine learning where FP32 and F16 are sufficient. Meanwhile there is also a very good reason to suspect that this may be the first place AMD implements HBM for an APU, as it would be in-line with their previous comments about doing HBM with more than just discrete GPUs, and this is a high-margin market that would be suitable for a higher-cost feature like HBM.Ultimately for AMD their plans for the datacenter are ambitious, but if executed well seem to be achievable. After being pushed out of the x86 server market, AMD is making a concentrated effort to re-enter it via the Zen CPU, and AMD’s APUs offer an interesting, alternative take on getting there. That said, we should also expect to see AMD significantly leaning on open industry standards to get back into the datacenter space, and consequently making part of their argument the fact that they are not Intel, and consider themselves to be more open towards working with partners and customers.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/9234/amds-20162017-datacenter-roadmap-x86-arm-and-gpgpu\n",
      "Title: AMD’s K12 ARM CPU Now In 2017\n",
      "Author: Ryan Smith\n",
      "Date Published: 2015-05-06T18:00:00Z\n",
      "URL: https://www.anandtech.com/show/9232/amds-k12-arm-cpu-now-in-2017\n",
      "Content: Along with an update on their x86 plans, AMD has also presented an update on their ARM plans at financial analyst day today. The news there is a mixed blessing, depending on one’s point of view.AMD’s “Seattle” SoC – the ARM Cortex-A57 poweredOpteron A1100– will finally be shipping in H2 of this year, after first sampling towards the end of last year. This unfortunately is almost a year behind AMD’s original schedule, though AMD seems moderately optimistic about catching up once they have their first ARM silicon out the door.But the real focus of AMD’s comments on ARM for FAD involveK12, the AMD-developed ARM CPU core being designed alongside Zen. With AMD having opted to prioritize Zen development, K12 has been pushed back from 2016 to 2017, essentially taking the near-simultaneous launch of the two parts off the table. AMD for their part is attributing this change in schedule only to the decision to focus on Zen, however with Opteron A1100 delayed, it’s entirely possible this is also a knock-on effect that has pushed back the entire AMD ARM roadmap.In any case, even with the delay AMD is still eagerly moving ahead with their ARM plans. A1100, even though it’s late, will be the pathfinder for AMD’s ARM efforts, serving as a platform to further develop the AMD ARM ecosystem, getting developers acquainted with the technology and getting software ready for it. K12 in turn will come in after that ecosystem has already seen some development, allowing AMD to get their new hardware out to the market and already have software support for it. I hesitate to say that this makes A1100entirelya pathfinder product – until it’s available, it’s not clear how many customers might purchase it for production work – but clearly AMD’s big play is K12, not A1100.And though K12 is delayed, AMD tells us that this hasn’t changed how it’s being developed, which is to say that it’s being done in concert with Zen. So virtually everything AMD gets right with Zen will be integrated into K12 as well, just on the basis of the ARM ISA instead of x86. AMD still believes that the time is right for ARM in the server space – along with a very obvious place in the semi-custom SoC space – and that K12 will be an interesting alternative to Zen in that regard.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/9232/amds-k12-arm-cpu-now-in-2017\n",
      "Title: ARM Reveals Cortex-A72 Architecture Details\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2015-04-23T21:00:00Z\n",
      "URL: https://www.anandtech.com/show/9184/arm-reveals-cortex-a72-architecture-details\n",
      "Content: ​Today in London as part of ARM's TechDay 2015 event we had the pleasure to get a better insight into ARM's new Cortex-A72 CPU. ARM hadannounced the Cortex-A72in the beginning of February - leaving a lot of questions to be asked and sense of mystery in the air. The Cortex-A72 is a direct successor to the Cortex-A57 - taking the predecessor as a baseline in order to iterate and improve it.On the naming side of the equation, moving from 'A57' to 'A72' rather than 'A59' or similar, ARM explains that it is purely a marketing decision as they wanted to give better differentiation between its higher-performance cores from the mid-tier and low-power cores. There seemed to be some confusion between the more power efficienct A53 and the more powerful A57, whereby users would assume they are similar, and thus moving its new big core into the A7x series.We saw some absolute targeted performance numbers back during the February release, which promised some very interesting numbers that could be achieved over the A57.The problem was that it was not clear how much from performance and power efficiency came from the architectural changes and how much came from the the process on which these targeted performance data points are estimated from. It's clear that on the high-end ARM is promoting the A72 on the new FinFET processes from Samsung/GlobalFoundries and TSMC, which are referred to as 14nm and 16nm in the slides. Generally, due to the design and the node, the A72 will be able to achieve higher clocks than the A57, and we seem to be aiming around 2.5GHz on the 14/16nm nodes when high-end smartphones are concerned. Higher clocks may be present in server applications, where the A72 is also aimed at.Probably the most interesting slide next to the actual performance metrics of the A72 is the apples-to-apples comparison of the A57 to the A72 on the same process node. When on the 28nm node, we see the A72 having a respectable 20% power reduction when compared to the A57. As a reminder - we're talking about absolute power at the same clock speed, which does not consider performance and thus not a representation of efficiency.Notably, ARM is aiming for the A72 to be capable of extensive sustained performance at its target frequency. This is something that smaller form factor A57 designs (e.g. phones) have struggled with due to just how powerful A57 is, which has lead to more bursty designs that can only run A57 at its top speeds for short periods of time. We are presented with figures such as sustained 750mW operation per core on 16FF+ at clocks of ~2.5GHz.While the power numbers are interesting we also have to put them into context of the achieved work. ARM has made several optimizations to the architecture to improve performance when compared to the A57. We'll get into more detail in just a bit - but what we are looking at is a general 16-30% increase on IPC depending on the kind of workload. Together with the power reduction, we now see how ARM is able to advertise such large efficiency gains for the same fixed workload.A72 Architecture - The Upgrades Over A57ARM seems to have managed to achieve an improvement in all three areas of the PPA metric; Performance, Power and Area - the trifecta of semiconductor design goals. This was achieved by doing a re-optimization of (almost) every logical block from the A57. There has been some considerable redesign in the CPU's architecture, some of which include a new branch-predictor and improvements in the decoder pipeline to allows for better throughoutput.On the level of the instruction fetch block we see a brand new branch-predictor that follows a new sophisticated algorithm that improves performance and reduces power through reduced misprediction and speculation, which has been cut down by 50% for mispredictions and 25% for speculation when compared to the A57. Superfluous branch-predictor accesses have also been suppressed - in workloads where the predictor is not able to do its job efficienctly it is then bypassed completely. There also has been general power optimization in the RAM-organization by coupling the different IP blocks better together, something ARM looks to provide with their own physical IP.Moving down the pipeline, A72's decoder/rename capabilities have seen their own set of improvements.The decoder itself is still a 3-wide decoder, but ARM has gone through it to try to improve both performance and power consumption in other ways. To improve performance, the effective decode bandwidth has been increased, and the decoder has received some AArch64 instruction-fusion enhancements. Meanwhile power consumption has been tempered at multiple levels, including optimizing decoding directly, and in other power optimizations to the buffers and flow-control hardware that work around the decoder.However it's on the dispatch/retire stage that the architecture sees the biggest improvements to performance. Going hand-in-hand with the decoder's ability to fuse instructions, ARM's dispatch unit can then break those ops back down into more granular micro-ops for feeding into the execution units, transforming it from a 3-wide to an effective 5-wide machine at the dispatch stage. The net result of this increases decoder throughput (by reducing the number of individual instructions decoded) while also increasing the total number of micro-ops created by the dispatcher and eventually executed per cycle. ARM is quoting an average of 1.08 micro-ops per instruction in code, which will aid the cases where in A57 the 3-wide dispatch unit was eventually dispatch limited. Again on the dispatch-level, ARM has done more extensive work on their register file by reducing the number of read-ports by introducting port-sharing andfurtherreducing superfluous access.ARM CPU Core ComparisonCortex-A15Cortex-A57Cortex-A72ARM ISAARMv7 (32-bit)ARMv8 (32/64-bit)Decoder Width3 opsMaximum Pipeline Length19 stages16 stagesInteger Pipeline Length14 stagesBranch Mispredict Penalty15 cyclesInteger Add2Integer Mul1Load/Store Units1 + 1 (Dedicated L/S)Branch Units1FP/NEON ALUs2x64-bit2x128-bitL1 Cache32KB I$ + 32KB D$48KB I$ + 32KB D$L2 Cache512KB - 4MB512KB - 2MB512KB - 4MBOn the side of the execution units we see introduction of new, next-generation FP/Advanced SIMD units. The new units allow for much lower instruction latency as the FP pipeline length is reduced from 9 to 6. FMUL is reduced from 5 cycles down to 3, FADD goes from 4 to 3, FMAC from 9 to 6, and the CVT units go from 4 to 2 units.The reduction of the FP pipeline length brings down the maximum pileline length of the architecture down from 19 to 16.The integer units also see an improvement, as the Radix-16 divider has seen its bandwidth doubled, while the CRC unit now becomes a pipelined block with just 1-cycle latency, a 3x increase in bandwidth over the A57. Again, we see a repeating pattern here as ARM claims it tried to squeeze the most power efficiency from all the units by improving the physical implementation.Another large performance improvement over the A57 is found on the Load/Store unit. Here, ARM claims that bandwidth to L1/L2 has been improved by up to 30%. This was achieved by introducting a sophisticated L1/L2 data prefetcher which, again, is at the same time more efficient as improvements in the L1-hit pipeline, fowarding network, and way-predictor reduce the needed power.We've been generally impressed with what the A72 brings to the table. It's clear that new architecture is an evolutional upgrade ot the A57, and the improvements in performance, power, and area, when looked at from an aggregate view, bring substantial differences and upgrades when compared to the A57. With the A57 having come to market in Q3 of last year and it now shipping in high-volume SoCs such as the Snapdragon 810 and Exynos 7420, we are looking at the possibility of seeing its successor come to market in shipping devices in less than a year's time. The obvious partners that might ramp prodution the soonest are MediaTek and Qualcomm, at least if they are able to hit their target schedules. There should presumably still be un-announced parts from other ARM partners as well. It's clear that ARM has increased the cadence of releasing refreshes of its IP portfolio and the quick succession of the A72 seems to be part of that.The A72 looks to be a logical update to the A57 addressing some weakpoints such as peak power and power efficiency combined with an ~10% area reduction. We already saw Mediatek showing off an A72 package at MWC, so it will be interesting to see how the IP actually performs in silicon and what ARM's partners will be able to do with the core and the time to market.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/9184/arm-reveals-cortex-a72-architecture-details\n",
      "Title: Synology Launches ARM-based DS1515 and RS815 Value Series NAS Units\n",
      "Author: Ganesh T S\n",
      "Date Published: 2015-04-14T12:01:00Z\n",
      "URL: https://www.anandtech.com/show/9157/synology-launches-armbased-ds1515-and-rs815-value-series-nas-units\n",
      "Content: Synology introduced their x15+ series in the second half of 2014. The models were all based on the Intel Rangeley platform (x86). It is now time for a refresh of the Value Series using ARM-based SoCs - the x15 models. There are two units being introduced today, the 5-bay DS1515 in the tower form factor (MSRP: $650) and the 4-bay RS815 in a new short-depth rackmount form factor (MSRP: $600).DS1515Based on the Annapurna Labs Alpine AL-314 quad-core Cortex-A15 SoC, the unit has four GbE LAN ports, two USB 3.0 and two eSATA ports. The eSATA ports can be used to connect up to two DX513 / DX213 expansion units. This can provide up to a maximum of 10 bays additional to the five on the main unit. The DS1515 comes with 2GB of RAM.Gallery:Synology DS1515The unit comes with Synology's widely respected DiskStation Manager DSM 5.2 OS supporting a wide variety of networking protocols, applications and add-on packages. The AL-314 SoC comes with hardware encryption engines and a dedicated floating point unit. The presence of four LAN ports help in setting up a high-performance high-availability cluster. Claimed throughput numbers indicate up to 403.7 MBps reads and 421.8 MBps writes.RS815The RS815 solution is internally the same as theRS814introduced last year. Carrying the same Marvell ARMADA XP MV78230 that we reviewed a couple of years back in the LenovoEMC ix4-300d, the performance numbers come in at 216.7 MBps reads and 121.8 MBps writes. The eSATA port allows the connection of a RX415 expansion unit. This can provide up to a maximum of 4 bays additional to the four on the main unit.Gallery:Synology RS815The important update is the short-depth chassis design. The RS815 is only 29 cm deep, compared to the RS814's 46 cm. This allows for denser deployments and compatibility with industrial server environments.It is refreshing to see Synology continuing to invest in ARM-based models f﻿or the Value Series. While these are not true 64-bit solutions yet, they will ensure that Intel x86-based solutions are not the only game in town for high-performance NAS units. The presence of four GbE ports on the DS1515 brings a host of exciting use-cases to the table. The Alpine platform mightjust about cut the grade for 10G NAS units, but it should be an excellent choice for NAS units with GbE LAN ports. The RS815, on the other hand, takes a tried and tested platform and fits it in a new chassis to expand its application areas.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article already exists: https://www.anandtech.com/show/9157/synology-launches-armbased-ds1515-and-rs815-value-series-nas-units\n",
      "Title: GIGABYTE Server Releases ARM Solutions using AppliedMicro and Annapurna Labs SoCs\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2015-03-24T16:51:00Z\n",
      "URL: https://www.anandtech.com/show/9107/gigabyte-server-releases-arm-solutions-using-appliedmicro-and-annapurna-labs-socs\n",
      "Content: As Johan points out in hisdeep dive of ARM in the server market, given a focused strategy new ARM solutions can offer the potential to disrupt some very typical x86 applications. Migrating from the ARM instruction sets or the ARM architecture into something in silicon is one part of the equation, then producing something more tangible has been the quest of a few solution providers. Typically these solutions all focus on enterprise as one would expect, and it would take time to filter down depending on use case and application. It would seem that today, for the opening of theWHD.globalevent in Germany, GIGABYTE’s server arm is launching a couple of ‘Server on a Chip’ solutions.First up is a model built around the AppliedMicro X-Gene under the ARMv8-A architecture. TheMP30-AR0uses the 45W X-Gene1, a 40nm eight-core solution running at 2.4 GHz using pairs of cores shared L2 cache with an overriding 8MB L3 cache.The system uses quad channel DDR3 with ECC, exploiting two DIMMs per channel for a total of 128GB. Two 10GbE SFP+ ports are supplied, along with two RJ-45 1 GbE ports from a Marvell 88E1512 controller. Four SATA 6 Gbps ports are part of the SoC, along with two PCIe 3.0 x8 slots in an x16 form factor. No USB 3.0, but USB 2.0 and a VGA output for the AST2400 server control IC included, which means another control network port on the rear-IO.The MP30-AR0 is compliant with ARM Server Base System Architecture (SBSA) and Server Base Root Requirements (SBBR) standards, and is designed for cloud and scale-out computing. While GIGABYTE’s server division has been hard at work enabling their products to be sold at retail, the ARM based platforms will most likely be a distributor b2b only offering, at least of now. This motherboard/SoC system will also be available in a 1U server rackmount (theR120-P30) with four hotswappable bays and a single PCIe riser card, as shown above.On the storage side, GIGABYTE Server is releasing theD120-S3G, a rackmount powered by the Annapurna Labs Alpine AL5140, which translates as a 1.7 GHz Quad A15 solution running at a 10W TDP relying on ARMv7 for the instruction set.This system seems more for cold-storage, offering support for 16 SATA 6Gbps drives with RAID 5 and RAID 6 both supported. The motherboard has only one memory slot, but two gigabit Ethernet ports are flanked with two 10GbE integrated SFP+ ports as well. An AST2400 supplies the network control, and GIGABYTE is stating support for LTS Linux Kernel 3.10 and Ubuntu 14.04. If the product page is anything to go by, this is still technically a work in progress as they have not officially announced any other connectivity.No word on release dates or pricing, although demonstrations at events can mean they might go on sale within the next couple of months.Gallery:GIGABYTE Server Releases ARM Solutions using AppliedMicro and Annapurna Labs SoCs\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/9107/gigabyte-server-releases-arm-solutions-using-appliedmicro-and-annapurna-labs-socs\n",
      "Title: ARM At GDC 2015: Geomerics Enlighten 3 Released\n",
      "Author: Ryan Smith\n",
      "Date Published: 2015-03-03T17:00:00Z\n",
      "URL: https://www.anandtech.com/show/9040/arm-at-gdc-2015-geomerics-enlighten-3-released\n",
      "Content: One of ARM’s more unusual acquisitions in recent years has been Geomerics, a fellow UK company who specializes in video game lighting technology. Geomerics is a far cry from ARM’s day-to-day business of developing hardware blocks and ISAs to license to customers who want to put together their own chips, but Geomerics has been a long-term play for the company. By investing in a company with strong ties to the video gaming industry, ARM would in turn gain an important tool in helping to bring higher quality lighting to SoC-class GPUs, and also help to ensure that such important middleware was including SoC-class GPUs in their feature & performance targets.With GDC 2015 taking place this week, the ARM is seeing the first real payoff from their acquisition with the release of the latest version of Geomerics’ lighting technology, Enlighten 3. Enlighten 3 in turn is designed to be one of the most advanced global illumination systems on the market, designed to scale up from mobile to desktop PCs. Previous versions of Enlighten were already in several games and engines, including the Frostbite 2 engine backing Battlefield 3, and now with Enlighten 3 the company is hoping to extend its reach further with its inclusion into the ever popular for mobile Unity 5 engine, and as an add-on for the similarly popular Unreal Engines 3 and 4.From a feature standpoint Enlighten 3 introduces several new features, including a greatly improved indirect lighting system. Also on the docket is a richer materials system, allowing for improved support for transparent surfaces, which in turn allows for the lighting to be updated to reflect when the transparency of a surface has changed. Alternatively, for scenarios without real-time lighting, the middleware also has increased the quality of lightmaps it can bake.Ultimately ARM tells us that they believe 2015 will be a big year for Geomerics in the mobile space, saying they expect a number of mobile titles to use the technology. To that end, as part of their GDC launch, ARM and Geomerics are showcasing several Enlighten 3 demos, including an in-house demo they are calling Subway, and a demo showcasing Enlighten 3 running inside Unreal Engine 4.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/9040/arm-at-gdc-2015-geomerics-enlighten-3-released\n",
      "Title: Synology DS2015xs Review: An ARM-based 10G NAS\n",
      "Author: Ganesh T S\n",
      "Date Published: 2015-02-27T13:20:00Z\n",
      "URL: https://www.anandtech.com/show/8985/synology-ds2015xs-review-an-armbased-10g-nas\n",
      "Content: Introduction and Testbed SetupSynology is one of the most popular COTS (commercial off-the-shelf) NAS vendors in the SMB / SOHO market segment. The NAS models introduced by them in 2014 were mostly based on Intel Rangeley (the Atom-based SoCs targeting the storage and communication market). However, in December, they sprang a surprise by launching the DS2015xs, an ARM-based model with dual 10GbE ports. Wecovered the launchof the Synology DS2015xs in December, and provided some details about the Annapurna Labs AL514 SoC in it.ARM-based SoCs for SMB / SOHO NAS units typically support up to 4 bays and come with dual GbE links. Intel's offerings have had a virtual monopoly in the other tiers of the market. Synology's DS2015xs, with its native 10G capabilities, brings in another contender into the market.The DS2015xs is 8-bay NAS unit presented as a step-up from theDS1815+. While the DS1815+ can expand up to a total of 18 bays with two DX513 expansion chassis, the DS2015xs is compatible with the 12-bay DX1215 expander (for a total of 20 bays). The main step-up from the DS1815+ is the presence of two built-in 10G SFP+ links (supporting direct-attach copper cables). The gallery below takes us around the unit's chassis.Gallery:Synology DS2015xs - ChassisThe specifications of the Synology DS2015xs are provided in the table belowSynology DS2015xs SpecificationsProcessorAnnapurna Labs AL514 SoC (Quad-Core Cortex-A15 @ 1.7 GHz)RAM4 GBDrive Bays8x 3.5\"/2.5\" SATA II / III HDD / SSD (Hot-Swappable)Network Links2x 1 GbE RJ-45 + 2x 10GbE SFP+External I/O Peripherals2x USB 3.0, 1x Infiniband for Expansion BayExpansion SlotsN/AVGA / Display OutN/AFull Specifications LinkSynology DS2015xs SpecificationsPriceUSD 1400The Synology DS2015xs runs the latest DiskStation Manager OS, which, subjectively speaking, is one of the best COTS NAS operating systems in the market. Geared towards both novice and power users, it also provides SSH access. Some additional aspects can be gleaned through SSH. For example, the unit runs on Linux kernel version 3.2.40. The AL514 SoC has hardware acceleration for cryptography and two in-built USB 3.0 ports. There are also four network links (we know from external inspection that two are 10GbE, while the others are 1GbE) with unified drivers for both types of interfaces.In the rest of the review, we will first take a look at the performance of the unit as a direct-attached storage device. This is followed by benchmark numbers for both single and multi-client scenarios across a number of different client platforms as well as access protocols. We have a separate section devoted to the performance of the NAS with encrypted shared folders. Prior to all that, we will take a look at our testbed setup and testing methodology.Testbed Setup and Testing MethodologyThe Synology DS2015xs can take up to 8 drives. Users can opt for different RAID types depnding on their requirements. We expect typical usage to be with multiple volumes in a RAID-5 or RAID-6 disk group. However, to keep things consistent across different NAS units, we benchmarked a SHR volume with single disk redundancy (RAID-5). Tower / desktop form factor NAS units are usually tested with Western Digital RE drives (WD4000FYYZ). However, the presence of 10-GbE on the DS2015xs meant that SSDs had to be used to bring out the maximum possible performance. Therefore, evaluation of the unit was done by setting up a RAID-5 volume with eight OCZ Vector 4 120 GB SSDs. Our testbed configuration is outlined below.AnandTech NAS Testbed ConfigurationMotherboardAsus Z9PE-D8 WS Dual LGA2011 SSI-EEBCPU2 x Intel Xeon E5-2630LCoolers2 x Dynatron R17MemoryG.Skill RipjawsZ F3-12800CL10Q2-64GBZL (8x8GB) CAS 10-10-10-30OS DriveOCZ Technology Vertex 4 128GBSecondary DriveOCZ Technology Vertex 4 128GBTertiary DriveOCZ Z-Drive R4 CM88 (1.6TB PCIe SSD)Other Drives12 x OCZ Technology Vertex 4 64GB (Offline in the Host OS)Network Cards6 x Intel ESA I-340 Quad-GbE Port Network AdapterChassisSilverStoneTek Raven RV03PSUSilverStoneTek Strider Plus Gold Evolution 850WOSWindows Server 2008 R2Network SwitchNetgear ProSafe GSM7352S-200The above testbed runs 25 Windows 7 VMs simultaneously, each with a dedicated 1 Gbps network interface. This simulates a real-life workload of up to 25 clients for the NAS being evaluated. All the VMs connect to the network switch to which the NAS is also connected (with link aggregation, as applicable). The VMs generate the NAS traffic for performance evaluation.Thank You!We thank the following companies for helping us out with our NAS testbed:Thanks toIntelfor theXeon E5-2630L CPUsand theESA I-340 quad port network adaptersThanks toAsusfor theZ9PE-D8 WS dual LGA 2011 workstation motherboardThanks toDynatronfor theR17 coolersThanks toG.Skillfor theRipjawsZ 64GB DDR3 DRAM kitThanks toOCZ Technologyfor thetwo 128GB Vertex 4 SSDs, twelve 64GB Vertex 4 SSDs and the OCZ Z-Drive R4 CM88Thanks toSilverStonefor theRaven RV03 chassisand the850W Strider Gold Evolution PSUThanks toNetgearfor theProSafe GSM7352S-200 L3 48-port Gigabit Switch with 10 GbE capabilities.DAS Evaluation Setup and MethodologyIn addition to our standard NAS evaluation suite, the Synology DS2015xs also warrants investigation under ideal network conditions as a direct-attached storage unit. The presence of 10G network links in the unit has prompted Synologyto market it as a fast DAS unit for video production workflows. In order to evaluate this aspect, we augmented ourDAS testbedwith an Emulex OneConnect OCe11102(R)-N 2-port 10GbE SFP+ PCIe NIC and a OCZ Vector 120 GB SSD.The Emulex PCIe NIC doesn't support teaming under Windows 8.1. Therefore, we had to install Windows Server 2012 R2 on the additional SSD to make our DAS testbed dual-boot for evaluating NAS units. The DHCP Server feature was also activated on the teamed port to which the NAS's 10G ports were connected. On the NAS side, the ports were set up for teaming too and configured to receive an IP address from a DHCP server. The MTU for the interface was configured to be 9000 bytes. The details of the tests that were run in this mode will be presented along with the performance numbers in the next section.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8985/synology-ds2015xs-review-an-armbased-10g-nas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM A53/A57/T760 investigated - Samsung Galaxy Note 4 Exynos Review\n",
      "Author: Ryan Smith\n",
      "Date Published: 2015-02-10T12:30:00Z\n",
      "URL: https://www.anandtech.com/show/8718/the-samsung-galaxy-note-4-exynos-review\n",
      "Content: It's been over three months since Josh had the opportunity toreview the Note 4in its details. The defining characteristic of that review is that it was a look at the variant with Qualcomm's Snapdragon 805 SoC running at the heart of the device. This version is found in devices shipping in North America, Western Europe, China and Japan. While these markets have now been served by Qualcomm's silicon offerings, Samsung is now back on track at trying to expand its market-share of in-house Exynos SoCs. As such, all other markets (with small exceptions) seem to be getting served Exynos variants of the Note 4. While we normally try to cover all aspects of device performance, for this review we will focus on the SoC and the differences that result from this change.Before we continue on, we'd like to thank international phone specialist28Mobile.comfor providing us with an Exynos unit of the Galaxy Note 4.Model BreakdownSamsung Galaxy Note 4 ModelsSamsung Galaxy Note 4SM-N910(F/A/V/T/P/06W/8V/G/0Z)Samsung Galaxy Note 4SM-N910(C/S/L/K/U/H)SoC2.7 GHz Snapdragon 8051.9GHz Exynos 5433R​AM3GB 128-bit LPDDR3-160025.6GB/s bandwidth3GB 64-bit LPDDR3-165013.2GB/s bandwidthNANDSamsung 32/64GB NAND+ microSDSamsung 32/64GB NAND+ microSDDisplay5.7” 1440p Super AMOLED5.7” 1440p Super AMOLEDNetwork2G / 3G / 4G LTEQualcomm MDM9x35 Cat6 LTE (F)Qualcomm MDM9x25 Cat4 LTE (*)2G / 3G / 4G LTEIntel XMM7260 Cat.6 LTE (U)Samsung M303 Cat.6 LTE (S/K/L)Ericsson M7450 Cat.4 LTE (C)2G / 3GEricsson M7450 (H)Dimensions153.5 x 78.6 x 8.5 mm, 176g153.5 x 78.6 x 8.5 mm, 176gCamera16MP Rear Facing w/ OIS, 1/2.6\"CMOS size, F/2.0 aperture(Sony IMX240)3.7MP FFC w/ F/1.9 aperture(S.LSI S5K6D1)16MP Rear Facing w/ OIS, 1/2.6\"CMOS size, F/2.0 aperture(Sony IMX240)3.7MP FFC w/ F/1.9 aperture(S.LSI S5K6D1)Battery3220 mAh, 3.85V, 12.397 Whr3220 mAh, 3.85V, 12.397 WhrOSAndroid 4.4.4 with TouchWiz UXAndroid 4.4.4 with TouchWiz UXConnectivity802.11a/b/g/n/ac + BT 4.1,USB2.0, GPS/GNSS, MHL,DLNA, NFC802.11a/b/g/n/ac + BT 4.1,USB2.0, GPS/GNSS, MHL,DLNA, NFCSIM SizeMicroSIMMicroSIMThe model breakdown of the Note 4 is quite a mess. The S805 version breaks down into about 9 different local versions, each with different RF frequency support and varying modem configurations. The Exynos variants are even more convoluted in terms of modem choice. We've been reporting that we might see Intel's XMM7260 as the predominant modem in Samsung's SoCs, but that seems to have changed and the Intel modem is only shipped in a small percentage of total units on the SM-N910U model, which ships in South-East Asia.In their home country of Korea Samsung continues to rely on their own modem, recently marketed as the \"Exynos Modem\" series. Samsung has been steadily developing their own modem hardware over the years and it seems the M303 is at feature parity with the Intel XMM7260 and Qualcomm's MDM9x35. This year Samsung has been pushing their modems outside of Korea in several devices such as the Galaxy S5 Mini, so I expect them in the future to be more prevalent in other global markets.The surprise supplier and big winner of this generation is Ericsson; the M7450 is found in unarguably the largest portion of Exynos variants, the SM-910C and SM-910H, shipping throughout South America, Central and Eastern Europe, Africa, the Middle East, and some Asian countries. Brianfirst reportedon this modem over 1.5 years ago and we haven't heard much about it since. The surprise here is that Ericsson managed to get such a big design win beforeannouncingthat they will discontinue development on modems altogether. The even more intriguing story is that the M7450 is used in the 3G version of the Note 4, usually a traditional Intel design win.Whatever model you end up with, and as long as the frequency bands match your provider, the choice between Snapdragon and Exynos remains the biggest differentiation. Everything else from camera to WiFi to battery size remains the same in an identical form-factor.For this review we're investigating the SM-910C running on firmware KTU84P.N910CXXU1ANJ8.Exynos 5433 - The First Mobile ARM A57 SoCI hadreportedback in September that Samsung's new Exynos 5433 found in the Note 4 is actually the first implementation of ARM's A53/A57 and T760 new SoC IP. While it took Samsung some time to actually officially announce the part, they eventually did confirm it. The interesting disclosure here is that they choose to market it in the Exynos 7 family, which certainly makes more sense than keeping it in the Exynos 5 category of SoCs.Also in my initial article I shared my opinion that I doubted that Samsung would update the Note 4 to AArch64 - this was based on the fact that the kernel treats the SoC as an A7/A15 part and most of the software stack remained 32-bit. Events since then seem to point out that they will eventually upgrade it to 64-bit, since we're seeingofficial patchesin upstream Linux with the chip being introduced with a full ARMv8 device tree. This is interesting to see and might point out that Samsung will go the effort to upgrade the BSPs to AArch64; however it remains unclear whether we'll see this on the Note 4. My personal opinion remains that we won't be seeing this overhaul in Samsung's 5.0 Lollipop update.Let's take a look again at how things have evolved in terms of spec sheet since then:Samsung Exynos 5 Octa 2014 lineupSoCSamsungExynos 5422SamsungExynos 5430SamsungExynos 5433CPU4x Cortex A7 r0p5 @1.3GHz512KB L2 cache4x Cortex A15 r2p4 @1.9GHz2MB L2 cache4x Cortex A7 r0p5 @1.3GHz512KB L2 cache4x Cortex A15 r3p3 @1.8GHz2MB L2 cache4x Cortex A53 r0p1@1.3GHz256KB* L2 cache4x Cortex A57 r1p0 @1.9GHz2MB L2 cacheMemoryController2x 32-bit @ 933MHzLPDDR314.9GB/s b/w2x 32-bit @ 825MHzLPDDR313.2GB/s b/w2x 32-bit @ 825MHzLPDDR313.2GB/s b/wGPUMali T628MP6@ 533MHzMali T628MP6@ 600MHzMali T760MP6@ 700MHzMfc.ProcessSamsung28nm HKMGSamsung20nm HKMGSamsung20nm HKMGI've been able to confirm that the GPU on the 5433 is an MP6 configuration, meaning it sports the same amount of cores as the previous Mali T628 implementations in the 5430, 5422 and 5420. I've also acquired a Galaxy Alpha which I'll review at a later date, but noticed that Samsung is also shipping 825MHz memory on that device. As such, the bandwidth of the 5430 is corrected from the publicized 17GB/s down to 13.2GB/s.Samsung has retained the cache sizes found in the older 54xx SoCs, meaning 512KB on the little cluster*We see a reduction to 256K on the L2 of the A53 cluster, and a continuation of 2MB on the big cluster. I would have liked to see the A53's getting 1MB of L2 but it seems we'll have to wait for future SoCs to see an improvement there.In other regards, the 5433 differs very little from the 5430. It has almost identical auxiliary block IP, sharing the same ISP, hardware decoders and encoders, various interfaces, and a similar bus architecture.* Correction 24/03/2015: Our initial info of 512K L2 cache on the A53 cluster was wrong and has henceforth been corrected to 256K (Source)Miscellaneous differencesWith the modem and SoC platform being the two main differences between the two Samsung devices, there are some other minor changes of components. On the audio side Qualcomm's WCD9330 is replaced by WolfsonMicro's WM5110 audio SoC. The Wolfson IC uses its integrated DSPs for voice processing, replacing the need for an additional Audience eS705 audio processing chip that is used in the Snapdragon variant. This might result in different call and audio quality between the two versions. The WM5110 also takes over device voice activation duties for the Exynos Note 4, which you can use with SVoice or Google Now.I also noticed that yet again Samsung did not allow for native 44.1KHz audio playback in the default configuration of Android's AudioFlinger on the Exynos version, causing forced resampling of 44.1KHz content to 48KHz. Neither version allows for higher quality output, even though both audio solutions support 24-bit 192KHz audio in hardware. I'm not sure what Samsung's audio team is doing here but it's getting increasingly frustrating to see this issue continue, starting with the Galaxy S4.In terms of media playback, Samsung has not exposed the HEVC/H265 hardware decoder of the Exynos 543X's to the media codec capabilities list of the system. The device is still shipping with the decoder block's firmware, so again this is either an oversight or something has gone wrong during development of the device. The Qualcomm version exposes the HEVC decoder of the Snapdragon 805.A large functional difference between recent Exynos SoCs and Qualcomm variants comes with the capability of the display controller. Due to 1440*2560*32bpp*60fps picture bandwidth exceeding the transport capacity of a 4-lane MIPI DSI link from the SoC's display controller to the display driver, Qualcomm devices need to double up on this interface and use dual-DSI links.Image courtesy of Synaptics/Renesas (source)The Qualcomm S805 uses two DSI ports with 4 lanes each to drive the display in halves of 720x2560 pixels, while the Exynos SoCs since the 5420 are able to make do with just a single 4-lane port with help of Mobile Image Compression. MIC compresses the data stream and thus avoids this bandwidth bottleneck. The disadvantage here is that the display driver itself must be able to decompress the stream.With Samsung developing their own AMOLED display drivers they can however take advantage of this vertical integration and implement this in their phones and tablets. The power advantages here can vary up to 150mW if the image stream is able to be highly compressed, or about 100mW on average. This of course has the biggest impact on highly dynamic content, as static content power consumption has mostly been resolved by Panel Self Refresh, which has seen wide adoption over the last 18 months.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8718/the-samsung-galaxy-note-4-exynos-review\n",
      "Title: ARM Announces Cortex-A72, CCI-500, and Mali-T880\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2015-02-03T22:30:00Z\n",
      "URL: https://www.anandtech.com/show/8957/arm-announces-cortex-a72\n",
      "Content: Today ARM is announcing three brand-new premium IP designs targeted at high-end mobile SoCs. We're still only starting to get widespread commercial availability of ARM's latest generation of SoCs, which includes the Cortex-A57 in big.LITTLE configuration coupled with the A53 as little cores, and the newest T760 Mali GPUs. But, while those designs are still ramping up through offerings from Samsung, Qualcomm, HiSilicon and co. this year, ARM isn't staying still and already looking forward to 2016 and beyond.Cortex-A72 - a new high end coreAt the center of today's announcements a new high-end performance core which succeeds the A57 in flagship devices. ARM was very vague about the architectural characteristics of the new design, disclosing for now only estimates of the chip's performance and power targets. ARM promises a 3.5x sustained performance increase over the A15 generation of SoCs while remaining in the same power budget. One has to note that we're talking about performance targets on TSMC's 16nm FinFET+ node versus previous nodes such as 28 and 20nm, which in itself should bring large clock and power advantages.The A72 targets roughly 1.9X the sustained performance of current 20nm A57 SoCs, meaning the Exynos 5433 and the Snapdragon 810 can be taken as the base for comparisons. ARM doesn't yet mention peak performance so we may be talking about overall power efficiency gains that enable future SoCs to throttle much less. ARM will be divulging more information on the architecture of the A72 in the coming months, and we're hoping to have a better picture on the actual IPC and efficiency gains of the new flagship core by then.The Cortex-A72, being a \"big\" core, can be partnered up with the already existing A53 LITTLE core architectures. ARM has said in the past that the A53 took in-order designs to new heights, and while work on a successor is underway, it seems that for now we'll be sticking with the A53 architecture for a while longer.HiSilicon, MediaTek and Rockchip are listed among more than then launch partners which have already licensed the Cortex-A72 processor, so expect to see a variety of vendors offering the new ARM IP in 2016.CoreLink CCI-500 SoC interconnectIt's been over 3 years since ARM initially announced their CCI-400 (Cache Coherent Interconnect), which saw widespread usage as the corner-stone technology enabling big.LITTLE heterogeneous multiprocessing in all consumer SoCs from the Exynos 5410 to the latest Snapdragon 810. While ARM also offered high-end alternatives such as the CCN-5XX (Cache Coherent Network) range of interconnects, these were targeted more at server-applications and not meant for mobile SoCs in smartphones or tablets.The CCI-500is a large upgrade over the CCI-400 as it introduces a variety of new functionality over its predecessor. The largest change in functionality is the addition of a snoop filter on the interconnect itself. Until now snoop control was only possible between CPUs within a single cluster. The addition of a snoop filter on the interconnect allows for power efficiency benefits as the amount of transactions when doing cache lookups is decreased, enabling both reduced overhead on the interconnect and also higher idle residency times on the CPU cores. This reduced overhead also frees up memory bandwidth on the interconnect, and ARM claims this enables for 30% better memory performance on the CPU ports.The new interconnect also doubles up on its system bandwidth: We now have twice the number of ACE (AXI Coherency Extension) ports, enabling usage of a maximum of four CPU clusters (instead of the two that are possible with the CCI-400). We'll be continuing to see the usage of only two clusters in mobile designs, but the new IP gives licensees the flexibility to deviate according to their needs.The increased bandwidth and numbers of ports on the interconnect also opens up the possibility of quad-channel memory controllers, resulting in 128-bit memory buses. The Snapdragon 805 was the first mobile product to feature such capability, although Qualcomm used a non-cache-coherent interconnect in their design.Mali T880 GPULastly, ARM also announced a new member of the T800 series of Mali GPUs. In addition to theT820, T830 and T860comes the T880. ARM was again light on details of what this new configuration brings, only promising a 1.8x increase performance over 2014 Mali T760 GPUs and a 40% reduction in energy consumption for the same workloads.With today's announcements, ARM appears to be addressing its weaknesses in mobile SoCs by focusing on sustained performance and efficiency of its big core architecture. We also have the much needed upgrade in the memory/interconnect subsystem and an expansion in its GPU IP offering.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8957/arm-announces-cortex-a72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Netgear Launches ARM Cortex-A15-based ReadyNAS 200 Series\n",
      "Author: Ganesh T S\n",
      "Date Published: 2015-01-15T10:30:00Z\n",
      "URL: https://www.anandtech.com/show/8900/netgear-launches-arm-cortexa15based-readynas-200-series\n",
      "Content: Netgear's major announcement at CES 2015 was the ReadyNAS 200 series of NAS units targeting the SOHO market and power users. This lineup has two members, a 2-bay RN202 and a 4-bay RN204. The ReadyNAS 200 series is based on a dual-core Cortex-A15 SoC from Annapurna Labs. The system has 2 GB of RAM and two GbE ports. 802.3ad dynamic link aggregation is supported, and transfer rates of around 200 MBps are possible (similar to what QNAP claims for their TS-x31+ series). The units run ReadyNAS OS 6.2 and have a MSRP of $360 and $500 for the 2-bay and 4-bay variants.As I mentioned in our COTS NAS buyer's guide last month, Netgear's ReadyNAS OS 6.x is quite interesting because of the choice of a btrfs file system. We also get snapshots with minimal overhead (due to the COW - Copy on Write - nature of the file system) and protection against bit-rot. The ReadyNAS 200 series provides a powerful, yet affordable alternative to the ReadyNAS 300 and ReadyNAS 500 series of NAS units.At the show, Netgear also launched the AirCard, a battery-powered 4G hotspot device connecting to the Sprint network. The $200 price also includes 1 GB of data, with the option to purchase more further down the road. In the ProSAFE lineup, we also got Click Switches - 8- and 16- port GbE unmanaged switches with USB charging ports. The focus of the product is on ease of use and versatility. On the powerline networking front, Netgear had their HomePlug AV2 PL1200 and PLP1200 devices on display.Netgear decided to hold off on any major router announcements at CES. Most of the other networking vendors announcing flagship products were planning to ship in Q2 or later. This obviously points to the hardware platforms not being ready. It does make plenty of sense for Netgear to announce products closer to their release dates. In addition, they got in first on the Broadcom XStream platform with the Nighthawk X6 R8000 and second on the 4x4 MU-MIMO Quantenna solution (after Asus) with the Nighthawk X4 R7500. They have both flagships currently in the market. The focus at CES was more on the software updates (such as ReadyCLOUD integration, configurable power output for the Wi-Fi radios etc.) that Negear had in the pipeline for the R7000 platform.That said, they did show off a Nighthawk AC1900 range extender - basically, the same internal hardware as the R7000, but, without routing support, and priced a good $30 cheaper.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8900/netgear-launches-arm-cortexa15based-readynas-200-series\n",
      "Title: ARM Challenging Intel in the Server Market: An Overview\n",
      "Author: Johan De Gelas\n",
      "Date Published: 2014-12-16T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/8776/arm-challinging-intel-in-the-server-market-an-overview\n",
      "Content: Introduction to ARM Servers\"Intel does not have any competition whatsoever in the midrange and high-end (x86) server market\". We came to that rather boringconclusionin our review of the Xeon E5-2600 v2. That date was September 2013.At the same time, the number of announcements and press releases about ARM server SoCs based on the new ARMv8 ISA were almost uncountable. AppliedMicro was announcing their 64-bit ARMv8 X-Gene back in late 2011. Calxeda sent usa real ARM-based serverat the end of 2012. Texas Instruments, Cavium, AMD, Broadcom, and Qualcomm announced that they would be challenging Intel in the server market with ARM SoCs. Today, the firstretail productshavefinallyappearedin theHP Moonshot server.There has been no lack of muscular statements about the ARM Server SoCs. For example, Andrew Feldman, the founder of micro server pioneer Seamicro and the former head of the server department at AMD stated: \"In the history of computers, smaller, lower-cost, and higher-volume CPUs have always won. ARM cores, with their low-power heritage in devices, should enable power-efficient server chips.\" One of the most infamous silicon valley insiders evenwent so far as to say, \"ARM servers are currently steamroller-ing Intel in key high margin areas but for some reason the company is pretending they don’t exist.\"Rest assured, we will not stop at opinions and press releases. As people started talking specifications, we really got interested. Let's see how the Cavium Thunder-X, AppliedMicro X-Gene, Broadcom Vulcan, and AMD Opteron A1100 compare to the current and future Intel Server chips. We are working hard to get all these contenders in our lab, and we are having some success, but it is too soon for a full blown shoot out.Micro Servers and Scale-out ServersMicro servers were the first the target of the ARM licensees. Typically, a discussion about Micro servers quickly turns into a wimpy versus brawny core debate. One of the reasons for that is that Seamicro, the inventor of the micro server, first entered the market with Atom CPUs. The second reason is that Calxeda, the pioneer of ARM based servers, had to work with the fact that the Cortex-A9 core was a wimpy core that could not deal with most server workloads.Wikipediaalso associates micro servers with very low power SoCs: “Very low power and small size server based on System-on-Chip, typically centered around ARM processor”.Micro servers are typically associated with low end servers that serve static HTML, cache web objects, and/or function as slow storage servers. It's true that you will not find a 150W high-end Xeon inside a micro server, but that does not mean that micro servers are defined by low power SoCs. In fact, the most successful micro servers are based on 15-45W Xeon E3s. Seamicro, the pioneer of micro servers, clearly indicated that there was little interest in the low power Atom based systems, but that sales spiked once they integrated Xeon E3s.Currently micro servers are still a niche market. But micro servers are definitely not hype; they are here to stay, although we don't think they will be as dominant as rack servers or even blade servers in the near future. To understand why we would make such a bold statement, it is important to understand the real reason why micro servers exist.Let us go back to the past decade (2005-2010). Virtualization was (and is) embraced as the best way to make enterprises with many heterogeneous applications running on underutilized servers more efficient. RAM capacity and core counts shot up. Networking and storage lagged but caught up – more or less – as flash storage, 10 Gbit Ethernet, and SRIOV became available. But the trend to notice was that virtualization made servers more I/O feature rich: the number and speed of network NICs and PCI-e expansion slots for storage increased quickly. Servers based on the Xeon E5 and Opterons have become \"software defined datacenters in a box\" with virtual switching and storage. The main driver for buying complex servers with high processor counts and more I/O devices is simple: professionals want the benefits that highly integrated virtualization software brings. Faster provisioning, high availability (HA), live migration (vMotion), disaster recovery (DR), keeping old services alive (running on Windows 2000 for example): virtualization made everything so much easier.But what if you did not need those features because your application is spread among many servers and can take a few hardware outages? What if you do not need the complex hardware sharing features such as SRIOV and VT-d? The prime example is an application like Facebook, but quite a few smaller web farms are in a similar situation. If you do not need the features that come with enterprise virtualization software, you are just adding complexity and (consultancy/training) costs to your infrastructure.Unfortunately, as always, the industry analysts came withunrealistichigh predictionsfor the new micro server market: in 2016, they would be 10% of the market, no less than \"a 50 fold jump\"! The simple truth is that there is a lot of demand for \"non-virtualized\" servers, but they do not all have to be as dense and low power as the micro servers inside theBoston Viridis. The \"very low power\", extremely dense micro servers with their very low power SoCs are not a good match for most workloads out there, with the exception of some storage and memcached machines. But there is a much larger market for servers denser than the current rack servers, but less complex and cheaper than the current blade servers, and there's a demand for systems with a relatively strong SoC, currently the SoCs with a TDP in the 20W-80W range.Not convinced? ARM and the ARM licensees are. The first thing that Lakshmi Mandyam, the director of ARM servers systems at ARM, emphasized when we talked to her is that ARM servers will be targeting scale-out servers, not just micro servers. The big difference is that micro servers are using (very) low power CPUs, while scale-out servers are just servers that can run lots and lots of threads in parallel.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8776/arm-challinging-intel-in-the-server-market-an-overview\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Synology's DS2015xs brings ARM to High-Performance NAS Units\n",
      "Author: Ganesh T S\n",
      "Date Published: 2014-12-09T08:00:00Z\n",
      "URL: https://www.anandtech.com/show/8777/synologys-ds2015xs-brings-arm-to-highperformance-nas-units\n",
      "Content: In the current NAS market, it is downright impossible to talk of ARM and high performance together. The most powerful ARM-based NAS units have been based on Marvell's ARMADA processors. They usually come with dual gigabit network links and typically target the SOHO and low-end SMB market. Intel's offerings have had a virtual monopoly in the other tiers of the market. Synology is set to change all that with their latest offering - the DS2015xs with native 10G capabilities.DS2015xs: Features & SpecificationsThe DS2015xs is 8-bay NAS unit presented as a step-up from the DS1815+. While the DS1815+ can expand up to a total of 18 bays with two DX513 expansion chassis, the DS2015xs is compatible with the 12-bay DX1215 expander (for a total of 20 bays). The main step-up from the DS1815+ is the presence of two built-in 10G SFP+ copper links.Claimed performance numbers include transfer speeds of up to 2000 MBps and 59K sequenital IOPS.Platform DetailsThe SoC at the heart of the DS2015xs is the AL-514 fromAnnapurna Labs, an Israeli startup that is still in stealth mode. The company has declined to speak to the media as of now. However, tracing some coverage of Israeli VC firms reveals that Annapurna Labs was founded in 2011 with the intent of bringing ARM-based communication processors to the market. Datasheets of SoCs from Annapurna Labs are not currently available to the public, but Synology was kind enough to divulge the following details (which, I suspect, can be gleaned via SSH access to the DS2015xs):The AL-514 has four ARM Cortex-A15 cores running at 1.7 GHzThe Cortex-A15 cores are configured with LPAE (large physical address extension) that allows addressing of more than 4 GB of RAM (the DS2015xs supports up to 8 GB)The SoC has two 10G Ethernet MAC IPs integratedBeyond these, details are scarce. We hope Annapurna Labs comes out of stealth mode soon and supplies us with more details. We might possibly be able to gather more information about the platform once we get the DS2015xs in hand.Moving on to the relevance of the AL-514 for the ARM ecosystem, it is clear that at least Synology considers it to be a worthy competitor to Intel's Rangeley. The DS2015xs is being positioned as a clear step-up from the Rangeley-based DS1815+. That said, the fact is that A15 is a 32-bit processor, and it is not entirely suitable (despite the LPAE extensions) for high-performance servers. ARM has since moved on to ARMv8 processors (true 64-bit) for both the smartphone and the server / communication infrastructure market.Related Announcements - DS3615xs and DX1215In other related news, Synology is also launching theDS3615xs, a native 12-bay desktop form factor solution (expandable to 36 bays with support for two DX1215 expanders). It is a straight-up upgrade to the previous-generation DS3612xs which used a Sandy Bridge-based Core i3-2100. The DS3615xs uses the Haswell Core i3-4130. It comes with four gigabit links, but also has support for a PCIe expansion card (to potentially add 10G capabilities). Like the previous generation xs-units, it comes with ECC RAM (4GB DDR3 by default, expandable to 32GB). Pricing and release date in the Americas for the DS3615xs is not yet available.The DX1215 is a 12-bay expansion unit for select Synology DiskStation models. Unlike the eSATA-based DX513, the DX1215 connects to the main unit using Infiniband. Further details are availablehere.Concluding RemarksThe DS2015xs will retail for $1400 - a price that really makes 10G affordable, while undercutting the introductory pricing of the previous generation xs units significantly. We have to admit that the xs positioning of the unit was initially very surprising. After all, the Cortex A15 really can't stand up to the Core i3 (on which the previous generation xs units have been based) in terms of raw CPU performance. There is no support for ECC RAM either. Synology clarified that the CPU performance would definitely not match up. However, the presence of communication and storage accelerators - particularly, natively integrated 10G capabilities, RAID acceleration and hardware-assisted encryption - enable the DS2015xs to support bandwidths that have typically not been reached at this price point.While the Annapurna AL-514 SoC seems more like a stop-gap solution for this market segment (particularly when true 64b ARM solutions are starting to make an appearance), consumers are fortunately shielded by Synology's DiskStation Manager (DSM) OS. The operating system manages to isolate the user experience with various NAS aspects from the underlying platform architecture. The end-user experience is the same, regardless of whether the NAS being used is based on ARM or x86.On the whole, Synology seems to have hit a home run with the price to performance ratio in the DS2015xs. We hope to soon have a unit in hand to test out how it performs with our standardized lab environment. We would also like to hear from readers on what sort of applications they are running on their Synology NAS units. This would help us determine how much of a performance difference there is between, say, the DS1815+ and the DS2015xs, for applications that matter to end users.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8777/synologys-ds2015xs-brings-arm-to-highperformance-nas-units\n",
      "Title: Sponsored Post: ARM Wearables Week\n",
      "Author: Sponsored Post\n",
      "Date Published: 2014-11-15T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/8723/sponsored-post-arm-wearables-week\n",
      "Content: The incredible rise of the smartphone market was a phenomena that caught virtually everyone off-guard. Though cellular phones had been ramping up in complexity for nearly 20 years, the sudden jump in consumer demand from “dumb” to “smart” is virtually unprecedented; very few technologies have been so thoroughly adopted in such a short period of time. At the center of all of this was ARM, whose processors and architecture powered nearly every single phone at the heart of the smartphone revolution.Now with mobile device sales stabilizing and devices approaching “good enough” status, the consumer electronics industry finds itself looking at the future and what comes next. Looking to repeat the smartphone revolution, all eyes are on wearable computing, which is looking to build off of the technologies and lessons of smartphones to start bringing some of that processing power and functionality into some of the smallest and most personal devices yet.As part of the broader conversation and development of wearables, next week ARM will be holding theirARM Wearables Weekevent. For this event ARM will be focusing on the technical issues facing wearables and how they can solve them – processing power, battery life, and meeting those aforementioned needs while fitting the entire package in an acceptable form factor. WithCortex-Mandmbedamong the many tools in their portfolio, ARM believes that the technology is right and the time is right for wearables to take off.Highlights of the week will include interviews with industry experts and the Wearables Week Webinar with Omate, which will be providing a case study for wearables by looking at the development of Omate’s products. ARM will also be doing teardowns of various wearable devices such as the Samsung Galaxy Gear 2 Neo and LG G, to show how these devices are put together.So be sure to check out theARM Wearables Week websitefor the above and more as ARM continues to roll out new content throughout the week.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8723/sponsored-post-arm-wearables-week\n",
      "Title: ARM Announces Mali 800 Series GPUs - T860, T830, & T820\n",
      "Author: Ryan Smith\n",
      "Date Published: 2014-10-28T02:31:00Z\n",
      "URL: https://www.anandtech.com/show/8649/arm-announces-mali-800-series-gpus-t860-t830-t820\n",
      "Content: Due to a lack of total vertical integration and heavy focus on IP licensing, one of the more interesting aspects of the SoC development pipeline is that we get to see the architectures and chips developed and announced in a very decoupled fashion. As opposed to the PC industry where there is heavy vertical integration and architectures are usually announced close to (if not at the same time as) the first silicon and even the first finished products, in the SoC space we will frequently see the complete development cadence in public – architectures, chips, and finally finished products. This has its own ups and downs, and while it means we’ll potentially hear about a new architecture long before it’s available in a product, on the other hand we get a lot more visibility into what’s coming down the development pipeline, at least for companies that develop IP for licensing or purchase it for use in their SoCs.This brings us to the matter of ARM and today’s announcements. As a massive ISA, CPU, and GPU licenser, ARM’s product lineup is the textbook case for early visibility. At a time when the first high-end 20nm SoCs are just now hitting consumer devices in products such as the Samsung Galaxy Note 4 International and Apple iPhone 6, ARM’s development pipeline and product marketing teams are already looking at next year’s products and what processor designs to prepare for them. For ARM’s GPU group in particular, whom now functionally operates on a yearly release cadence, the roll-out of Mali 700 equipped devices means that it’s time to announce the designs for next year’s GPUs.To that end, today ARM is announcing the Mali 800 series. Designed for inclusion in 2015+ SoCs, the Mali 800 series is the immediate successor to the current Mali 700 series. From an architectural standpoint Mali 800 is still based on the same common Midgard architecture that the Mali 600 and Mali 700 series are based on, and as a result from an architectural standpoint there isn’t much to discuss.Midgard and its unusual all-ILP/no-TLP architectureis still a modern GPU architecture that supports up to OpenGL ES 3.1, the Android Extension Pack, and Direct3D 11.1, so from that standpoint ARM has little reason to change.Consequently while still based on Midgard, the 800 series is a successive round of optimization for the Midgard designs. For this generation ARM has focused on further improving Midgard’s area and power efficiency while adding a handful of new features not found in the existing Mali 700 series. This refresh spans from ARM’s highest-end designs to lowest-end designs, and coupled with Mali’s multi-core scalability spans the SoC GPU market from top to bottom.The Midgard Shader CoreARM Mali 800 SeriesT860T830T820Core Configurations1-161-41-4ALU Pipes Per Core221Texture Units Per Core111FLOPs (FP32 MAD)20 - 320 FLOPs20 - 80 FLOPs10 - 40 FLOPsOpenGL ESES 3.1 + AEPES 3.1 + AEPES 3.1 + AEPDirect3DFL 11_1FL 9_3FL 9_310-Bit YUVYesOptionalOptionalMali-T860We’ll start off with ARM’s new high-end Mali design, the Mali-T860. With ARM reforming their product naming with the Mali 700 series ARM’s lineup is now much easier to follow, and as given away by the T860’s design it’s the immediate successor to the T760.Like T760 before it, T860 is ARM’s most feature packed and most powerful Mali design. The underlying design uses the more common Midgard 2 arithmetic pipelines per core configuration, with the overall design being scalable to up to 16 cores. Coupled with the single texture unit per core, the throughput of a T860 design can scale from 20 FLOPs (10 MADs) and 1 texel per clock up to 320 FLOPs and 16 texels per clock. As a result this design can also be scaled up and down as needed to cover both phones and tablets just by varying the number of cores.From a functionality standpoint, T860 will be the only Mali 800 part to support the 800 series’ fullest feature set. Specifically, support for Direct3D feature level 11_1 is limited to T860. Otherwise common to the entire 800 series, T860 also supports OpenGL ES 3.1, the Android Extension Pack, and OpenCL 1.2.Meanwhile one notable feature addition for the Mali 800 family is support for native (and full speed) 10-bit YUV input and output. At the moment this feature addition is going to be of limited value, but HEVC is expected to make significant use of 10-bit YUV, so adding support here is laying the groundwork for HEVC in future products, and for that matter will go hand-in-hand with ARM’s new video processing block and display controller block, which are also being announced today.Elsewhere from a performance standpoint ARM is offering the usual high level performance estimates. However it should be noted that these are compared to the two-generation old T628, and there aren’t similar numbers to work from for T760. In any case, compared to T628 ARM expects an equal configuration T860 to be some 45% more energy efficient on the same process node. And seeing as how mobile performance gains are almost entirely an exercise in energy efficiency, this would represent a very significant increase in energy efficiency (and ultimately sustainable performance) for their designs.ARM Frame Buffer Compression -From Our Look at Midgard Earlier This YearThat said, by making a two-generation old comparison ARM also gets to roll up the benefits of their AFBC frame buffer compression technology, which was first introduced on the Mali 700 series. AFBC is something the company is significantly banking on due to the high bandwidth savings, and ARM considers one of their greatest feature advantages for the 800 series as well as the 700 series.Finally, something to also keep in mind though is that while ARM’s same-node comparison is the fairest way to look at architectural efficiency, you’re highly unlikely to see T628 manufactured on 20nm+ processes. So on top of the architectural gains over the years, the real world performance gains for T860 should be better still due to the newer process node.Mali-T830 & Mali-T820Also being announced today alongside the T860 and rounding out the new Mali 800 family are the T830 and T820. These parts are best described as ARM’s low-end and mainstream designs, and are the successors to the T720. With Mali T860 essentially scaling down to cover most of the mid-range, the T820 and T830 are intended to be lower performance, lower power consuming parts that are optimized around power and die size needs.For the Mali 800 series ARM is bifurcating the T720’s market a bit to offer different blends of die size and performance. The T720’s immediate successor is the T820, and like its predecessor is a one arithmetic pipeline design that is focused first and foremost on die size. Meanwhile new to the 800 series, though still a successor of sorts to the T720, is the Mali-T830. This is a more powerful design that while still focused on die size efficiency brings the number of pipelines to two per core (like T860), offering better performance in exchange for a slightly larger die size.Other than the difference in the number of pipelines, the T820 and T830 designs are extremely similar. Both of them can be scaled up to 4 cores, allowing for some performance scaling. This puts the throughput of T820 designs at a range of 10 FLOPs and 1 texel per clock to 40 FLOPs and 4 texels per clock, while T830 will scale from 20/1 to 80/4 respectively.From a feature standpoint, as previously mentioned only T860 gets the fullest Mali feature set while the other Mali 800 parts will be a bit more modest. T830 and T820 only support Direct3D up to feature level 9_3, while for the more mobile-centric world they will be on par with the T860 and support OpenGL ES 3.1 and the Android Extension Pack. Meanwhile 10-bit YUV support is present here, however for T820 and T830 it is an optional feature that will depend on which specific version of the core is licensed, so we will likely see a mix of retail products that do and do not include it.Finally from a performance standpoint ARM is once again offering some high level guidance compared to the Mali 600 series, specifically the Mali-T622 in this case. Depending on the 800 design used, ARM tells us that performance should be up to 55% better or area efficiency will be 50% better. Presumably the area efficiency comparison is for T820 while the performance comparison is for T830.Closing ThoughtsLaunching alongside the new Mali GPUs today are a series of updates for the rest of ARM’s graphics stack, which will see the Mali-V video block and Mali-DP display controllers updated respectively. Along with the general strength of the Mali GPUs, expect to see ARM focus heavy on the synergy between these parts, including their common support for AFBC and of course the benefits of having all graphics components developed together.We’re covering these in another article, but we wanted to quickly point out where the Mali GPUs fit in the bigger picture of ARM’s announcements today.Finally, while ARM doesn't have complete control over consumer devices (since they only sell designs to chipmakers) they are providing a rough estimate of when to expect Mali 800 GPUs to begin appearing in devices. According to ARM we should expect to start seeing Mali 800 devices starting in late 2015, or roughly a year from now. This is consistent with the Mali 700 series, which having been announced almost a year ago to this day has started to show up in consumer devices very recently. To that end we would expect to start seeing Mali 800 SoC designs announced in the first half of next year, with consumer designs to follow as per ARM's timetable.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8649/arm-announces-mali-800-series-gpus-t860-t830-t820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Announces Mali-V550 Video Processor & Mali-DP550 Display Processor\n",
      "Author: Ryan Smith\n",
      "Date Published: 2014-10-28T02:30:00Z\n",
      "URL: https://www.anandtech.com/show/8650/arm-announces-maliv550-video-processor-malidp550-display-processor\n",
      "Content: As part of ARM’s fall refresh of their Mali graphics product lineup, today ARM is announcing refreshes and new products in a number of product segments. All told ARM is releasing new GPUs, a new video process, and a new display processor. Having already taken a look atthe new Mali 800 series GPUsseparately, let’s dive into the other part of today’s announcements from ARM, focusing on the video processor and the display processor.As ARM seeks to be a nearly top-to-bottom processor IP vendor, alongside their well-known Cortex CPU designs and Mali-T GPU designs the company also offers the other processors needed to complete the graphics stack. Alongside the GPU designs these are the Mali-V video processors for real-time video encode and decode, and the Mali-DP display processors. In the scope of today’s announcements both parts are receiving feature updates to add new functionality and to keep them up to par with the capabilities of the new Mali GPUs. Overall ARM is making a significant emphasis on 4K this year, lining up the hardware necessary to decode and render to 4K, and to do so within the bandwidth constrains of an SoC.Mali-V550Starting things off we have ARM’s new video processor, the Mali-V550. With the V550 ARM is introducing full HEVC support into their video processor, building off of the H.264 support present in the earlier V500. Like the V500 the V550 is a video decode and encode processor, so with today’s update ARM’s product stack gains the ability to do both HEVC decoding and encoding.Overall V550 retains V500’s basic architectural design, which not unlike ARM’s GPUs is based around the concept of cores. In this case the number of cores can be scaled up to support additional streams or higher resolutions/framerates, with ARM supporting 1080p60 on a single core and scaling up to 2160p120 on the largest 8 core configuration. This of course goes for both encode and decode performance.Along with the addition of HEVC support, V550 also introduces a couple of other new features to the Mali video processor. ARM can now process 10-bit YUV video, which is expected to see significant usage with HEVC and goes hand-in-hand with the 10-bit YUV support added to the Mali 800 GPUs.Meanwhile ARM is introducing a new video encoding feature for V550 dubbed Motion Search Elimination, which is intended to reduce video latency and power requirements. MSE borrows heavily from ARM’stransaction elimination technology, which was introduced on the Mali 700 series and reduced memory bandwidth by using CRCs to identify unchanged screen tiles and to avoid writing those redundant tiles out to the frame buffer. In the case of MSE the idea is quite similar, only this time ARM applies the tile principle to video encoding instead of rendering. By identifying unchanged tiles, ARM can then skip the motion estimation step of H.264 video encoding for those tiles, which saves on time and power doing what would otherwise be a redundant encoding step.Eliminating motion estimation should have an immediate impact on power consumption, as it’s generally consideredthe most time consuming step of the encoding process. However along with the power savings from skipping motion estimation, ARM is also touting this as a means of reducing encoding latency, something which would be very important for wireless display technologies such as Miracast. The benefits would be variable at best, but for transmitting desktops and other semi-static content it should have an impact on encode latency and therefore overall input lag on such setups.One thing to note though is that like 10-bit YUV, MSE will require the rest of the processing pipeline to support this feature. In this case the display processor (which is piping the display output back to the video processor) needs to support the technology.Mali-DP550And speaking of the display processor, we have ARM’s final announcement of the day, which is the Mali-DP550 display controller. Like the V550 video processor, the DP550 is an ecosystem play that sees some general feature enhancements along with reciprocal support for other new features introduced on ARM’s latest GPUs and video processors.From a high level overview, ARM’s display processors serve as both a display controller and a sort of simple GPU of their own, handling basic operations such as scaling and layer composition along with interfacing with the display itself. By doing this in fixed function hardware as opposed to the more flexible but power hungry GPU, ARM is able to save some power by avoiding sending off this simple work to the GPU.For DP550, ARM has scaled up its performance to handle 4K resolutions along with more complex composition tasks. Whereas DP500 could only composite up to 3 layers and work with sub-4K panels, DP550 can drive 4K panels while compositing up to 7 layers. From a phone and tablet perspective it seems questionable if we’re going to see 4K in those devices any time soon, but for ARM’s secondary TV market, being 4K capable on DP550 will be a big deal.Meanwhile DP550 adds reciprocal support for other features added to the Mali 800 GPUs and Mali-V550 video processor, including motion search elimination and 10-bit YUV for video. Also of note, ARM has added a co-processor interface to DP550 to allow it to directly interface with 3rdparty processor blocks, though ARM hasn’t gone into detail on what they expect those 3rdparty blocks to be.Finally, like the Mali 800 GPUs, ARM is releasing these designs to SoC integrators today. Which means we should start seeing the V550 and DP550 appear in retail products by the end of next year.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8650/arm-announces-maliv550-video-processor-malidp550-display-processor\n",
      "Title: ARM Announces “mbed” IoT Device Platform\n",
      "Author: Ryan Smith\n",
      "Date Published: 2014-10-01T19:15:00Z\n",
      "URL: https://www.anandtech.com/show/8583/arm-announces-mbed-iot-device-platform\n",
      "Content: Following up on the incredible success of smartphones, tablets, and other handheld-size mobile devices, device manufacturers have been toying with ideas on what comes next. A common theme across many of these ideas has been the Internet of Things concept, which sees microcontrollers and Internet connectivity embedded into increasingly small or otherwise unusual devices where network connectivity wasn’t present before. From a technology perspective this is an exercise in seeing what you can do with products and environments where everything is networked, and meanwhile from a market perspective this is about driving the next wave of market growth for chip and device manufacturers.But while the IoT concept seems simple on the surface – just put a chip in everything – what device makers have found out is that they have yet to discover what a good implementation would look like and what parts they need to build it. For that reason we have seen numerous companies toy with the IoT concept over the last year, launching new hardware components such asIntel’s Edisonand the ARM Cortex-M7, or software components likeMediaTek’s LinkIt. Which brings us to today’s news from ARM and their latest IoT project.Being announced today at ARM TechCon 2014, ARM is unveiling their “mbed” (all lower case) IoT Device Platform, which is ARM’s new software platform for IoT devices and the servers feeding them. mbed is a surprisingly blunt project from ARM, who thanks to the success of their platform and their work over the years on their Cortex-M series of CPUs already has most of the hardware in place. Confident in their hardware, ARM is seeking to tackle what they see as the current issue holding IoT back: software.The mbed platform is a combination of client and server software, consisting of a lightweight OS for client devices – mbed OS – and the matching server software to interact with it, the mbed Device Server. Like ARM’s hardware IP, both mbed OS and mbed Device Server are intended to be building blocks for finished products. Specifically, the idea being that developers will take the mbed components and build the application logic they need on top of a solid software foundation provided by ARM. By providing the OS and Device Server with the necessary support for various networking, wireless, and security standards built in, then as ARM’s thinking goes IoT software development will be dramatically simplified as ARM will have taken care of the hard parts, leaving developers free to focus on the application logic itself, reducing development costs and the time to marketFor the mbed OS component, the OS is a lightweight, low-power kit OS designed to run on Cortex-M processors. ARM for their part will build in the necessary hardware features and even some common libraries, with a focus on providing building blocks for developers looking to design finished products. At this point ARM’s announcement predates the software by a bit, so mbed OS is still under development with early access for partners in this quarter with a shipping version in 2015.Meanwhile the mbed Device Server is essentially a software bridge designed to allow the mbed OS devices to interact with web services. Unlike the stand-alone OS, Device Server is intended to be integrated into larger (cloud) server setups, with Device Server providing the means for the easy interaction with and management of mbed OS clients. As ARM likes to note the Device Server is based around open standards, with the idea being that they’re providing another kit (this time for the server) to give developers a place to start rather than creating a closed ecosystem around the mbed OS and Device Server components. Finally, unlike the OS, the Device Server is already running and available to developers.In the short term mbed is all about kickstarting the IoT market, which is a big reason as to why ARM is giving away large chunks of it for free. The mbed OS is entirely free, and the Device Server is free for development. Only production setups running Device Server would need to pay a license fee. ARM wants to get mbed spread as widely as possible, and with their strong position in the hardware market they are more than willing to give away the software if it will spur on IoT hardware sales. Or as they see it, the time it takes to develop good software is currently gating the sales of products incorporating their IoT-focused hardware.Looking at the bigger picture, while ARM has the right idea overall they are not the only company pursuing this software/toolkit market. Intel’s Edison platform ships with its own OS, and MediaTek’s LinkIt platform also includes its own LinkIt OS for many of the same purposes. However in all of these cases ARM and other companies can only provide the building blocks; they still must rely on developers to put together the killer app that will jumpstart the market. Largely for this reason the potential success for the IoT market is out of the hands of ARM and other hardware IP providers, but by providing a solid set of building blocks and working with a very large set of partners such as Marvell and Freescale, they are doing what they can to make that success happen.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8583/arm-announces-mbed-iot-device-platform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: HP, AppliedMicro and TI Bring New ARM Servers to Retail\n",
      "Author: Stephen Barrett\n",
      "Date Published: 2014-09-30T13:19:00Z\n",
      "URL: https://www.anandtech.com/show/8580/hp-appliedmicro-and-ti-bring-new-arm-servers-to-retail\n",
      "Content: Yesterday HP announced retail availability of two ARM based servers, the ProLiant m400 and m800. Each are offered in a server cartridge as part of the Moonshot System. A single 4.3U Moonshot chassis can hold 45 server cartridges. Usually higher numbers mean better, but in this case the m400 and m800 are so significantly different I wouldn’t consider them competitors. The m800 is focused on parallel compute and DSP, while the m400 is focused on compute, memory bandwidth, IO bandwidth and features the first 64-bit ARM processor to reach retail server availability.HP ProLiant ARM Serversm400m800Processors14ProcessorAppliedMicro X-GeneCustom 64-bit ARMv8TI KeyStone II 66AK2HCortex-A15 ARMv7A + DSPCompute cores per processor8 CPU4 CPU8 DSPClock Speed2.4 GHz1.0 GHzCache MemoryEach core: 32KB L1 D$ and I$Each pair: 256KB L2All cores: 8MB L3Each DSP core: 1MB L2MemoryQuad Channel8 SODIMM SlotsDDR3-1600 Low VoltageMax: 64GB (8x8GB)Single Channel4 SODIMM SlotsDDR3-1600 Low VoltageMax: 32GB (4x8GB)Network ControllerDual 10GbEDual 1GbEStorageM.2 2280M.2 2242PCIe3.02.0Starting with the m400, HP designed in a single AppliedMicro X-Gene SoC at 2.4 GHz. AppliedMicro has beendiscussing the X-Gene processorfor several years now, and with this announcement becomes the first vendor to achieve retail availability of a 64-bit ARMv8 SoC other than Apple. Considering Apple doesn’t sell their processors stand-alone, this is a significant milestone. AppliedMicro has significantly beatenAMD’s A1100 processorto market, as AMD has not yet entered production. Marquee features of the X-Gene SoC include 8 custom 64-bit ARM cores, which at quad-issue should be higher performance than A57, quad channel DDR3 memory, and integrated PCIe 3.0 and dual 10GbE interfaces. Look out for a deep dive on the X-Gene SoC in a future article.The m800 is a 32-bit ARM server containing four Texas Instruments KeyStone II 66AK2H SoCs at 1.0 GHz. Each KeyStone II SoC contains four A15 CPU cores alongside eight TI C66x DSP cores and single channel DDR3 memory, for a total of 16 CPU and 32 DSP cores. IO steps back to dual GbE and PCIe 2.0 interfaces. It is clear from the differences in these servers that m400 and m800 target different markets. There isn’t yet a best-of-both-worlds server combining the core count and memory + IO interfaces of the m400 and m800 together.Each server is available with Ubuntu and IBM Informix database preinstalled, and will be demonstrated at ARM TechCon October 1-3 in Santa Clara, California.Source:HP\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8580/hp-appliedmicro-and-ti-bring-new-arm-servers-to-retail\n",
      "Title: Samsung's Exynos 5433 is an A57/A53 ARM SoC\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2014-09-16T15:40:00Z\n",
      "URL: https://www.anandtech.com/show/8537/samsungs-exynos-5433-is-an-a57a53-arm-soc\n",
      "Content: There has been a lot of confusion going on over the last few weeks on what exactly Samsung's Exynos 5433 is. Joshua and I were pretty much convinced that it was a standard big.LITTLE A15/A7 chip configuration due to naming consistencies and evidence in firmware. Even though the Note 4 was already announced with region-specific models employing this chip, Samsung S.LSI has yet to divulge any kind of official information on the matter or even publicly announce the chip.With the release ofnew source code, we can now confirm that the Exynos 5433 is indeed the first Cortex A57/A53 SoC to market. We see a 4x Cortex A57, 4x Cortex A53 big.LITTLE CPU configuration employed in the part, here's a little overview of what we currently know:Samsung Exynos 5 Octa 2014 lineupSoCSamsungExynos 5422SamsungExynos 5430SamsungExynos 5433CPU4x Cortex A7 r0p5 @ 1.3GHz4x Cortex A15 r2p4 @ 1.9GHz4x Cortex A7 r0p5 @ 1.3GHz4x Cortex A15 r3p3 @ 1.8GHz4x Cortex A53 @1.3GHz4x Cortex A57 r1p0 @1.9GHzMemoryController2x 32-bit @ 933MHz14.9GB/s b/w2x 32-bit @ 1066MHz17.0GB/s b/w2x 32-bit @ 825MHz13.2GB/s b/wGPUMali T628MP6@ 533MHzMali T628MP6@ 600MHzMali T760MP6@ 700MHzMfc.ProcessSamsung28nm HKMGSamsung20nm HKMGSamsung20nm HKMGThe big question is why Samsung choose to name this chip Exynos 5433 and not market it as a 64-bit chip in a new product lineup? The answer could be simply that we won't ever see the 5433 running in AArch64 mode. The chip's firmware and drivers are running on a \"CAL\" / Chip-Abstraction-Layer on the lowest level of the driver stacks. In fact, beyond the CPU cores (and GPU), the Exynos 5433 looks very similar to the Exynos 5430 which employs A15/A7 cores.While we won't be seeing the Exynos 5433 running 64-bit code any time soon, it still takes advantage of the architectural improvements of ARM's Cortex A57 and A53 cores and their ARMv8 instruction set (running in AArch32 mode). Power consumption should also be improved due to the new A50's power management and new retention features. The silicon, similarly to the 5430, is manufactured on Samsung's new 20nm process.Atlas (A57) and Apollo (A53) cores in the power management driversAlso employed for the first time is ARM's new Mali T760 GPU running at 700MHz. We already published anarchitectural diveinto the T760 detailing what's new.I wasn't able to determine the number of cores on this GPU due to ARM's transparent and scalable driver architecture in regards to shader cores, this is something we'll have to wait for in the eventual official announcement or in a hands-on investigation.OpenCL information points out to 6 compute units, hence we can derive an MP6 configuration on the Mali (Shoutout to Lodix for his findings).While the Exynos 5433 seems nothing more than a \"brain-transplant\" in terms of SoC design, the newer Exynos 7 chip is a genuinely new part. Over the last 3 weeks Samsung has been busysubmitting patchesto the Linux kernel mailing lists adding support for their new SoC lineup. The Exynos 7420 seems to be on track for Samsung's next flagship lineup, this time in full 64-bit AArch64 mode with Android L. The details of the chip are still sparse, but we'll be seeing the same A57/A53 CPU combination together with an Mali T760, powered by an LPDDR4-capable memory controller.The Exynos 5433 is definitely a surprise that many didn't expect. Qualcomm's Snapdragon 810 isn't officially due until Q1 2015, and we don't know yet when we'll be seeing it in consumer devices. Here Samsung has quite a lead as the Note 4 variants with the 5433 are shipping in the coming weeks. While I'm still a bit perplexed at Samsung's silence and lack of announcements, the fact that many regions are supplied a Snapdragon S805 in the Note 4 may have to do something with it, as they wouldn't want to cause buyer's remorse.Edit 22/10/2014:Mali MP6 configuration has been confirmed.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8537/samsungs-exynos-5433-is-an-a57a53-arm-soc\n",
      "Title: ARM's Cortex M: Even Smaller and Lower Power CPU Cores\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2014-08-19T01:14:00Z\n",
      "URL: https://www.anandtech.com/show/8400/arms-cortex-m-even-smaller-and-lower-power-cpu-cores\n",
      "Content: ARM (and its partners) were arguably one of the major causes of the present day smartphone revolution. While AMD and Intel focused on using Moore’s Law to drive higher and higher performing CPUs, ARM and its partners used the same physics to drive integration and lower power. The result was ultimately the ARM11 and Cortex A-series CPU cores that began the revolution and continue to power many smartphones today. With hopes of history repeating itself, ARM is just as focused on building an even smaller, even lower power family of CPU cores under the Cortex M brand.We’ve talked about ARM’s three major families of CPU cores before: Cortex A (applications processors), Cortex R (real-time processors) and Cortex M (embedded/microcontrollers). Although Cortex A is what we mostly talk about, Cortex M is becoming increasingly important as compute is added to more types of devices.Wearables are an obvious fit for Cortex M, yet the initial launch of Android Wear devices bucked the trend and implemented Cortex A based SoCs. A big part of that is likely due to the fact that the initial market for an Android Wear device is limited, and thus a custom designed SoC is tough to justify from a financial standpoint (not to mention the hardware requirements of running Android outpace what a Cortex M can offer). Looking a bit earlier in wearable history and you’ll find a good number of Cortex M based designs including the FitBit Force and the Pebble Steel. I figured it’s time to put the Cortex M’s architecture, performance and die area in perspective.We’re very much in the early days of the evolution of Cortex M. The family itself has five very small members: M0, M0+, M1, M3 and M4. For the purposes of this article we’ll be focusing on everything but Cortex M1. The M1 is quite similar to the M0 but focuses more on FPGA designs.Before we get too far down the architecture rabbit hole it’s important to provide some perspective. At a tech day earlier this year, ARM presented this data showing Cortex M die area:By comparison, a 40nm Cortex A9 core would be roughly around 2.5mm^2 range or a single core. ARM originally claimed the Cortex A7 would be around 1/3 - 1/2 of the area of a Cortex A8, and the Cortex A9 is roughly equivalent to the Cortex A8 in terms of die area, putting a Cortex A7 at 0.83mm^2 - 1.25mm^2. In any case, with Cortex M we’re talking about an order of magnitude smaller CPU core sizes.The Cortex M0 in particular is small enough that SoC designers may end up sprinkling in multiple M0 cores in case they need the functionality later on. With the Cortex M0+ we’re talking about less than a hundredth of a square millimeter in die area, even the tightest budgets can afford a few of these cores.In fact, entire SoCs based on Cortex M CPU cores can be the size of a single Cortex A core. ARM provided this shot of a Freescale Cortex M0+ design in the dimple of a golf ball:ARM wouldn’t provide me with comparative power metrics for Cortex M vs. Cortex A series parts, but we do have a general idea about performance:Estimated Core PerformanceARM Cortex M0/M0+ARM Cortex M3/M4ARM11ARM Cortex A7ARM Cortex A9Qualcomm Krait 200DMIPS/MHz0.84/0.941.251.251.92.53.3In terms of DMIPS/MHz, Cortex M parts can actually approach some pretty decent numbers. A Cortex M4 can offer similar DMIPS/MHz to an ARM11 (an admittedly poor indicator of overall performance). The real performance differences come into play when you look at shipping frequencies, as well as the type of memory interface built around the CPU. Cortex M designs tend to be largely SRAM and NAND based, with no actual DRAM. You'll note that the M3/M4 per clock performance is identical, that's because the bulk of what the M4 adds is in the form of other hardware instructions not measured by Dhrystone performance.Instruction set compatibility varies depending on the Cortex M model we’re talking about. The M0 and M0+ both implement ARM’s v6-M instruction profile, while the M3 and M4 support ARM’s v7-M. As you go up the family in terms of performance you get access to more instructions (M3 adds hardware divide, M4 adds DSP and FP instructions):Each Cortex M chip offers a superset of the previous model’s instructions. So a Cortex M3 should theoretically be able to execute code for a Cortex M0+ (but not necessarily vice versa).You also get support for more interrupts the higher up you go on the Cortex M ladder. The Cortex M0/M0+ designs support up to 32 interrupts, but if you move up to the M3/M4 you get up to 240.All Cortex M processors have 32-bit memory addressability and the exact same memory map across all designs. ARM’s goal with these chips is to make moving up between designs as painless as possible.While we’ve spent the past few years moving to out-of-order designs in smartphone CPUs, the entire Cortex M family is made up of very simple, in-order architectures. The pipelines themselves are similarly simplified:Cortex M0, M3 and M4 all feature 3-stage in-order pipelines, while the M0+ shaves off a stage of the design. In the 3-stage designs there’s an instruction fetch, instruction decode and a single instruction execute stage. In the event the decoder encounters a branch instruction, there’s a speculative instruction fetch that grabs the instruction at the branch target. This way regardless of whether or not the branch is taken, the next instruction is waiting with at most a 1 cycle delay.These aren’t superscalar designs, there’s only a 1-wide path for instruction flow down the pipeline and not many execution units to exploit. The Cortex M3 and M4 add some more sophisticated units (hardware integer divide in M3, MAC and limited SIMD in M4), but by and large these are simple cores for simple needs.The range of operating frequencies for these cores is relatively low. ARM typically expects to see Cortex M designs in the 20 - 150MHz range, but the cores are capable of scaling as high as 800MHz (or more) depending on process node. There’s a corresponding increase in power consumption as well, which is why we normally see lower clocked Cortex M designs.Similar to the Cortex A and R lines, the Cortex M family has a roadmap ahead of it. ARM recently announced a new CPU design center in Taiwan, where Cortex M based cores will be designed. I view the Cortex M line today quite similarly to the early days of the Cortex A family. There’s likely room for a higher performing option in between Cortex M4 and Cortex A7. If/when we get such a thing I feel like we may see the CPU building block necessary for higher performance wearable computing.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8400/arms-cortex-m-even-smaller-and-lower-power-cpu-cores\n",
      "Title: AMD’s Big Bet on ARM Powered Servers: Opteron A1100 Revealed\n",
      "Author: Stephen Barrett\n",
      "Date Published: 2014-08-11T16:00:00Z\n",
      "URL: https://www.anandtech.com/show/8362/amds-big-bet-on-arm-powered-servers-a1100-revealed\n",
      "Content: It has been a full seven months since AMD released detailed information aboutits Opteron A1100 server CPU, and twenty two months since announcement. Today, at the Hot Chips conference in Cupertino, CA, AMD revealed the final pieces about its ARM powered server strategy headlining the A1100.The Case for Low Power Server CPUsBefore we discuss the new Opteron A1100 details, let us review the background of why AMD designed an ARM powered CPU. It all comes down to the devices and services we now take for granted: cell phones, tablets, cloud storage, and cloud services. AMD presented a slide about a year ago that summed it up nicely.The amount of internet users is growing by 8 to 12% every year. Apple, Google, Microsoft, Facebook, you-name-it, all invest huge sums of money into server farms to provide the services we have come to rely on. This trend gains more and more momentum as software companies like Microsoft try to emulate the success of Apple and Google by selling hardware (Apple) and providing free services (Google) that are ad-supported.Building the infrastructure to support all these devices and users is a massive undertaking. Typically, companies buy traditional high powered servers (read: Intel Xeon) and partition their computing power up between many tasks as needed. However, this isn’t always the best strategy. For IO tasks, you are always bottlenecked by something other than the CPU, so there is not a reason to throw high cost high power CPUs at the problem. For webserver tasks, response time is paramount. However, with the huge number of users connecting, webservers have become an ‘embarrassingly parallel’ problem you can address with multi core CPUs - as long as there is enough muscle behind each CPU.The ‘enough muscle’ issue has hindered previouslow power high density webserverattempts. When we tested theCalxeda ARM compute cluster, there were only certain edge cases where it was more efficient than a dual core Xeon server running virtual machines. Calxeda themselves admitted that their processors, utilizing ARM Cortex A9s, were in the early adopter phase of ARM powered webservers. Calxeda stated it wouldn’t be until ARMv8 (where virtualization is supported) and Cortex A57 that ARM based servers would ‘cross the chasm’ and enter the mainstream.With the Opteron A1100, AMD skipped the early adopter phase and chose something with a higher chance of initial success.Meet the A1100: CPUs and IOThere are three types of ARM licenses: POP, processor, and architecture. A POP license stands for Processor Optimization Pack and provides the licensee with everything they need to send a chip to the fab. A processor license provides the details of an ARM core like Cortex A9 so you can implement it into your own SoC, but you are not allowed to customize it. Finally, there is the ultimate license, an architecture license. An architecture license provides all details of ARM instruction set (ISA) and CPU implementation so a licensee can implement their own custom CPU core using the ARM ISA however they see fit. AMD is a processor and architecture licensee. If AMD decides it can be competitive by shipping an SoC with an ARM designed CPU (processor license), they can do so without the effort designing their own ARM ISA CPU. If AMD wants to differentiate itself with a custom designed CPU using the ARM ISA, AMD can use its architecture license to do that, similar to Qualcomm’s Krait CPU cores. AMD has decided to do both. Today we discuss its processor license.AMD’s first SoC containing an ARM CPU is code named Seattle, the Opteron A1100. Seattle features no less than eight 64-bit ARMv8 ISA, Cortex A57 cores. Depending on availability, this could be the first Cortex A57 CPU to hit any market, not just the server market. AMD will follow up in 2015 with a lower power version that is pin compatible with another x86 CPU, both of which are part ofProject Skybridge. In 2016 AMD will leverage its architecture license and ship K12, a fully custom CPU design using the ARMv8 ISA.Gallery:Opteron A1100 CPUEach pair of Cortex A57s in the A1100 shares a 1MB L2 cache (totaling to 4MB of L2), and they all roll up to a shared 8MB L3 cache. To address the server market, all caches are ECC protected except for the L1 instruction cache, which is parity protected instead. Instruction cache protection is not quite as important (invalid instruction just means a pipeline stall). AMD utilizes ARM bus interfaces and debugging support throughout the design. The Cortex A57 also implements cryptography extensions that are quoted by ARM to accelerate things like https by 3-10x over previous ARM designs.The SoC has a dual channel (2x64-bit) DDR3/4 interface to up to 128GB of 1866MHz memory. Just like the caches, the memory path also supports ECC of the single-bit error correct / double-bit error detect variety. Registered (RDIMM), unregistered (UDIMM), and small-outline (SODIMM) memory modules are support by the A1100 SoC, but actual motherboards will likely support only one type of memory. The same goes for DDR3 vs. DDR4.As the A1100 is a SoC, it integrates IO directly into the single chip instead of relying on an off-chip IO hub. Integrated components include 8 SATA 3 (6Gb/s) ports, two 10 Gbit Ethernet (10GBASE-KR) ports, one 10/100/1000 Ethernet port, 8 lanes of Gen3 PCI-Express (supporting 8x, 4x/4x, and 4x/2x/2x), I2C, SPI, and UART. The inclusion of this breadth of storage IO (8 SATA3 ports) along with the 2x10 Gbit Ethernet is particularly interesting as it gives us hints of how AMD will position the Opteron A1100 on the market. More on this later.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article already exists: https://www.anandtech.com/show/8362/amds-big-bet-on-arm-powered-servers-a1100-revealed\n",
      "Title: Watch our Hangout with ARM GPU Fellow Jem Davies\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2014-07-09T15:57:00Z\n",
      "URL: https://www.anandtech.com/show/8243/watch-our-hangout-with-arm-gpu-fellow-jem-davies\n",
      "Content: For those of you who weren't able to catch the live stream be sure to watch our interview with ARM Fellow and all around GPU expert Jem Davies. In our Hangout Jem talked about how he got involved with GPUs at ARM (spoiler alert: his boss told him to go out and buy a GPU company). He also shared his thoughts on Mali's unique architecture and how it may evolve over time. We discussed the future of mobile gaming, solving memory bandwidth challenges, and the potential for ARM scaling its GPU architectures from wearables all the way up to supercomputers.I reference it a few times, but you should also check outRyan Smith's deep dive into ARM's Midgard GPU Architecture. ARM is the second mobile-focused GPU vendor to allow us unfettered access to the details of their GPUs and we're very thankful for it.I'd like to thank Jem once again for taking the time to chat with me, and if you haven't already seen them I'd suggest watching some of our other Hangouts with ARM:\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8243/watch-our-hangout-with-arm-gpu-fellow-jem-davies\n",
      "Title: Join us for a Live Hangout with ARM Fellow Jem Davies and Chat about GPUs at 12PM ET Today\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2014-07-07T15:54:00Z\n",
      "URL: https://www.anandtech.com/show/8240/join-us-for-a-live-hangout-with-arm-fellow-jem-davies-and-chat-about-gpus-at-12pm-et-today\n",
      "Content: Afteranswering your questions in our Ask the Expertspost, ARM Fellow and GPU expert Jem Davies agreed to join a live video discussion with me. At 12:00PM ET today (50 minutes from now) we will be hosting a live Google Hangout with Jem Davies. I'll update this post with the stream just before we begin.We will be accepting questions live if you are able to join the Hangout. As usual, we'll be posting the feed directly to our YouTube Channel once it's over.Update- Stream below:\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8240/join-us-for-a-live-hangout-with-arm-fellow-jem-davies-and-chat-about-gpus-at-12pm-et-today\n",
      "Title: Watch our Hangout with ARM's CTO Mike Muller\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2014-07-04T16:22:00Z\n",
      "URL: https://www.anandtech.com/show/8235/watch-our-hangout-with-arms-cto-mike-muller\n",
      "Content: Yesterday I spent an hour talking to ARM's CTO Mike Muller about everything from the Internet of Things, ARM in servers, wearables, a roadmap to ARM on the desktop and the history of the company. Mike is one of the original ARM founders and has a great perspective on ARM and the industry. If you missed the live hangout you can check out the video below:On Monday at noon ET I'll be doing the same with ARM Fellow Jem Davies where we'll be talking about mobile GPUs.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8235/watch-our-hangout-with-arms-cto-mike-muller\n",
      "Title: ARM’s Mali Midgard Architecture Explored\n",
      "Author: Ryan Smith\n",
      "Date Published: 2014-07-03T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/8234/arms-mali-midgard-architecture-explored\n",
      "Content: So far this is shaping up to be a banner year for SoCs. From a market perspective the mobile hardware space is still in a period of significant growth, but more importantly from a hardware point of view these products and especially the GPUs in these products have made significant strides in performance and in features. SoC GPUs will approach feature parity with desktop GPUs this year, and from a performance perspective they’re nearing the performance of the last-generation game consoles, a long-aspired goal given the “good enough” status attached to those devices.Meanwhile at the same time that these products are maturing at a technical level, we’ve seen the various SoC firms mature at a professional level. The “wild west” days of SoCs have given way to mature markets of longer product cycles, longer product lives, and a more stable market overall. This both good and bad news for the various players in the SoC market as firms get squeezed out – SoC integrators such as TI and STMicroelectronics have been the first of such victims – but it also means that as companies become better established and more deeply entrenched, they can be more open about their projects and their products, and discuss them in greater detail than before without needing to be concerned about getting scooped by a competitor.Here at AnandTech we’re particularly fond of doing architectural deep dives; our chance to talk to the people behind various processors and learn from them about how their products work and how they came together. Thanks to the maturation of the SoC market we’re finally getting a level of access in the SoC market that we haven’t had before, and in turn for the first time we get to tell the stories of the people behind these mind-bogglingly complex devices while better learning how they operate and as such how they compare. It’s admittedly a level of access we take for granted in the PC space, where companies such as Intel, AMD, and NVIDIA are regularly open, but it’s hard to contain our excitement about gaining this kind of access to the myriad of players in the SoC space.This year then has been especially productive in that regard, and as of today it’s going to get even better. After we took a look atImagination’s PowerVR Rogue architectureearlier this year, ARM contacted us and indicated that they would like to do the same; that they would like to take a seat at the “open architecture” table. To give us the access we need to discover how their GPUs work, and in turn tell you what we’ve learned.To that end we’ve gladly let ARM pull up a seat, and today we’ll be taking our first in-depth look at ARM’s newest Mali SoC GPU architecture: Midgard. Now as with Imagination what we’re seeing today is most, but not all of the picture, as ARM has their secrets and they wish to keep some of them. But today we get to learn all about Midgard’s shader cores while also learning a thing or two about its pixel rendering pipeline, power optimizations, and other aspects of what makes Midgard tick. In other words, more than enough to keep us busy for one day.But before we dive in we’d also like to quickly call attention to anAsk The Experts session we held with ARM’s Jem Davies, an ARM Fellow and VP of Technology in the Media Processing Division. While our deep dive is focusing on Midgard’s architecture, Jem has been answering all sorts of additional Mali-related questions, including business strategy and ARM’s views on GPU computing.Finally, as this is the second article in our continuing series on SoC GPUs, we will be picking up from where we left off after ourlast article. While all of our articles are meant to be accessible to some degree, if you haven’t caught any of our previous articles I’d highly recommendour primer on how GPUs workfor a quick overview of the technology before we dive into the nuts and bolts of ARM’s Midgard architecture.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8234/arms-mali-midgard-architecture-explored\n",
      "Title: Join Us for a Live Google Hangout with ARM's CTO - Happening Now!\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2014-07-02T16:35:00Z\n",
      "URL: https://www.anandtech.com/show/8183/join-us-for-a-live-google-hangout-with-arms-cto-and-cmo-tomorrow-at-1pm-et\n",
      "Content: As I hinted at duringour interview withKrisztián Flautner, ARM was quite pleased with how things went withour Peter Greenhalgh ATEthat it's going to be giving us access to more key folks over the coming weeks/months. I want to thank all of you for your questions forKrisztián in our last Ask the Experts post, and I want to thankKrisztián for taking the time to respond to you all directly. If you haven't read through the Q&A I'd recommend doing so.Today I'll be doing a live Google Hangout with Mike Muller, ARM's Chief Technology Officer. Mike Muller was one of the original founders of ARM. We originally scheduled this hangout for late June but had some technical issues with the stream and had to reschedule.The Hangout will happen today, Wednesday, July 2, 2014 at 1PM ET. I'll update this post with an embedded feed when we get started. If you've got any questions you want to ask ARM's CTO, give them some thought - we may be able to get them answered live tomorrow.Update: Here's the feed, we will be starting in 15 minutes:\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8183/join-us-for-a-live-google-hangout-with-arms-cto-and-cmo-tomorrow-at-1pm-et\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Ask the Experts - ARM Fellow Jem Davies Answers Your GPU Questions\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2014-06-30T15:52:00Z\n",
      "URL: https://www.anandtech.com/show/8226/ask-the-experts-arm-fellow-jem-davies-answers-your-gpu-questions\n",
      "Content: When we ran ourAsk the Experts with ARM CPU guru Peter Greenhalghsome of you had GPU questions that went unanswered. A few weeks ago we set out to address the issue and ARM came back with Jem Davies to help. Jem is an ARM Fellow and VP of Technology in the Media Processing Division, and he's responsible for setting the GPU and video technology roadmaps for the company. Jem is also responsible for advanced product development as well as technical investigations of potential ARM acquisitions. Mr. Davies holds three patents in the fields of CPU and GPU design and got his bachelor's from the University of Cambridge.If you've got any questions about ARM's Mali GPUs (anything on the roadmap at this point), the evolution of OpenGL, GPU compute applications, video/display processors, GPU power/performance, heterogeneous compute or more feel free to ask away in the comments. Jem himself will be answering in the comments section once we get a critical mass of questions.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8226/ask-the-experts-arm-fellow-jem-davies-answers-your-gpu-questions\n",
      "Title: Ask the Experts: Krisztián Flautner VP of R&D, ARM\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2014-06-03T16:00:00Z\n",
      "URL: https://www.anandtech.com/show/8115/ask-the-experts-krisztin-flautner-vp-of-rd-arm\n",
      "Content: Late last year we did an installment ofAsk the Experts with ARM's Peter Greenhalgh, lead architect for the Cortex A53. The whole thing went so well, in no small part to your awesome questions, that ARM is giving us direct access to a few more key folks over the coming months.Krisztián Flautner is Vice President of Research and Development at ARM, and as you can guess - he's focused on not the near term, but what's coming down the road for ARM. ARM recently celebrated its 50 billionth CPU shipment via its partners, well Krisztián is more focused on the technologies that will drive the next 100 billion shipments.Krisztián holds PhD, MSE and BSE degrees in computer science and engineering from the University of Michigan. He leads a global team that researches everything from circuits to processor/system architectures and even devices. And he's here to answer your questions.If there's anything you want to ask the VP of R&D at ARM, this is your chance. Leave a comment with your question andKrisztián will go through and answer any he's able to answer. If you've got questions about process tech, Moore's Law, ARM's technology roadmap planning or pretty much anything about where ARM is going, ask away!\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8115/ask-the-experts-krisztin-flautner-vp-of-rd-arm\n",
      "Title: ARM Announces CPU Design Center in Taiwan\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2014-06-02T04:55:00Z\n",
      "URL: https://www.anandtech.com/show/8088/arm-announces-cpu-design-center-in-taiwan\n",
      "Content: As part of Computex 2014, ARM has announced their first CPU Design Center in Taiwan, focusing on the next generation of Cortex-M class cores for IoT and wearables. Initially this will mean 40-50 individuals within the talent pool in 2014 before scaling based on demand. A large part of the press event has been geared towards demand for ARM processors and the segmentation therein, focusing on the uses of Cortex-M within an ecosystem that counts sixteen billion processors shipped into embedded to date.ARM will be working with research institutes to grow and scale, with the aim to produce important engineers with experience in the field. ARM sees Taiwan as a focal point of the industry with respect to the large number of ARM’s semiconductor partners in the region, hence the choice of the region for their fourth CPU design center. One of the outputs of the new design center, due to the increase of interest in wearables, ARM sees the segment as two main parts – the low end sensor based wearables, and the high power models requiring sensor calculation relating to features such as HUDs, particularly in the professional and medical markets.ARM also mentioned it is developing MBED, a platform for embedded developers incorporating an OS and a system of tools to help bring ideas to fruition. THe platform will be open source, with libraries and web based tools allowing developers to mix and match microcontrollers, radios, sensors and software stacks.Gallery:ARM Announces CPU Design Center in Taiwan\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/8088/arm-announces-cpu-design-center-in-taiwan\n",
      "Title: ARM Expects ~1B Entry Level Smartphones in 2018, $20 Smartphones Coming This Year\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2014-05-06T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/7993/arm-expects-1b-entry-level-smartphones-in-2018-20-smartphones-coming-this-year\n",
      "Content: When we first started covering mobile, nearly every silicon vendor I spoke with issued the same statement: eventually, all phones are smartphones. Continued scaling on process technology, combined continued development of small/power efficient CPU cores, will ensure that even the lowest cost mobile devices will be smartphones.At its second ever Tech Day, ARM shared some data about how the smartphone market is evolving. We often mention that the growth in the smartphone industry will shift from high-end devices to mid-range and entry level devices. The graph above shows just that. By 2018 ARM expects over a billion entry level (< $150) smartphone shipments per year, around 2x what it is today.Two factors will drive entry level shipments: the performance of entry-level devices, and their overall cost. ARM believes the floor for an entry level smartphone running Android (today with a single-core Cortex A5 based SoC) is $20, and that we'll see the first devices on sale at that price point in the next few months. Manufacturing limits will likely prevent cost scaling below $20. Keep in mind that a single Cortex A5 is faster than the ARM11 in the original iPhone, which retailed for $599 in 2007.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7993/arm-expects-1b-entry-level-smartphones-in-2018-20-smartphones-coming-this-year\n",
      "Title: ARM's Impact on the Chip Market: 100M China-Designed SoCs Shipped in 2013\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2014-05-06T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/7994/arms-impact-on-the-chip-market-100m-chinadesigned-socs-shipped-in-2013\n",
      "Content: At its second ever Tech Day, ARM shared a pretty interesting slide about its impact on the mobile SoC market.ARM's business modelallows for pretty much anyone to be a player in the SoC space. This is in stark contrast to the PC business that's dominated by a single silicon player, with perhaps one lower volume second source. ARM's IP licensing business has paved the way for a number of low cost SoC vendors, particularly those based in China, to enjoy substantial marketshare. While we've only revieweda single MediaTek based device on AnandTech, the numbers out there are increasing tremendously.Tablets in particular are the perfect target for low cost SoCs given that you can successfully ship a WiFi-only design.ARM's chart above shows just how successful its China-based SoC vendors have been in the tablet space, shipping over 100M SoCs in 2013 (~40% of ARM's tablet business).\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7994/arms-impact-on-the-chip-market-100m-chinadesigned-socs-shipped-in-2013\n",
      "Title: ARM Shares Updated Cortex A53/A57 Performance Expectations\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2014-05-06T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/7995/arm-shares-updated-cortex-a53a57-performance-expectations\n",
      "Content: With thefirst Cortex A53 based SoCsdue to ship in the coming months, and Cortex A57 based designs to follow early next year, ARM gave us a quick update on performance expectations for both cores. Given the timing of both designs we'll see a combination of solutions built on presently available manufacturing processes (e.g. 28nm) as well as next gen offerings (20/16FF). The graph above gives us an updated look at performance expectations (in web browsing workloads) for both ARM 64-bit cores.If we compare across the same process nodes (28nm in both cases), the Cortex A53 should give us nearly a 50% increase in performance compared to ARM's Cortex A7. The Cortex A57 should offer roughly the same increase in performance compared to Cortex A15 as well. Although the A57 will do so at higher power, power efficiency may be better depending on the workload thanks to the added performance. Thankfully we won't see many A57 designs built on 28nm in mobile (AMD's first Cortex A57 design will be aimed at servers and is built on a 28nm process).If you combine architectural improvements with a new process node, the gains are substantial. Move to 20nm or 16FF for these designs and the improvement over their 32-bit predecessors easily exceeds 50%.ARM also provided some Geekbench 3 performance data comparing the Cortex A57 to A15, both in 32-bit and 64-bit mode. We already know Geekbench 3 is particularly sensitive to the new instructions that come along with AArch64, but even in 32-bit mode there's still a 15 - 30% increase in performance over the Cortex A15 at the same clocks.Qualcomm has already announced its Snapdragon410,610 and 615will use ARM's Cortex A53, while its808 and 810will use a combination of Cortex A53s and A57s.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7995/arm-shares-updated-cortex-a53a57-performance-expectations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD Announces K12 Core: Custom 64-bit ARM Design in 2016\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2014-05-05T17:01:00Z\n",
      "URL: https://www.anandtech.com/show/7990/amd-announces-k12-core-custom-64bit-arm-design-in-2016\n",
      "Content: In 2015AMD will launch project SkyBridge, a pair of pin-compatible ARM and x86 based SoCs. Leveraging next generation Puma+ x86 cores or ARM's Cortex A57 cores, these SoCs form the foundation of the next phase in AMD's evolution where ARM and x86 are treated like equal class citizens. As I mentioned in today's post however, both of these designs really aim at the lower end of the performance segment. To address a higher performance market, AMD is doing what many ARM partners have done and is leveraging an ARM architecture license to design their own microarchitecture.In 2016 AMD will release its first custom 64-bit ARMv8 CPU core, codenamed K12.Jim Kelleris leading the team that is designing the K12, as well as a corresponding new 64-bit x86 design.AMD is pretty quiet about K12 details at this point given how far away it is. Given the timing I'm assuming we're talking about a 14/16nm FinFET SoC. On the slide above we see that AMD is not only targeting servers and embedded markets, but also ultra low power client devices for its 64-bit ARM designs (presumably notebooks, chromebooks, tablets). AMD has shied away from playing in the phone market directly, but it could conceivably play in that space with its semi-custom business (offering just a CPU/GPU core with other IP).Update:AMD added that server, embedded and semi-custom markets are obvious targets for K12.There's also this discussion of modularity, treating both ARM and x86 cores as IP modules rather than discrete designs. AMD continues to have a lot of expertise in SoC design, all it really needs is a focus on improving single threaded performance. I can only hope (assume?) that K12 won't be Bulldozer-like and will hopefully prioritize single threaded performance. It's important to point out that there hasn't been a single reference to the Bulldozer family of CPU cores in any of these announcements either...Update:Jim Keller added some details on K12. He referenced AMD's knowledge of doing high frequency designs as well as \"extending the range\" that ARM is in. Keller also mentioned he told his team to take the best of the big and little cores that AMD presently makes in putting together this design.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7990/amd-announces-k12-core-custom-64bit-arm-design-in-2016\n",
      "Title: AMD Announces Project SkyBridge: Pin-Compatible ARM and x86 SoCs in 2015, Android Support\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2014-05-05T16:45:00Z\n",
      "URL: https://www.anandtech.com/show/7989/amd-announces-project-skybridge-pincompatible-arm-and-x86-socs-in-2015\n",
      "Content: This morning AMD decided to provide an update on its CPU core/SoC roadmap, particularly as it pertains to the ARM side of the business. AMD already committed to releasing a28nm 8-core Cortex A57 based Opteron SoC this year. That particular SoC is aimed at the enterprise exclusively and doesn't ship with an on-die GPU.Next year, AMD will release a low-power 20nm Cortex A57 based SoC with integrated Graphics Core Next GPU. The big news? The 20nm ARM based SoC will be pin compatible with AMD's next-generation low power x86 SoC (using Puma+ cores). The ARM SoC will also be AMD's first official Android platform.I don't expect we'll see standard socketed desktop boards that are compatible with both ARM and x86 SoCs, but a pin compatible design will have some benefits for embedded, BGA solutions. AMD expects to target embedded and client markets with these designs, not servers.AMD's motivation behind offering both ARM and x86 designs is pretty simple. The TAM (Total Addressable Market) for x86 is decreasing, while it's increasing for ARM. AMD is no longer married to x86 exclusively and by offering OEMs pin compatible x86/ARM solutions it gets to play in both markets, as well as benefit if one increases at the expense of the other.Note that we're still talking about mobile phone/tablet class CPU cores here (Cortex A57/Puma+). AMD has yet to talk about what it wants to do at the high end, but I suspect there's a strategy there as well.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7989/amd-announces-project-skybridge-pincompatible-arm-and-x86-socs-in-2015\n",
      "Title: Scaling Windows - The DPI Arms Race\n",
      "Author: Brett Howse\n",
      "Date Published: 2014-04-15T18:00:00Z\n",
      "URL: https://www.anandtech.com/show/7939/scaling-windows-the-dpi-arms-race\n",
      "Content: For several years now, mobile device manufacturers have been in a race to push the pixel density of mobile devices higher and higher. The race began with the iPhone 4 “Retina” display – an at the time impressive 330 pixels per inch (PPI) 960x640 3.5” display. Keen to trump the Retina moniker, makers of Android devices soon churned out devices with displays with PPIs of 440 and higher, with the current push to 2560x1440 displays in 5.5” or smaller sizes which yield an amazing 500+ PPI. Next up was a similar race in the tablet space, with 1280x800 soon giving way to 2560x1600 displays, but this time in a 7” to 10” form factor.All the while, the lowly PC and Mac chugged along with displays that could hardly be called impressive. The standard LCD display of just a few years ago would hover somewhere around 96 PPI, and it was often lower. A 17” LCD with a resolution of 1280x1024 wasn’t an accident – it was exactly 96 PPI, which is what the PC and Mac would render at by default. High resolution laptops would barely squeak past the 120 PPI range. These lower densities – thoughdecent for the longer view distances of desktop monitors– have until recently not been improved on, highlighting the gap in progress between the two devices categories.Further complicating matters, desktops and mobile devices have always differed in how they use resolution when it is increased. On a mobile device, higher resolution has been used to increase image quality, while higher resolution displays on a desktop were released as part of physically larger displays and used to increase the amount of work you can do. Mobile devices have had one big advantage: they are backed by new operating systems that are built for higher resolution out of the box, and there is no back catalog of legacy applications to deal with. Phones and tablets can easily deal with high resolution displays, but for the PC and Mac, things are not so simple.In 2012, Apple launched the 15.4” Retina MacBook Pro. At the time it was far and away the highest PPI laptop available. It took a lot of work for Apple to ensure a high resolution display was usable because for really the first time, increased resolution on a computer was used to improve image quality rather than simply to increase screen real estate. How they achieved this was nicely explained byAnand back in 2012. However, OS X wasn’t perfect; certain applications didn’t behave as well as they should have, which resulted in some applications having blurry text or other UI issues. Still, Apple was able to make the Retina display work, and for the applications that were Retina aware, the result was a fantastic experience. If developers updated their applications, their clients could enjoy the high resolution clarity that had already taken over the mobile space.But what about Windows? Windows Vista, and then Windows 7, both had support for higher DPI (Dots Per Inch) settings; even lowly Windows XP had some support for DPI scaling. The main issue was that there was no market force pushing for High DPI (in the operating system and APIs, it’s referenced as DPI as opposed to the PPI of a display) like there was with the Retina MacBook Pro. OEMs were happy to sell consumers low cost, low resolution 1366x768 TN panels for years. If people don’t demand better, most OEMs are unlikely to provide them better than the basics in such a low margin industry.High Resolution LaptopsBrandModelScreen SizeScreen ResolutionPixels per inchAcerAspire S713.3\"2560x1440221ASUSZenbook UX301LA13.3\"2560x1440221DellXPS 1111.6\"2560x1440253DellXPS 1515.6\"3200x1800235HPSpectre 13t-300013.3\"2560x1440221LenovoYoga 2 Pro13.3\"3200x1800276LenovoX1 Carbon14\"2560x1440210PanasonicToughpad 4k20\"3840x2560231RazerBlade14\"3200x1800262SamsungATIV Book 913.3\"3200x1800276ToshibaKIRAbook13.3\"2560x1440221What changed was a combination of High DPI tablets and the Retina MacBook Pro putting pressure on the PC industry to offer something better. It has taken a long time, but finally quality displays are something that are important enough to consumers for every single major OEM to now offer at least one, if not multiple, devices with High DPI.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7939/scaling-windows-the-dpi-arms-race\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Partners Ship 50 Billion Chips Since 1991 - Where Did They Go?\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2014-03-31T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/7909/arm-partners-ship-50-billion-chips-since-1991-where-did-they-go\n",
      "Content: A few weeks ago ARM celebrated its partners shipping over 10 billion ARM based chips in 2013. AsARM makes a royalty on every IP license shipped, it was a banner year for the company. The bigger story was that the 10 billion in 2013 brought the cumulative total for ARM based processors to over 50 billion (note that these are discrete ICs, multiple cores within a single design are not counted multiple times). ARM's press activities were limited to talking about the big final number, but ARM has a pretty broad IP portfolio. What I wanted was a breakdown of where the 50 billion went, so I asked.What I got in response were tables of data. I was asked not to share specific numbers, but using the data in graphs was ok - and that's all I wanted to do. We'll start with where the 50 billion went in terms of markets (pictured above). Mobile obviously took the majority of shipments, followed by embedded markets. Remember that ARM cores are used all over the place, including in things like HDD and SSD controllers. The modems that work alongside the main apps processors in mobile devices are also frequently home to ARM processor IP. Intel's Quark project actually came about because Intel needed a low power/low cost core to use internally for various projects and eventually decided to offer it to anyone who wanted it. For those companies that don't have the desire/ability to build and validate their own low power CPU core, they often turn to ARM.The enterprise slice may be a bit misleading depending on what you define as enterprise. We often refer to enterprise in terms of primary CPU shipments into servers. In this case we're talking about chips that go into things like routers and wireless access points. ARM obviously hopes to take a big portion of the high dollar enterprise CPU market with its ARMv8 based IP in the coming years, but it's not there yet.The smallest slice, labeled home, is still nearly 3 billion shipments. Here we're talking about things like consumer set top boxes as well as wearables.Note that 37.5 of the 50 billion chips shipped in the past five years (2009 - 2013). That shouldn't come as a surprise given the overlap between that time period and the rise of modern smartphones and tablets.While ARM wasn't willing to give me shipments by specific core, it was willing to give me family data:Two thirds of all ARM mobile shipments are really old ARM7 and ARM9 based designs (remember my point about modems above). Here we get the first hint that the reign of the ARM11 designs (the foundation of the original iPhone) was a small blip in the grand scheme of things - the Cortex A family is really what allowed mobile to grow.The embedded market is dominated by these lower power cores, although the newer Cortex M designs have made a huge dent. The same is true for the enterprise market, which is indicative of what I said earlier about ARM's enterprise market not yet including primary CPU sockets.The trends are extremely telling. ARM7 (and ARM9) shipments peaked back in 2011 and have been in a slow decline ever since. Cortex M based designs have been skyrocketing since their introduction and show the most aggressive growth of any ARM line. The Cortex A line shows a similar slope over the past two years, with the ARM11 shipments crossing over in 2012.The next two charts show the same data but focusing on the past 7 years and past 4 years, respectively:You can easily correlate the rise in ARM's shipments with the explosion in mobile. It's also interesting to point out that, for the most part, shipments are growing with higher performing product families. A smart man once told me that no one wins by betting against performance. Although ARM definitely has its fair share of area and power optimized designs, ultimately it's the serious focus on performance that's been responsible for the surge over the past few years.It's worth pointing out that although the shipment numbers we're talking about here are in the billions, there's a point to be made about margin. ARM pointed out that Cortex-A shipments overtook x86 in 2012, but with most Cortex-A based designs shipping at well below $30 it's important to put volume in context.There's a real opportunity for ARM and its partners to start pushing for even higher end designs in my opinion. Thus far all of the talk about ARM enterprise CPUs has been focused on effectively repurposing smartphone designs for the datacenter. You could argue that the Cortex A57 is more enterprise focused than mobile focused, but the fact remains that it's still small/low power enough to get into a phone. I believe one of the next opportunities for disruption will be if ARM (and/or its partners) build a truly big core, something aimed exclusively at the enterprise (and could be repurposed for notebook/desktop use). I've got to believe that all the big players in the ARM space are working on such a thing. And the implications of even moderate success of such a thing are pretty big (particularly if you look at the impact to server CPU ASPs).\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7909/arm-partners-ship-50-billion-chips-since-1991-where-did-they-go\n",
      "Title: ARM Cortex A17: An Evolved Cortex A12 for the Mainstream in 2015\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2014-02-11T06:00:00Z\n",
      "URL: https://www.anandtech.com/show/7739/arm-cortex-a17\n",
      "Content: ARM has been doing a good job figuring out its PR strategy as of late. In the span of a couple of years we went from very little outward communication to semi-deep-dives on architecture and a regular cadence of IP disclosures. ARM continues its new trend today with the announcement of its 2015 mid-range CPU IP: the Cortex A17.As its name implies, the Cortex A17 is a 32-bit ARMv7-A CPU design (64-bit ARMv8 cores belong to the Cortex A50 series - e.g. A53/A57). The best way to think about Cortex A17 is as an evolution of the recently announced Cortex A12, rather than anything to do with the Cortex A15. ARM's Cortex A17 takes the basic 2-wide out-of-order architecture of the Cortex A12 and improves it. Specific details are still light at this point, but I'm told that the front end and execution engine are similar to Cortex A12, with most of the performance/efficiency gains coming from improvements to the memory subsystem.The result is a design that is roughly 60% faster than a Cortex A9r4 at a given frequency/process/memory interface (Cortex A12 is 40% faster than A9r4 under the same conditions). Using ARM's own DMIPS/MHz ratings I threw together a little table of relative/estimated performance ratings to help put all of this in perspective:ARM 2014/2015 CPU IP lineupCPU IPTargetEstimated DMIPS/MHzbig.LITTLEShipping in Devices/SystemsCortex A57High-end mobile/servers5*Yes (w/ A53)2015Cortex A53Low-end mobile2.3Yes, LITTLE, w/ A572H 2014Cortex A17Mid-range mobile4.0*Yes, big, w/ A7Early 2015Cortex A15High-end mobile4.0*Yes, big, w/ A7NowCortex A12Mid-range mobile3.5No2H 2014Cortex A9High-end mobile2.5NoNowCortex A7Low-end mobile1.9Yes, LITTLE, w/ A15/A17Now*Estimate based on ARM's claimsOn a given process node, the Cortex A17 can occupy around 20% more area than a Cortex A9 or a marginal increase over a Cortex A12 design. Running the same workload, ARM expects the Cortex A17 to be 20% more energy efficient than the Cortex A9 (race to sleep), but I'd expect higher peak power consumption from the A17. The Cortex A17 name was deliberately chosen as ARM expects to be able to deliver similar performance to the Cortex A15(in mobile apps/benchmarks, likely not in absolute performance), but in a much smaller area and at a lower power. I can't help but wonder if this is what the Cortex A15 should have been from the very beginning, at least for mobile applications.ARM expects many early Cortex A17 designs to be built on a 28nm process, with an eventual shift over to 20nm once the cost of that process drops. ARM supplied an interesting slide showcasing the number of transistors $1 will buy you as a function of process node:If you're a fabless semiconductor, it looks like 28nm will be the sweet spot for manufacturing for a little while.Keep in mind that the target market for the Cortex A17, like the Cortex A12, is somewhere in between a device like the Moto G and the latest flagship Galaxy S device from Samsung.big.LITTLE SupportIf you remember back to our analysis of the Cortex A12, the first version of the core didn't support ARM's big.LITTLE (lacking the requisite coherent interface) but a future version was promised with big.LITTLE support. The Cortex A17 is that future version.In a big.LITTLE configuration, the Cortex A17 will function as the \"big\" core(s) while the Cortex A7 will serve as the \"LITTLE\" core(s).Rather than giving the Cortex A12 a new major revision number, ARM improved the design, added big.LITTLE support and called the finished product the Cortex A17. It's an interesting approach to dealing with the fact that ARM can rev/improve a single IP offering many times over the course of its life. In case it isn't already obvious, there won't be a big.LITTLE version of the Cortex A12.ARM expects some overlap between Cortex A17 and Cortex A12. If a customer is looking to ship in 2014, Cortex A12 will be the only option for them in the mid-range from ARM. If a customer wants big.LITTLE or they are starting a design now, Cortex A17 is the obvious fit. I expect Cortex A17 will contribute to a relatively short lifespan for Cortex A12 in the grand scheme of things.ARM sees some of the biggest opportunities in addressing the entry level and performance mainstream smartphone markets going forward. With the Cortex A17 aiming at the latter, ARM sees a potential market of around 450 million devices in 2015. The lack of 64-bit support makes ARM's mid-range lineup a little odd, especially considering the Cortex A53 and Cortex A57 will ensure both entry level and high-end smartphones will be 64-bit enabled. While I don't have an issue with a good mid-range device shipping without 64-bit support, I'm not sure how handset and tablet OEMs will feel. With Apple, Intel (and likely Qualcomm), embracing 64-bit-only strategies in mobile, I do wonder just how much success these A12/A17 architectures will have over the long run.ARM tells me we should see the first devices using Cortex A17 CPU cores shipping in early 2015. Cortex A17 IP will be available to ARM customers for implementation by the end of this quarter.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7739/arm-cortex-a17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM and Partners Deliver First ARM Server Platform Standard\n",
      "Author: Johan De Gelas\n",
      "Date Published: 2014-01-29T18:05:00Z\n",
      "URL: https://www.anandtech.com/show/7721/arm-and-partners-deliver-first-arm-server-platform-standard\n",
      "Content: The demise of innovator Calxeda and the excellent performance per watt of the new Intel Avoton server were certainly not good omens for the ARM server market. However, there are still quite a few vendors that are aspiring to break into the micro server market.AMD seems to have the best position with by far the most building blocks and experience in the server world. The64 bit 8-core ARMv8 based Opteron A1100should see the light in the second half of 2014. Broadcom is also well placed andhas announcedthat it will produce a 3 GHz 16 nm quadcore server ARMv8 server CPU. ARM SoC marketleaderQualcommhas shown some interest too, but without any real product yet. Capable outsiders areCavium with \"Project Thunder\"and AppliedMicro with thex-gene family.But unless any of the players mentioned above can grab Intel-like marketshare of the micro server market, the fact remains that supporting all ARM servers is currently a very time consuming undertaking. Different interrupt controllers, different implementation of FP units... at this point in time, the ARM server platform simply does not exist. It is a mix of very different hardware running their own very customized OS kernels.So the first hurdle to take is to develop a platform standard. And that is exactly what ARM is announcing today: theplatform standard for ARMv8-A based (64-bit) servers, known as theARM ‘Server Base System Architecture’ (SBSA) specification.The new specification is supported by a very broad range of companies ranging fromsoftware companies such as Canonical, Citrix, Linaro, Microsoft, Red Hat and SUSE,OEMs (Dell and HP) and the most important component vendors active in this field such as AMD, Cavium, Applied Micro and Texas Instruments. In fact, the Opteron A1100 that was just announced adheres to this new spec.All those partners of course formulated comments, but the best comment came fromFrank Frankovsky, president and chairmanof the Open Compute Project Foundation.\"These standardization efforts will help speed adoptionof ARM in the datacenter by providing consumers and software developers with the consistencyand predictability they require, and by helping increase the pace of innovation in ARMtechnologiesby eliminating gratuitous differentiation in areas like device enumeration and bootprocess.\"The primary goal is to ensure enough standard system architecture to enable asingle OS imageto run on all hardware compliant to this specification. That may sound like a fairly simple thing, but in reality it's extremely important to solidifying the ARM ecosystem and making it a viable alternative in the server space.A few examples of the standard:The base server system shall implement a GICv2 interrupt controllerAs a result, the maximum number of CPUs in the system is 8all CPUs must have Advanced SIMD extensions andcryptography extensions.the system uses generic timers (Specified by ARM)CPUs must implement the described power state semanticsUSB 2.0 controllers must conform to EHCI 1.1, 3.0 to XHCI 1.0, SATA controllers to AHCI v1.3We can only applaud these efforts: it will eliminate a lot of useless time investments, lower costs and help make ARM partners a real option in servers. With the expected launch of manyARM Cortex A57based server SoCs this year, it looks like 2014 can be a breakthrough year for ARM servers. We looking forward to do anothermicro serverreview.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7721/arm-and-partners-deliver-first-arm-server-platform-standard\n",
      "Title: It Begins: AMD Announces Its First ARM Based Server SoC, 64-bit/8-core Opteron A1100\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2014-01-28T23:35:00Z\n",
      "URL: https://www.anandtech.com/show/7724/it-begins-amd-announces-its-first-arm-based-server-soc-64bit8core-opteron-a1100\n",
      "Content: Around 15 months ago, AMD announced that it would be building 64-bit ARM based SoCs for servers in 2014. Less than a month into 2014, AMD made good on its promise and officially announced the Opteron A1100: a 64-bit ARM Cortex A57 based SoC.The Opteron A1100 features either 4 or 8 AMD Cortex A57 cores. There's only a single die mask so we're talking about harvested die to make up the quad-core configuration. My guess is over time we'll see that go away entirely, but since we're at very early stages of talking about the A1100 there's likely some hedging of bets going on. Each core will run at a frequency somewhere north of 2GHz.The SoC is built on a 28nm process at Global Foundries.Each pair of cores shares a 1MB L2 cache, for a total of up to 4MB of L2 cache for the chip. All cores share a unified L3 cache of up to 8MB in size. AMD designed a new memory controller for the Opteron A1100 that's capable of supporting both DDR3 or DDR4. The memory interface is 128-bits wide and supports up to 4 SODIMMs, UDIMMs or RDIMMs. AMD will be shipping a reference platform capable of supporting up to 128GB of Registered DDR3 DIMMs off of a single SoC.Also on-die is an 8-lane PCIe 3.0 controller (1 x8 or 2 x4 slot configurations supported) and an 8-port 6Gbps SATA controller. AMD assured me that the on-chip fabric is capable of sustaining full bandwidth to all 8 SATA ports. The SoC features support for 2 x 10GbE ports and ARM's TrustZone technology.AMD will be making a reference board available to interested parties starting in March, with server and OEM announcements to come in Q4 of this year.It's still too early to talk about performance or TDPs, but AMD did indicate better overall performance than its Opteron X2150 (4-core 1.9GHz Jaguar) at a comparable TDP:AMD Opteron A1100 vs. X2150CPU Core ConfigurationCPU FrequencySPECint_rate EstimateSPECint per CoreEstimated TDPAMD Opteron A11008 x ARM Cortex A57>= 2GHz801025WAMD Opteron X21504 x AMD Jaguar1.9GHz28.1722WAMD alluded to substantial cost savings over competing Intel solutions with support for similar memory capacities. AMD tells me we should expect a total \"solution\" price somewhere around 1/10th that of a competing high-end Xeon box, but it isn't offering specifics beyond that just yet. Given the Opteron X2150 performance/TDP comparison, I'm guessing we're looking at a similar ~$100 price point for the SoC. There's also no word on whether or not the SoC will leverage any of AMD's graphics IP.The Opteron A1100 is aimed squarely at those applications that either need a lot of low power compute or tons of memory/storage. AMD sees huge demand in the memcached space, cold storage servers and Apache web front ends. The offer is pretty simple: take cost savings on the CPU front and pour it into more DRAM.Early attempts at ARM based server designs were problematic given the lack of a 64-bit ARM ISA. With ARMv8 and the Cortex A53/A57 CPUs, that's all changed. I don't suspect solutions like the Opteron A1100 to be a knockout success immediately, but this is definitely the beginning of something very new. Of all of the players in the ARM enterprise space, AMD looks like one of the most credible threats. It's also a great way for AMD to rebuild its enterprise marketshare with a targeted strike in new/growing segments.AMD's Andrew Feldman included one of his trademark reality check slides in his Opteron A1100 presentation today:Lower cost, high volume CPUs have always won. That's how Intel took the server market to begin with. The implication here is that ARM will do the same to Intel. Predicting 25% of the server market by 2019 may be feasible, but I'm not fond of making predictions for what the world will look like 5 years from now.The real question is what architecture(s) AMD plans to use to get to a leadership position among ARM CPUs and a substantial share of the x86 CPU market. We get the first hint with the third bullet above: \"smaller more efficient x86 CPUs will be dominant in the x86 segment\".\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7724/it-begins-amd-announces-its-first-arm-based-server-soc-64bit8core-opteron-a1100\n",
      "Title: AnandTech Google Hangouts Live with ARM's Peter Greenhalgh\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2013-12-20T17:49:00Z\n",
      "URL: https://www.anandtech.com/show/7598/anandtech-google-hangouts-live-with-arms-peter-greenhalgh-december-20th-12pm-et\n",
      "Content: He came andanswered your questions, his companygave away a couple of Samsung Galaxy Note 3sand now ARM's Cortex A53 lead architect, Peter Greenhalgh, is joining me for a live video interview (via Google Hangouts) tomorrow at noon ET.Update:We are starting now. You canask questions on Google+or just watch the stream on G+ or YouTube.And we're done. Check out the full interview below.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7598/anandtech-google-hangouts-live-with-arms-peter-greenhalgh-december-20th-12pm-et\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Holiday 2013 Giveaway: Samsung Galaxy Note 3\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2013-12-18T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/7592/arm-holiday-2013-giveaway-samsung-galaxy-note-3-with-biglittle-architecture\n",
      "Content: As I hinted at in ourAsk the Experts post with ARM's Peter Greenhalgh, a good turnout from all of you in the comments might help secure some giveaways from ARM. You guys held up your part of the bargain, now it's our turn. ARM is giving away two international Samsung Galaxy Note 3 smartphones (N9005) to two lucky AnandTech readers. Brian reviewed theUS version of the Galaxy Note 3 here. The N9005 is an unlocked international variant of the Galaxy Note 3. This particular variant has all of the LTE bands you'd want for europe/international use, but not for use on any US operators (I've listed band support in the table below). That being said, HSPA+ support shouldn't be an issue. We're giving away 32GB black versions here. A full list of its specs are below:Samsung Galaxy Note 3(SM-N9005)SoC2.3 GHz Qualcomm Snapdragon 800 (MSM8974)Display5.7-inch Super AMOLED (1080p)RAM3 GB LPDDR3WiFi802.11a/b/g/n/ac (BCM4339) + BT 4.0Cellular BandsGSM/GPRS/EDGE: 850/900/1800/1900MHzHSPA+: 850/900/1900/2100MHzLTE: 800/850/900/1800/2100/2600MHzStorage32GB + microSDXC (up to 64 GB)I/OmicroUSB 3.0, MHL 2.0, IR LED (remote), NFCOSAndroid 4.3Battery3200 mAh, 3.8V, 12.1 WhrSize / Mass151.2 x 79.2 x 8.3mm, 168gCamera13 MP w/AF, LED (Rear Facing) – 1080p60, 720p120, 4k302 MP (Front Facing)To enter the giveaway please leave a comment below (please only post one). As with all of our giveaways, this is only open to US residents with US mailing addresses. Unfortunately this is a legal requirement as each country/territory requires its own set of rules in order to be in compliance. Good luck!Entries will be accepted from9:00 AM ET on 12/18/2013through9:00 AM ET on 12/19/2013. We will draw 2 winner(s) who will be selected by12/19/2013.Official Rules and Regulations for AnandTech SweepstakesNo Purchase Required to Enter or WinUpon entering any contest, sweepstakes, or promotion (a “Promotion”) offered by anandtech.com (the “Site”), a website owned and operated by AnandTech, Inc. (“AnandTech”), you must agree to the following Official Rules and Regulations (the “Rules”) as well as any additional rules governing a specific Promotion that AnandTech publishes on the Site.BEFORE ENTERING A PROMOTION, READ THESE RULES AND THE SITE’S TERMS, CONDITIONS AND PRIVACY INFORMATION. BY ENTERING THE PROMOTION, YOU AGREE TO COMPLY WITH THE RULES AND THE SITE’S TERMS, CONDITIONS AND PRIVACY INFORMATION.NO PURCHASE NECESSARY TO ENTER OR WIN. PURCHASE DOES NOT INCREASE CHANCES OF WINNING.Eligibility.Promotions are open to entrants who are 18 years of age or older at time of entry, and a legal resident of the United States (excluding Puerto Rico). Entries are limited to individuals only who are not presently banned from AnandTech’s website or comments section; commercial enterprises and business entities are not eligible. Directors, officers, employees, contractors, and agents of AnandTech (excluding volunteer AnandTech forum moderators) and members of their immediate families (spouses, parents, siblings, and children) are not eligible. Subject to all applicable federal, state, and local laws and regulations. Void where prohibited. Participation constitutes entrant’s full and unconditional agreement to these Rules and AnandTech’s decisions, which are final and binding in all matters relating to a Promotion.Entry Period.Each Promotion will contain a specific time period within which entries will be accepted (a “Promotion Period”). The Promotion Period for this Promotion shall run from9:00 AM ET on 12/18/2013through9:00 AM ET on 12/19/2013. Only entries received during the Promotion Period will be accepted.How to Enter.Each Promotion will describe an entry procedure. AnandTech is not responsible for lost, late, illegible, stolen, incomplete, invalid, unintelligible, misdirected, technically corrupted or garbled entries, which will be disqualified, or for problems of any kind whether mechanical, human or electronic. Proof of submission will not be deemed to be proof of receipt by AnandTech. All entries must be in English. Individuals are automatically entered in this Promotion by signing up for AnandTech’s Comments athttp://anandtech.com/Account/Registerand posting a reply to this post. If randomly selected as a winner individuals must provide full name, complete mailing address, telephone number, AnandTech user name and birth date within 3 days of being contacted.Limits on Entry.An individual may enter a Promotion once only. The use of any automated launching or entry software or any other means that permits an entrant to automatically enter repeatedly or in excess of the entry limitations is prohibited.Prizes.Winning a gift, prize, or other promotional item (a “Prize”) in a Promotion is contingent upon fulfilling all requirements in these Rules. Winners will be selected in a random drawing of eligible entries received during the Promotion Period. AnandTech will notify Prize winners using the contact information provided in the winning entry. Failure to claim a Prize by the time or in the manner specified in the particular Promotion will invalidate any claim to the Prize. Prizes are not transferable. The odds of winning a Prize or the Grand Prize depend on the number of entries received by AnandTech. TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, ALL PRIZES ARE PROVIDED “AS IS” AND ARE NOT EXCHANGEABLE FOR FAIR MARKET VALUE. TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, ANANDTECH DISCLAIMS ALL WARRANTIES WITH RESPECT TO THE PRIZES, INCLUDING THE IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, AND NON-INFRINGEMENT. For this Promotion,2winner(s) will be selected by12/19/2013and will each receive aSamsung Galaxy Note 3 (SM-N9005)described above (“Grand Prize”). The Total U.S. Retail Value of the Grand Prize is$700. The Grand Prize may not be substituted for cash. The Grand Prize winner will be solely responsible for all applicable taxes, fees, and surcharges associated with receipt and/or use of the Grand Prize. After the Grand Prize winner has been notified and has complied with all applicable Rules, AnandTech will post the Grand Prize winner’s name on this website.Publicity.The winner of a Promotion agrees to allow AnandTech to use his or her name, photograph, likeness, voice, prize information, and biographical information for publicity and promotional purposes without further compensation where permitted by law.Governing Law.These Rules and all Promotions are governed by and controlled by the laws of the State of North Carolina, without reference to the applicable choice of law provisions. All actions, proceedings or litigation relating hereto will be instituted and prosecuted solely within Wake County, North Carolina. By entering a Promotion, entrants consent to the jurisdiction of the state courts of North Carolina and the federal courts located within North Carolina with respect to any action, dispute or other matter pertaining to or arising out of that Promotion.General Terms.Any failure by an entrant, including any prize winner, to comply with any of the Rules or the Site’s Terms, Conditions and Privacy Information may result in disqualification from the Promotion. All entries, whether they are eligible entries or ineligible entries, are the exclusive property of AnandTech. AnandTech is not responsible for any typographical errors in the Rules or in any other communication surrounding a Promotion or for any technical malfunction or error relating to the Promotion. AnandTech reserves the right to amend or interpret the Rules at any time, upon published notice to participants on its website. Promotion participants agree to release, indemnify, and hold harmless AnandTech and its directors, officers, employees and agents from any and all liability regarding the Promotion, including any injuries, losses, or damages (compensatory, direct, incidental, consequential, or otherwise) regarding the use or misuse of any Prize, any event beyond AnandTech’s control resulting in the disruption, cancellation, or postponement of the receipt of the Prize, or any typographical errors or technical malfunctions associated with the Promotion. AnandTech reserves the right to disqualify any entry that it, in its sole discretion, determines (i) to be in violation of the Rules, (ii) submitted by fraud or by tampering with the entry process, or (iii) contains inaccurate or fraudulent information. ANY ATTEMPT BY ANY INDIVIDUAL TO DELIBERATELY DAMAGE THE SITE OR UNDERMINE THE LEGITIMATE OPERATION OF THE PROMOTION IS A VIOLATION OF CRIMINAL AND CIVIL LAWS. IN THE EVENT SUCH AN ATTEMPT OCCURS, ANANDTECH RESERVES THE RIGHT TO SEEK DAMAGES FROM SUCH INDIVIDUAL TO THE FULLEST EXTENT PERMITTED BY LAW.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7592/arm-holiday-2013-giveaway-samsung-galaxy-note-3-with-biglittle-architecture\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Answered by the Experts: ARM's Cortex A53 Lead Architect, Peter Greenhalgh\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2013-12-18T04:56:00Z\n",
      "URL: https://www.anandtech.com/show/7591/answered-by-the-experts-arms-cortex-a53-lead-architect-peter-greenhalgh\n",
      "Content: Last week we heldan awesome Ask the Experts Q&A with ARM's Peter Greenhalgh, lead architect for the Cortex A53. Peter did a great job answering questions in the comments, but for those of you who missed them we're compiling all of the Q&A here for you to go through.On Friday, December 20th at 12:00PM ET, Peter will be joining me for a live Google Hangouts chat. We'll be posting more details on that later this week. For now, enjoy the responses!Question from shodanshokHi, it would be interesting to know two thing:- the cache memories (L1/L2) are write-back or write-through? Inclusive or exclusive?- multiprocessor capabilities are limited to 4 cores or they can scale to 8+ cores without additional glue logic?Thanks.AnswerHi Shodanshok,All cacheable attributes are supported, but Cortex-A53 is optimised around write-back, write-allocate. The L2 cache is inclusive on the instruction side and exclusive on the data side.A Cortex-A53 cluster only supports up to 4-cores. If more than 4-cores are required in a platform then multiple clusters can be implemented and coherently connected using an interconnect such as CCI-400. The reason for not scaling to 8-cores per cluster is that the L2 micro-architecture would need to either compromise energy-efficiency in the 1-4 core range to achieve performance in the 4-8 core range, or compromise performance in the 4-8 core range to maximise energy-efficiency in the 1-4 core range. This isn’t a hard and fast rule for all clusters, but is the case for a cluster at the Cortex-A53 power/performance point. For the majority of mobile use cases it is best to focus on energy efficiency and enable more than 4-cores through multi-cluster solutions.Question from lukarakWe have seen MediaTek introducing an 8xA7 SOC, instead of going to the big.LITTLE configuration of some sorts. Do you expect the same thing to happen with the A53 and A57 generation for low budget SOCs or will this generation's combo be a little easier and cheaper to implement?AnswerHi Lukarak,We expect to see a range of platform configurations using Cortex-A53. A 4+4 Cortex-A53 platform configuration is fully supported and a logical progression from a 4+4 Cortex-A7 platform. A Cortex-A57 in the volume smartphone markets is less likely, but that’s a decision in the hands of the ARM partners. It will be interesting to see the range of Cortex-A53 platforms and configurations announced by partners over the coming months.Question from ehsan.nitolHi there, I have some questions.We have already seen how well Qualcomm's Cortex A7 can perform thanks to Moto G. How much will it improve with the new Cortex A53? What will be the core and performance wise difference? How will you compare it against Cortex A9, A12 and A15 in terms of performance, battery consumption and all.With the Exynos Octa core processor Battery Test we haven't seen much battery improvements compared to Qualcomm's Snapdragon 600 and 800 Processor. How will it perform this time?What is ARM planning do with its Mali GPU? What will be next after Cortex A53 and A57?AnswerHi Ehsan,Cortex-A53 has the same pipeline length as Cortex-A7 so I would expect to see similar frequencies when implemented on the same process geometry. Within the same pipeline length the design team focussed on increasing dual-issue, in-order performance as far as we possibly could. This involved symmetric dual-issue of most of the instruction set, more forwarding paths in the datapaths, reduced issue latency, larger & more associative TLB, vastly increased conditional and indirect branch prediction resources and expanded instruction and data prefetching. The result of all these changes is an increase in SPECInt-2000 performance from 0.35-SPEC/Mhz on Cortex-A7 to 0.50-SPEC/Mhz on Cortex-A53. This should provide a noticeable performance uplift on the next generation of smartphones using Cortex-A53.Question from Techguy99XWhy are the current A7 quad core phones performing similar to the A9 quad (exynos 4412 , tegra 3), although A9 is more advanced and OoO? What is the main difference between A5 and A7, becuase the A7 is just a bit faster than the A5?AnswerHi Techguy99X,Overall platform performance is dependent on many factors including processor, interconnect, memory controller, GPU, video and more. While the Cortex-A9 is a higher performance processor both in IPC and frequency, ARM partners are continuously improving their platforms and porting them to new process geometries. This allows a new generation Cortex-A7 based platform to improve on an older generation Cortex-A9 based platform.Compared to Cortex-A5, Cortex-A7 increased load-store bandwidth, allowed more common data-processing operations to dual-issue and made some small improvements in the branch-predictors.Question from barleyguyWhen designing a processor and deciding on which performance attributes to emphasize, do you target current workloads for short term market concerns, or do you target possible future workloads for the market a year or two from now? Or is performance tuning more workload agnostic, and do you say \"I want this to be fast for everything\"?For example, since ARM processors are very popular in the Android market, do you tune for content comsumption and gaming? Or since Android may be trending towards more of a primary computing device in the future, is it important to tune for desktop applications?And finally, what are the considerations of performance tuning for thermally constrained devices?AnswerHi Barleyguy,A good question. A general purpose processor has to be good on all workloads. After all, we expect to see Cortex-A53 in everything from mobile phones to cars, set-top-box to micro-servers and routers. However we do track the direction software is evolving – for example the increased use of Javascript which puts pressure on structures such as indirect predictors. Therefore we design structures within the processor to cope with future code as well as existing expected workloads.Question from psychobriggsyDo you have any plans to support various forms of turbo functionality within your next generation ARM cores? An a potential example, in a 28nm quad-core A53 setup at 1.2GHz, you could support dual-core at >1.4GHz and single core at >1.6GHz within the same power consumption (core design allowing, of course), yet single threaded performance could improve significantly.ARM cores have been historically low power, however that doesn't mean there aren't more power savings to be made. Examples include deeper sleep states, power gated cores, and so on - features that Intel and AMD have had to include in order to reduce their TDPs whereas ARM cores haven't need them (yet). What are the future power saving methods that ARM is considering for its future cores (that you can give away)?AnswerHi Psychobriggsy,A Turbo mode is typically a form of voltage overdrive for brief periods of time to maximise performance, which ARM partners have been implementing on mobile platforms for many years. Whether this is applied to 1,2 or more cores is a decision of the Operating System and the platform power management software. If there is only one dominant thread you can bet that mobile platforms will be using Turbo mode. Due to the power-efficiency of Cortex-A53 on a 28nm platform, all 4 cores can comfortably be executing at 1.4GHz in less than 750mW which is easily sustainable in a current smartphone platform even while the GPU is in operation.In terms of further power saving techniques, power gating unused cores is a technique that has been used since the first Cortex-A9 platforms appeared on the market several years ago. The technique is so fundamental that I think many in the mobile industry use it automatically and forget that it’s a highly beneficial power saving technique. But you are correct that there is more milage to come from deeper sleep states which is why both Cortex-A53 and Cortex-A57 support state retention techniques in caches and the NEON unit to further improve leakage power.Question from XebecPeter, thanks for offering your time to Anandtech!I was curious if you could talk a bit about how easy/difficult A53-derived SoCs might be to integrate into solutions that are already using A7/A9 type chips? i.e. Devices like Beagleboards, Raspberry Pis, ODROIDs, etc. Is there anything that makes the A53 particularly difficult or easy to suit to these types of devices?Also, for Micro and \"regular\" servers, do you see A57/A53 big.LITTLE being the norm, or do you anticipate a variety of A53-only and A57-only designs? Any predictions on market split between the A5x series here?Respectfully,JohnAnswerHi Xebec,Cortex-A53 has been designed to be able to easily replace Cortex-A7. For example, Cortex-A7 supports the same bus-interface standards (and widths) as Cortex-A7 which allows a partner who has already built a Cortex-A7 platform to rapidly convert to Cortex-A53.With servers I think we will see a mix of solutions. The most popular approach will be to use Cortex-A57 due to the performance that micro-architecture is capable of providing, but I still expect some Cortex-A53 servers and big.LITTLE too!Question from DoormatARM CPU vendors (Qualcomm, Nvidia, etc) seem to be choosing slower quad core over faster dual core, and I'm suspecting its all a marketing game (e.g. more cores is better, see Motorola's X8 announcement of an \"8 core\" phone). Do those non-technical decisions impact the decisions of the engineers in developing the ARM architecture?AnswerHi Doormat,You are quite correct that there are a variety of frequencies and core-counts being offered by ARM partners. However, for ARM design micro-architectures these do not have an effect on micro-architectures as we must be able to support a variety of target frequencies and core-counts across many different process geometries.Question from Factory FactoryHow does designing a CPU \"by hand\" differ from using an automated layout tool? What sort of trade-offs does/would using automated tools cause for ARM's cores?Second question: With many chips from many manufacturers now implementing technologies like fine-grained power gating, extremely fine control of power and clock states, and efficient out-of-order execution pipelines, where does ARM go from here to keep its leadership in low-power compute IP?AnswerHi Factory,Hand layout versus automated layout is an interesting trade-off. From one perspective, full hand-layout for all circuits in a processor is rarely used now. Aside from cache RAMs which are always custom, hand-layout is reserved for datapath and queues which are regular structures that allow a human to spot the regularity and ‘beat’ an automated approach. However, control logic is not amenable to hand-layout as it’s very difficult to beat automated tools which means that the control logic can end up setting the frequency of the processor without significant effort.In general the benefit from hand-layout has been reducing in recent years. Partly this is due to the complexity of the design rules for advanced process generations reducing the scope for more specific circuit tuning techniques to be used. But another factor is the development of advanced standard cell libraries that have a large variety of cells and drive strengths which lessens the need for special circuit techniques. When we’re developing our processors we’re fortunate to have access to a large team in ARM designing standard cell libraries and RAMs who can advise us about upcoming nodes (for example 16nm and 10nm). In turn the processor teams can suggest & trial new advanced cells for the libraries which we call POPs (Processor Optimization Packages) that improve frequency, power and area.A final trade-off to consider is process portability. After an ARM processor is licensed we see it on many different process geometries which is only possible because the designs are fully synthesizable. For example, there are Cortex-A7 implementations on all the major foundries from 65nm to 16nm. In combination with the advanced standard cell libraries for these processes there is little need to go to a hand-layout approach and we instead enable our partners to get to market more rapidly on the process geometry and foundry of their choosing.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7591/answered-by-the-experts-arms-cortex-a53-lead-architect-peter-greenhalgh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Ask the Experts: ARM's Cortex A53 Lead Architect, Peter Greenhalgh\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2013-12-10T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/7574/ask-the-experts-arms-cortex-a53-lead-architect-peter-greenhalgh\n",
      "Content: Given the timing of yesterday'sCortex A53 based Snapdragon 410 announcement, our latest Ask the Experts installment couldn't be better. Peter Greenhalgh, lead architect of the Cortex A53, has agreed to spend some time with us and answer any burning questions you might have on your mind about ARM, directly.Peter has worked in ARM's processor division for 13 years and worked on the Cortex R4, Cortex A8 and Cortex A5 (as well as the ARM1176JZF-S and ARM1136JF-S). He was lead architect of the Cortex A7 and ARM's big.LITTLE technology as well.Later this month I'll be doing a live discussion with Peter via Google Hangouts, but you guys get first crack at him. If you have any questions about Cortex A7, Cortex A53, big.LITTLE or pretty much anything else ARM related fire away in the comments below. Peter will be answering your questions personally in the next week.Please help make Peter feel at home here on AnandTech by impressing him with your questions. Do a good job here and I might be able to even convince him to give away some ARM powered goodies...\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7574/ask-the-experts-arms-cortex-a53-lead-architect-peter-greenhalgh\n",
      "Title: Qualcomm Announces Snapdragon 410 Based on 64-bit ARM Cortex A53\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2013-12-09T22:45:00Z\n",
      "URL: https://www.anandtech.com/show/7573/qualcomm-announces-snapdragon-410-based-on-64bit-arm-cortex-a53-and-adreno-306-gpu\n",
      "Content: ARM told us to expect some of the first 64-bit ARMv8 based SoCs to ship in 2014, and it looks like we're seeing just that. Today Qualcomm is officially announcing its first 64-bit SoC: the Snapdragon 410 (MSM8916).Given that there's no 64-bit Android available at this point, most of the pressure to go to 64-bit in the Android space is actually being driven by the OEMs who view 64-bit support as a necessary checkbox feature at this point thanks to Apple's move with the A7. Combine that with the fact that the most ready 64-bit IP from ARM is the Cortex A53 (successor to the Cortex A5/A7 line), and all of the sudden it makes sense why Qualcomm's first 64-bit mobile SoC is aimed at the mainstream market (Snapdragon 400 instead of 600/800).I'll get to explaining ARM's Cortex A53 in a moment, but first let's look at the specs of the SoC:Qualcomm Snapdragon 410Internal Model NumberMSM8916Manufacturing Process28nm LPCPU4 x ARM Cortex A53 1.2GHz+GPUQualcomm Adreno 306Memory Interface1 x 64-bit LPDDR2/3Integrated Modem9x25 core, LTE Category 4, DC-HSPA+At a high level we're talking about four ARM Cortex A53 cores, likely running around 1.2 - 1.4GHz. Having four cores still seems like a requirement for OEMs in many emerging markets unfortunately, although I'd personally much rather see two higher clocked A53s. Qualcomm said the following about 64-bit in its 410 press-release:\"The Snapdragon 410 chipset will also be the first of many 64-bit capable processors as Qualcomm Technologies helps lead the transition of the mobile ecosystem to 64-bit processing.”Keep in mind that Qualcomm presently uses a mix of ARM and custom developed cores in its lineup. The Snapdragon 400 line already includes ARM (Cortex A7) and Krait based designs, so the move to Cortex A53 in the Snapdragon 410 isn't unprecedented. It will be very interesting to see what happens in the higher-end SKUs. I don't assume that Qualcomm will want to have a split between 32 and 64-bit designs, which means we'll either see a 64-bit Krait successor this year or we'll see more designs that leverage ARM IP in the interim.As you'll see from my notes below however, ARM's Cortex A53 looks like a really good choice for Qualcomm. It's an extremely power efficient design that should be significantly faster than the Cortex A5/A7s we've seen Qualcomm use in this class of SoC in the past.The Cortex A53 CPU cores are paired with an Adreno 306 GPU, a variant of the Adreno 305 used in Snapdragon 400 based SoCs (MSM8x28/8x26).The Snapdragon 410 also features an updated ISP compared to previous 400 offerings, adding support for up to a 13MP primary camera (no word on max throughput however).Snapdragon 410 also integrates a Qualcomm 9x25 based LTE modem block (also included in the Snapdragon 800/MSM8974), featuring support for LTE Category 4, DC-HSPA+ and the usual legacy 3G air interfaces.All of these IP blocks sit behind a single-channel 64-bit LPDDR2/3 memory interface.The SoC is built on a 28nm LP process and will be sampling in the first half of 2014, with devices shipping in the second half of 2014. Given its relatively aggressive schedule, the Snapdragon 410 may be one of the first(if not the first)Cortex A53 based SoCs in the market.A Brief Look at ARM's Cortex A53ARM's Cortex A53 is a dual-issue in-order design, similar to the Cortex A7. Although the machine width is unchanged, the A53 is far more flexible in how instructions can be co-issued compared to the Cortex A7 (e.g. branch, data processing, load-store, & FP/NEON all dual-issue from both decode paths).The A53 is fully ISA compatible with the upcoming Cortex A57, making A53 the first ARMv8 LITTLE processor (for use in big.LITTLE) configurations with an A57The overall pipeline depth hasn't changed compared to the Cortex A7. We're still dealing with an 8-stage pipeline (3-stage fetch pipeline + 5 stage decode/execute for integer or 7 for NEON/FP).The vast majority of instructions will execute in one cycle, leaving branch prediction as a big lever for increasing performance. ARM significantly increased branch prediction accuracy with the Cortex A53, so much that it was actually leveraged in the dual-issue, out-of-order Cortex A12.ARM also improved the back end a bit, improving datapath throughput.The result of all of this is a dual-issue design that's pushed pretty much as far as you can without going out-of-order. Below are some core-level performance numbers, all taken in AArch32 mode, comparing the Cortex A53 to its A5/A7 competitors:Core Level Performance ComparisonAll cores running at 1.2GHzDMIPSCoreMarkSPECint2000ARM Cortex A51920-350ARM Cortex A722803840420ARM Cortex A9 r4p1--468ARM Cortex A5327604440600Even ignoring any uplift from new instructions or 64-bit, the Cortex A53 is going to be substantially faster than its predecessors. I threw in hypothetical SPECint2000 numbers for a 1.2GHz Cortex A9 to put A53's performance in even better perspective. You should expect to see better performance than a Cortex A9r4 at the same frequencies, but the A9r4 is expected to hit much higher frequencies (e.g. 2.3GHz for Cortex A9 r4p1 in NVIDIA's Tegra 4i).ARM included a number of power efficiency improvements and is targeting 130mW single-core power consumption at 28nm HPM (running SPECint 2000). I'd expect slightly higher power consumption at 28nm LP but we're still talking about an extremely low power design.I'm really excited to see what ARM's Cortex A53 can do. It's a potent little architecture, one that I wish we'd see taken to higher clock speeds and maybe even used in higher end devices at the same time. The most obvious fit for these cores however is something like the Moto G, which presently uses the 32-bit Cortex A7. Given Qualcomm's schedule, I wouldn't be surprised to see something like a Moto G update late next year with a Snapdragon 410 inside. Adding LTE and four Cortex A53s would really make that the value smartphone to beat.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7573/qualcomm-announces-snapdragon-410-based-on-64bit-arm-cortex-a53-and-adreno-306-gpu\n",
      "Title: Intel to Fab Altera FPGAs with ARM IP\n",
      "Author: Jarred Walton\n",
      "Date Published: 2013-10-30T19:11:00Z\n",
      "URL: https://www.anandtech.com/show/7473/intel-to-fab-altera-fpgas-with-arm-ip\n",
      "Content: In a storyposted today on EETimes, Altera announced at the ARM Developers Conference that they have entered into a partnership with Intel to have their next generation 64-bit ARM chips produced at Intel’s fabs. According to the report, Altera will be using Intel's upcoming 14nm FinFET process technology to manufacture a Cortex-A53 quad-core SoC, which will be surrounded by FPGA logic.The Intel/Altera partnership wasfirst announced back in February 2013, and it's worth noting that FPGAs are not an area where Intel currently competes. Even though ARM logic will be on the new chips, this likely won't lead to direct competition with Intel's own chips. The bigger deal of course is that while Intel's 22nm process would give anyone willing to pay Intel’s price a leg up on the competition, 14nm is a full step ahead of the competition.Intel has apparently inked deals with other companies as well.The Inquirerhas this quote from an Intel spokesperson: “We have several design wins thus far and the announcement with Altera in February is an important step towards Intel's overall foundry strategy. Intel will continue to be selective on customers we will enable on our leading edge manufacturing process.”The key there is the part about being “selective”, but I would guess it’s more a question of whether a company has the volume as well as the money to pay Intel, rather than whether or not Intel would be willing to work with them. There are many possibilities – NVIDIA GPUs on Intel silicon would surely be interesting, and given that AMD has gone fabless as well we could also see their future CPUs/GPUs fabbed by Intel. There are many other ARM companies as well (Qualcomm), not to mention Apple. But those are all more or less in direct competition with Intel's own processors, so unless we're talking about potential x86 or Quark licensees, it's tough to predict where this will lead.If we take things back another step, the reality of the semiconductor business is that fabs are expensive to build and maintain. Then they need to be updated every couple of years to the latest technology, or at least new fabs need to be built to stay competitive. If you can’t run your fabs more or less at capacity, you start to fall behind on all fronts. If Intel can more than utilize all of their fabrication assets, it’s a different story, but that era appears to be coming to a close.The reason for this is pretty simple. We’re seeing a major plateau in terms of the computing performance most people need on a regular basis these days. Give me an SSD and I am perfectly fine running most of my everyday tasks on an old Core 2 Duo or Core 2 Quad. The difference between Bloomfield, Sandy Bridge, Ivy Bridge, and Haswell processors is likewise shrinking each generation – my i7-965X that I’m typing this on continues to run very well, thank you very much! If people and businesses aren’t upgrading as frequently, then you need to find other ways to keep your fabs busy, and selling production to other companies is the low hanging fruit.Regardless of the reasons behind the move, this potentially marks a new era in Intel fabrication history. It will be interesting to see what other chips end up being fabbed at Intel over the next year or two. Will we see real competitors and not just FPGA chips fabbed at Intel? Perhaps some day, but probably not in the short term.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7473/intel-to-fab-altera-fpgas-with-arm-ip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Samsung Updates Exynos 5 Octa (5420), Switches Back to ARM GPU\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2013-07-23T03:52:00Z\n",
      "URL: https://www.anandtech.com/show/7164/samsung-exynos-5-octa-5420-switches-back-to-arm-gpu\n",
      "Content: In thefirst part of our series on ARM, we mentioned that with every major microprocess design ARM tries to choose 3 licensees to get early access to technology. It's very clear that Samsung was among the early three to get ahold of Cortex A15 IP. Samsung was first on the mobile market with a Cortex A15 based SoC: the Exynos 5250 (aka Exynos 5 Dual). Featuring two cores running at up to 1.7GHz paired with an ARM Mali-T604 GPU, we first met theExynos 5250inSamsung's own Chromebook XE303last October.The next logical step would be a quad-core version, which we sort of got with theExynos 5410- or as it's more commonly known: Exynos 5 Octa. This part features four ARM Cortex A15 cores running at up to 1.6GHz and four ARM Cortex A7 cores running at up to 1.2GHz in a configuration ARM calls big.LITTLE. The specific implementation of big.LITTLE on Exynos 5410 is known as Cluster Migration; either the four Cortex A15 cores or four Cortex A7 cores can be active, but not both and not an arbitrary combination of cores from each island. They're either all on or all off. This is by far the easiest to implement from a software perspective, but is obviously the less interesting option from a heterogeneous SMP perspective. I'll be talking more about this in an upcoming ARM piece.On the graphics front,Samsung moved to Imagination Technologies for the Exynos 5410- implementing a PowerVR SGX 544MP3 setup. The Exynos 5410 saw limited use, appearing in some international versions of the Galaxy S 4 and nothing else. Part of the problem with the design was a broken implementation of the CCI-400 coherent bus interface that connect the two CPU islands to the rest of the SoC. In the case of the 5410, the bus was functional but coherency was broken and manually disabled on the Galaxy S 4. The implications are serious from a power consumption (and performance) standpoint. With all caches being flushed out to main memory upon a switch between CPU islands. Neither ARM nor Samsung LSI will talk about the bug publicly, and Samsung didn't fess up to the problem at first either - leaving end users to discover it on their own.Last week Samsung teased a new, improved Exynos 5 Octa - the Exynos 5420. Today we got the first details of the new SoC. The base CPU architecture remains unchanged. Samsung outfitted the Exynos 5420 with four A15s and four A7s, presumably in the same Cluster Migration big.LITTLE configuration. Clock speeds on both clusters are a bit higher now, 1.8GHz is the top speed for the Cortex A15 cores while 1.3GHz is where the A7s top out. Note that on the Cortex A15 side this exceeds where even ARM recommends clocking Cortex A15 for smartphones as far as power efficiency is concerned, but it should be fine for tablets. There's no word on whether or not the CCI-400 bug has been fixed, but I can only assume that it has been otherwise it'd be senseless to do another Exynos 5 Octa release this close to the original.Update: It looks like the CCI bug has been fixed.Exynos 5 ComparisonSoC525054105420Max Number of Active Cores244 (?)CPU Configuration2 x Cortex A154 x Cortex A15 + 4 x Cortex A74 x Cortex A15 + 4 x Cortex A7A15 Max Clock1.7 GHz1.6GHz1.8GHzA7 Max Clock-1.2GHz1.3GHzGPUARM Mali-T604 MP4Imagination PowerVR SGX544MP3ARM Mali-T628 MP6Memory Interface2 x 32-bit LPDDR3-16002 x 32-bit LPDDR3-16002 x 32-bit LPDDR3-1866Process32nm HK+MG28nm HK+MG28nm HK+MG(?)For the GPU Samsung switches back to ARM, this time using the Mali-T628 GPU in a 6-core configuration. Mali-T628 is actuallya second generation implementation of ARM's Midgard GPUarchitecture first demonstrated with the T604. The second generation brings higher IPC and higher clocks in the same physical area as the first-gen cores, the combination of the two results in up to a 50% increase in performance. The T604 was a four-core implementation, so we should see another 50% on top of that with the move to 6-cores in the 5420. A six-core configuration is a bit odd in that we've never seen one before, but the T628 is scalable from 4 - 8 cores so it's a valid config.On the memory interface front the Exynos 5420 retains a dual-channel LPDDR3 interface (2 x 32-bit) with support for up to 1866MHz memory, resulting in peak theoretical memory bandwidth of 14.9GBps.The biggest question about the new Exynos 5420 is whether or not the cache coherency issues have been worked out. The solution remains a bit on the large side for most price sensitive tablets, but it could make for an interesting use case in a higher-end tablet. In smartphones I'm still not sold on the idea of having four Cortex A15s running at up to 1.8GHz. Although big.LITTLE is one answer to the problem of getting the best of both worlds (low power and high performance), Qualcomm seems to have a pretty good solution with its Krait 300/400 cores. If Samsung were to enable one of the more interesting big.LITTLE scheduling models in its products however (e.g. big.LITTLE MP, all cores visible at once, intelligent scheduling based on perf needs) I'd be more interested.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7164/samsung-exynos-5-octa-5420-switches-back-to-arm-gpu\n",
      "Title: Is Haswell Ready for Tablet Duty? Battery Life of Haswell ULT vs Modern ARM Tablets\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2013-07-22T18:11:00Z\n",
      "URL: https://www.anandtech.com/show/7117/haswell-ult-investigation\n",
      "Content: With the exception of article production or live blogging, my on-the-road notebook usage model is filled with tons of idle time. Last week I was in a large conference room, sitting through presentations from 9AM - 5PM every day. There was an hour break for lunch, and a couple of 15 minute breaks spread throughout, but for the most part I had my notebook open, taking notes and occasionally pulling up websites to reference/research after having a thought.My in-meeting notebook usage is actually a lot like what my notebook usage was as a student in college. Very light web browsing (unless I really didn’t have to pay attention), coupled with background IM, email and tons of note taking. I have a feeling this usage model isn’t all that unique to me. On the contrary, I bet it’s quite common. Which makes this next part hilarious: notebook PCs actually did a terrible job of running this very scenario.Power efficiency was always a problem. The only notebooks you wanted to carry around with you were the ones that had tiny batteries. The larger notebooks had big batteries but also had big screens and power hungry components. We used to have a battery life test that simply measured how long it would take a notebook to die if we left it idling at the windows desktop. Just two years ago, it wasn’t unusual to see notebooks incapable of breaking 5 hours of idle battery life.The truth is that it wasn’t just display quality, terrible track pads and sluggish mechanical hard drives that drove people to tablets. Great platform idle power coupled with very efficient mobile OSes really made the current tablet revolution possible.Back in the early 2000s, Matthew Witheiler (our first graphics editor) was on a tablet PC kick. He searched high and low for anyone who’d bring a tablet to market. In college at the time, I understood why he wanted a tablet. The experience fell short at the very same points every time. Tablets back then were too big, too slow and had terrible battery life. The PaceBook PaceBlade lasted under 3 hours on a single charge back when we reviewed it in 2002. It also took 11 seconds to wake up from standby, and 84 seconds to boot (the Transmeta Crusoe TM5600 inside was slower than a 433MHz Celeron at the time).The current crop of ARM based tablets largely fixed this problem. Theyaren’t all that quick if you compare to modern high-end CPU and GPU architectures, but they benefit from much lighter weight apps and OSes that are more efficient.When Intel first started talking about Haswell and Ultrabooks, it did so under the banner of fixing the “ARM problem” and merging the best of tablets and notebook PCs. Looking around at the first implementation of Haswell ULT and the Ultrabooks based on it, they just look like better versions of the systems that came before them. Haswell ULT definitely posts better battery life than any previous Intel Core microarchitecture, but everything the world did with it seemed so very...predictable. EvenApple just slotted Haswell ULT into the same chassis as Sandy Bridge and Ivy Bridge ULV.The idea for this article struck me as I was in meetings last month. Sitting in that conference room for 8 hours straight each day would’ve killed my rMBP13 without plugging it in. The 2013 MacBook Air on the other hand did just fine. At one point there was some drama around a few power outlets not working. Much like using a tablet, I didn’t care. Even when I had only 50% of my battery charged, I had more than enough juice to get through the day without hunting for a power outlet. Given a very light usage model, Haswell ULT behaved like an ARM tablet platform. The difference being that if/when I needed more performance, it was available.This whole situation convinced me to run a test that a few AnandTech readers had asked me for a few weeks ago: run our tablet battery life workload on the 2013 MacBook Air. Even our lightest Mac battery life workload is still heavier than what we run on smartphones/tablets, so the light workload battery life numbers aren’t really representative of a tablet usage model with Haswell ULT. Luckily our tablet battery life tests are fairly portable, so I prepped the 2013 13-inch MBA the same way I would one of our tablets: brightness calibrated to 200 nits running the very same workload as what we would on a tablet. You’ll notice two bars for the 2013 MacBook Air, one indicating its result and one with that result scaled down to simulate what would happen if it had 78.7% of its actual battery capacity - putting it on equal footing to the 42.5Wh iPad 4. With workload and performance constant, it’s safe to assume that battery life scales linearly at best with battery capacity. In other words, our MacBook Air numbers at 42.5Wh should be indicative of what we’d expect if the 13-inch MBA actually had a 42.5Wh battery rather than 54Wh unit.First off, our WiFi web browsing test:We regularly load web pages at a fixed interval until the battery dies (all displays are calibrated to 200 nits as always). The differences between this test and our previous one boil down to the amount of network activity and CPU load.On the network side, we've done a lot more to prevent aggressive browser caching of our web pages. Some caching is important otherwise you end up with a baseband/WiFi test, but it's clear what we had previously wasn't working. Brian made sure that despite the increased network load, the baseband/WiFi still have the opportunity to enter their idle states during the course of the benchmark.We also increased CPU workload along two vectors: we decreased pause time between web page loads and we shifted to full desktop web pages, some of which are very js heavy. The end result is a CPU usage profile that mimics constant, heavy usage beyond just web browsing. Everything you do on your device ends up causing CPU usage peaks - opening applications, navigating around the OS and of course using apps themselves. Our 5th generation web browsing battery life test should map well to more types of mobile usage, not just idle content consumption of data from web pages.This is what I hinted at duringPodcast #21: total platform power of the 2013 13-inch MacBook Air is lower than Apple’s 4th generation iPad. Even if you take into account battery capacity, the 13-inch MBA lasts around 18% longer on a single charge.What we aren’t taking into account however are the different display panels. The MacBook Air uses a 13.3-inch 1440 x 900 panel, compared to a 9.7-inch 2048 x 1536 panel on the 4th gen iPad. I’m not sure how big of a difference the delta would make. DisplayMate measured 7W for the 3rd gen iPad’s backlight, compared to 2.7W for the iPad 2. If we assume the delta is around 4.2W, that’s roughly another 10% hit that the Haswell ULT platform would have to take in order to bring its display power consumption in line with the iPad. With an 18% advantage in battery life in this test, it looks like even moving to a similar panel would deliver equal if not slightly better platform power consumption for Haswell ULT. Similarly, deploying Haswell ULX instead (lower TDP/SDP version of Haswell) could drive battery life even higher.Tablets are very often used for video playback, so this next test is just as important as a more interactive workload:Here I'm playing a 4Mbps H.264 High Profile 720p rip I made of the Harry Potter 8 Blu-ray. The full movie plays through and is looped until the battery dies. Once again, the displays are calibrated to 200 nits.The video playback results show exactly where Intel needs to focus on improving power efficiency. Granted I’m using QuickTime here, which I can only assume offloads video decode to Intel’s video engine. The video playback story looks better than it did on Microsoft’s Surface Pro, but it’s still not great at all. Modern ARM based SoCs have extremely low power video decoders integrated into the silicon. I wonder if Haswell’s video decode engine just isn’t as low power as what you can get in most ultra mobile SoCs today. Intel’s public documentation tends to focus on transcoding power efficiency relative to software based encode/decode, but not decode power efficiency alone.What This MeansWith Haswell ULT, Intel finally got its platform power story in order. Haswell ULT and, eventually, Haswell ULX platforms appear to have idle power characteristics that are at least within the range of high-end ARM based tablets. It’s finally possible to use Core in a tablet (the thermal considerations can be negated by going with a Y-series or even lower power SKU). Video decode power consumption remains a question in my mind. Assuming the results I saw weren’t due to software, I’d be willing to bet that video decode power efficiency becomes a target for improvement in Broadwell and/or Skylake.Microsoft Surface Pro (left) vs. 4th gen iPad (right)There are obvious implications for the next-generation ofMicrosoft’s Surface Pro. It’s unclear whether Microsoft will wait until Broadwell to reduce the thickness of Surface Pro, or if it’ll go with a Y-series Haswell ULX part this year and release something that’s much thinner immediately. The challenges Microsoft would face there are similar to those Apple faced with the 2013 MacBook Air, namely Microsoft would have to be accepting of a CPU performance regression but a significant improvement in battery life (and form factor). Broadwell should deliver (some of) the best of both worlds, but that’s another year/generation of waiting for Microsoft.What about Apple? I am not convinced that Apple would leave the intersection of iPad and MacBook Air alone. Tablets are under heavy pricing pressure, and Apple itself has established upper bounds to iPad pricing. As the world continues to shift towards tablets and lower cost/margin computing devices, Apple needs a solution to keep ASPs high. With iPad sales shifting to the mini, a higher end convergence solution between (replacing?) the iPad and 11-inch MBA might not be a bad idea.At this year’s WWDC, Apple made it very clear that idle power optimizations were high on the list for OS X Mavericks. Reducing the number of CPU cycles used by active but visually occluded application windows, and putting idle applications in a nap mode. These optimizations obviously benefit the Mac notebook lineup, but they’re also very important should Apple try to build a Surface Pro competitor.The platform could run OS X with a modified launchpad in tablet mode, or the standard OS X desktop in docked mode. Perhaps I’m just projecting Windows 8/Surface onto Apple, but I feel like the possibility is there.Final WordsIf you look at the first Haswell ULT systems, they generally don’t appear all that different from the Ivy Bridge ULV systems that came before them. The biggest change is a tremendous increase in battery life, due to idle power platform optimizations, but in terms of functionality they’re largely unchanged. This brings two thoughts to mind:The first is that Haswell ULT will ultimately do nothing to change the current trajectory of the PC industry. The problem isn’t in the silicon (for the most part), but rather in the traditional implementation of the silicon by Intel’s OEM partners. From Apple to the army of Ultrabook OEMs, Haswell ULT has only been used to enable good ultraportable notebooks and nothing more exotic. Companies invested in a return to growth in the PC industry won’t find it as a result of Haswell ULT. The question you should be asking instead is how much worse would things have been had Haswell ULT not been as good as it is.The second is that the best has yet to come. I have high hopes for the second generation of Microsoft’s Surface Pro. Microsoft could build the second generation into a true convergence device that further blurs the lines between tablet and productivity notebook. For the first time in quite a while Microsoft could have a product that shows significant improvement year over year, for multiple years in a row. The first Surface Pro was good, a Haswell ULT/ULX based device could really make the experience more tablet-like and a Broadwell ULT/ULX successor could make it even thinner.The next few generations won’t be a walk in the park for Intel however. There’s a ton of catching up to do. Just because Intel now has a single-chip Haswell SoC solution doesn’t mean that Intel and the ARM ecosystem are at parity in terms of capabilities. Qualcomm is quick to point out that the CPU island in its Snapdragon SoCs can be around 15% of the total die area, the rest of the SoC being devoted to GPU, ISP, video encode/decode, connectivity, etc... While I don’t expect a high-end Core based SoC to be only 15% CPU cores, I do expect that Intel will need to integrate similarly high-quality, high-performance and low power IP blocks in its flagship silicon. At this point all we’ve established is that on a largely CPU driven workload that Haswell ULT can be competitive (from a power efficiency standpoint) with a high-end ARM based SoC. The video playback results alone point out there’s so much more to the story that matters.The OS vendors have to similarly make sure they're adequately prepared for this transition. The name of the game is making all usage appear as idle as possible. We've seen improvements along this front in Windows 8, and promised in OS X Mavericks. To blur the lines between tablet and notebook hardware, you need to do the same between tablet and notebook OSes.Intel prepping its Core family of microarchitectures for low power tablet duty matters quite a bit to notebook OEMs. I don’t believe the computing world will top out at $499, but I do believe that any solution above $499 will have to be something more unique than just a really thin notebook. In my Surface Pro review I talked about that device being a tablet that could serve as a notebook, or as a desktop when docked to a large display/kb/mouse. I’m not much of a visionary, but I feel like such a flexible device might not be a bad idea.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7117/haswell-ult-investigation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The ARM Diaries, Part 2: Understanding the Cortex A12\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2013-07-17T16:30:00Z\n",
      "URL: https://www.anandtech.com/show/7126/the-arm-diaries-part-2-understanding-the-cortex-a12\n",
      "Content: A couple of weeks agoI began this series on ARM with a discussion of the company’s unique business model. In covering semiconductor companies we’ve come across many that are fabless, but it’s very rare that you come across a successful semiconductor company that doesn’t even sell a chip. ARM’s business entirely revolves around licensing IP for its instruction set as well as its own CPU (and now GPU and video) cores.Before we get into discussions of specific cores, it’s important to talk about ARM’s portfolio as a whole. In the PC space we’re used to focusing on Intel’s latest and greatest microarchitectures, which are then scaled in various ways to hit lower price targets. We might see different core counts, cache sizes, frequencies and maybe even some unfortunate instruction set tweaking but for the most part Intel will deliver a single microarchitecture to cover the vast majority of the market. These days, this microarchitecture is simply known as Core.Back in 2008, Intel introduced a second microarchitecture under the Atom brand to target lower cost (and lower power) markets. The combination of Atom and Core spans the overwhelming majority of the client computing market for Intel. The prices of these CPUs range from the low double digits with Atom to many hundreds of dollars for the highest end Core processors (the most expensive desktop Haswell is $350, however mobile now extends up to $1100). There are other designs that target servers (which are then repurposed for ultra high-end desktops), but those are beyond the scope of this discussion for now.If we limit our discussion to personal computing devices (smartphones, tablets, laptops and desktops), where Intel uses two microarchitectures ARM uses three. The graphic below illustrates the roadmap:You need to somewhat ignore the timescale on the x-axis since those dates really refer to when ARM IP is first available to licensees, not when products are shipping to consumers, but you get an idea for the three basic vectors of ARM’s Cortex A-series of processor IP. Note that there are also Cortex R (embedded) and Cortex M (microcontroller) series of processor IP offered as well, but once again those are beyond the scope of our discussion here.If we look at currently available cores, there’s the Cortex A15 on the high end, Cortex A9 for the mainstream and Cortex A7 for entry/low cost markets. If we’re to draw parallels with Intel’s product lineup, the Cortex A15 is best aligned with ultra low power/low frequency Core parts (think Y-series SKUs), while the Cortex A9 vector parallels Atom. Cortex A7 on the other hand targets a core size/cost/power level that Intel doesn’t presently address. It’s this third category labeled high efficiency above that Intel doesn’t have a solution for. This answers the question of why ARM needs three microarchitectures while Intel only needs two: in mobile, ARM targets a broader spectrum of markets than Intel.Dynamic RangeIf you’ve read any of our smartphone/tablet SoC coverage over the past couple of years you’ll note that I’m always talking about an increasing dynamic range of power consumption in high-end smartphones and tablets. Each generation performance goes up, and with it typically comes a higher peak power consumption. Efficiency improvements (either through architecture, process technology or both) can make average power in a reasonable workload look better, but at full tilt we’ve been steadily marching towards higher peak power consumption regardless of SoC vendor. ARM provided a decent overview of the CPU power/area budget as well as expected performance over time of its CPU architectures:Looking at the performance segment alone, we’ll quickly end up with microarchitectures that are no longer suited for mobile, either because they’re too big/costly or they draw too much power (or both).The performance vector of ARM CPU IP exists because ARM has its sights set higher than conventional smartphones. Starting with the Cortex A57, ARM hopes to have a real chance in servers (and potentially even higher performance PCs, Windows RT and Chrome OS being obvious targets).Although we see limited use of ARM’s Cortex A15 in smartphones today (some international versions of the Galaxy S 4), it’s very clear that for most phones a different point on the power/performance curve makes the most sense.The Cortex A8 and A9 were really the ARM microarchitectures that drove smartphone performance over the past couple of years. The problem is that while ARM’s attentions shifted higher up the computing chain with Cortex A15, there was no successor to take the A9’s place. ARM’s counterpoint would be that Cortex A15 can be made suitable for lower power operation, however its partners (at least to date) seemed to be focused on extracting peak performance from the A15 rather than pursuing a conservative implementation designed for lower power operation. In many ways this makes sense. If you’re an SoC vendor that’s paying a premium for a large die CPU, you’re going to want to get the most performance possible out of the design. Only Apple seems to have embraced the idea of using die area to deliver lower power consumption.The result of all of this is that the Cortex A9 needed a successor. For a while we’d been hearing about a new ARM architecture that would be faster than Cortex A9, but lower power (and lower performance) than Cortex A15. Presently, the only architecture in between comes from Qualcomm in the form of some Krait derivative. For ARM to not let its IP licensees down, it too needed a solution for the future of the mainstream smartphone market. Last month we were introduced to that very product: ARM’s Cortex A12.Slotting in numerically between A9 and A15, theinitial disclosureunfortunately didn’t come with a whole lot of actual information. Thankfully, we now have some color to add.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7126/the-arm-diaries-part-2-understanding-the-cortex-a12\n",
      "Title: The ARM Diaries, Part 1: How ARM’s Business Model Works\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2013-06-28T04:06:00Z\n",
      "URL: https://www.anandtech.com/show/7112/the-arm-diaries-part-1-how-arms-business-model-works\n",
      "Content: It must frustrate ARM just how much attention is given to Intel in the ultra mobile space, especially considering the chip giant’s effectively non-existent market share. Since 2008 Intel has tried, year after year, to break into smartphones and tablets with very limited success. Despite having the IP and technical know-how to do so, it wasn’t until 2012 that we saw Intel act like a company with even a sliver of a chance. Today, things are finally starting to change. Intel’s 22nm SoC process and updated Atom microarchitecturelook very competitive, and we’ll see the first tablet products based on them later this year - with phones following sometime in early 2014. As Intel is about to start acting like a competitor, ARM is starting to talk a lot more about its magic.We’ve had well over a decade of Intel sharing its beliefs with us, but this is ARM’s first attempt at doing the same. What will follow over the next few posts are a bunch of disclosures, some related some not, attempting to bring everyone up to speed on where ARM is today and where ARM will be in the near future. The best place to start is with ARM’s business model.In the PC industry, the concept of a fabless semiconductor manufacturer isn’t unusual. NVIDIA has always been one, and now AMD is one as well. Fabless semiconductors create all of the designs for their chips, but they’re physically manufactured at a foundry partner (e.g. TSMC, Global Foundries, Samsung). The fabless semi approach greatly helps reduce costs, but your designs are ultimately at the mercy of your foundry partner. Capacity, quality of process and timeline for process are more or less out of your control. Sometimes this is a non-issue, but other times it dramatically impacts your ability to bring products to market (e.g. quality control for early TSMC 40nm, timeline for GF 28nm or early capacity for TSMC 28nm).ARM goes one step beyond the fabless semi: it doesn’t even sell any chips into the marketplace. ARM instead, designs IP (instruction set architecture, microprocessor, graphics, interconnects) and licenses it to anyone who wants to use it. ARM’s customers will then take the IP they’ve licensed and design it into silicon. These customers can be fabless semiconductor companies or companies that own fabs.It’s a very unique business model, especially if you compare it to that of the market share leader in the PC silicon space (Intel). From Intel’s perspective, it made the mistake of licensing the x86 ISA early on in its life, but quickly retreated from that business. It instead builds its own architectures, designs them into chips for various markets, and manufactures the designs at its own foundries. Intel is a truly vertically integrated chip design and fabrication house. It’s a lot of work, but Intel is rewarded by having extremely high margins on all of its products.The ultra mobile world is very different, at least today. In the PC world, Intel drives platform definition and ends up being the biggest part of the BoM (Bill of Materials) as a result. In smartphones and tablets, the main applications processor is easily under 10% of the cost of the device. More often than not, we’re talking about low single digit percentages of the total BoM (e.g. $15 SoC for a $400 device, or 3.75%). Intel’s theory is that this will eventually change as silicon complexity increases inside ultra mobile devices, but until now (and likely for the near future) the market requires/enables a different sort of business model.How ARM WorksThe ARM business model is incredibly simple to understand, it’s just different than what we’re used to in the PC space. At a high level, ARM offers three different types of licenses: POP, processor and architecture.A processor license is the license to use a microprocessor or GPU that ARM has designed. You can’t really change the design, but you get to implement it however you’d like. For example, Samsung’s Exynos 5 Octa implements four ARM Cortex A7 cores and four ARM Cortex A15 cores - these are processor licenses. ARM will provide guidelines as to how to implement these designs in silicon, but ultimately it’s up to you and your physical implementation teams to do so and get good frequency/power out of your design.A processor optimization pack (POP), takes a processor license to the next level. If you aren’t great at physical implementations, ARM will sell you an optimized processor design that you can take and manufacture at a specific foundry which will result in some degree of guaranteed performance. If you look at what happened with the Cortex A8, Apple and Samsung had their own physical implementations of the core that resulted in better frequency/power than a lot of other designs. Apple and Samsung had access toIntrinsitywho hardened the Cortex A8 design, but not all companies had the bandwidth/budget to do the same. POPs are ARM’s equivalent solution for those customers who need very good implementations but can’t do so by themselves. POPs are available for various processor/foundry/process node combinations. For example, ARM offers a 28nm HPM POP at TSMC for the Cortex A12.The final option is an architecture license. Here, ARM would license you one of its architectures (e.g. ARMv7, ARMv8) and you’re free to take that architecture and implement it however you’d like. This is what Qualcomm does to build Krait, and what Apple did to build Swift. These microprocessors are ISA compatible with ARM’s Cortex A15 for example, but they are their own implementations of the ARM ISA. Here you basically get a book and a bunch of tests to verify compliance with the ARM ISA you’re implementing. ARM will offer some support to help you with your design, but it’s ultimately up to you to design, implement and validate your own microprocessor design.In terms of numbers, ARM has around 1000 licenses in the market spread across 320 licensees/partners. Of those 320 licensees, only 15 of them have architecture licenses.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7112/the-arm-diaries-part-1-how-arms-business-model-works\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM Cortex A12: Between Cortex A9 and A15 in Power & Perf, Sampling in Late 2014\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2013-06-03T03:31:00Z\n",
      "URL: https://www.anandtech.com/show/7009/arm-cortex-a12-between-cortex-a9-and-a15-in-power-perf-sampling-in-late-2014\n",
      "Content: We’ve talked about the hole in ARM’s product lineup for quite a while now. The Cortex A9 is too slow to compete with the likes of Intel’s Atom and Qualcomm’s Krait 200/300 based SoCs. The Cortex A15 on the other hand outperforms both of those solutions, but at considerably higher power and die area requirements. The slide below from Samsung illustrates my point clearly:The comparison point here is the Cortex A15 and Cortex A7, but the latter should be quite performance competitive with a Cortex A9 so the comparison is still relevant. The Cortex A15 island in Samsung’s Exynos 5 Octa occupies 5x the die area as the A7 island, and consumes nearly 6x the power. In exchange for 5x the area and 6x the performance, the Cortex A15 offers under 4x the performance. It’s not exactly an area or power efficient solution, but a great option for anyone looking to push the performance envelope.Today, ARM is addressing that hole with the Cortex A12.This announcement isn’t a deep architectural disclosure, but we do have some high level details to share. Like AMD’s Jaguar, Intel’s Silvermont and even ARM’s A9, the Cortex A12 is a dual-issue out-of-order architecture. Unlike the Cortex A9, the Cortex A12 is fully out-of-order including the NEON/FP units (NEON/FP was in-order on Cortex A9).Pipeline length increased a bit compared to Cortex A9 (11 stages), however ARM told me to expect similar frequencies to what we have with the Cortex A9.The execution back end has also been improved, although I don’t have many details as to how. My guess is we should expect something a bit wider than Cortex A9 but not nearly as wide as Cortex A15.Memory performance is much improved compared to Cortex A9 as well, which we’ve already demonstrated as a significant weak point in the A9 architecture.All of the architectural enhancements are supposed to provide up to a 40% increase in performance (IPC) over Cortex A9 at the same frequency and process node. ARM isn’t talking power, other than to say that it can do the same workload at the same power as a Cortex A9. In order words, Cortex A12 should have higher power than Cortex A9 but faster execution reduces total energy consumed. With a higher max power we’ll see more dynamic range in power consumption, but just not nearly as much as with the Cortex A15.Cortex A12 also adds support for 40-bit memory addressability, an intermediate solution before we get to 64-bit ARMv8 based architectures. Finally, Cortex A12 features the same ACE bus interface as Cortex A7/A15 and can thus be used in big.LITTLE configurations with either core (but likely exclusively with the A7s). Given the lower power profile of Cortex A12, I'm not sure the complexity of doing a big.LITTLE implementation will be worth it though.ARM expects the Cortex A12 to be used in mainstream smartphones and tablets where cost and power consumption are a bit more important. The design makes a lot of sense, the only downside is its launch timeframe. ARM expects to be sampling Cortex A12 in late 2014 with the first devices showing up in 2015.Update: ARM clarified that SoCs based on Cortex A12 would be shipping to device vendors in mid-2014, with devices shipping to consumers by late 2014 to early 2015. ARM has optimized Cortex A12 processor packs at both Global Foundries (28nm SLP) and TSMC (28nm HPM).\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7009/arm-cortex-a12-between-cortex-a9-and-a15-in-power-perf-sampling-in-late-2014\n",
      "Title: ARM Mali-T622 & V500 Video Block Complement Cortex A12\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2013-06-03T03:30:00Z\n",
      "URL: https://www.anandtech.com/show/7010/arm-malit622-v500-video-block-complement-cortex-a12\n",
      "Content: Alongsidetoday's Cortex A12 announcement, ARM is also announcing two new IP blocks: the Mali-T622 GPU and the Mali V500 video encode/decode block.The Mali-T622 is a 2-core implementation of the2nd generation Mali-T600 GPU architecture that we first learned about with the 8-core T628. Each shader core features two ALUs, an LSU and a texture unit.On the video front, the Mali-V500 video encode/decode block is a multi-core engine used for all video acceleration. The V500 allegedly supports up to 100Mbps High Profile H.264, although details are scarce on more specifics. ARM claims support for up to 120 fps 4K video decode with an 8-core V500 implementation. Mali-V500 also features a protected video path, necessary for gaining content owner support for high-bitrate/high-resolution video decode.The V500 also supports ARM's Frame Buffer Compression (AFBC), a lossless compression algorithm that can supposedly reduce memory bandwidth traffic by up to 50%. There's presently no frame buffer compression in Mali GPUs today, but ARM expects to eventually roll AFBC out to Mali GPUs as well.Gallery:ARM Mali-T622 & V500 Video Block Complement Cortex A12\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/7010/arm-malit622-v500-video-block-complement-cortex-a12\n",
      "Title: Exploring the Floating Point Performance of Modern ARM Processors\n",
      "Author: Rahul Garg\n",
      "Date Published: 2013-06-02T18:30:00Z\n",
      "URL: https://www.anandtech.com/show/6971/exploring-the-floating-point-performance-of-modern-arm-processors\n",
      "Content: As a programmer who wants to write decent performing code, I am very interested in understanding the architectures of CPUs and GPUs. However, unlike desktop and server CPUs, mobile CPU and GPU vendors tend to do very little architectural disclosure - a fact that we've been working hard to change over the past few years. Often times all that's available are marketing slides with fuzzy performance claims. This situation frustrates me to no end personally. We've done quite a bit of low-level mobile CPU analysis at AnandTech in pursuit of understanding architectures where there is no publicly available documentation. In this spirit, I wrote a few synthetic tests to better understand the performance of current-gen ARM CPU cores without having to rely upon vendor supplied information. For this article I'm focusing exclusively on floating point performance.We will look at 5 CPU cores today: the ARM Cortex A9, ARM Cortex A15, Qualcomm Scorpion, Qualcomm Krait 200 and Qualcomm Krait 300. The test devices are listed below.Devices testedDeviceOSSoCCPUFrequencyNumber of coresSamsung Galaxy SIIX (T989D)Android 4.0Qualcomm APQ8060Scorpion1.5GHz2Boundary devices BD-SL-i.mx6Ubuntu OneiricFreescale i.mx6Cortex-A91.0GHz4Blackberry Z10Blackberry 10 (10.1)Qualcomm MSM8960Krait 2001.5GHz2Google Nexus 10Android 4.2.2Samsung Exynos 5250Cortex-A151.7GHz2HTC OneAndroid 4.1.2Qualcomm Snapdragon 600Krait 3001.7GHz4I wanted to test the instruction throughput of various floating point instructions. I wrote a simple benchmark consisting of a loop with a large number of iterations. The loop body consisted of many (say 20) floating point instructions with no data dependence between them. The tests were written in C++ with gcc NEON intrisincs where required, and I always checked the assembler to verify that the generated assembly was as expected. There were no memory instructions inside the loop and thus memory performance was not an issue. There were minimal dependencies in the loop body. I tested the performance of scalar addition, multiplication and multiply-accumulate for 32-bit and 64-bit floating point datatypes. All the tested ARM processors also support the NEON instruction set, which is a SIMD (single instruction multiple data) instruction set for ARM for integer and floating point operations. I tested the performance of 128-bit floating point NEON instructions for addition, multiplication and multiply-accumulate.Apart from testing throughput of individual instructions, I also wrote a test for testing throughput of a program consisting of two types of instructions: scalar addition and scalar multiplication instructions. The instructions were interleaved, i.e. the program consisted of an addition followed by a multiply, followed by another add, then another multiply and so on. There were no dependencies between the additions and following multiplies. You may be wondering the reasoning behind this mixed test. Some CPU cores (such as AMD's K10 core) have two floating point units but the two floating point units may not be identical. For example, one floating point unit may only support addition while another may only support multiplication. Thus, if we only test the additions and multiplications separately, we will not see the peak throughput on such a machine. We perform the mixed test to identify such cases.All the tests mentioned above measure the amount of time taken for a particular number of instructions and thus we get the instructions executed per-second. We also need to know the frequency to get the instructions executed per-cycle. Knowing the peak frequency of the device is not enough because CPUs have multiple frequency states and the tests may not be running at the advertised peak speeds. Thus, I also wrote code to monitor the percentage of time spent in each frequency state as reported by the kernel. The frequency was calculated as the average of the frequency states weighted by percentage of time spent in each state. The observed frequency on Scorpion (APQ8060) , Cortex A9 (i.mx6) and Cortex A15 (Exynos 5250) were 1.242 GHz, 992MHz and 1.7GHz respectively on all tests except where noted in the results below.However, as it turns out, the method I used for measuring the time spent in each frequency state does not work on aSMP designs like the Krait 200 based Snapdragon S4 and Krait 300 based Snapdragon 600. For Krait 200, the results reported here are for MSM8960 which shouldn't really have thermal throttling issues. My results on the MSM8960 also line up quite neatly with the assumption that the CPU spent most or all of its time in the test in the peak frequency state. Brian also ran the test on a Nexus 4 and the results were essentially identical as both have the same peak, which is additional confirmation that our results are likely correct. Thus I will assume a frequency of 1.5 GHz while discussing Krait 200 results. Results on Krait 300 (Snapdragon 600) however are more mixed. I am not sure if it is reaching peak frequency on all the tests and thus I am less sure of the per-cycle estimates on this chip. Brian also ran the tests on another handset (LG Optimus G Pro) with the same Snapdragon 600, and the results were qualitatively very similar.Now the results. First up, the raw data collected from the tests in gigaflops:Performance of each CPU in GFlops on different testsScorpion(APQ8060)Cortex-A9(i.mx6)Krait 200(MSM8960)Cortex-A15(Exynos 5250)Krait 300(Snapdragon 600)Add (fp64)1.230.991.331.55 @ 1.55 GHz1.6Add (fp32)1.190.991.461.691.72Mul (fp64)0.610.501.481.691.72Mul (fp32)1.220.991.491.691.72Mixed (fp64)0.820.991.481.631.72Mixed (fp32)1.230.991.471.691.72MAC (fp64)1.230.991.483.352.65MAC (fp32)2.471.981.473.393.13Add (fp32 NEON)4.941.995.866.776.89Mul (fp32 NEON)4.891.995.766.776.89MAC (fp32 NEON)9.883.985.9113.5512.5Before we discuss the results, it is important to keep in mind that the results and per-cycle timing estimates reported are what I observed from the tests. I did my best to ensure that the design of the tests was very conducive to achieving high throughput. However, it is possible there may be some cases where an architecture can achieve higher performance than what what I was able to get out of my tests. With that out of the way, lets look at the results.In the data, we need to distinguish between number of instructions and number of flops. I count scalar addition and multiply as one flop and scalar MACs as two flops. I count NEON addition and multiply as four flops and NEON MACs are counted as eight flops. Thus, we get the following per-cycle instruction throughput estimates:Estimated floating point instruction throughput per cycleScorpionCortex A9Krait 200Cortex A15Krait 300Add (fp64)11111Add (fp32)11111Mul (fp64)1/21/2111Mul (fp32)11111Mixed (fp64)2/31111Mixed (fp32)11111MAC (fp64)1/21/21/217/9MAC (fp32)111/2110/11Add (fp32 NEON)11/2111Mul (fp32 NEON)11/2111MAC (fp32 NEON)11/21/2110/11We start with the Cortex A9. Cortex A9 achieves throughput of 1 operation/cycle for most scalar instructions, except for fp64 MUL and fp64 MAC, which can only be issued once every two cycles. The mixed test reveals that though fp64 muls can only be issued every two cycles, Cortex A9 can issue a fp64 add in the otherwise empty pipeline slot. Thus, in the mixed test it was able to achieve throughput of 1 instruction/cycle. NEON implementation in Cortex A9 has a 64-bit datapath and all NEON instructions take 2 cycles. Qualcomm's Scorpion implementation of scalar implementations is similar to Cortex A9 except that it seems unable to issue fp64 adds immediately after fp64 muls in the mixed test. Scorpion uses a full 128-bit datapath for NEON and has twice the throughput of Cortex A9.Krait 200 features an improved multiplier, and offers 1 instruction/cycle throughput for most scalar and NEON instructions. Interestingly, Krait 200 has half the per-cycle throughput for MAC instructions, which is a regression compared to Scorpion. Krait 300 improves the MAC throughput compared to Krait 200, but still appears to be unable to reach throughput of 1 instruction/cycle possibly revealing some issues in the pipeline. An alternate explanation is that Snapdragon 600 reduced the frequency in the MAC tests for some unknown reason. Without accurate frequency information, currently it is difficult to make that judgment. Cortex A15 is the clear leader here, and offers throughput of 1 FP instruction/cycle in all our tests.In the big picture, readers may want to know how the the floating point capabilities of these cores compares to x86 cores. I consider Intel's Ivy Bridge and Haswell as datapoints for big x86 cores, and AMD Jaguar as a datapoint for a small x86 core. For double-precision (fp64), current ARM cores appear to be limited to 2 flops/cycle for FMAC-heavy workloads and 1 flops/cycle for non-FMAC workloads. Ivy Bridge can have a throughput of up to 8 flops/cycle and Haswell can do 16 flops/cycle with AVX2 instructions. Jaguar can execute up to 3 flops/cycle. Thus, current ARM cores are noticeably behind in this case. Apart from the usual reasons (power and area constraints, very client focused designs), current ARM cores also particularly lag behind in this case because currently NEON does not have vector instructions for fp64. ARMv8 ISA adds fp64 vector instructions and high performance implementations of the ISA such as Cortex A57 should begin to reduce the gap.For fp32, Ivy Bridge can execute up to 16 fp32 flops/cycle, Haswell can do up to 32 fp32 flops/cycle and AMD's Jaguar can perform 8 fp32 flops/cycle. Current ARM cores can do up to 8 flops/cycle using NEON instructions. However, ARM NEON instructions are not IEEE 754 compliant, whereas SSE and AVX floating point instructions are IEEE 754 compliant. Thus, comparing flops obtained in NEON instructions to SSE instructions is not apples-to-apples comparison. Applications that require IEEE 754 compliant arithmetic cannot use NEON but more consumer oriented applications such as multimedia applications should be able to use NEON. Again, ARMv8 will fix this issue and will bring fully IEEE 754-compliant fp32 vector instructions.To conclude, Cortex A15 clearly leads amongst the CPUs tested today with Krait 300 very close behind. It is also somewhat disappointing that none of the CPU cores tested displayed a throughput of more than 1 FP instruction/cycle in these tests. I end at a cautionary note that the tests here are synthetic tests that only stress the FP units. Floating point ALU peaks are only a part of a microarchitecture. Performance of real-world applications will depend upon rest of the microarchitecture such as cache hierarchy, out of order execution capabilities and so on. We will continue to make further investigations into these CPUs to understand them better.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/6971/exploring-the-floating-point-performance-of-modern-arm-processors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: More Details On NVIDIA’s Kayla: A Dev Platform for CUDA on ARM\n",
      "Author: Ryan Smith\n",
      "Date Published: 2013-03-20T00:00:00Z\n",
      "URL: https://www.anandtech.com/show/6847/more-details-on-nvidias-kayla-a-dev-platform-for-cuda-on-arm\n",
      "Content: In this morning’s GTC 2013 keynote, one of the items briefly mentioned by NVIDIA CEO Jen-Hsun Huang was Kayla, an NVIDIA project combining a Tegra 3 processor and an unnamed GPU on a mini-ITX like board. While NVIDIA is still withholding some of the details of Kayla, we finally have some more details on just what Kayla is for.The long and short of matters is that Kayla will be an early development platform for running CUDA on ARM. NVIDIA’s first CUDA-capable ARM SoC will not arrive until 2014 with Logan, but NVIDIA wants to get developers started early. By creating a separate development platform this will give interested developers a chance to take an early look at CUDA on ARM in preparation for Logan and other NVIDIA products using ARM CPUs, and start developing their wares now.As it stands Kayla is a platform whose specifications are set by NVIDIA, with ARM PC providers building the final systems. The CPU is a Tegra 3 processor – picked for its PCI-Express bus needed to attach a dGPU – while the GPU is a Kepler family GPU that NVIDIA is declining to name at this time. Given the goals of the platform and NVIDIA’s refusal to name the GPU, we suspect this is a new ultra low end 1 SMX (192 CUDA core) Kepler GPU, but this is merely speculation on our part. There will be 2GB of RAM for the Tegra 3, while the GPU will come with a further 1GB for itself.Update:PCGamesHardware has a picture of a slide from a GTC sessionlisting Kayla's GPU as having 2 SMXes. It's definitely not GK107, so perhaps a GK107 refresh?The Kayla board being displayed today is one configuration, utilizing an MXM slot to attach the dGPU to the platform. Other vendors will be going with PCIe, using mini-ITX boards. The platform on the whole is in the 10s of watts - but of course NVIDIA is quick to point out that Logan itself will be an order of magnitude less, thanks in part to the advantages conferred by being an SoC.NVIDIA was quick to note that Kayla is a development platform for ARM on CUDA as opposed to calling it a development platform for Logan; though at the same it unquestionably serves as a sneak-peak for Logan. This is in big part due to the fact that the CPU will not match what’s on Logan – Tegra 4 already is beyond Tegra 3 with its A15 CPU cores – and it’s unlikely that the GPU is an exact match either. Hence the focus on early developers, who are going to be more interested in making it work than the specific performance the platform provides.It’s interesting to note that NVIDIA is not only touting Kayla’s CUDA capabilities, but also the platform’s OpenGL 4.3 capabilities. Because Kayla and Logan are Kepler based, the GPU will be well ahead of OpenGL ES 3.0 with regards to functionality. Tessellation, compute shaders, and geometry shaders are present in OpenGL 4.3, among other things, reflecting the fact that OpenGL ES is a far more limited API than full OpenGL. This means that NVIDIA is shooting right past OpenGL ES 3.0, going from OpenGL ES 2.0 with Tegra 4 to OpenGL 4.3 with Logan/Kayla. This may also mean NVIDIA intends to use OpenGL 4.3 as a competitive advantage with Logan, attracting developers and users looking for a more feature-filled SoC than what current OpenGL ES 3.0 SoCs are slated to provide.Wrapping things up, Kayla will be made available in the spring of this year. NVIDIA isn’t releasing any further details on the platform, but interested developers can go sign up to receive updates over atNVIDIA’s Developer Zone webpage.On a lighter note, for anyone playing NVIDIA codename bingo, we’ve figured out why the platform is called Kayla. Jen-Hsun called Kayla “Logan’s girlfriend”, and it turns out he wasbeing literal. So in keeping with their SoC naming this is another superhero-related name.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/6847/more-details-on-nvidias-kayla-a-dev-platform-for-cuda-on-arm\n",
      "Title: Calxeda's ARM server tested\n",
      "Author: Johan De Gelas\n",
      "Date Published: 2013-03-12T23:14:00Z\n",
      "URL: https://www.anandtech.com/show/6757/calxedas-arm-server-tested\n",
      "Content: ARM based servers hold the promise of extremely low power and excellent performance per Watt ratios. It's theoretically possible to place an incredible number of servers into a single rack; there are already implementations with as many as 1000 ARM servers in one rack (48 server nodes in a 2U chassis). What's more, all of those nodes consume less than 5KW combined (or around 5W per quad-core ARM node). But whenever a new technologyis hyped, it's important to remain objective. The media loves to rave about new trends and people like reading about \"some new thing\"; however, at the end of the day the system administrator has to keep his IT services working and convince his boss to invest in new technologies.At first sight, the relatively low performance per core of ARM CPUs seems like a bad match for servers. The dominant CPU in the server market is without doubt Intel's Xeon. The success of the Xeon family is largely rooted in its excellent single-threaded (or per core) performance at moderate power levels (70-95W). Combine this exceptional single-threaded performance with a decent core count and you get good performance in almost any kind of application. Economies of scale and the resulting price levels are also very important, but the server market has been more than willing to pay a little extra if the response times are lower and the energy bills moderate.A data point proving that single-threaded performance is still important is the evolution of the T-series of Oracle (or Sun if you prefer). The Sun T3 had 16 cores with 128 threads; the T4 however had only 8 cores with 8 threads each, and CEO Larry Ellison touted more than once that single-threaded performance was massively improved, up to five times faster. Do we really need another server with a flock of slow but energy efficient cores? Has history not taught us that a few \"bulls\" is better than \"a flock of chickens\"?History has also shown that the amount of memory per server is very important. Many HPC and virtualization applications are limited by the amount of RAM. The current Cortex-A9 generation of ARM CPUs has a 32-bit address bus and does not support more than 4GB.And yet, the interest in ARM-based servers is growing, and there is more to it than just hype. Yes, ARM-based CPUs still lack the number crunching power and the massive amount of DIMM slots that Xeon's memory controller can handle, but ARM CPUs score extremely well when it comes to cost and power consumption.ARM based CPU have also made giant steps forward when it comes to performance. To give you a few data points: a dual ARM Cortex-A9 at 1.2GHz (Samsung Exynos 1.2GHz) introduced in 2011 compresses more than 10 times faster than the typical ARM 11 based cores in 2008. The SunSpider performance increased by a factor 20according to Anand's measurements on the iPhones(though part of that is almost certainly thanks to browser and software optimizations). The latest ARM Cortex-A15 is again quite a bit more powerful, offering about 50% higher performance. The A57 will add 64-bit support and is estimated todeliver 20 to 30% higher performance. In short, the single-threaded performance is increasing quickly, and the same is true for the amount of RAM that can be addresssed. The ARM Cortex-A9 is limited to 4GB but the Cortex-A15 should be able to address 16GB while the A57 will be able to address a lot more.It is likely just a matter of time before ARM products can start to chip away at segments of the server market. How much time? The best way to find out is to look at the most mature ARM server shipping today: the Calxeda basedBoston Viridis. Just what can this server handle today, where does it have the potential to succeed, and what are its shortcomings? Let's find out.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/6757/calxedas-arm-server-tested\n",
      "Title: Samsung's Exynos 5 Octa: Powered by PowerVR SGX 544MP3, not ARM's Mali\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2013-01-14T02:09:00Z\n",
      "URL: https://www.anandtech.com/show/6654/samsungs-exynos-5-octa-powered-by-powervr-sgx-544mp3-not-arms-mali\n",
      "Content: At CES, Samsung announced itsExynos 5 Octa SoCfeaturing four ARM Cortex A7s and four ARM Cortex A15s. Unusually absent from the announcement was any mention of the Exynos 5 Octa's GPU configuration. Given that the Exynos 5 Dual featured an ARM Mali-T604 GPU, we only assumed that the 4/8-core version would do the same. Based on multiple sources, we're now fairly confident in reporting that the with the Exynos 5 Octa Samsung included a PowerVR SGX 544MP3 GPU running at up to 533MHz.The PowerVR SGX 544 is a lot like the 543 used in Apple's A5/A5X, however with the addition of DirectX 10 class texturing hardware and 2x faster triangle setup. There are no changes to the unified shader ALU count. Taking into account the very aggressive max GPU frequency, peak graphics performance of the Exynos 5 Octa should be between Apple's A5X and the A6X (assuming Samsung's memory interface is just as efficient as Apple's):Mobile SoC GPU ComparisonPowerVR SGX 543MP2PowerVR SGX 543MP4PowerVR SGX 544MP3PowerVR SGX 554MP4Used InA5A5XExynos 5 OctaA6XSIMD NameUSSE2USSE2USSE2USSE2# of SIMDs8161232MADs per SIMD4444Total MADs326448128GFLOPS @ Shipping Frequency16.0 GFLOPS32.0 GFLOPS51.1 GFLOPS71.6 GFLOPSIt's good to see continued focus on GPU performance by the major SoC vendors, although I'd like to see a device ship with something faster than Apple's highest end iPad. At the show we heard that we might see this happen in the form of an announcement in 2013, with a shipping device in 2014.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/6654/samsungs-exynos-5-octa-powered-by-powervr-sgx-544mp3-not-arms-mali\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The ARM vs x86 Wars Have Begun: In-Depth Power Analysis of Atom, Krait & Cortex A15\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2013-01-04T12:32:00Z\n",
      "URL: https://www.anandtech.com/show/6536/arm-vs-x86-the-real-showdown\n",
      "Content: Late last month,Intel dropped by my officewith a power engineer for a rare demonstration of its competitive position versus NVIDIA's Tegra 3 when it came to power consumption. Like most companies in the mobile space, Intel doesn't just rely on device level power testing to determine battery life. In order to ensure that its CPU, GPU, memory controller and even NAND are all as power efficient as possible, most companies will measure power consumption directly on a tablet or smartphone motherboard.The process would be a piece of cake if you had measurement points already prepared on the board, but in most cases Intel (and its competitors) are taking apart a retail device and hunting for a way to measure CPU or GPU power. I describedhow it's done in the original article:Measuring power at the battery gives you an idea of total platform power consumption including display, SoC, memory, network stack and everything else on the motherboard. This approach is useful for understanding how long a device will last on a single charge, but if you're a component vendor you typically care a little more about the specific power consumption of your competitors' components.What follows is a good mixture of art and science. Intel's power engineers will take apart a competing device and probe whatever looks to be a power delivery or filtering circuit while running various workloads on the device itself. By correlating the type of workload to spikes in voltage in these circuits, you can figure out what components on a smartphone or tablet motherboard are likely responsible for delivering power to individual blocks of an SoC. Despite the high level of integration in modern mobile SoCs, the major players on the chip (e.g. CPU and GPU) tend to operate on their own independent voltage planes.A basic LC filterWhat usually happens is you'll find a standard LC filter (inductor + capacitor) supplying power to a block on the SoC. Once the right LC filter has been identified, all you need to do is lift the inductor, insert a very small resistor (2 - 20 mΩ) and measure the voltage drop across the resistor. With voltage and resistance values known, you can determine current and power. Using good external instruments (NI USB-6289) you can plot power over time and now get a good idea of the power consumption of individual IP blocks within an SoC.Basic LC filter modified with an inline resistorTheprevious articlefocused on an admittedly not too interesting comparison: Intel's Atom Z2760 (Clover Trail) versus NVIDIA's Tegra 3. After much pleading, Intel returned with two more tablets: a Dell XPS 10 using Qualcomm's APQ8060A SoC (dual-core 28nm Krait) and a Nexus 10 using Samsung's Exynos 5 Dual (dual-core 32nm Cortex A15). What was a walk in the park for Atom all of the sudden became much more challenging. Both of these SoCs are built on very modern, low power manufacturing processes and Intel no longer has a performance advantage compared to Exynos 5.Just like last time, I ensured all displays were calibrated to our usual 200 nits setting and ensured the software and configurations were as close to equal as possible. Both tablets were purchased at retail by Intel, but I verified their performance against our own samples/data and noticed no meaningful deviation. Since I don't have a Dell XPS 10 of my own, I compared performance to theSamsung ATIV Taband confirmed that things were at least performing as they should.We'll start with the Qualcomm based Dell XPS 10...\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/6536/arm-vs-x86-the-real-showdown\n",
      "Title: Qualcomm Announces S4 Play MSM8x26 and WTR2605 - Quad Core ARM Cortex A7\n",
      "Author: Brian Klug\n",
      "Date Published: 2012-12-06T00:27:00Z\n",
      "URL: https://www.anandtech.com/show/6498/qualcomm-announces-s4-play-msm8x26-and-wtr2605-quad-core-arm-cortex-a7\n",
      "Content: Yesterday, Qualcommannounced a new SoCfor its Snapdragon S4 Play category, the MSM8x26, and alongside it a new transceiver, WTR2605. The announcement was a little light on detail and I waited until confirmation of a few details, but now know more about these two new parts geared at the growing entry-level Chinese handset market.First off, MSM8x26 is a 28nm SoC consisting of four ARM Cortex A7 CPUs running at (1.2 GHz) alongside an Adreno 305 GPU. This is to my knowledge the first Qualcomm SoC using a Cortex A7 for CPU, previously we've seen a lot of Cortex A5 use at Qualcomm in parts like MSM8x25 (dual A5s), MSM8x25Q (quad A5s), and also onboard baseband as an optional AP for managing things like a router. MSM8x26 is the spiritual successor to MSM8x25Q, which was again quad core ARM Cortex A5s at 45nm with Adreno 203 graphics. MSM8x26 should bring a nice jump in performance on both CPU and GPU over that part, in addition to supporting 1080p video encode and decode, and support for 13 MP cameras. MSM8x26 will come in two flavors, 8226 with UMTS and TD-SCDMA, and 8626 with UMTS, CDMA, and TD-SDCMA, consistent with Qualcomm's part numbering scheme.The other part of the story is the new transceiver, WTR2605, whose name suggests a wafer-level package (W for wafer) and includes necessary improvements to accommodate dual SIM active and standby modes (DS-DS operation) popular in the entry level Chinese market MSM8x26 is geared at. I don't know anything further about the WTR2605 or how it compares in terms of RF ports to WTR1605L, which is Qualcomm's current flagship transceiver, but suspect it's an evolution of that design with changes to accommodate the dual SIM modes. We'll have a piece ready later in the week about WTR1605 and the state of Qualcomm's modem portfolio.Source:Qualcomm\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/6498/qualcomm-announces-s4-play-msm8x26-and-wtr2605-quad-core-arm-cortex-a7\n",
      "Title: Samsung Chromebook (XE303) Review: Testing ARM's Cortex A15\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2012-10-31T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/6422/samsung-chromebook-xe303-review-testing-arms-cortex-a15\n",
      "Content: Google announced theChrome OS project two years ago, and with it came the first Chromebook: the CR-48. The Chrome OS concept seemed revolutionary at the time. In 2010 we were well into the latest round of questioning whether today's PCs were fast enough. The Ultrabook revolution hadn't yet begun, and the iPad was starting to gain momentum. Capitalizing on the market being flooded with poor quality, yet affordable PC notebooks that still struggled with the same virus/malware issues they'd been facing for years, Google took the opportunity to attempt to revolutionize the PC OS.The Chrome OS desktopChrome OS was that attempt at a revolution. As an OS built around a web browser, Chrome OS offered many of the advantages that the Chrome browser itself brought to the table: sandboxing, guest mode and constant/painless updates. All user data is encrypted on the drive by default. Security was and remains a major feature of Chrome OS.Google's revolution extended to hardware as well. The Cr-48 notebook delivered a good keyboard, trackpad and solid state storage. Future Chromebooks would do the same. While the price points of these machines (<$500) kept ultra high resolution IPS displays out of the bill of materials, Google promised good build quality and solid state storage - two things you couldn't find in cheap notebooks of the time.The new Samsung ChromebookSince then, some of the traditional PC makers have woken up. Although confined to the $999+ price point, we're finally seeing attention paid to build quality, display quality and storage performance. Over the next couple of years there's going to be increased focus on bringing those premium features down to sub $700 price points.For Chrome OS and Google's Chromebooks to remain relevant, they also had to move down the pricing stack. With itsmost recent announcement, Google has done just that. The new Chromebook (Samsung XE303C12) is priced at $249, while maintaining much of what made its predecessors interesting.Even more interesting than its aggressive price point is the choice of SoC inside Google's new Chromebook:Samsung's Exynos 5 Dual, featuring two ARM Cortex A15 CPU cores. This move makes the new Chromebook the very first non-x86 machine to ship with Chrome OS. Given that I also happen to have a dual-core Atom based Chromebook from 2011, the new Exynos 5 based machine gave me a unique opportunity to get a preview of how ARM's next-generation CPU core would stack up against Atom.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/6422/samsung-chromebook-xe303-review-testing-arms-cortex-a15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM's Cortex A57 and Cortex A53: The First 64-bit ARMv8 CPU Cores\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2012-10-30T15:58:00Z\n",
      "URL: https://www.anandtech.com/show/6420/arms-cortex-a57-and-cortex-a53-the-first-64bit-armv8-cpu-cores\n",
      "Content: YesterdayAMD revealed that in 2014 it would begin production of its first ARMv8 based 64-bit Opteron CPUs. At the time we didn't know what core AMD would use, however today ARM helped fill in that blank for us with two new 64-bit core announcements: the ARM Cortex-A57 and Cortex-A53.You may have heard of ARM's Cortex-A57 under the codename Atlas, while A53 was referred to internally as Apollo. The two are 64-bit successors to the Cortex A15 and A7, respectively. Similar to their 32-bit counterparts, the A57 and A53 can be used independently or in abig.LITTLE configuration. As a recap, big.LITTLE uses a combination of big (read: power hungry, high performance) and little (read: low power, lower performance) ARM cores on a single SoC.By ensuring that both the big and little cores support the same ISA, the OS can dynamically swap the cores in and out of the scheduling pool depending on the workload. For example, when playing a game or browsing the web on a smartphone, a pair of A57s could be active, delivering great performance at a high power penalty. On the other hand, while just navigating through your phone's UI or checking email a pair of A53s could deliver adequate performance while saving a lot of power. A hypothetical SoC with two Cortex A57s and two Cortex A53s would still only appear to the OS as a dual-core system, but it would alternate between performance levels depending on workload.ARM's Cortex A57Architecturally, the Cortex A57 is much like a tweaked Cortex A15 with 64-bit support. The CPU is still a 3-wide/3-issue machine with a 15+ stage pipeline. ARM has increased the width of NEON execution units in the Cortex A57 (128-bits wide now?) as well as enabled support for IEEE-754 DP FP. There have been some other minor pipeline enhancements as well. The end result is up to a 20 - 30% increase in performance over the Cortex A15 while running 32-bit code. Running 64-bit code you'll see an additional performance advantage as the 64-bit register file is far simplified compared to the 32-bit RF.The Cortex A57 will support configurations of up to (and beyond) 16 cores for use in server environments. Based on ARM's presentation it looks like groups of four A57 cores will share a single L2 cache.ARM's Cortex A53Similarly, the Cortex A53 is a tweaked version of the Cortex A7 with 64-bit support. ARM didn't provide as many details here other than to confirm that we're still looking at a simple, in-order architecture with an 8 stage pipeline. The A53 can be used in server environments as well since it's ISA compatible with the A57.ARM claims that on the same process node (32nm) the Cortex A53 is able to deliver the same performance as a Cortex A9 but at roughly 60% of the die area. The performance claims apply to both integer and floating point workloads. ARM tells me that it simply reduced a lot of the buffering and data structure size, while more efficiently improving performance. From looking at Apple's Swift it's very obvious that a lot can be done simply by improving the memory interface of ARM's Cortex A9. It's possible that ARM addressed that shortcoming while balancing out the gains by removing other performance enhancing elements of the core.Both CPU cores are able to run 32-bit and 64-bit ARM code, as well as a mix of both so long as the OS is 64-bit.Completed Cortex A57 and A53 core designs will be delivered to partners (including AMD and Samsung) by the middle of next year. Silicon based on these cores should be ready by late 2013/early 2014, with production following 6 - 12 months after that. AMD claimed it would have an ARMv8 based Opteron in production in 2014, which seems possible (although aggressive) based on what ARM told me.ARM expects the first designs to appear at 28nm and 20nm. There's an obvious path to 14nm as well.It's interesting to note ARM's commitment to big.LITTLE as a strategy for pushing mobile SoC performance forward. I'm curious to see how the first A15/A7 designs work out. It's also good to see ARM not letting up on pushing its architectures forward.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/6420/arms-cortex-a57-and-cortex-a53-the-first-64bit-armv8-cpu-cores\n",
      "Title: AMD Will Build 64-bit ARM based Opteron CPUs for Servers, Production in 2014\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2012-10-29T20:48:00Z\n",
      "URL: https://www.anandtech.com/show/6418/amd-will-build-64bit-arm-based-opteron-cpus-for-servers-production-in-2014\n",
      "Content: Last year AMD officially became an ARM licensee, although the deal wasn't publicized at the time. Fast forward to June 2012 and we saw the first fruits of that deal: AMD announced it would integrateARM's Cortex A5 core into its 2013 APUs to enable TrustZone support.Today comes a much bigger announcement: AMD will be building Opteron processors based on a 64-bit ARM architecture. There are no product announcements today, but the 64-bit ARM Opterons will go into production in 2014. Today's announcement is about a processor license, not an ARM architecture license - in other words, AMD will integrate an ARM designed 64-bit core for this new Opteron.Update: AMD will integrateARM's new Cortex-A50 series of 64-bit ARMv8 CPU cores.The only other detail we know is that these ARM based Opterons will embedSeaMicro's Freedom Fabric, presumably on-die.AMD offering ARM based Opterons is really to target the microserver market. As for why AMD isn't using Jaguar for these parts, it's likely that by going with ARM it can lower the development time and cost to get into this market. The danger here is the total microserver market is expected to be around 10% of the overall server market, but that includes x86 + ARM. With x86 as the default incumbent, it's going to be an uphill battle for AMD/ARM to carve out a significant portion of that market.AMD was quick to mention that despite today's announcement, it will continue to build x86 CPUs and APUs for client and server markets.Overall the move sounds a lot like AMD trying to move quickly to capitalize on a new market. It's unclear just how big the ARM based server market will be, but AMD seems to hope that it'll be on the forefront of that revolution - should it happen. Embracing ARM also further aligns AMD with one of Intel's most threatening sources of competition at this point. The question is whether or not AMD is doing itself more harm than good by working to devalue x86 in the server space. I suspect it'll be years before we know the real impact of AMD's move here.The other major takeaway is that AMD is looking to find lower cost ways of bringing competitive platforms to market. I do think that a Jaguar based Opteron would likely be the best route for AMD, but it would also likely require a bit more effort than integrating an ARM core.Obviously competition will be more prevalent in the ARM server space, but here is where AMD hopes its brand and position in the market will be able to give it an advantage. AMD will also be relying heavily on the SeaMicro Freedom Fabric for giving its ARM based Opterons a leg up on the competition. This is one time where I really wish AMD hadn't spun off its fabs.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/6418/amd-will-build-64bit-arm-based-opteron-cpus-for-servers-production-in-2014\n",
      "Title: Google Reboots The Chromebook: ARM Meets Notebook For $249\n",
      "Author: Jason Inofuentes\n",
      "Date Published: 2012-10-19T02:11:00Z\n",
      "URL: https://www.anandtech.com/show/6384/google-reboots-the-chromebook-arm-meets-notebook-for-249\n",
      "Content: Google's been known to throw the spaghetti at the wall to see what sticks. And for every GMail and Android, there's a Wave and Buzz. At best, Google's Chromebook initiative is valiantly clinging to the wall, lost ground not withstanding. Today, they've unveiled their latest Chromebook collaboration with Samsung, and it's frankly quite exciting. The 11.6\" notebook weighs in at just 2.5 lbs, and gets to be the first device sporting Samsung's Exynos 5 Dual (5250). Sound familiar? That would be the first ARM Cortex-A15 SoC to show up in a commercially available device, and the first potentially mainstream ARM based PC to hit the market.The rest of the specs are relatively modest, the 11.6\" panel has a respectable 1366x768 resolution, a VGA camera, SD card slot, one each USB 3.0 and USB 2.0 ports, and an HDMI port. Bluetooth and WiFI are built-in, though this generation foregoes the 3G modem of the first Chromebook. Some noise has been made about the quoted 6.5 battery life; we've never taken too much stock in PR provided data. The Cortex-A15 can be a power hungry SoC when stressed, and there's no details on battery capacity, so we'll have to handle the hardware ourselves to size up the battery life.The SSD is a zippy but small 16GB, but local storage isn't really the point of a cloud-centric Chromebook. And to help allay storage fears, Google's including 100GB of Google Drive capacity for 2 years, with the the purchase of the $249 laptop. That additional cloud storage sweetens the value of the device, too; paying for the service out of pocket would cost $120 over the two year span. So, is this the Chromebook that will finally win us over? We'll find out soon, preorders start today, and sales start Monday.Gallery:Google Reboots The Chromebook: ARM Meets Notebook For $249Update: Turns out a 3G option is available, though unannounced. Service is included in the cost of the device for 2-years, up to 100MB a month, and is provided by Verizon. The 3G SKU will cost you $329, and is available forpre-ordernow.Also, I misspoke on the matter of this being the first ARM based PC, I hope the edit adds some clarity.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/6384/google-reboots-the-chromebook-arm-meets-notebook-for-249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Boston Viridis ARM Server Gets x86 Binary Translation Support\n",
      "Author: Johan De Gelas\n",
      "Date Published: 2012-10-18T09:31:00Z\n",
      "URL: https://www.anandtech.com/show/6380/boston-viridis-arm-server-gets-x86-binary-translation-support\n",
      "Content: We covered the launch of the Calxeda-based Boston Viridis ARM server back inJuly. The server is makings its appearance at the UK IP EXPO 2012. Boston has beenbloggingabout their work on the Viridis over the last few months, and one of the most interesting aspects is the fact that x86 binary translation nowworkson the Viridis. The technology is fromEltech, and they have apparently given the seal of approval to the Calxeda platform by indicating that the Boston Viridis was the fastest platform they had tested.Eltech seems to be doing dynamic binary translation, i.e, x86 binaries are translated on the fly. That makes the code a bit bulky (heavier on the I-Cache). The overhead is relatively large compared to, say, VMware's binary translator (BT) that does x86 to x86, becauseof the necessity to translate between two different ISAs.Eltech uses a 1 MB translator cache (similar to the translator cache of VMware's BT), which means they can reuse earlier translations. The translation overhead will thus decrease quickly over time if most of the critical loops fit in the translator cache. But it also means that only code with a relatively small footprint will run fast, e.g. get the promised 40-65% of native performance.Most server applications have a relatively large instruction memory footprint, so it is unclear whether this approach will help to run any heavy server software. Some HPC softwares have a small memory footprint, but since the HPC users tend to pursue performance most of the time, this technology is unlikely to convince them to use ARM servers instead of x86.In general, the BT software will be useful in the - not uncommon - case where one may have a complex web application comprised of multiple software modules where one small piece of software is not open-source and the vendor does not offer an ARM based binary. So, the Eltech solution does handle a small piece of the puzzle. x86 emulation is thus a nice to have feature, but most ARM based servers will be running fully optimized and recompiled linux software. That is the target market for products such as the Boston Viridis.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/6380/boston-viridis-arm-server-gets-x86-binary-translation-support\n",
      "Title: ARM Announces 8-core 2nd Gen Mali-T600 GPUs\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2012-08-06T15:49:00Z\n",
      "URL: https://www.anandtech.com/show/6136/arm-announces-8core-2nd-gen-malit600-gpus\n",
      "Content: In our discrete GPU reviews for the desktop we've often noticed the tradeoff between graphics and compute performance in GPU architectures. Generally speaking, when a GPU is designed for compute it tends to sacrifice graphics performance or vice versa. You can pursue both at the same time, but within a given die size the goals of good graphics and compute performance are usually at odds with one another.Mobile GPUs aren't immune to making this tradeoff. As mobile devices become the computing platform of choice for many, the same difficult decisions about balancing GPU compute and graphics performance must be made.ARM announced its strategy to dealing with the graphics/compute split earlier this year. In short, create two separate GPU lines: one in pursuit of great graphics performance, and one optimized for graphics and compute.Today all of ARM's shipping GPUs fall on the blue, graphics trend line in the image above. The Mali-400 is the well known example, but the forthcoming Mali-450 (8-core Mali-400 with slight improvements to IPC) is also a graphics focused part.The next-generation ARM GPU architecture, codenamed Midgard but productized as the Mali-T600 series will have members optimized for graphics performance as well as high-end graphics/GPU compute performance.The split looks like this:The Mali-T600 series is ARM's first unified shader architecture. The parts on the left fall under the graphics roadmap, while the parts on the right are optimized for graphics and GPU compute. To make things even more confusing, the top part in each is actually a second generation T600 GPU, announced today.What does the second generation of T600 give you? Higher IPC and higher clock speeds in the same die area thanks to some reworking of the architecture and support for ASTC (an optional OpenGL ES texture compression specwe talked about earlier today).Both the T628 and T678 are eight-core parts, the primary difference between the two (and between graphics/GPU compute optimized ARM GPUs in general) is the composition of each shader core. The T628 features two ALUs, a LSU and texture unit per shader, while the T658 doubles up the ALUs per core.Long term you can expect high end smartphones to integrate cores from the graphics & compute optimized roadmap, while the mainstream and lower end smartphones wll pick from the graphics-only roadmap. All of this sounds good on paper, however there's still the fact that we're talking about the second generation of Mali-T600 GPUs before the first generation has even shipped. We will see the first gen Mali-T600 parts before the end of the year, but there's still a lot of room for improvement in the way mobile GPUs and SoCs are launched...\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/6136/arm-announces-8core-2nd-gen-malit600-gpus\n",
      "Title: Boston Releases Servers Based on Calxeda's ARM SoCs\n",
      "Author: Kristian Vättö\n",
      "Date Published: 2012-07-09T16:05:00Z\n",
      "URL: https://www.anandtech.com/show/6070/boston-releases-servers-based-on-calxedas-arm-socs\n",
      "Content: Calxeda EnergyCore SoCBoston has released its Viridis server which uses Calxeda's ARM System-on-Chips (SoCs). Each SoC consists of four ARM Cortex-A9 cores and up to 48 SoCs can be installed into a standard 2U enclosure. The SoCs come on what Boston calls POCket boards. There are four SoCs per board and each board also has four miniDIMM connectors and four SATA ports (one per SoC). The POCket board is a separate PCB which looks a lot like a PCIe card. Every board has a 10Gbps Ethernet link which is the interconnector between the main motherboard and POCket board.Boston Viridis SpecificationsSoCCalxeda EnergyCore (4x per POCket board, 48x per enclosure)ArchitectureARM Cortex-A9Number of cores4 per SoC (192 in total)Frequency1.1 - 1.4GHzMemory4GB per SoC (192GB in total)Storage4x SATA per SoC (192 in total)Form Factor2UWhen using a regular 42U rack, Viridis can provide up to 1,008 SoCs with a total of 4,032 cores of processing power. Viridis is also extremely power efficient since each SoC has a TDP of only 5W (for example Intel's low-power Xeon E5 offerings have a TDP of 50W). Hence Viridis is the best suited for environments that have a highly parallel workload that can benefit from the high amount of cores and require extreme power efficiency.Using ARM architecture obviously limits software selection (e.g. Windows of any flavor is not supported). Boston is, however, stating that Ubuntu 11.10 is supported, along with popular software such as Perl, Python and MySQL. How well these applications are optimized for the architecture remains to be seen, but any significant ARM threat to the x86 server space is likely to benefit enterprise customers across the board. Competition in high margin businesses is never a bad thing.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/6070/boston-releases-servers-based-on-calxedas-arm-socs\n",
      "Title: AMD 2013 APUs To Include ARM Cortex-A5 Processor For TrustZone Capabilities\n",
      "Author: Ryan Smith\n",
      "Date Published: 2012-06-13T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/6007/amd-2013-apus-to-include-arm-cortexa5-processor-for-trustzone-capabilities\n",
      "Content: At AMD’s 2012 Financial Analyst Day, as part of their presentation on their future strategy AMD’s CTO Mark Papermaster announced thatAMD would be looking into integrating 3rdparty IP into future AMD APUs. At the time there was a strong assumption that this would be mobile focused – perhaps in the form of a cellular modem or an ARM core – and it turns out the assumptions were both right and wrong. Today AMD is announcing that they are in fact going to start integrating ARM cores into future APUs starting in 2013, but not in the way you’re probably thinking.If you look at AMD’s long term strategy, not only do they need to continue to compete with Intel on the technology front, but they also want to better position themselves to compete on the tablet front. AMD has the basic hardware for this with their APU families, particularly for tablets with their Zacate based entry-level APUs, but they have a feature gap in certain markets. Both Intel and ARM have hardware trusted platform/security technologies but AMD lacks such a technology.For various reasons we’ll get to in a moment, AMD believes they need some kind of hardware security platform technology to continue to compete in the market in the future. Intel’s Trusted Execution Technology is not part of the x86 specification and is therefore not shared, so AMD would need to come up with their own technology. Designing and implementing such a technology is not only resource intensive but by its very nature it fragments the market, which is something AMD doesn’t necessarily have the clout to get away with all the time. So rather than design their own technology they’ve chosen to license an existing technology, and this brings us to ARM.In order to implement a hardware security platform on their future APUs, AMD has chosen to enter into a strategic partnership with ARM for the purpose of gaining access to ARM’s TrustZone technology. By licensing TrustZone, AMD gains a hardware security platform that’s already in active use, which means they avoid fragmenting the market and the risks that would bring. Furthermore AMD saves on the years of work – both technical and evangelical – that they would have needed had they rolled their own solution. Or more simply put, given their new willingness to integrate 3rdparty IP, licensing was the easy solution to getting a hardware security platform quickly.But because TrustZone is an ARM technology (both in name and ISA) AMD needs an ARM CPU to execute it. So the key to all of this will be the integration of an ARM processor into an AMD APU, specifically ARM’s Cortex-A5 CPU. The Cortex-A5 is ARM’s simplest ARMv7 application processor, and while it’s primarily designed for entry-level and other lower-performance devices, as it turns out it fits AMD’s needs quite nicely since it won’t be used as a primary application processor.ARM TrustZone Hardware Model; Normal World Would Be On x86This also means that the ARM and x86 CPU cores will fit together in an interesting manner unlike any existing ARM or Intel x86 CPU. By integrating a low-power/low-performance ARM CPU in this manner an application will be split up over multiple CPUs, with the TrustZone secure backend executing on the Cortex-A5 while the frontend logic will be executing as normal on AMD’s x86 CPU and GPU cores. This gives AMD a dedicated security co-processor with all the benefits and drawbacks thereof, while on full ARM processors and on Intel’s x86 processors TrustZone and TXT respectively are hardware features of a single CPU.By implementing a hardware security platform in this manner AMD not only gains a relatively quick turnaround time on the hardware, but on the software side too. AMD is specifically looking to leverage existing ARM applications for their tablet ambitions by taking advantage of the fact that existing TrustZone application cores can easily (if not directly) be ported over to AMD’s APUs. Developers would still need to put in some effort to write the necessary x86 frontends (in all likelihood written in scratch for Win8 as opposed to any kind of Android), but the hard part of implementing and validating the TrustZone functionality would simply carry over, leaving the new x86 frontend to talk to the existing ARM TrustZone application core. AMD isn’t in any position to talk about specific software yet, but we’re told that they’ve been working with select software partners even before this announcement in order to get a jump on developing applications.As for the hardware details AMD hasn’t named any specific APUs that will be receiving the Cortex-A5, but they have told us that they intend to start with the low-power APUs in order to go after the tablet market. That means we’re almost certainly looking at the 2013 successor to the Zacate APU found at the heart of AMD’s Brazos platform. However AMD won’t be stopping there, and in 2014 and beyond AMD will continue to add it to further APUs until AMD’s entire APU lineup from mobile to desktop to server contains the Cortex-A5 and TrustZone functionality.Having covered the technology, let’s also quickly discuss why AMD is pursuing this move. As AMD is pitching this it’s not just closing a feature gap but also about what it enables. A big focus of this of course is on trusted computing in the classical sense, meaning DRM for consumer applications and on platform lockdown and auditing for business IT purposes. But as we’ve seen Intel do with their acquisition of McAfee some years back, there’s also a strong focus on securing systems from malware in the form of new anti-virus technologies and in newer applications such as mobile payments. Even cloud services get a mention in here, since TrustZone can be used to make sure malware isn’t watching in on a session from the client.It’s worth noting that AMD also has a bit of self-interest in here. AMD’s Chief Information Officer Mike Wolfe is spearheading this announcement with a focus on how AMD intends to use this technology internally. AMD recently implemented a Bring Your Own Device (BYOD) policy for employees to let them use their own computers at work. BYOD is popular with employees because it allows them to use the device they like the most, but it’s a potential headache for IT since it means many different devices that need to be supported and secured. As a result only a select number of generally high-end devices are allowed in AMD’s BYOD environment because most low-end x86 devices lack hardware security platform technology. By implementing this in their entire range of APUs, AMD expects to be self-serving here by expanding the range of devices they can support. At the same time AMD and Wolfe expect other companies to adopt BYOD too, in which case this will help to quickly set up AMD to serve a potentially large market.Wrapping things up, we would be remiss to ignore the elephant in the room, which of course is the inclusion of an ARM core in the first place. A lot of speculation has been going on that AMD is considering adopting the ARM architecture on a broader basis – particularly if HSA takes off and makes the underlying architecture less important – and this certainly is going to fuel more of that. The Cortex-A5 in AMD’s future APUs will be a fully functional ARM processor and in theory it is possible to run full ARM applications on the processor (OS differences not withstanding), though at this point in time AMD hasn’t released the full details on how accessing the ARM processor will work. Even if AMD just intends to use ARM for TrustZone today, this opens the door to comprehensive native ARM code execution in the future if AMD wanted to go that way; but at the same time this could end up being as far as AMD ever goes.In any case we aren’t expecting AMD to go into any more detail about this announcement here at AFDS, but there are still two days of keynotes to go. Otherwise we’d expect AMD to discuss this in greater detail once they’re ready to unveil more details about their 2013 APUs. So until then stay tuned.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/6007/amd-2013-apus-to-include-arm-cortexa5-processor-for-trustzone-capabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Portland Group Announces OpenCL Compiler for ST-Ericsson ARM-Based NovaThor SoCs\n",
      "Author: Ryan Smith\n",
      "Date Published: 2012-02-28T23:25:00Z\n",
      "URL: https://www.anandtech.com/show/5607/the-portland-group-announces-opencl-compiler-for-stericsson-armbased-novathor-socs\n",
      "Content: Amidst all the major hardware announcements at MWC2012 there are also some announcements coming out of the software side. The Portland Group (PGI) sent word this afternoon that their OpenCL compiler for ST-Ericsson’s ARM-based NovaThor SoCs is finished and will be available late next month. ARM has thrown their support behind OpenCL as of last year, but PGI’s compiler would be among the first commercial OpenCL compilers for ARM, and something we’d expect the rest of the field to follow sooner than later.ThePGI OpenCL Compiler For ARMwill initially target ST-Ericsson’s NovaThor U8500 SoC, which is based on a dual-core Cortex-A9 CPU and coupled with an ARM Mali 400 MP GPU. As the name hints at, this is an OpenCL compiler and runtime for ARM, with all local OpenCL execution taking place on the U8500’s ARM A9 cores. Ultimately PGI will support additional NovaThor SoCs with this compiler, such as therecently announced L8540.For the time being OpenCL execution will be limited to the ARM cores. We’d eventually expect support in some manner to broaden to GPU cores, but not until sometime in 2013 when ST-Ericsson’s PowerVR Series 6 based A9600 SoC ships.Finally, on the operating system side of things PGI will be basing their support around Android, which makes sense as so far NovaThor SoCs have only appeared in Android devices. Notably they list the target OS as Android 2.3.3 (Gingerbread), which will be the OS shipping on the first U8500 devices such as theSony Xperias U and P. ICS isn’t expected for those phones until next quarter, though it’s not immediately clear when PGI’s compiler will reach parity.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/5607/the-portland-group-announces-opencl-compiler-for-stericsson-armbased-novathor-socs\n",
      "Title: Microsoft Provides Windows on ARM Details\n",
      "Author: Andrew Cunningham\n",
      "Date Published: 2012-02-09T23:00:00Z\n",
      "URL: https://www.anandtech.com/show/5527/microsoft-provides-windows-on-arm-details\n",
      "Content: We've known that Microsoft has been planning an ARM-compatible version of Windows since well before we knew anything else about Windows 8, but the particulars have often been obscured both by unclear signals from Microsoft itself and subsequent coverage of those unclear signals by journalists. Steven Sinofsky has taken to the Building Windows blog today to clear up some of this ambiguity, and in doing so has drawn a clearer line between the version of Windows that will run on ARM, and the version of Windows that will run on x86 processors.Up until now, we've operated under the assumption that a new version of Windows called Windows 8 would be released this year, and that it would run on both x86 (32-bit and 64-bit - throughout this article I'll use x86 to refer to both architectures) and ARM processors - Sinofsky's post makes it clear that the ARM version of Windows, officially referred to as Windows on ARM (WOA), is considered to be a separate product from Windows 8, the same way that products like Windows Server and Windows Embedded share a foundation with but are distinct from Windows 7. Windows on ARM has a \"high degree of commonality\" and \"very significant shared code\" with Windows 8 - much of the user's interaction with the OS will be the same on either platform, and much of the underlying technologywe've seen in our Windows 8 coverage so farwill be present in both versions, but they're distinct products that will be treated differently by Microsoft.This post is quite lengthy and represents what is likely to be our best look at WOA for at least a little while - we'll get to see some of its features in theWindows 8 Consumer Previewwhen it is released at the end of the month, but for now Windows on ARM is only being tested internally, and on customized hardware that will be sent to some developers and hardware partners at about the same time. It will be a little while before we see anything remotely similar to shipping hardware.The Windows Desktop, Office, and x86 AppsOne of the biggest recurring questions I've seen about Windows on ARM is whether the standard Windows desktop would be available for use on those devices as it will be on Windows 8 machines - the answer isyes, it definitely will be. The desktop can be invoked from the Start screen, and once there users can perform standard Windows Explorer operations, launch the desktop version of Internet Explorer, and other tasks either via touch (for which Explorer has apparently been optimized) or via keyboard and mouse input. The desktop will only consume resources when it is launched, meaning that there are no performance or battery life implications for users who stick with the Metro interface for everything - the desktop is there if you want it, but one of Microsoft's stated goals with the Metro interface is to make it so that you don'tneedto use the desktop as a fallback.Microsoft will also be bundling versions of Microsoft Word, Excel, PowerPoint, and OneNote with Windows on ARM systems. These Office apps will be a part of the newOffice 15family of products (suggesting, but not guaranteeing, that we may see a full x86 version of that suite before the end of the year as well), but will be touch-optimized versions of the applications rather than ports of the standard suite. The Windows on ARM products will \"maintain fidelity\" with their x86 counterparts (meaning that a file created in Word or Powerpoint on an x86 machine will look the same on an ARM machine), but will otherwise be redesigned to fit the platform - an early version of Excel is shown above.That said, Microsoft is firm in its insistence that it will not support running, emulating, or porting existing x86 apps on the Windows on ARM desktop - apps can only be downloaded and installed through the Windows Store, and only apps written to target the new WinRT APIs can be distributed through the store (however, the store will be able to distribute and update both ARM and x86 versions of apps in the event that the app uses any code native to either architecture). Microsoft suggests that current Windows developers should be able to take significant bits of their existing code and wrap them in a Metro layer, but acknowledges that bringing over existing apps will require a bit of work - WinRT is clearly the wave of the future where Windows is concerned, but it'll be up to individual developers to decide how, when, and if to bring their programs over.Supported Devices and Release DateWindows on ARM is being written to run on ARM SoCs from NVIDIA, Qualcomm, and Texas Instruments, and it will only be available on devices designed to run it - you won't be able to buy a license for Windows on ARM and install it on an existing tablet, or a tablet designed to run Android. Microsoft is working with partners to deliver compatible hardware, and the company's goal is to start shipping devices running Windows on ARM at the same time as x86 devices running Windows 8 (currently slated for late this year).In addition to SoC type, Microsoft will have a set of broad guidelines for Windows on ARM tablets that are similar to those for current Windows phones (the \"chassis specification,\" in Microsoft parlance) - likely a set of supported screen resolutions and a list of required hardware devices designed to provide a middle ground between the uniformity of the iPad and the diversity-to-the-point-of-insanity of Android tablets. On Windows phones, these requirements are in place to give consumers some choice while also limiting developer headaches and ensuring a standardized look and feel across different devices from different manufacturers - the requirements for Windows on ARM will have the same aims, and we'll talk a little bit more about some of the hardware that will be common to WOA devices later on in this post.In treating Windows on ARM as a separate product, Microsoft has left itself some wiggle room to let its release date slip without holding up Windows 8 (wiggle room is very important to the post-Vista Windows team, and they generally don't give hard dates unless they expect to be able to hit them). Microsoft obviously wants to ship before the end of the year because, let's face it, they don't want to give Apple, Google, Amazon, and the rest another holiday season all to themselves, but at this point in the game a botched or half-baked release in time for Christmas could actually beworsefor Microsoft's market and mindshare than a well-executed release a few months later. Expect a concurrent release with Windows 8, but know that Microsoft hasn't yet completely committed to it.When it is shipped, Windows on ARM should come as a single edition of Windows from a feature standpoint (though the company notes that no decisions regarding new Windows product editions have been finalized) - Microsoft promises to \"adjust the features ... such that [WOA] is competitive in the marketplace and offers a compelling value proposition to customers of all types.\" That doesn't tell us much, but I think we should expect the consumer-oriented features that you'd find in a Home Premium version of Windows along with business-minded features (like domain joining and device encryption) thrown in to increase WOA's appeal to enterprises. Whether the decision to ship a single Windows on ARM SKU will have any effect on the x86 version's army of different editions remains to be seen.Drivers, Updates, and HardwareSo, since Windows on ARM will only be available on devices designed specifically for it, Microsoft can actually keep track of what hardware WOA devices are guaranteed to be using. This means that all software, from OS patches to device firmware to specific drivers, can and will be distributed using Windows Update. Apple has achieved something similar in OS X - Macs are many and subtly varied, especially when you take multiple model years into consideration, but ultimately there is a finite set of hardware in the field, and Apple can keep every Mac in use up-to-date with drivers, firmware, and OS updates through Software Update, rather than the broad array of different first and third-party updaters required to patch those separate elements on an x86 Windows box (and I promise that I'm just comparing the two to give you a frame of reference, not because I consider one system to be inherently superior to the other).To reduce the number of drivers it will have to keep up to date, Microsoft is relying heavily on \"class drivers\" to support hardware in both WOA and Windows 8 - for those of you just tuning in, a class driver is designed to support all hardware manufactured to certain standards, rather than targeting specific devices. They're why you can freely plug in different USB keyboards and flash drives to a Windows computer and have them recognized by the machine without needing to pop in a driver disk first.A lot of the work Microsoft is putting into class drivers is also applicable to Windows 8 - we've already looked at new class drivers forUSB 3.0 controllers,mobile broadband chips, andmotion sensors, and we should also see class drivers for printers, Bluetooth, Embedded MultiMediaCard (eMMC) storage, and drivers for different busses and input devices (like the Windows, power, and volume buttons).Where Microsoft can't create class drivers, it's trying to enforce some common specifications - WOA devices will all have DirectX-capable GPUs and drivers, which will power Metro apps, the Windows UI, and GPU acceleration in Internet Explorer among other things. This baseline has enabled Microsoft to improve on the fallback software GPU driver to enable a nicer-looking display on devices without a specific driver (and also for system diagnostic and information screens). This new soft GPU driver will also be available in Windows 8, where it will replace the standard VGA driver that has been a part of Windows for just about as long as Windows has been around.WOA systems will also require UEFI firmware and Trusted Platform Module (TPM) hardware across the board to support its secure boot and data encyption features, both of which will also be available to Windows 8 devices with the correct hardware (TPMs have been used to encrypt hard drives with BitLocker since Windows Vista and UEFI is slowly replacing BIOS in OEM PCs, but Windows 8 should push the adoption of both in a wider range of computers).ConclusionsTo see what Microsoft is trying to do with Windows on ARM, the most applicable template to examine is the one the company followed with Windows Phone 7. In both cases, Microsoft is entering an established market where competitors have established footholds through very different strategies (in each case, Apple and its tighty-controlled iOS on one end, Google with its infinitely malleable Android on the other, and a few other competitors fighting for scraps in between) and has tried to forge a middle path. Windows Phone 7 has been a bit of a slow starter because of Microsoft's low profile in the smartphone field and because of some lackluster handsets, but the platform has some very vocal fans - if the company can achieve a similar balance in Windows on ARM and get it to market on competitive hardware by the end of the year, that (combined with Android's relative weakness in the tablet market so far) might just be enough to establish Windows as a major player in the tablet space.As usual with these Building Windows post summaries, I've relayed and distilled the most pertinent information for Windows users and enthusiasts here. If you'd like to read the full post, which also includes some details about how Microsoft is testing Windows on ARM in its labs and some of the more technical details involved in \"porting\" Windows from x86 to ARM, it is linked below for your convenience.Source:Building Windows 8 blog\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/5527/microsoft-provides-windows-on-arm-details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD is Ambidextrous, Not Married to Any One Architecture, ARM in the Datacenter?\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2012-02-02T18:59:00Z\n",
      "URL: https://www.anandtech.com/show/5501/amd-is-ambidextrous-not-married-to-any-one-architecture-arm-in-the-datacenter\n",
      "Content: We've been hammering this point home all day, but AMD just mentioned it again. The company wants to be a solutions provider, one that's ambidextrous and not married to any one architecture. AMD is likely talking about ARM here and seems willing to offer both ARM and x86 based SoCs depending on the market segment/customer requirements.What's important to note is that thus far AMD has talked about these ambidextrous solutions with respect to the datacenter and not client systems, and definitely not smartphones. If you were looking for AMD to get into the ARM based SoC race in phones, that's not what's going to happen. An AMD architected ARM based enterprise solution is interesting though. It's unclear to me what the main advantage of ARM would be there, particularly given that AMD has its own low power x86 core with Bobcat, but it's an interesting notion.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/5501/amd-is-ambidextrous-not-married-to-any-one-architecture-arm-in-the-datacenter\n",
      "Title: TI Shows Off OMAP 5 & ARM Cortex A15 at CES\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2012-01-12T22:02:00Z\n",
      "URL: https://www.anandtech.com/show/5406/ti-shows-off-omap-5-arm-cortex-a15-at-ces\n",
      "Content: We weren't allowed to take photos but we've just seen TI's OMAP 5 reference platform up and running Ice Cream Sandwich with full GPU acceleration enabled. The 28nm chip just came back from the fab earlier this month and it's already up and running. TI indicated that 28nm was very healthy.The high level specs of the SoC are awesome: two ARM Cortex A15s (3-wide OoO cores), 2MB L2 cache, a PowerVR SGX 544MP2 GPU, and a dual-channel LPDDR2-533 (1066MHz data rate) memory interface. OMAP 5 also features integrated 3Gbps SATA and USB 3.0, although PCIe isn't supported.Target clocks for the A15s are up to 2GHz, although the chip is currently running at ~1GHz and SGX 544 at ~300MHz since it just came back from the fab.The first devices based on OMAP 5 aren't expected to ship until early 2013, with some aggressive customers potentially shipping at the very end of this year. If Qualcomm getsKraitout on time, it looks like it'll have the majority of the 2012 market to itself with the real battle with TI taking place next year.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/5406/ti-shows-off-omap-5-arm-cortex-a15-at-ces\n",
      "Title: Google TV Goes ARM with Marvell's ARMADA 1500\n",
      "Author: Ganesh T S\n",
      "Date Published: 2012-01-05T13:01:00Z\n",
      "URL: https://www.anandtech.com/show/5296/google-tv-goes-arm-with-marvells-armada-1500\n",
      "Content: It wouldn't be far off the mark to call Google TV as one of the unmitigated disasters of 2010 - 2011. Through the failure of the Logitech Revue, it was responsible for Logitech's below-par performance last year, and also for the stepping down of its CEO. Anand coveredIntel's winding down of the Digital Home Groupand it could be said that Google TV / Intel's concept of Smart TV not taking off as expected was one of the reasons.However, Google doesn't give up on its efforts without a fight. With access to the Android market and an upgrade to Honeycomb, Google TV received some life support last October. However, pricing and device power consumption were the two other prime factors which needed to get addressed. The first generation Google TV devices were all based on the Intel's CE4100. Despite being a highly capable platform, it suffered from a number of issues such as high silicon cost (leading to higher priced Google TV units) and unreasonably high power consumption. With Intel's shuttering of the Digital Home Group, it was inevitable that Google and its partners would end up moving to an ARM based platform. Given that ARM has remained the architecture of choice for Android smartphones, this was also a move predicted by many.WecoveredMarvell's foray into the DMA (Digital Media Adapter) market with their ARMADA 1000 platform. Today, Marvell is officially launching the next generation ARMADA 1500 (88DE3010) SoC. They also announced their team up with Google and indicated that all the Google TV boxes at the 2012 CES would be powered by Marvell silicon.The ARMADA 1500 (88DE3100) is the follow up to the ARMADA 1000 (88DE3010) introduced a couple of years back. The 88DE3010 is the same chip which is being used in theNixeus Fusion XSwhich started shipping recently. It is also the chip used in some high end (in terms of cost) 3D Blu-ray players like theKaiboer K860iand theAsus O!Play BD players(BDS-500 and BDS-700).\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/5296/google-tv-goes-arm-with-marvells-armada-1500\n",
      "Title: ARM & GlobalFoundries Demo 2.5GHz+ 28nm Cortex A9 & 20nm Test Vehicle\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2011-12-15T00:34:00Z\n",
      "URL: https://www.anandtech.com/show/5247/arm-globalfoundries-demo-25ghz-28nm-cortex-a9-20nm-test-vehicle\n",
      "Content: GlobalFoundries sent over aPR showcasing two significant milestonesin its march towards being a major foundry player in the mobile space. The first is the announcement of a dual-core Cortex A9 test chip built on GF's 28nm HPP (High Performance Plus) process. The test chip operates at 2.5GHz although it is apparently capable of higher frequencies according to the ambiguous statement from GF. The chip's operating voltage is a low 0.85V. Both the frequency and voltage targets are good for a Cortex A9 implementation, although again this is only a test chip.Many companies are expecting to break the 2GHz barrier on high performance 28nm processes starting late next year and moving on into 2013. These designs aren't likely to be used in smartphones, but instead we'll see them in tablets and netbook replacements (e.g. Windows 8 on ARM).The second milestone is the the tapeout of a 20nm Technology Qualification Vehicle (TQV). This is a test chip designed to, well, test GF's 20nm process as it would be used by a vendor for producing Cortex A9 based SoCs. The move to 20nm is much further out for GF, but there's lots of development work prior to release that necessitates the production of test silicon.Both announcements are designed to showcase GF's close partnership with ARM, as well as its continued momentum in the manufacturing space. Process technology in general is going to become a very important piece of the puzzle in the mobile space, particularly as companies like Intel enter the market next year.Gallery:ARM & GlobalFoundries Demo 2.5GHz+ 28nm Cortex A9 & 20nm Test Vehicle\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/5247/arm-globalfoundries-demo-25ghz-28nm-cortex-a9-20nm-test-vehicle\n",
      "Title: ARM's Mali-T658 GPU in 2013, Up to 10x Faster than Mali-400\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2011-11-10T01:30:00Z\n",
      "URL: https://www.anandtech.com/show/5077/arms-malit658-gpu-in-2013-up-to-10x-faster-than-mali400\n",
      "Content: ARM's licensed CPU cores dominate the mobile space. This year the core of choice for high-end smartphones and tablets is ARM's Cortex A9 and late next year it'll be the Cortex A15. ARM also offers GPU cores to its partners, although we've seen far more limited adoption compared to its CPU offerings. The limited adoption has nothing to do with a lack of performance though as we found inour review of Samsung's Galaxy S 2. ARM's Mali-400 MP4 is the fastest GPU we've tested on Android and second to only Imagination Technologies' PowerVR SGX 543MP2 across all of our mobile data.TheMali-400, likeNVIDIA's GeForce used in Tegra 2/3, doesn't feature a unified shader architecture - it has discrete pixel and vertex shader hardware. The Mali-400 is the last implementation of what ARM calls its Utgard architecture. Next year we'll see the Mali-400's successor: the Mali-T604.The Mali-T604 was announced last year and it's the first implementation of ARM's new Midgard architecture. The T604 appears to be ARM's first unified shader architecture. Each T604 core is a combination of two arithmetic pipes and one texture pipe, although the width and capabilities of each are unknown. Like the Mali-400, the Mali-T604 will be available in 1 - 4 core configurations. The first T604 based SoCs will be available in the second half of 2012 on 28/32nm silicon. ARM is promising up to 68 GFLOPS of compute from T604 (presumably that's for a 4-core configuration at high clocks).What comes after T604? ARM's Mali-T658 of course.The T658 is a second generation Midgard implementation with twice the arithmetic pipes per core compared to the T604. ARM also enables up to 8-core configurations with T658. We'll see the first T658 implementations on 28/32nm sometime in 2013. It's unclear what other architectural changes have been made compared to the T604, but at bare minimum we can hope for a doubling of execution resources. ARM is promising up to a 10x increase in performance compared to \"mainstream\" Mali-400 implementations (perhaps single-core Mali-400).Samsung is listed as a launch partner for Mali-T658, which isn't surprising given the company's use of Mali-400 in its Exynos 4210 (the SoC inside many of the Galaxy S 2s).It's good to see ARM continue the evolution of its Mali graphics cores. Unlike in the PC market, there's almost no coupling between CPU and GPU IP providers in the mobile SoC space. ARM hopes to change that by offering very compelling GPUs in addition to its widely used CPUs cores. Since everything is already on a single chip however, it's unclear whether the mobile market will follow the same path - at least in the near term. The Mali-T604 and T658 will have their work cut out for them. There are new GPUs from Imagination, NVIDIA and Qualcomm that will be out over the next two years as well.Gallery:ARM's Mali-T658 GPU in 2013, Up to 10x Faster than Mali-400\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/5077/arms-malit658-gpu-in-2013-up-to-10x-faster-than-mali400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AppliedMicro Announces 64-bit ARM Based X-Gene SoCs\n",
      "Author: Kristian Vättö\n",
      "Date Published: 2011-10-28T20:30:00Z\n",
      "URL: https://www.anandtech.com/show/5027/appliedmicro-announces-xgene-arm-based-socs-for-cloud-computing\n",
      "Content: AppliedMicro has released specifications of their upcoming X-Gene SoC (Server-on-a-Chip this time, not System).AppliedMicro X-Gene SpecificationsArchitectureARMv8CoresFrom 2 to up to 128FrequencyUp to 3GHzProcessTSMC 40/28nmPower UsageUp to 2W per coreAbove is a simple table showing the key specifications. ARMv8 is ARM's brand new architecture, which was announced on Thursday. ARMv8 brings 64-bit addressing to ARM architecture, which makes ARM a more attractive solution for server market. X-Gene is very scalable - core count ranges from two to up to 128, while the frequency is up to 3GHz (yes, even with 128 cores). AppliedMicro has chosen TSMC as the manufacturer of the SoCs and the process will be TSMC's 40nm and 28nm.X-Gene is a SoC, meaning that key server and network components are integrated onto the same chip. This is much cleaner approach when compared to for example Intel's, where you have several independent chips, such as the CPU(s) and chipset controller. X-Gene even has an integrated 10Gbit Ethernet controller, which should be a welcome addition for enterprises with a need for high-speed networking. Support for multi-chip configurations is also present, enabled by a 100Gbit/s interface (just for comparison, Intel's QPI is good for up to 204.8Gbit/s).The biggest advantage of X-Gene is its power efficiency. At full load, the power usage is only 2 watts per core. When idling, the power usage is one fourth, 0.5 watts per core. For the 128-core chip at 3GHz, the power usage works out to be 256W, or 64W when idling. 256W may sound like a big number but it's actually on-par with for example two Intel X5680s, which are 130W each. And that is when excluding the power used by the chipset and other components, which are integrated into X-Gene. Of course, performance is a big question mark but if AppliedMicro's tests are to believe, X-Gene is up to three times faster than Intel's Sandy Bridge based E3 Xeons when looking at similar power profile. It should be noted these numbers are based on pre-silicon projections, so a lot can change before the final products hit the market.The scalability of X-Gene allows a broad suite of market-end applications. The low-end chips with only a couple of cores are suitable for more consumer-friendly devices like NASs and routers - whereas the chips with higher core count are ideal for more complex setups, such as data centers. The first samples of X-Gene are expected in the second half of 2012.Stay tuned for a more thorough analysis of this announcement!\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/5027/appliedmicro-announces-xgene-arm-based-socs-for-cloud-computing\n",
      "Title: ARM's Cortex A7: Bringing Cheaper Dual-Core & More Power Efficient High-End Devices\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2011-10-19T16:31:00Z\n",
      "URL: https://www.anandtech.com/show/4991/arms-cortex-a7-bringing-cheaper-dualcore-more-power-efficient-highend-devices\n",
      "Content: How do you keep increasing performance in a power constrained environment like a smartphone without decreasing battery life? You can design more efficient microarchitectures, but at some point you’ll run out of steam there. You can transition to newer, more power efficient process technologies but even then progress is very difficult to come by. In the past you could rely on either one of these options to deliver lower power consumption, but these days you have to rely on both - and even then it’s potentially not enough. Heterogeneous multiprocessing is another option available - put a bunch of high performance cores alongside some low performance but low power cores and switch between them as necessary.NVIDIA recently revealedit was doing something similar to this with its upcoming Tegra 3 (Kal-El) SoC. NVIDIA outfitted its next-generation SoC with five CPU cores, although only a maximum of four are visible to the OS. If you’re running light tasks (background checking for email, SMS/MMS, twitter updates while your phone is locked) then a single low power Cortex A9 core services those needs while the higher performance A9s remain power gated. Request more of the OS (e.g. unlock your phone and load a webpage) and the low power A9 goes to sleep and the 4 high performance cores wake up.While NVIDIA’s solution uses identical cores simply built using different transistors (LP vs. G), the premise doesn’t change if you move to physically different cores. For NVIDIA, ARM didn’t really have a suitable low power core thus it settled on a lower power Cortex A9. Today, ARM is expanding the Cortex family to include a low power core that can either be used by itself or as an ISA-compatible companion core in Cortex A15 based SoCs. It’s called the ARM Cortex A7.ArchitectureStarting with the Cortex A9, ARM moved to an out-of-order execution core (instructions can be reordered around dependencies for improved parallelism) - a transition that we saw in the x86 space back in the days of the Pentium Pro. The Cortex A15 continues the trend as an OoO core but increases the width of the machine. The Cortex A7 however takes a step back and is another simple in-order core capable of issuing up to two instructions in parallel. This should sound a lot like the Cortex A8, however the A7 is different in a number of areas.The A8 is a very old design with work originally beginning on the core in 2003. Although ARM offered easily synthesizable versions of the core, in order to hit higher clock speeds you needed to include a lot of custom logic. The custom design requirements on A8 not only lengthened time to market but also increased development costs, limiting the A8’s overall reach. The Cortex A7 on the other hand would have to be fully synthesizable while being able to deliver good performance. ARM could leverage process technology advancements over the past few years to deliver clock speed and competitive power consumption, but it needed a revised architecture to meet the cost and time to market requirements.The Cortex A7 features an 8-stage integer pipeline and is capable of dual-issue. Unlike the Cortex A8 however, the A7 cannot dual-issue floating point or NEON instructions. There are other instructions that turn the A7 into a single-issue machine as well. The integer execution cluster is quite similar to the Cortex A8, although the FPU is fully pipelined and more compact than its older brother.Limiting issue width for more complex instructions helps keep die size in check, which was a definite goal for the core. ARM claims a single Cortex A7 core will measure only 0.5mm2 on a 28nm process. On an equivalent process node ARM expects customers will be able to implement an A7 in 1/3 - 1/2 the die area of a Cortex A8. As a reference, an A9 core uses about the same (if not a little less) die area as an A8 while an A15 is a bit bigger than both.Architecture ComparisonARM11ARM Cortex A7ARM Cortex A8ARM Cortex A9Qualcomm ScorpionQualcomm KraitDecodesingle-issuepartial dual-issue2-wide2-wide2-wide3-widePipeline Depth8 stages8 stages13 stages8 stages10 stages11 stagesOut of Order ExecutionNNNYPartialYPipelined FPUYYNYYYNEONN/AY (64-bit wide)Y (64-bit wide)Optional MPE (64-bit wide)Y (128-bit wide)Y (128-bit wide)Process Technology90nm40nm/28m65nm/45nm40nm40nm28nmTypical Clock Speeds412MHz1.5GHz (28nm)600MHz/1GHz1.2GHz1GHz1.5GHzDespite the limited dual issue capabilities, ARM is hoping for better performance per clock and better overall performance out of the Cortex A7 compared to the Cortex A8. Branch prediction performance is improved partly by using a more modern predictor, and partly because the shallower pipeline lessens the mispredict penalty. The Cortex A7 features better prefetching algorithms to help improve efficiency. ARM also includes a very low latency L2 cache (10 cycles) with its Cortex A7 design, although actual latency can be configured by the partner during implementation.Note that in decoding bound scenarios, the Cortex A7 will offer the same if not lower performance than a Cortex A8 due to its limited dual-issue capabilities. The mildly useful DMIPS/MHz ratings of ARM’s various cores are below:Estimated Core PerformanceARM11ARM Cortex A7ARM Cortex A8ARM Cortex A9Qualcomm ScorpionQualcomm KraitDMIPS/MHz1.251.92.02.52.13.3The big news is the Cortex A7 is 100% ISA compatible with the Cortex A15, this includes the new virtualization instructions, integer divide support and 40-bit memory addressing. Any code running on an A15 can run on a Cortex A7, just slower. This is a very important feature as it enables SoC vendors to build chips with both Cortex A7 and Cortex A15 cores, switching between them depending on workload requirements. ARM calls this a big.LITTLE configuration.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/4991/arms-cortex-a7-bringing-cheaper-dualcore-more-power-efficient-highend-devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ARM & Cadence Tape Out 20nm Cortex A15 Test Chip\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2011-10-18T14:30:00Z\n",
      "URL: https://www.anandtech.com/show/4976/arm-cadence-tape-out-20nm-cortex-a15-test-chip\n",
      "Content: Although we won't see the firstARM Cortex A15based designs until the second half of next year, and even then only on 28/32nm processes, ARM and design tools supplier Cadence have announced the first tape-out of a 20nm Cortex A15 based test chip. Tape out signals the end of an overall design phase and the release of the design to the foundry for manufacturing. The Cortex A15 is expected to be a significant step forward for ARM, bringing its designs further up the chain into the low-end x86 notebook market in addition to current smartphone/tablet targets. Cortex A15 based designs will also go head to head withQualcomm's Krait based Snapdragon S4.The test chip will be fabbed at TSMC on it's next-generation 20nm process, a full node reduction (~50% transistor scaling) over its 28nm process. With the first 28nm ARM based products due out from TSMC in 2012, this 20nm tape-out announcement is an important milestone but we're still around two years away from productization.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/4976/arm-cadence-tape-out-20nm-cortex-a15-test-chip\n",
      "Title: ARM's Mali-400 MP4 is the Fastest Smartphone GPU...for Now\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2011-09-11T17:21:00Z\n",
      "URL: https://www.anandtech.com/show/4760/arms-mali400-mp4-is-the-fastest-smartphone-gpufor-now\n",
      "Content: Earlier this morning we published ourlong awaited review of the Samsung Galaxy S 2. In it we dedicated a few pages to investigating Samsung's own Exynos 4210 SoC. The chip a full featured dual-core Cortex A9 design, comparable to TI's OMAP 4. The big news however is the Exynos 4210 is the first SoC in a smartphone to use ARM's Mali-400 GPU.Samsung implemented a 4-core version of the Mali-400 in the 4210 and its resulting performance is staggering as you can see above. Although it's still not as fast as the PowerVR SGX 543MP2 found in the iPad 2, it'sanywhere from 1.7 - 4x faster than anything that's shipping in a smartphone today.The Mali-400 MP4 is put to good use in the Galaxy S 2 as our own Brian Klug found it to be the smoothest experience by an order of magnitude compared to any currently available Android phone.The downsides to the Mali-400 MP4? It doesn't have the best triangle throughput, which could be an issue in future games that may scale along that vector rather than simply increasing pixel shader complexity.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/4760/arms-mali400-mp4-is-the-fastest-smartphone-gpufor-now\n",
      "Title: Windows 8 Running on ARM, NVIDIA Kal-El Notebook Demoed\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2011-06-02T02:54:00Z\n",
      "URL: https://www.anandtech.com/show/4411/windows-8-running-on-arm-nvidia-kalel-notebook-demoed\n",
      "Content: Microsoft just showed Windows 8 running on three different ARM platforms: a single-core 1.2GHz Qualcomm Snapdragon, a dual-core TI OMAP 4430 and a quad-core NVIDIA Kal-El notebook.The same interface we showed you earlier exists on these systems, and the same applications can run across both systems (assuming the apps have been ported to ARM).You get a standard Windows 8 desktop as well as the new tiled start screen. USB devices will work and MS even did a demo of copying files off of a USB thumb drive.NVIDIA had a Kal-El based notebook and tablet on display. Microsoft showed task manager displaying all four threads during H.264 decode acceleration.Gallery:Windows 8 Running on a Quad-Core Kal-El Notebook\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/4411/windows-8-running-on-arm-nvidia-kalel-notebook-demoed\n",
      "Title: TI Reveals OMAP 5: The First ARM Cortex A15 SoC\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2011-02-08T01:59:00Z\n",
      "URL: https://www.anandtech.com/show/4153/ti-reveals-omap-5-the-first-arm-cortex-a15-soc\n",
      "Content: TI sure does have impeccable timing. Not 12 hours after we published ourLG Optimus 2X and NVIDIA Tegra 2 review, complete witha discussion of the 2011 SoC space, did TI announce its OMAP 5 SoC.OMAP 5 will go into production in the second half of 2011 and ship in phones during the second half of 2012. It’s a 28nm SoC with significant architectural upgrades compared to the OMAP 4.While the OMAP 4 integrates a pair of ARM Cortex A9 cores, OMAP 5 features twoARM Cortex A15 cores. TI already announced that it wasARM’s first licensee of the Cortex A15, so the OMAP 5 announcement is not too surprising.The ARM Cortex A15 cores can run at speeds of up to 2GHz, although specific frequencies will depend on the OEM implementation. At 2GHz TI claims the pair of Cortex A15s should be 3x the speed of the 1GHz Cortex A9s in the OMAP 4330. At the same clock speed, TI is boasting a 50% performance advantage from the A15 over the A9.With the OMAP 3 TI had a 256KB L2 used by its single Cortex A8. The OMAP 4 quadrupled L2 cache size to 1MB and shared it among both of its Cortex A9s. TI’s OMAP 5 doubles L2 cache size to 2MB, operating at the CPU clock speed, and it’s once again shared by the SoC’s two CPU cores (in this case Cortex A15s). The OMAP 5 retains the same dual-channel LPDDR2 memory interface from the OMAP 4.Architecturally we still don’t know much about what makes up a Cortex A15, although I suspect we’ll get more of that in the coming months.Update:It looks like ARM has released some info on the A15. It appears to be a wider, deeper, even more OoO architecture. More on this later.In our Tegra 2 review I mentioned that the transition from ARM11, to Cortex A8 and now to Cortex A9 left us with a generational performance improvement each step of the way. The move to A15 will be no different. ARM’s Cortex A8 went mainstream in the performance segment in 2010, Cortex A9 will do the same in 2011 and with any luck we’ll see A15 before the end of 2012. It’s this sort of yearly cadence that ARM and its partners must keep up in order to really catch up and surpass what Intel has been promising with Atom. Atom came out in 2008 and it won’t be until late 2012 that its architecture is truly refreshed. For a company that survived the mistakes of NetBurst, Intel doesn’t seem to have learned much of a lesson there.In addition to its dual Cortex A15 CPUs, the OMAP 5 will feature two ARM Cortex M4 cores as well. The M4 isn’t very useful as a general purpose microprocessor, it only supports the ARM Thumb/Thumb-2 instruction sets (not the full 32-bit ARMv7-A ISA). The M4 does have a suite of signal processing extensions that can be used to accelerate audio/video encode and decode. One application of the M4s will be still picture enhancement. A goal for the next-generation of SoCs is to begin to bridge the gap between smartphone camera quality and high-end point-and-shoot and DSLR cameras. Obviously we’ll always be bound by the poor optics possible on smartphones, however there’s still a lot that can be done in hardware to improve the quality of what’s captured.The OMAP 5 supports up to four cameras. One demo I saw Intel put together a while ago was of a smartphone with two equal resolution/quality cameras on its back. The only difference between the two was the focal length of the lens. Whenever you took a photo with the demo camera you’d actually capture the scene at two (vastly) different focal lengths. The SoC (in this case an Atom) would use the captured data to produce one image where the entire scene was in focus, with improved sharpness/detail over a single camera solution. It’s possible that OMAP 5 based smartphones may feature similar technologies and use its Cortex M4s to merge/interpolate data from the camera sensors.As ARM’s CPUs grow in power consumption, the amount of fixed function or specialized silicon set aside to offload various tasks will increase. The Cortex A15s should be used only for those applications that absolutely need them, anything else should be offloaded.The OMAP 5 SoC integrates Imagination Technologies’ PowerVR SGX 544 GPU, although it’s unclear how many cores will be present in TI’s implementation. Each SGX 544 core has four USSE2 pipes compared to four USSE pipes in the SGX 540, and two USSE pipes in the SGX 530. Each USSE2 pipe offers an increase in compute power compared to the old USSE pipes, however I don’t have details on specific differences in internal organization.TI announced two versions of the OMAP 5: the 5430 and 5432. The OMAP 5430 features a 14 x 14mm PoP with LPDDR2 memory support, while the 5432 is a larger 17 x 17mm BGA package with DDR3/DDR3L support. The 5430 will likely be used in smartphones while the 5432’s larger size would be better suited for tablets.On paper alone the OMAP 5 looks to be very powerful, but it needs to be. Also shipping in 2012 will beQualcomm’s MSM8960and we may seeProject Denveras well. The application processor race hasn’t even begun to heat up, but it’s starting to get interesting.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/4153/ti-reveals-omap-5-the-first-arm-cortex-a15-soc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NVIDIA's Project Denver: NV Designed, High Performance ARM Core: Updated!\n",
      "Author: Brian Klug\n",
      "Date Published: 2011-01-05T21:00:00Z\n",
      "URL: https://www.anandtech.com/show/4099/nvidias-project-denver-nv-designed-high-performance-arm-core\n",
      "Content: NVIDIA's CEO, Jen-Hsun Huang just announced Project Denver - its first CPU architecture design ever, based on ARM's ISA. This is a custom design done by NVIDIA in conjunction with ARM and targeted at the high performance computing (HPC) market.This is a huge announcement from NVIDIA, but not entirely unexpected. Prior to Project Denver NVIDIA licensed ARM IP but developed its own IP everywhere else for use in Tegra. Going forward, NVIDIA is turning into a full fledged SoC architecture company. This is a huge step in NVIDIA becoming a major player in the SoC evolution going forward.Update:NVIDIA provided some more details on the announcement. Project Denver is targeted at everything from PCs to HPC/servers. This is completely a high end play going after the x86 stronghold. Project Denver ties in completely with Microsoft's announcement to bring Windows 8 to ARM next year.NVIDIA also announced that it will be licensing ARM's Cortex A15 core, presumably for use in lower end devices (e.g. smartphones). I wouldn't be surprised if NVIDIA eventually moves to its own architecture based on ARM across the board.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/4099/nvidias-project-denver-nv-designed-high-performance-arm-core\n",
      "Title: ARM Aims at Intel, Cortex A15 Headed for Smartphones, Notebooks and Servers\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2010-09-09T04:01:00Z\n",
      "URL: https://www.anandtech.com/show/3905/arm-brands-eagle-cortex-a15-headed-for-smartphones-notebooks-and-servers\n",
      "Content: Last monthTI announced it was the first to license ARM’s next-generation Eagle core. Today, ARM is announcing the official name of that core: it’s the ARM Cortex A15.Architectural details are light, and ARM is stating that first silicon will ship in 2012 at 32/28nm. Here’s what we do know. The Cortex A15 will be a multi-core CPU, designs can have as few as a single core but most will have 2 - 4 cores depending on their target market.The cores will all be superscalar out-of-order designs and support Long Physical Address Extensions (greater than 32-bit memory addressing). I suspect the cores will be an evolution of theCortex A9. The Cortex A15 will support extensions to the ARMv7 instruction set to enable hardware virtualization support (among other things).The Cortex A15 will feature private L1 caches but a shared L2 cache (similar to the A9). The L2 cache is stated to be low latency and up to 4MB in size, although smartphones will probably see smaller versions. ARM is promising FP and SIMD performance improvements, but it isn't saying anything more than that.ARM is listing performance as 5x a Cortex A8 but we don’t have a good estimate vs. Cortex A9. Clock targets are as follows:1) 1 - 1.5GHz single or dual-core for smartphones and mobile devices2) 1 - 2GHz dual or quad-core for netbooks/notebooks/nettops3) 1.5 - 2.5GHz quad-core for home and web serversARM is targeting more than just smartphones with the Cortex A15. This will be the architecture that ARM takes into the low end notebook and netbook market. That’s right, with the Cortex A15 ARM is going after AMD and Intel - it wants to fend off the impending x86 assault on its territory.In addition to notebooks/netbooks based on Cortex A15, ARM will also be targeting the server market with its next architecture. As Xeon and Opteron grow more powerful, so does the need for simpler, lower power consumption servers. We’ve seensome companiesattempt to address this market, but expect the floodgates to open in a couple of years as ARM officially supports it. The Cortex A15 will also enable virtualization support, specifically for the server market.It’s too early to know anything about how well the Cortex A15 will do, but it’s clear that Atom (and maybe even Bobcat) are going to have to face a threat from below just as ARM is gearing up to face the threat from above.Given thatthe first Cortex A9 designs have yet to shipit’ll be a little while before we see smartphones, tablets, notebooks and servers based on Cortex A15. Today’s announcement is simply ARM’s statement of intent. ARM doesn’t plan on staying in the smartphone market forever, it has bigger things planned.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/3905/arm-brands-eagle-cortex-a15-headed-for-smartphones-notebooks-and-servers\n",
      "Title: TI First to License ARM's Next-Generation Eagle Core\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2010-08-09T12:59:00Z\n",
      "URL: https://www.anandtech.com/show/3844/ti-first-to-license-arms-nextgeneration-eagle-core\n",
      "Content: In oursmartphoneandtabletreviews we make sure to spend a good amount of time talking about the silicon powering these devices. There’s no reason that handset and tablet manufacturers shouldn’t be held to the same standards as the PC vendors we’ve worked with for years.Today the fastest phones are either based onARM’s Cortex A8core or a similar architecture as in the case ofQualcomm’s Snapdragon. Starting either late this year or more likely sometime next year we’ll see the first SoCs based on ARM’s first out of order core, the Cortex A9, shipping in phones. The roadmap doesn’t end there though.Later this year ARM will officially announce the successor to theCortex A9, codenamed Eagle. Today, Texas Instruments is announcing that it is the first company to license the ARM Eagle core.The announcement goes further. Not only is TI licensing the core, but it also helped define the specifications for the core. TI has been working on the design with ARM since June 2009. As a result, TI expects to be the first to market with SoCs based on ARM’s Eagle core.Unfortunately there’s not much to say about Eagle itself until ARM makes its announcement later this year. TI’s Cortex A9 based SoCs (OMAP 4) will be shipping in Q4, showing up in devices in early 2011. Based on that schedule I wouldn’t expect to see Eagle anytime sooner than 2012.Eagle’s performance is slated to be much more competitive with future derivatives ofIntel’s Moorestown SoC, while power consumption should be similar to existing designs thanks to the 2x-nm manufacturing process it will most likely be built on.We’re still waiting to hear more details about the Eagle architecture but with today’s announcement, something from ARM can't be too far away.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/3844/ti-first-to-license-arms-nextgeneration-eagle-core\n",
      "Title: Marvell's ARMADA: Custom Designed ARM SoCs Break 1GHz\n",
      "Author: Anand Lal Shimpi\n",
      "Date Published: 2009-10-19T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/2860\n",
      "Content: Intel used to be an ARM architecture licensee until 2006, when it sold its XScale division to Marvell. Intel had grown too large, too defocused, and in turn its core business had suffered. Don’t be confused, the focus wasn’t to be shifted back to desktop, but rather back to x86.It wouldn’t be until 2008 that Intel would reveal its more focused strategy unto the world:Atom.Intel's Atom processor coreWhile ARM and its licensees played off Atom as not being remotely threatening, all of them knew that it was only a matter of time. Publicly they reasserted ARM’s dominance in the market. Four billion ARM chips shipped last year alone. Intel sold on the order of tens of millions of Atoms. But privately, the wheels were in motion.ARM inked a deal withGlobalfoundries, AMD’s manufacturing arm, to bring ARM based SoCs to the fab. This gives ARM the sort of modern manufacturing it needs to compete with Intel. The second thing that’s changed is ARM licensees are now much more eager to talk about their architectures and what makes them special.ARM offers two licensing arrangements to its partners: a processor license or an architecture license. A processor license allows the partner to take an ARM designed core and implement it in their SoC. An architecture license allows the partner to take an ARM instruction set and use it in their own processor. The former is easier to implement, while the latter allows the licensee the ability to optimize the architecture for its specific needs.The Palm Pre - Powered by ARMCompanies like Samsung and TI hold ARM processor licenses. The Cortex A8 used in theiPhone 3GS(Samsung) and thePalm Pre(TI) is licensed directly from ARM. Marvell however has been an ARM architecture licensee for the past 5 years.It’s an ARMADAMarvell is introducing a fleet of new SoCs (system on a chip) and the brand is called ARMADA. Get it?Marvell is introducing four series of ARMADA and their target markets are below:SoCMarketARMADA 100 SerieseBook Readers, digital photo frames, portable NAV devices, etc...ARMADA 500 SeriesMIDs, netbooksARMADA 600 SeriesSmartphones, MIDsARMADA 1000 SeriesBlu-ray players, TVs, set-top boxesThese are SoCs so they’ve got CPU, GPU, I/O and networking all included on a single chip. The entire ARMADA line is built on TSMC’s 55nm process. The 100 is super low performance, useful in eBook readers, digital photo frames, IP cameras, etc... The 1000 is a multi-core version of the 100 with additional blocks designed for Blu-ray players, digital TVs and HD set-top boxes.Both the 100 and 1000 are based on Marvell’s Sheeva PJ1 ARM core. This core uses the ARMv5 instruction set like the ARM9 processor, but performance-wise it should be comparable to an ARM11 implementation.It’s a single issue in-order core with data forwarding support. The core is a hybrid between the original Marvell CPU team and the XScale CPU team that Marvell acquired in 2006. The pipeline depth is between 5 and 8 stages depending on the instruction group.The core has two separate ALUs (simple single cycle and complex two cycle), a load/store unit and a multiply unit. The ARMv5 instruction set doesn’t explicitly require floating point so there’s a separate coprocessor for all fp operations. Integer SIMD is handled through a separate Wireless MMX2 unit.Marvell wouldn’t reveal die sizes but indicated that the PJ1 is comparable to ARM11 based designs in both size and power characteristics.The more interesting SoCs are in the ARMADA 500 and 600 families. They use the Sheeva PJ4 core, Marvell’s answer to theCortex A8.The ARM Cortex A8 is an in-order, dual issue microprocessor with a 13-stage integer pipeline clocked at around 600 - 800MHz today. Marvell’s PJ4 core implements the same ARMv7 instruction set, but the architecture is much different. It’s still an in-order, dual issue core but the integer pipeline is 6 - 9 stages depending on the instruction.The shorter pipeline apparently doesn’t come at the expense of clock speed. Through the use of some custom logic Marvell is able to deliver clock speeds greater than 1GHz.Both L1 and L2 caches are supported, just like the Cortex A8.The biggest issue I can see with Marvell’s PJ4 is that it doesn’t support ARM’s NEON SIMDfp instruction set. Marvell argues that Wireless MMX2 penetration is higher than NEON. Given the limited use of Cortex A8 in the market today, I don’t see a lack of NEON compatibility as a major issue for now but it could be one down the road depending on developer uptake.On paper the PJ4 would appear to have much higher IPC and clock speed than the Cortex A8. Marvell was unwilling to share any power or performance data at this time, so it remains to be seen exactly how well Marvell’s architecture competes in the real world but on paper, at a high level, it looks good.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/2860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Corsair A115 CPU Cooler Review: Massive Air Cooler Is Effective, But Expensive\n",
      "Author: E. Fylladitakis\n",
      "Date Published: 2024-01-22T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/21240/the-corsair-a115-cpu-cooler-review\n",
      "Content: With recent high-performance CPUsexhibiting increasingly demanding cooling requirements, we've seen a surge in releases of new dual-tower air cooler designs. Though not new by any means, dual-tower designs have taken on increased importance as air cooler designers work to keep up with the significant thermal loads generated by the latest processors. And even in systems that aren't running the very highest-end or hottest CPUs, designers have been looking for ways to improve on air cooling efficiency, if only to hold the line on noise levels while the average TDP of enthusiast-class processors continues to eke up. All of which has been giving dual-tower coolers a bigger presence within the market.At this point many major air cooler vendors are offering at least one dual-tower cooler, and, underscoring this broader shift in air cooler design, they're being joined by the liquid-cooling focused Corsair. Best known within the PC cooling space for their expansive lineup of all-in-one (AIO) liquid PC CPU coolers, Corsair has enjoyed a massive amount of success with their AIO coolers. But perhaps as a result of this, the company has exhibited a notable reticence towards venturing into the air cooler segment, and it's been years since the company last introduced a new CPU air cooler. This absence is finally coming to an end, however, with the launch of a new dual-tower air cooler.Our review today centers on Corsair's latest offering in the high-end CPU air cooler market, the A115. Designed to challenge established models like theNoctua NH-D15, the A115 is Cosair's effort to jump in to the high-end air cooling market with both feet and a lot of bravado. The A115 boasts substantial dimensions to maximize its cooling efficiency, aiming not just to meet but to surpass the cooling requirements of the most demanding mainstream CPUs. This review will thoroughly examine the A115's performance characteristics and its competitive standing in the aftermarket cooling market.Corsair A115 CPU Cooler SpecificationsTypeTower Cooler (Twin)Dimensions153 x 155 x 164.8Fans2 x 140 mm Corsair AF140 fan1600RPM (max)RGBNoSupported SocketsIntel: LGA1700, LGA1200, LGA115xAMD: AM5, AM4Warranty5 YearsPrice$99.99Packaging & BundleCorsair supplies the A115 in a fairly large cardboard box with the company’s signature yellow/black design theme. The aesthetic design is rather minimal, with a picture of the cooler itself decorating the front side of the package. Plenty of information about the cooler can be found on the rear of the packaging but, more importantly, there is a QR code that leads to the comprehensive online manual.Upon unboxing the Corsair A115 CPU cooler, it becomes evident that the packaging adopts a methodical and organized approach. Each piece of the mounting hardware, essential for installation, is neatly segregated in individual paper bags, simplifying the setup process. This meticulous packaging reflects Corsair's attention to detail and commitment to user convenience. The mounting hardware provided is comprehensive, ensuring compatibility with most current CPU sockets. This includes support for Intel sockets (1700 / 1200 / 115x) as well as AMD sockets (AM5 / AM4).The Corsair A115 CPU CoolerThe Corsair A115 makes its presence known in the tower CPU cooler category with its impressive design. It features six highly efficient 6 mm sintered copper heat pipes, expertly designed to transfer heat from the base to the twin fin arrays. One array is a little smaller, providing better clearance to adjacent heatsinks and/or RAM modules. Standing at a height of 164.8 mm, this cooler is substantial in size and requires careful consideration for compatibility, particularly in larger ATX cases. And for that matter, consideration is needed when moving it around as well; with a weight of 2.2 kg (4.85 lbs), the A115 is extremely heavy, making it a very good idea to avoid vigorously moving a system around with the cooler installed in order to prevent potential damage.On the Corsair A115, the fins are designed with efficiency and noise reduction in mind. While the cooler doesn't have recessed fins specifically to reduce turbulence noise, the overall design still aims to minimize acoustic output while maintaining airflow efficiency. The sawtooth fins, although not asymmetric like in some other models, are crafted to optimize aerodynamic performance, enhancing air circulation and heat dissipation. The company logo is very subtly etched at the top of one fin array, barely visible.The Corsair A115 also pays attention to compatibility with various motherboard layouts. Its design accommodates VRM heatsinks commonly found on motherboards, ensuring that it does not interfere with other components. This thoughtful design consideration ensures a high degree of adaptability to different system configurations. While these design choices might slightly affect the total heat transfer surface area, they significantly boost the cooler's versatility, offering a well-balanced solution in terms of efficiency and compatibility with a range of PC builds. Regardless, the heat transfer surface area remains ample, at the expense of unexpectedly wide fins that make the A115 one of the bulkiest heatsinks we have ever tested.Examining the hulking mass as a whole, it's clear that Corsair aimed to put together a pure tower air cooler that could stand up to today's hot chips, as well as standards set by their own AIO liquid coolers. That's no small feat – but then, accomplishing this requires no small cooler, either.The Corsair A115 cooler boasts a rail mounting design for its fans, a feature that greatly enhances its compatibility with a variety of RAM configurations. This innovative rail design allows for adjustable height settings of the fan, providing considerable clearance over the RAM slots. This adjustability is especially beneficial for systems equipped with taller DIMMs or larger RAM modules. However, it's important to keep in mind that adjusting the front fan height to accommodate taller RAM modules will also increase the total height of the cooler. This necessitates wider case to accommodate the adjusted setup. Additionally, raising the front fan excessively may impact both the performance and the overall aesthetic cohesion of the build. Although height adjustments are not useful for the center fan, the rail makes its removal a matter of seconds, allowing the quick installation/removal of the cooler when necessary.The base of the Corsair A115 is a little more intricate than what we typically see in standard tower cooler designs. It is a split design, with a copper contact plate and a top support plate with steel retention arms sandwiching the heatpipes in between them. The A115 comes with pre-applied thermal XTM70 paste, meticulously applied to guarantee optimal spread upon installation.A key highlight of the Corsair A115 is its inclusion of high-quality PWM fans, which are a step away from the LED-lit fans seen in other models. The A115's AF140 fans do not feature LED lighting, focusing instead on pure performance and clean aesthetics. These fans are designed with fluid-dynamic motors and blades optimized for guided airflow, promoting effective heat dissipation. For the current retail price of the cooler, however, we hoped to see MagLev fans included. The 140 mm fans on the A115 can reach up to 1600 RPM.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/21240/the-corsair-a115-cpu-cooler-review\n",
      "Title: The Intel CES 2024 Client Computing Keynote Live Blog (Starts at 3:30pm PT/23:30 UTC)\n",
      "Author: Ryan Smith\n",
      "Date Published: 2024-01-08T23:25:00Z\n",
      "URL: https://www.anandtech.com/show/21218/the-intel-ces-2024-client-computing-live-blog\n",
      "Content: With CES's media day nearing its completion, Intel is hosting the first live, on-site PC-related keynote of the day. Intel Client Computing Group head Michelle Johnston Holthaus is hosting what Intel is calling their \"Intel Client Open House Keynote\", which will be covering the many Intel CPU announcements of the day, as well as showcasing the equally numerous laptops and other devices that partners will be releasing over the coming days and weeks.So come join us at 3:30pm for a look at the latest in client computing from Intel!06:31PM EST- And wasting no time, here we go06:31PM EST- Intel is starting things off with a Core Ultra branded promo video06:32PM EST- And here's Michelle Holthaus06:33PM EST- 3 key priorities for Intel06:33PM EST- 1) Reimagining how they deliver key expierences06:33PM EST- 2) Investing in driving forward the age of the AI PC (Core Ultra)06:33PM EST- 3) Locking arms with the OS and IHV partners to deliver innovation across hardware and software06:34PM EST- \"We are kicking off 2024 with a bang\"06:34PM EST- Recapping today's new Core mobile processor announcements. 14th Gen Core HX for DTRs, and Core (Series 1) U-series for mainstream thin & light06:34PM EST- Now transitioning to AI06:34PM EST- \"AI is everywhere\"06:35PM EST- Intel launched Core Ultra (Meteor Lake) last month. It's their first part with an NPU06:35PM EST- Core Ultra is Intel's most power effiicient client processor ever06:35PM EST- Up to 79% better than the competition06:36PM EST- New CPU core (built on Intel 4) is up to 11% better than the competition06:36PM EST- \"Power. Performance. Graphics. And AI. And we have it all\"06:36PM EST- And thanking Intel's partners for helping to bring Core Ultra to life06:37PM EST- Intel wants to bring AI to over 100 million systems by 202506:37PM EST- Showing a new, non-traditional device. The MSI Claw gaming handheld06:38PM EST- Intel considers the handheld market a showcase for power efficiency06:38PM EST- Also showing off a more traditional laptop with a unit from Samsung06:38PM EST- 40% better battery life than the previous-generation Galaxy Book 306:39PM EST- Intel's partners will also be delivering EVO and vPro devices using Core Ultra chips06:39PM EST- Intel will have a lot more to share about commercial offerings in the coming weeks06:39PM EST- We can expect more than 750 device designs using the Core and Core Ultra processors06:39PM EST- Many of them are already available at retail06:40PM EST- AI is a \"big, big\" part of what makes Core Ultra so important06:40PM EST- \"AI has the potential to impact every aspect of our lives\"06:40PM EST- Intel is all-in on AI that benefits humanity06:41PM EST- Generative AI in particular is going to \"transform the PC\"06:41PM EST- AI PCs will be your complete personal assistant06:41PM EST- All of which adds up to saving users' millions and millions of hours06:42PM EST- Intel believes that the PC is at an inflection point. With external estimates of 19% of PCs shipping with AI capabilities this year06:42PM EST- Using the CPU, GPU, and NPU together06:42PM EST- \"The true power of AI comes in seeing it in action\"06:43PM EST- (i.e. here come the partner demos)06:43PM EST- Now on stage: Jim Johnson, SVP of Intel's client business group06:43PM EST- Intel launched an AI acceleration program last month to help accelerate users' experiences06:44PM EST- Now rolling a brief testamonial video from software partners06:44PM EST- Now for some Microsoft Copilot demonstrations06:45PM EST- Showcasing the Copilot (repurposed Menu) key on their Core Ultra notebook06:45PM EST- Jim is having an image generated by a cloud server while also creating some code for him06:46PM EST- Product demo: Omnibridge06:47PM EST- Which is designed to help deaf users communicate06:47PM EST- They're doing real-time sign language reading, as well as real-time voice transcription06:48PM EST- With all of this being run on a Core Ultra laptop06:48PM EST- \"Cloud, and now done locally\"06:48PM EST- And now back to Michelle06:49PM EST- (Stagehands are bringing out bottles of water. Must be chat time)06:49PM EST- Now joining Michelle on stage, analyst Patrick Moorhead to talk about AI PCs06:50PM EST- The hardware needs to be delivered now to get innovation moving with AI PCs06:51PM EST- Moorhead is \"calling a supercycle\" starting later this year06:51PM EST- Several more guests are joining them on stage, including executives from Dell, HP, Lenovo, and Microsoft06:52PM EST- Commenting on how rare it is to get this collection of people together06:52PM EST- The group represents 60% of the PC sales volume06:53PM EST- Lenovo thinks 2024 is an inflection point for the PC industry06:54PM EST- \"AI PC will bring something completely new\"06:55PM EST- Lenovo is getting ready with what they think will be the broadest PC portfolio in the world06:56PM EST- They're also confident that this will trigger an acceleration of the PC replacement cycle. And a moderate expansion of the TAM06:56PM EST- HP is also very excited as well06:56PM EST- \"This is more than a PC revolution. This will be a computing revolution\"06:57PM EST- \"There is so much we're going to invent and create together\"06:58PM EST- Meanwhile Dell reiterates the idea that it's an inflection point for the industry. Which brings ample opportunity for innovation06:59PM EST- People are going to love their PCs with AI built into them07:00PM EST- As for Microsoft, Copilot is anchoring their thinking07:00PM EST- Copilot is the marquee experience07:01PM EST- MS says they've received a \"tremendous amount\" of positive feedback on the key07:01PM EST- And this is just the beginning07:02PM EST- MS wants Windows (and PCs) to be the destination for the best AI experiences07:02PM EST- Which means tweaking the OS to take advantage of both local and cloud resources07:03PM EST- AI is great for consumers. Interesting for enterprises07:04PM EST- Now for some new PC bragging07:04PM EST- Dell starting with a new XPS system. These were announced at the end of last week07:05PM EST- Edge-to-edge display, a non-physical capacitive function key row, and of course, Core Ultra CPUs07:07PM EST- Now to HP with their latest Spectre laptop07:07PM EST- Who wants to make PCs even more personal07:07PM EST- Moving from a computing device to a personal companion07:09PM EST- And Lenovo has brought a new Yoga Pro 9i. Their flagship, most powerful consumer device07:10PM EST- 2kg, 16-inch device07:12PM EST- Delivering AI PCs to users over the next year will take everyone on the stage; all of Intel's device and software partners07:12PM EST- And that's a wrap on the chat session07:12PM EST- Now, here's a quick look at what Intel will be bringing in the later half of the year07:12PM EST- Arrow Lake coming later this year. First desktop Intel chip with an AI accelerator07:13PM EST- Meanwhile the next-generation Lunar Lake is making good progress07:13PM EST- Intel is promising significant IPC improvements in the CPU core, and 3x the AI perf on the GPU and the CPU07:13PM EST- Already shipping samples to Intel's partners07:14PM EST- Meanwhile Core Ultra is ramping now in the market. Shipped millions of units in the last few weeks07:15PM EST- And that's a wrap on the client keynote. Thanks for joining us!\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/21218/the-intel-ces-2024-client-computing-live-blog\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ASRock Industrial 4X4 BOX-7840U mini-PC Review: AMD Phoenix in an UCFF Avatar\n",
      "Author: Ganesh T S\n",
      "Date Published: 2023-12-28T13:30:00Z\n",
      "URL: https://www.anandtech.com/show/21199/asrock-industrial-4x4-box7840u-minipc-review-phoenix-in-an-ucff-avatar\n",
      "Content: ASRock Industrial has been one of the most prolific vendors in the ultra-compact form-factor (UCFF) PC space over the last few years. They have been releasing 4\"x4\" systems based on the latest AMD and Intel platforms within a few weeks of their announcements by the processor vendors. On the AMD front, the company released the Rembrandt Refresh-based4X4 BOX-7735Uearlier this year, and followed that up a couple of quarters later with the Phoenix-based4X4 BOX-7040series.The 4X4 BOX-7040 series builds upon the I/O and iGPU improvements in the 4X4 BOX-7735U with the incorporation of AMD's latest Zen 4 CPU cores and RDNA3-based iGPU. The move to a 4nm TSMC process (from the 6nm one used for Rembrandt Refresh) and the CPU / iGPU improvements should theoretically delivery much better performance and power efficiency for a range of workloads.The Ryzen 7 7840U is a 28W TDP part that bears striking similarity to the Ryzen 7 7840HS that was already evaluated in a 65W mode inBeelink's GTR7. As a result of the lower power limit, the base clock is lowered. The iGPU configuration, I/Os, and other operating parameters remain the same. The U-series is primarily meant for notebook platforms, but ASRock Industrial (and other mini-PC vendors) have been deploying them in actively-cooled UCFF cases. The presence of a fan gives vendors a bit of leeway in configuring the power limits - and, as an extension, performance.ASRock Industrial ships the 4X4 BOX-7840U in the 'Normal Mode' with AMD's suggested TDP of 28W. However, a toggle in the BIOS can push the system into a 'Performance Mode' with a souped-up TDP of around 40W. This review explores the performance profile of the PC in both modes, and provides detailed insights into the differentiating aspects of AMD's Phoenix in a UCFF mini-PC platform.Introduction and Product ImpressionsUCFF systems have been steadily replacing bulky tower desktops for many use-cases over the last decade. This category has been experiencing growth in both home consumer and industrial settings, with the latter prompting the B2B / industrial computing arms of many motherboard vendors to provide more attention to such systems. ASRock Industrial (spun out of ASRock's business unit in 2018) has been creating UCFF systems based on both AMD and Intel platforms since 2019.This review delves in detail into the company's AMD Phoenix-based flagship UCFF offering - the4X4 BOX-7840U. Based on AMD's high-end Phoenix 28W offering (Ryzen 7 7840U), the new system is meant to be a follow-up to the Rembrandt-R-based4X4 BOX-7735Ureleased earlier this year. The Ryzen 7 7840U continues with the same 8C/16T configuration of the Ryzen 7 7735U. However, the fabrication process has moved from TSMC's 6nm FinFET to 4nm FinFET, and the CPU cores are now based on Zen 4. The integrated GPU has also been re-architected, moving from the RDNA2-based Radeon 680M in Rembrandt-R to the RDNA3-based 780M in Phoenix.ASRock Industrial's UCFF systems are non-descript machines that do not opt for a fancy case design. The functional chassis used in previous 4X4 BOX systems is retained for the 4X4 BOX-7840U also. This does mean an extension of the lifespan of the glossy polycarbonate fingerprint magnet casing, but that is set tochange in 2024. The I/O port locations are exactly the same as in the previous generation. The internal board is also largely the same, as the USB4 ports and DDR5 support were already pencilled in during the design of the 4X4 BOX-7735U.The company's 4X4 BOX-7040 series has only two members - one based on the Ryzen 5 7640U and the other based on the Ryzen 7 7840U. Some of the key relevant aspects are brought out in AMD's introductory slide to the product family back at the 2023 CES.ASRock Industrial's choice of I/O ports for the Intel and AMD systems in the same generation is worth a note. While the rear USB ports are only USB 2.0 in the AMD ones, they are USB 3.2 Gen 2 in the Intel versions. However, the two front panel Type-C ports support the full USB4 feature set including PCIe tunneling in the AMD systems. The Intel boards are skimped on, with only one of the Type-C ports capable of full USB4 / Thunderbolt 4 support and the other falling back to USB 3.2 Gen 2 with Alt DP support.The company offers both barebones version of the system as well as themotherboardalone. The former is typically sold in retail, while the motherboard is meant for the B2B channel. The barebones version package comes with a 120W DC power adapter (19V @ 6.32A), VESA mount (and associated screws), a geo-specific power cord, the main unit, and a product overview / user guide.The barebones version needs two DDR5 SODIMMs and a M.2 2280 SSD to complete the build. Crucial's DDR5-5600 SODIMM kit (2x16GB) was complemented by a Samsung 990 PRO 2TB Gen 4 NVMe SSD for this purpose.Access to the SODIMM and M.2 slots is via the underside, similar to the previous generation systems. Removal of the four screws at the bottom allows the panel to be popped off.Despite the support for a 2.5\" SATA drive in the system, ASRock Industrial strongly recommends not using it in order to aid with proper airflow. The installation process is otherwise similar to the older 4X4 BOX systems, and we were up and running with a freshly installed OS in no time (after installing the chipset drivers from the product support page). The Ryzen 7 7840U does include the XDNA engine (now rebranded as Ryzen AI in Phoenix Refresh), but the drivers for that (recognized as an IPU in Device Manager) are not yet available through Windows Update.The full specifications of the review sample (as tested) are provided in the table below. As we will note in the next section, the BIOS allows the system to be configured in either of two modes with different TDPs, as specified in the Processor entry.Systems Specifications(as tested)ASRock 4X4 BOX-7840U (Performance)ASRock 4X4 BOX-7840U (Normal)ProcessorAMD Ryzen 7 7840UZen 4 (Phoenix) 8C/16T, 3.3 - 5.1 GHzTSMC 4nm, 16MB L3, 28WTarget TDP : 40WAMD Ryzen 7 7840UZen 4 (Phoenix) 8C/16T, 3.3 - 5.1 GHzTSMC 4nm, 16MB L3, 28WTarget TDP : 28WMemoryCrucial CT16G56C46S5.M8G1 DDR5-5600 SODIMM46-45-45-90 @ 5600 MHz2x16 GBCrucial CT16G56C46S5.M8G1 DDR5-5600 SODIMM46-45-45-90 @ 5600 MHz2x16 GBGraphicsAMD Radeon 780M (RDNA3 / Phoenix) - Integrated(12 CUs @ 2.7 GHz)AMD Radeon 780M (RDNA3 / Phoenix) - Integrated(12 CUs @ 2.7 GHz)Disk Drive(s)Samsung SSD 990 PRO MZ-VP92T0B(2 TB; M.2 2280 PCIe 4.0 x4 NVMe;)(Samsung 7thGen. V-NAND 176L (136T) 3D TLC; Samsung Pascal S4LV008 Controller)Samsung SSD 990 PRO MZ-VP92T0B(2 TB; M.2 2280 PCIe 4.0 x4 NVMe;)(Samsung 7thGen. V-NAND 176L (136T) 3D TLC; Samsung Pascal S4LV008 Controller)Networking1x 2.5 GbE RJ-45 (Realtek RTL8125BG)1x GbE RJ-45 (Realtek RTL8111EPV)Mediatek MT7922 (RZ616) Wi-Fi 6E (2x2 802.11ax - 1.9 Gbps)1x 2.5 GbE RJ-45 (Realtek RTL8125BG)1x GbE RJ-45 (Realtek RTL8111EPV)Mediatek MT7922 (RZ616) Wi-Fi 6E (2x2 802.11ax - 1.9 Gbps)AudioRealtek ALC256 (3.5mm Audio Jack in Front)Digital Audio with Bitstreaming Support over HDMI and Display Port (Type-C)Realtek ALC256 (3.5mm Audio Jack in Front)Digital Audio with Bitstreaming Support over HDMI and Display Port (Type-C)Video2x HDMI 2.02x Display Port 1.4a over USB4 Type-C2x HDMI 2.02x Display Port 1.4a over USB4 Type-CMiscellaneous I/O Ports2x USB 2.0 (Rear)2x USB4 Type-C (Front)1x USB 3.2 Gen 2 Type-A (Front)2x USB 2.0 (Rear)2x USB4 Type-C (Front)1x USB 3.2 Gen 2 Type-A (Front)Operating SystemWindows 11 Enterprise (22621.2861)Windows 11 Enterprise (22621.2861)Pricing(Street Pricing on December 27th, 2023)US$570(barebones)US $837 (as configured, no OS)(Street Pricing on December 27th, 2023)US$570(barebones)US $837 (as configured, no OS)Full SpecificationsASRock Industrial 4X4 BOX-7840U SpecificationsASRock Industrial 4X4 BOX-7840U SpecificationsIn the next section, we take a look at the system setup and follow it up with a detailed platform analysis.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/21199/asrock-industrial-4x4-box7840u-minipc-review-phoenix-in-an-ucff-avatar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NVIDIA at SC23: H200 Accelerator with HBM3e and Jupiter Supercomputer for 2024\n",
      "Author: Ryan Smith\n",
      "Date Published: 2023-11-13T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/21136/nvidia-at-sc23-h200-accelerator-with-hbm3e-and-jupiter-supercomputer-for-2024\n",
      "Content: With faster and higher capacity HBM3E memory set to come online early in 2024, NVIDIA has been preparing its current-generation server GPU products to use the new memory. Back in August we saw NVIDIA’s plans to release an HBM3E-equipped version of the Grace Hopper GH200 superchip, and now for the SC23 tradeshow, NVIDIA is announcing their plans to bring to market an updated version of the stand-alone H100 accelerator with HBM3E memory, which the company will be calling the H200.Like its Grace Hopper counterpart, the purpose of the H200 is to serve as a mid-generation upgrade to the Hx00 product line by rolling out a version of the chip with faster and higher capacity memory. Tapping the HBM3E memory thatMicronand others are set to roll out n, NVIDIA will be able to offer accelerators with better real-world performance in memory bandwidth-bound workloads, but also parts that can handle even larger workloads. This stands to be especially helpful in the generative AI space – which has been driving virtually all of the demand for H100 accelerators thus far – as the largest of the large language models can max out the 80GB H100 as it is.Meanwhile, with HBM3E memory not shipping until next year, NVIDIA has been using the gap to announce HBM3E updated parts at their leisure. Following this summer’s GH200 announcement, it was only a matter of time until NVIDIA announced a standalone version of the Hx00 accelerator with HBM3E, and this week NVIDIA is finally making that announcement.NVIDIA Accelerator Specification ComparisonH200H100A100 (80GB)FP32 CUDA Cores16896?168966912Tensor Cores528?528432Boost Clock1.83GHz?1.83GHz1.41GHzMemory Clock~6.5Gbps HBM3E5.24Gbps HBM33.2Gbps HBM2eMemory Bus Width6144-bit5120-bit5120-bitMemory Bandwidth4.8TB/sec3.35TB/sec2TB/secVRAM141GB80GB80GBFP64 Vector33.5 TFLOPS?33.5 TFLOPS9.7 TFLOPSINT8 Tensor1979 TOPS?1979 TOPS624 TOPSFP16 Tensor989 TFLOPS?989 TFLOPS312 TFLOPSFP64 Tensor66.9 TFLOPS?66.9 TFLOPS19.5 TFLOPSInterconnectNVLink 418 Links (900GB/sec)NVLink 418 Links (900GB/sec)NVLink 312 Links (600GB/sec)GPUGH100(814mm2)GH100(814mm2)GA100(826mm2)Transistor Count80B80B54.2BTDP700W700W400WManufacturing ProcessTSMC 4NTSMC 4NTSMC 7NInterfaceSXM5SXM5SXM4ArchitectureHopperHopperAmpereBased on the same GH100 GPU as found in theoriginal H100, the new HBM3E-equipped version of the H100 accelerator will be getting a new model number, H200, to set it apart from its original predecessor and align it with theGH200 superchip(whose HBM3E version is not getting a distinct model number).Looking at the specifications being disclosed today, the H200 basically looks like the Hopper half of GH200 as its own accelerator. The big difference here, of course, is swapping out HBM3 for HBM3E, which is allowing NVIDIA to boost both memory bandwidth and capacity – as well as NVIDIA enabling the 6thHBM memory stack, which was disabled in the original H100. This will increase the H200’s memory capacity from 80GB to 141GB, and memory bandwidth from 3.35TB/second to what NVIDIA is preliminarily expecting to be 4.8TB/second – an approximately 43% increase in bandwidth.Working backwards here based on total bandwidth and memory bus width, this indicates that H200’s memory will be running at roughly 6.5Gbps/pin, a roughly 25% frequency increase versus the original H100’s 5.3Gbps/pin HBM3 memory. This is actually well below the memory frequencies that HBM3E is rated for – Micron wants to hit 9.2Gbps/pin – but since it’s being retrofit to an existing GPU design, it’s not surprising to see that NVIDIA’s current memory controllers don’t have the same range.The H200 will also keep GH200’s unusual memory capacity of 141GB. The HBM3E memory itself physically has a capacity of 144GB – coming in the form of six 24GB stacks – however NVIDIA is holding back some of that capacity for yield reasons. As a result, customers don’t get access to all 144GB on board, but compared to H100 they are getting access to all six stacks, with the capacity and memory bandwidth benefits thereof.As we’ve noted in past articles, shipping a part with all 6 working stacks will essentially require a perfect chip, as H100’s specs very generously allowed NVIDIA to ship parts with a non-functional stack. So this is likely to be a lower volume, lower yielding part than comparable H100 accelerators (which are already in short supply).Otherwise, nothing NVIDIA has disclosed so far indicates that H200 will have better raw computational throughput than its predecessor. While real-world performance should improve from the memory changes, the 32 PFLOPS of FP8 performance that NVIDIA is quoting for an HGX H200 cluster is identical to the HGX H100 clusters available on the market today.Finally, as with HBM3E-equipped GH200 systems, NVIDIA is expecting H200 accelerators to be available in the second quarter of 2024.HGX H200 Announced: Compatible With H100 SystemsAlongside the H200 accelerator, NVIDIA is also announcing their HGX H200 platform, an updated version of the 8-way HGX H100 that uses the newer accelerator. The true backbone of NVIDIA’s H100/H200 family, the HGX carrier boards house 8 SXM form factor accelerators linked up in a pre-arranged, fully-connected topology. The stand-alone nature of the HGX board allows it to be plugged in to suitable host systems, allowing OEMs to customize the non-GPU parts of their high-end servers.Given that HGX goes hand-in-hand with NVIDIA’s server accelerators, the announcement of the HGX 200 is largely a formality. Still, NVIDIA is making sure to announce it at SC23, as well as making sure that HGX 200 boards are cross-compatible with H100 boards. So server builders can use HGX H200 in their current designs, making this a relatively seamless transition.Quad GH200 Announced: 4 GH200s Baked Into a Single BoardWith NVIDIA now shipping both Grace and Hopper (and Grace Hopper) chips in volume, the company is also announcing some additional products using those chips. The latest of which is a 4-way Grace Hopper GH200 board, which NVIDIA is simply calling the Quad GH200.Living up to the name, the Quad GH200 places four GH200 accelerators on to a single board, which can then be installed in larger systems. The individual GH200s are wired up to each other in an 8-chip, 4-way NVLink topology, with the idea being to use these boards as the building blocks for larger systems.In practice, the Quad GH200 is the Grace Hopper counterpart to the HGX platforms. The inclusion of Grace CPUs technically makes each board independent and self-supporting, unlike the GPU-only HGX boards, but the need to connect them to host infrastructure remains unchanged.A Quad GH200 node will offer 288 Arm CPU cores and a combined 2.3TB of high-speed memory. Notably, NVIDIA does not mention using the HBM3E version of GH200 here (at least not initially), so these figures seem to be with the original, HBM3 version. Which means we’re looking at 480GB of LPDDR5X per Grace CPU, and 96GB of HBM3 per Hopper GPU. Or a total of 1920GB of LPDDR5X and 384GB of HBM3 memory.Jupiter Supercomputer: 24K GH200s at 18.2 Megawatts, Installing in 2024Finally, NVIDIA is announcing a new supercomputer design win this morning withJupiter. Ordered by the EuroHPC Joint Undertaking, Jupiter will be a new supercomputer built out of 23,762 GH200 nodes. Once it comes online, Jupiter will be the largest Hopper-based supercomputer announced thus far, and is the first one that is explicitly (and publicly) targeting standard HPC workloads as well as the low-precision tensor-driven AI workloads that have come to define the first Hopper-based supercomputers.Contracted to Eviden and ParTec, Jupiter is a showcase of NVIDIA technologies through and through. Based on the Quad GH200 node that NVIDIA is also announcing today, Grace CPUs and Hopper GPUs sit at the core of the supercomputer. The individual nodes are backed by a Quantum-2 InfiniBand network, no doubt based on NVIDIA’s ConnectX adapters.The company is not disclosing specific core count or memory capacity figures, but since we know what a single Quad GH200 board offers, the math is simple enough. At the top end (assuming no salvaging/binning for yield reasons), this would be 23,762 Grace CPUs, 23,762 Hopper H100-class GPUs, and roughly 10.9 PB of LPDDR5X and anther 2.2PB of HBM3 memory.The system is slated to offer 93 EFLOPS of low-precision performance for AI uses, or over 1 EFLOPS of delivered high-precision (FP64) performance for traditional HPC workloads. The latter figure is especially notable, as that would make Jupiter the first NVIDIA-based exascale system for HPC workloads.That said, NVIDIA’s HPC performance claims should be taken with a word of caution, as NVIDIA is still counting tensor performance here – 1 EFLOPS of FP64 is something 23,762 H100s can only provide with FP64 tensor operations. The traditional metric for theoretical HPC supercomputer throughput is vector performance rather than matrix performance, so this figure isn’t entirely comparable to other systems. Still, with HPC workloads also making significant use of matrix math in parts, it’s not an entirely irrelevant claim, either. Otherwise, for anyone looking for the obligatory Frontier comparison, the straight vector performance of Jupiter would be around 800 TFLOPS, versus over twice that for Frontier. How close the two systems get in real-world conditions, on the other hand, will come down to how much matrix math is used in their respective workloads (LINPACK results should be interesting).No price tag has been announced for the system, but power consumption has: a toasty 18.2 Megawatts of electricity (~3MW less than Frontier). So whatever the true price of the system is, like the system itself, it will be anything but petite.According to NVIDIA’s press release, the system will be housed at the Forschungszentrum Jülich facility in Germany, where it will be used for “the creation of foundational AI models in climate and weather research, material science, drug discovery, industrial engineering and quantum computing.” Installation of the system is scheduled for 2024, though no date has been announced for when it’s expected to come online.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/21136/nvidia-at-sc23-h200-accelerator-with-hbm3e-and-jupiter-supercomputer-for-2024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qualcomm Snapdragon X Elite Performance Preview: A First Look at What’s to Come\n",
      "Author: Ryan Smith\n",
      "Date Published: 2023-10-30T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/21112/qualcomm-snapdragon-x-elite-performance-preview-a-first-look-at-whats-to-come\n",
      "Content: Last week we saw the announcement of Qualcomm’s first post-Nuvia laptop SoC design, theSnapdragon X Elite. A new brand name being anchored by a new, custom Qualcomm CPU core, the Snapdragon X Elite will be Qualcomm’s most aggressive entry into the laptop SoC market to date, with Qualcomm dedicating far more in the way of engineering resources – and marketing resources – into developing their latest generation Arm SoC for Windows laptops. Backed by Qualcomm’s custom Arm CPU core, Oryon, the company is aiming to make the Snapdragon X Elite a watershed moment for the Snapdragon brand, both carving out a piece of the lucrative Windows laptop market while also setting the stage for a generation of even more powerful smartphone SoCs in 2024.As outlined in the company’s keynote andrelated announcementsat last week’s Snapdragon Summit, Qualcomm is incredibly proud of (and gung-ho on) the Snapdragon X Elite, and they are eager to show off what it can do. With benchmark scores that, in some cases, are beating some of the best chips from Intel, AMD, and Arm, Qualcomm believes they have a hot hand with the Snapdragon X Elite, and they want to show it off as soon as possible. Even despite the fact that actual retail devices won’t be available until mid-2024 – at least another 6 months from now.As part of that promotional push, alongside the public disclosures at last week’s event, Qualcomm also held an embargoed event for press and analysts to further demonstrate the Snapdragon X Elite and its performance. In a closed-door benchmarking session, the company set out a series of Snapdragon X Elite reference design laptops pre-loaded with popular benchmark software, most of which the company had also used in its performance claims from last week’s keynote sessions. The purpose of the session was to demonstrate in front of the gathered crowd that Qualcomm’s reported benchmark scores were no exaggeration or anomaly, and that the company had devices up and running right now that could hit those chart-topping scores. As the saying goes, “seeing is believing,” and Qualcomm’s benchmarking session was all about making sure the Snapdragon X Elite was being seen in action.With devices not arriving until next year, today’s disclosure is very much a preview. And a hands-off one at that; while the press was allowed to observe the benchmarks in action, we weren’t allowed to touch the laptops (least we try to run CPU-Z or Crysis on them), as Qualcomm is holding back on disclosing some of the technical details of Oryon and the Elite SoC altogether. So while we can report on what we saw, it’s important to note that these are Qualcomm’s benchmark numbers, and not our own. Though with Qualcomm picking highly standardized benchmarks, they’re trying to keep things as comparable and above-board as possible.Competitive Context: Timing Is EverythingPart of Qualcomm showing off so many benchmark results so soon is, simply put, a matter of pride. It’s not every day your company gets to show off a new SoC anchored by a custom CPU core developed by a team of engineers, that, by and large, draws from the CPU architects behind Apple’s highly performant A/M series SoCs, never mind their collective experience from even earlier projects. Theestablishment of the Nuvia teamalmost 4 years ago made waves at the time thanks to the stacked talent and the potential to disrupt the current CPU core ecosystem, and while the Qualcomm acquisition changed that trajectory from servers to mobile devices, the potential for disruption is still there.If nothing else, the introduction of a clean sheet high-performance CPU design makes Oryon the most interesting story in hardware tech right now, and the most interesting thing to happen in the CPU space since the introduction of Apple’s M1. And if the Snapdragon X Elite as a whole can deliver on those performance claims – CPU, GPU, and NPU – then that makes things all the more interesting. As a result, Qualcomm is very, very eager to demonstrate the hardware that going to be behind their next-generation laptop, handset, and automotive designs.At the same time, however, revealing Snapdragon X Elite and publishing benchmark figures for it is all about timing. Qualcomm wants to control the narrative as much as possible – they want the Snapdragon X Elite to be seen now and compared to rival hardware now. Notnext day, next week, ornext month.Ultimately, the 6+ month gap until retail devices launch means that the competition for Qualcomm’s upcoming SoC isn’t going to be today’s chips such as the Apple M2 series or Intel’s various flavors of Alder/Raptor Lake. Almost everyone is going to have time to roll out a new generation of chips between now and then. So while Qualcomm’s SoC may be ready right now, we’ve yet to see what they’ll be competing against in premium devices. That doesn’t make today’s benchmark disclosure any less enlightening, but it means that Qualcomm is aiming at a moving target – beating Apple, AMD, or Intel today is not a guarantee that it’ll still be the case in 6 months.In any case, besides being able to pick their competition, an early preview also gives Qualcomm a chance to line up OEM partners for their new SoC. The 8cx chips, three generations in all, have been successively unpopular. Despite the fact that the software side of the Windows-on-Arm ecosystem finally came together in the last couple of years with Windows 11, the8cx Gen 3has been the least-adopted chip from an OEM standpoint; only a handful of devices, such as the Lenovo ThinkPad X13s and Microsoft’s not-a-Snapdragon Surface Pro 9, were released based on it. So Qualcomm would like to reverse that trajectory by lining up more OEMs and more designs for what is intended to be a much higher-profile chip launch for the company.The Reference Laptops: Big & SmallerDiving right into matters, for their benchmark demonstration, Qualcomm put together two reference design laptops. Dubbed “demo config A” and “demo config B”, the systems are essentially DTR-style and thin & light laptops, somewhat in the range of Apple’s 14-inch and 16-inch MacBook Pro designs. Qualcomm won’t be selling laptops based on these designs, but they are meant to show off what OEMs can do with the hardware, and how the hardware would perform in similar designs.\"Demo Config A\", a high-performance 16.5-inch DTR reference laptop designShowcasing reference designs for performance purposes is very typical overall for Qualcomm. This is also how the company handles their smartphone testing – in most years we’ll get to benchmark a Snapdragon mobile SoC running in a Qualcomm Reference Device (QRD) as a preview before retail devices are available. Even though they’re not a vertically integrated company, they still need to develop devices to test their own hardware, and they like to use those to initially show off their designs.Qualcomm Snapdragon X Elite Reference Test SystemsAnandTechHigh Perf System(Config A)Thin and Light(Config B)2 Core Max Turbo4.3GHz4.0GHzAll Core Max Turbo3.8GHz3.4GHzRAMLPDDR5X-8533LPDDR5X-8533Display16.5-inch LCD3840x216014.5-inch OLED2880x1800Thickness16.8mm15mmDevice TDP80W23WCoolingActiveActiveBattery87Wh58WhBesides differing in physical size, the two laptop designs have different performance and TDP envelopes. The high performance (A) system gets an equally high performance Snapdragon X Elite chip, with a maximum clockspeed of 4.3GHz for 2 core turbo, and 3.8GHz for all-core turbo. The smaller thin and light system (B) is clocked lower, at 4.0GHz for 2 core turbo and 3.4GHz for all-core turbo. The laptops have lower TDPs as well; Qualcomm is disclosing what they call “device TDPs” for the laptops, which are 80W and 23W respectively. This puts the thin and light laptop on the edge of being able to be passively (fanless) cooled, but here Qualcomm is using active cooling on both laptops.Notably, here, both laptops are paired with LPDDR5X memory running at 8533Mbps/pin. While other laptop vendors also support LPDDR5(X), none of them support these data rates with their current chips (most of which top out at LPDDR5X-6400). So the Snapdragon Elite X systems are getting a roughly 33% memory bandwidth bonus right off the bat – something that is especially helpful in GPU benchmarks.\"Demo Config B\", a portable, 14.5-inch reference laptop designOtherwise there is little to say about the systems, especially as we did not get the chance to touch them. They’re essentially meant to give us a look at the Snapdragon X Elite’s performance in TDP-constrained and unconstrained scenarios, showcasing how performance still remains high even while scaling down in power.The Numbers: Observed vs. Qualcomm Official GuidanceAs well as providing the laptops, Qualcomm also provided official guidance numbers for the hardware, mirroring the results that were in their keynote slides from earlier days. The purpose of their official numbers was to disclose the official performance range for their hardware, with the demo units on hand to demonstrate that they’re actually attaining those numbers. In other words, the official guidance is the performance Qualcomm has been getting in the lab, while the press was there to observe that they were attaining those numbers with real-world hardware.As an aside, I’m told that most of Qualcomm’s official data was put together by formerAnandTechguru Andrei Frumusanu. Which helps to explain why Qualcomm’s official figures are based on averaging out three runs of the various benchmarks, with the results presented as ranges. It’s a level of statistical finesse that few (if any) manufacturers provide.Qualcomm Snapdragon X Elite Benchmark ScoresAnandTechHigh Perf - ObservedHigh Perf - QC Expected RangeThin and Light - ObservedThin and Light - QC Expected RangeCinebench 2024 ST132131-132124122-123Cinebench 2024 MT12271211-1233997925-973Geekbench 6.2 ST29712939-297927802722-2798Geekbench 6.2 MT1537115087-153821402913849-14007Linux: Geekbench 6.2 ST*3236N/AN/AN/ALinux: Geekbench 6.2 MT*17387N/AN/AN/APCMark 10 Applications1349812869-131121273712433-13516GFXBench Aztec Ruins - Normal354 fps355-357 fps294 fps294-296 fps3DMark Wildlife Extreme45 fps44.65-44.78 fps39 fps39.0-39.2 fpsUL Procyon AI*17661750-180018131750-1800For their demonstration, Qualcomm lined up Cinebench, Geekbench, UL’s PCMark 10 Applications, Procyon AI, and 3DMark Wildlife Extreme benchmarks, as well as Kishonti’s GFXBench 5.0 Aztec Ruins. At first blush, it’s an unusual mix of benchmarks for a laptop demonstration, as many of these benchmarks are better known for being used in smartphone and tablet reviews. So it is not especially a PC-centric set of benchmarks.But with Qualcomm wanting to put their best foot forward – in part by ensuring everything run here was with native Arm binaries – it’s not surprising to see them sticking to a core set of benchmarks that are as equally optimized and tested for Arm as they are x86. According to Qualcomm, the benchmark decision was based on, in part, using benchmarks they frequently encountered in laptop reviews. In either case, it means that we’re looking at a slate of Arm-native benchmarks, without any x86 emulation at hand.Overall, all of the results we observed from the reference laptops fell within Qualcomm’s official performance ranges – if not exceeding them slightly in the case of PCMark 10 Applications. While Qualcomm’s choice of benchmarks may or may not be cherry-picked to an extent, the numbers they’re reporting are accurate.I’ve also put asterisks on a few benchmarks, though for different reasons. For the Procyon AI benchmark, the Qualcomm devices were running against Qualcomm’s own Snapdragon Neural Processing Engine (SNPE) runtime, which allowed them to access the NPU functions of the SoC. So this was not done using a vendor-neutral API, such as DirectML. Furthermore, in Qualcomm’s keynote slides, they were comparing data to AMD and Intel chips running CPU paths; so the end result is an NPU vs. CPU comparison, a very apples-to-oranges setup.As for the Linux Geekbench results, because Qualcomm does not yet have fan control working under Linux, these systems were running with their fans on full blast. Whereas the Windows systems were running with more typical fan ramp curves, and thus didn’t enjoy the Linux laptops’ effectively unlimited thermal environment. Regardless, the primary purpose of the Linux demo was to showcase that Linux was working on the Snapdragon Elite X as well – that it’s not just for Windows – as Qualcomm has aims of getting the SoC into Linux laptops as well.Putting the Numbers in Context: Snapdragon X Elite vs. The WorldThe validity of Qualcomm’s official numbers aside, they mean very little on their own. We need some kind of performance comparison to give them some context for just how well Snapdragon X Elite is performing.Unfortunately, Qualcomm’s eclectic choice of benchmarks makes any kind of third-party performance comparison very difficult. AnandTech itself does not review much in the way of laptops, and even across our publisher’s entire footprint, our sister sites likeTom’s Hardware,Laptop Mag,Tom’s Guide, andTechRadardon’t use most of these benchmarks in laptop reviews. Never mind the fact that some, like Cinebench 2024, are barely two months old.As a result, we don’t have sufficient (and consistent) data of our own to use in a performance comparison. So what we’re left with are Qualcomm’s own performance slides, which present data from select Intel, AMD, and Apple systems.Specifically, Qualcomm has collected data from a 2023 Razer Blade 15 (Core i7-13800H) running at an unconstrained TDP, a 2023 Asus ROG Zephyrus G14 (Ryzen 9 7940HS) running at an 80W power limit, and a 2022 13-inch MacBook Pro (M2). Notably, this group does not include Intel’s top mobile chip, the Core i9-13900H, but it does cover AMD’s top chip. Meanwhile the M2-powered MBP13 is the least-constrained option for showing off the vanilla M2 chip, but it lacks the greater CPU and GPU core counts of the Pro/Max variants used in the 14-inch and 16-inch MacBook Pro models.Notably absent are any other Qualcomm-powered notebooks. So we don’t have any Snapdragon 8cx Gen 3 data to use as a comparison.Qualcomm was quick to show off their Cinebench 2024 results in their keynote address, and understandably so given how popular of a CPU benchmark it is. Pulling ahead in single-threaded performance is no small task, given the constraints of both extracting more IPC from a given workload, and the lack of headroom to improve clockspeeds as vendors once could.Multi-threaded results are a bit more lopsided; most of the other chips in this chart only have 8 CPU cores. Even in the case of the 13800H, only 6 cores are performance cores (and the other 8 efficiency cores), which in a test of pure CPU throughput would put the Intel system on the back foot.Geekbench 6.2 is a narrower cluster of results. Of course, Qualcomm has themselves winning in single-threaded performance twice-over (both SDX Elite systems), though as Geekbench is a composite score of multiple sub-benchmarks to begin with, it’s hard to say how the chips compare in any given task. Things open up with multi-threaded results, as is expected, though again, not by as much as Cinebench.Switching over to GPU benchmarks, we have GFXBench Aztec Ruins. This is arguably the oddest choice of inclusion for a laptop benchmark suite, as, especially in “normal” mode, Aztec Ruins is designed for smartphone benchmarking. Never mind the fact that this benchmark is 5 years old.Qualcomm’s results have them well in the lead here. But given how unconventional this benchmark is in the PC space, I have significant doubts over whether it’s been meaningfully optimized for within Intel and AMD’s driver sets. The M2 is likely the only meaningful comparison.3DMark Wildlife Extreme, by contrast, is a more of a relevant GPU benchmark. But it’s still a cross-platform benchmark written with smartphones in mind – in this case using the more strenuous “extreme” setting. Typically, the graphics score of this benchmark is reported, rather than the frame rate. So it’s not a pure GPU benchmark.In any case, Qualcomm has in recent years gone in heavy with their integrated GPU designs in the smartphone space – usually outpacing the most recent Apple mobile SoC – so it’s not too surprising to see evidence of that here as well. Anything on the level of the M2 should have little trouble outperforming current x86 iGPUs, though I am a bit surprised that Qualcomm isn’t pulling further ahead of the M2 as well.Regardless, being that these are Windows PCs, I’m more eagerly awaiting benchmarks using Windows games. While Qualcomm will have to pay the x86 translation penalty, real games are going to be much more relevant towards real-world performance than a synthetic benchmark is.Finally, for the sake of completeness, I’m also including Qualcomm’s last slide for the UL Procyon AI benchmark. As noted earlier, these results are highly misleading without additional context: only the Qualcomm chips are running on an accelerated (NPU) backend. The Intel and AMD chips are using the generic Windows ML CPU backend.With that said, this situation is not Qualcomm’s fault – the Intel chip doesn’t have an equivalent NPU, and AMD’s isn’t sufficiently exposed to be used in this benchmark – so their results aren’t incorrect. But this is, at best, an aspirational benchmark, showing the benefits of having an NPU versus doing things on the CPU. Which, by the time the Snapdragon X Elite ships, all of the major vendors will be shipping hardware with NPUs.Closing Thoughts: An Interesting First LookOverall, Qualcomm’s early benchmark disclosure offers an interesting first look at what to expect from their forthcoming laptop SoC. While the competitive performance comparisons are poorly-timed given that next-generation hardware is just around the corner from most of Qualcomm’s rivals, the fact that we’re talking about the Snapdragon X Elite in the same breath as the M2 or Raptor Lake is a major achievement for Qualcomm. Coming from the lackluster Snapdragon 8cx SoCs, which simply couldn’t compete on performance, the Snapdragon X Elite is clearly going to be a big step up in virtually every way.Ultimately there’s still a great deal we don’t know about the Snapdragon X Elite SoC and its Oryon CPU cores, both in regards to its architecture and its performance (especially in x86 emulation). But Qualcomm has made enough progress thus far that the Snapdragon X Elite warrants keeping an eye on as the first devices get closer to their expected mid-2024 release date.Gallery:Snapdragon X Elite Benchmark Slide Deck\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/21112/qualcomm-snapdragon-x-elite-performance-preview-a-first-look-at-whats-to-come\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qualcomm Snapdragon Tech Summit Live Blog: Compute Spotlight\n",
      "Author: Ryan Smith\n",
      "Date Published: 2023-10-25T18:55:00Z\n",
      "URL: https://www.anandtech.com/show/21109/qualcomm-snapdragon-tech-summit-live-blog-compute-spotlight\n",
      "Content: We're here in sunny Hawaii for day two of Qualcomm's annual Snapdragon Summit.Yesterday, of course, was the company's prime keynote, where they announced their forthcoming Snapdragon X Elite SoC for laptops, as well as the Snapdragon 8 Gen 3 for smartphones. As well, the company gave us our first look at their Oryon CPU architecture, which will be driving the X Elite, and eventually Qualcomm's smartphones, automotive chips, and more.For day two of their event, today Qualcomm is going to take us through the technical side of their new chips, revealing a bit more about how they work and what new features and abilities have been added in this generation. So please join us for our live blog coverage of Qualcomm's technical track disclosures.03:01PM EDT- And here we go. Qualcomm's day two keynote is kicking off03:01PM EDT- Starting with an intro video. \"The PC is not enough\"03:04PM EDT- Recapping yesterday's big 4 announcements03:04PM EDT- Snapdragon X Elite03:04PM EDT- Snapdragon 8 Gen 303:05PM EDT- Qualcomm S7 Pro audio ASIC03:05PM EDT- Snapdragon Seemless multi-device connectivity03:06PM EDT- (This keynote is scheduled to go for 3 hours. We'll just be covering the X Elite and 8 Gen 3 sections, which are first)03:06PM EDT- Starting with Honor03:07PM EDT- Honor's CEO is now on the stage03:07PM EDT- Honor has a lot of talent. But innovation is slowing down. There are a lot of challenges03:08PM EDT- But it's also an era of potential. AI models, battery technology, and more03:09PM EDT- Honor has been a major partner of Qualcomm. They've made many, many devices based on Qualcomm's hardware03:09PM EDT- Discussing the Honor Magic V2 foldable phone03:09PM EDT- 9.9mm thick03:10PM EDT- 4.7mm when unfolded03:10PM EDT- 5000 mAh battery03:11PM EDT- Now discussing some of the phone's features, such as parallel spaces03:12PM EDT- Each space can have its own WhatsApp accounts03:13PM EDT- Or playing the same game with two instances/accounts at once03:14PM EDT- And, of course, Honor is big on developing more AI uses for smartphones03:16PM EDT- And where to draw the line between on-device and cloud processing. Privacy versus performance03:16PM EDT- On device AI performance is getting better quickly, thanks to recent hardware improvements such as Qualcomm's updated NPU03:18PM EDT- 6 to 7 billion parmeters is needed to get a useful on-device AI. 1 billion parameters is \"just a toy\"03:19PM EDT- And showing a demo of AI in action. Having a local model collate videos and cut them together03:21PM EDT- In this case the AI is selecting for things such as the person to be in the videos, whether they're smiling, what they're doing (dancing), etc03:22PM EDT- Announcing Magic 603:23PM EDT- 8 Gen 3 based phone03:24PM EDT- Also demoing \"MagicRing\" multi-device sharing (based on Snapdragon Seamless, I'd expect)03:25PM EDT- Sharing keyboard and mouse from the laptop over to the unfolded phone03:26PM EDT- And confirmed, it's Snapdragon Seamless-based03:27PM EDT- Honor will also be making a Snapdragon X Elite-based laptop03:28PM EDT- And that's Honor. Now back to Kedar03:29PM EDT- Now talking about Zoom and video calling03:29PM EDT- And rolling a video from Zoom03:29PM EDT- (Thus far this has been more of a partner showcase than a compute spotlight)03:30PM EDT- Lauding the hardware encode capabilities of Qualcomm's SoCs03:31PM EDT- Delivering good experiences for users requires working closely with partners like Honor and Zoom03:31PM EDT- And now we're getting to the technical dive03:32PM EDT- Starting with Snapdragon X Elite03:33PM EDT- Users want a sleek form factor with good battery life. And Snapdragon X Elite can deliver that03:34PM EDT- \"A massive leap above anything else in its class\"03:34PM EDT- Recapping the Oryon announcement03:34PM EDT- Wanted to deliver the ultimate performance without compromising on power efficiency03:35PM EDT- Tri-cluster design, 4 cores each. Up to 3.8GHz under normal circumstances03:35PM EDT- Or 2C turbo up to 4.3GHz03:35PM EDT- First Arm architecture CPU to hit over 4GHz03:36PM EDT- 42MB of total cache. Optimizations for virtualization and memory address translation03:37PM EDT- \"A sophisticated branch predictor\"03:37PM EDT- X Elite is designed to work in a range of laptop designs03:38PM EDT- Ultraportables up to large format laptops03:38PM EDT- Cinebench results03:38PM EDT- 2x faster performance at the same power03:39PM EDT- Now on to the GPU03:40PM EDT- 2x peak performance, or iso-performance at 23% of the power03:40PM EDT- First Snapdragon compute platform with updatable drivers03:41PM EDT- (Sounds like out-of-band display driver updates, separate from what MS distributes?)03:41PM EDT- Demo time with Blackmagic's Davinci Resolve03:42PM EDT- Resolve is a complete video editing, grading, and post-production tool03:42PM EDT- And Resolve has a suite of AI-based abilities these days03:43PM EDT- Voice isolation, dialogue leveler, and more03:43PM EDT- Blackmagic will be launching a native Arm version of Resolve next year for Windows03:43PM EDT- Blackmagic has been working with Qualcomm to make this happen03:44PM EDT- Briefly talking about performance versus a 12 core (Intel) processor03:44PM EDT- And they're not done optimizing Resolve or adding new AI-based capabilities03:45PM EDT- (Getting a native version of Resolve on Windows is a big win for Qualcomm. That is arguably the most important video editing tool on the market. It doesn't hurt that they offer the base version for free, either)03:45PM EDT- Now on to AI performance03:46PM EDT- Qualcomm has invested significant resources (and seemingly die space) on including a faster NPU03:47PM EDT- Support for AI models over 13B parameters03:47PM EDT- And with a smaller 7B model, X Elite can run at 30 tokens per second03:47PM EDT- Stable diffusion benchmarks03:48PM EDT- Over 2.5x faster with SD 1.5. More with highly optimized versions03:48PM EDT- New power delivery system for the NPU03:48PM EDT- (The tensor cores are on their own rail)03:48PM EDT- And Qualcomm's secret sauce of micro tile inferencing03:49PM EDT- 45 TOPS of AI performance03:49PM EDT- 50% faster AI compute performance on the GPU as well03:50PM EDT- UL Procyon benchmarks. Upwards of 10x faster than Intel/AMD chips03:51PM EDT- (Checking the notes, it looks like this is an NPU vs. CPU comparison)03:51PM EDT- The Hexagon NPU shows up in the Task Manager on Windows03:52PM EDT- Now for another partner showcase. HP03:54PM EDT- On-device local AI inference will bring new benefits03:55PM EDT- HP is very excited about all of this03:56PM EDT- And now switching over to talking about user experiences03:56PM EDT- Users asking their phones to do more and more has driven so much of the innovation in Snapdragon03:57PM EDT- X Elite brings the best of Qualcomm's mobile tech to the PC03:57PM EDT- External QC Snapdragon 5G modems03:57PM EDT- And Wi-Fi 7 (also external)03:58PM EDT- QC leading the push from USB to MIPI interface for web cameras03:58PM EDT- Which should deliver better phone-like front cameras03:58PM EDT- Smartphone level video quality to the PC03:59PM EDT- X Elite includes Snapdragon Sound for lossless spatial audio support04:00PM EDT- Partner time again. Rolling a Lenovo video04:01PM EDT- Talking about some of the Qualcomm-powered devices Lenovo has released over the years04:01PM EDT- Such as previous 8cx-based devices04:02PM EDT- Lenovo is going to expand their collaboration to take advantage of new AI abilities04:02PM EDT- More personalized and intuitive devices for consumers04:03PM EDT- And more efficient and collaborative teams for businesses04:04PM EDT- Now switching to discussing Snapdragon devices for commercial customers04:04PM EDT- Snapdragon for Enterprise and Ecosystem04:04PM EDT- Half of PC sales are commercial04:04PM EDT- So QC has been working with CIOs and IT leaders to address their needs04:04PM EDT- Privacy and security are more important than ever04:05PM EDT- Over 500 enterprises have tested/deployed Snapdragon devices04:05PM EDT- Recapping thbe security features of Qualcomm's PC platform04:06PM EDT- Pluton support. Secure enclave. Device resiliance. MS Secure Core certification04:06PM EDT- AI is the key to drive new business opportunities04:07PM EDT- Snapdragon X Elite is \"the total package\"04:08PM EDT- Partner/demo time04:08PM EDT- Uniphone04:09PM EDT- Today Uniphone is running all of their AI processes in the cloud04:09PM EDT- But with the latest Snapdragon hardware, they can run it on-device04:09PM EDT- Working to bring it to X Elite laptops04:10PM EDT- Voice-to-text transcription, senitment analysis, and more run locally04:10PM EDT- Now rolling a demo video04:11PM EDT- ~100ms latency on on-device transcription04:12PM EDT- The application was built in a matter of weeks04:12PM EDT- Praising Qualcomm's dev kit and toolset04:13PM EDT- Now back to QC and the Snapdragon ecosystem04:13PM EDT- \"We couldn't have done it alone\"04:13PM EDT- \"Every part of the value chain is optimized for snapdragon\"04:14PM EDT- QC wants to make it easier for devs to port their applications and enhance them for AI04:14PM EDT- Performance metrics on early builds are looking promissing04:15PM EDT- Now rolling a customer testamonial video04:16PM EDT- Customers talking about what they've been doing with Snapdragon hardware, and how it's made their software faster04:17PM EDT- QC has a rich ecosystem of partners ready to innovate on the next generation of devices04:18PM EDT- \"This will change the computing world\"04:18PM EDT- Now rolling another video. This time hardware partners04:19PM EDT- Snapdragon X Elite will also be a capable gaming chip04:20PM EDT- And QC is working with several studios on their games04:20PM EDT- Rolling another clip video04:21PM EDT- \"We are on a mission to build the best PC platform in the world\"04:21PM EDT- Now recapping Snapdragon X Elite speeds and feats04:22PM EDT- Coming mid-202404:23PM EDT- And that's a wrap on the Snapdragon X Elite session04:23PM EDT- Up next: Snapdragon 8 Gen 3 session. That should start in a few minutes04:24PM EDT- While we're on intermission, here's a quck photo of the X Elite chip in a plastic shell04:28PM EDT- Sadly, Qualcomm doesn't let us keep those chip packages. Which is probably for the best, as there's no shortage of geeks here who have screwdriver sets04:28PM EDT- Or in Charlie Demerjian's (SemiAccurate) case: micro-calipers04:31PM EDT- And the opposite view04:32PM EDT- There's also a Snapdragon 8 Gen 3 package floating around. However, it doesn't seem to have an actual silicon die in it; I'm pretty sure it's just the organic substrate04:33PM EDT- For the SD 8G3 segment of today's keynote, Qualcomm is dubbing this their \"Mobile Spotlight\"04:35PM EDT- With SD 8G3 phones launching tomorrow, there should be more to talk about in terms of final hardware. However, since QC dosen't make the phones, it's their customers who will reserve the right to make the big announcements04:35PM EDT- And here we go04:36PM EDT- Bringing the 8G3 product lead to the stage04:37PM EDT- Talking about how phones and use cases have evolved since 200804:37PM EDT- A QC chip powered the very first Android phone04:37PM EDT- Made on a65nm process04:38PM EDT- AI will be the next disruptive technology04:39PM EDT- 8G3 is \"the titan\" of on-device intelligence04:39PM EDT- 8G3 dev started about 3 years ago04:39PM EDT- Starting with the Hexagon NPU04:39PM EDT- 98% faster than the previous-gen Hexagon04:40PM EDT- 40% higher perf-per-watt04:40PM EDT- Raised clockspeed of the vector unit04:40PM EDT- And they have LPDDR5x memory support at 8.5Gbps04:41PM EDT- The low-power sensing hub has also been updated04:41PM EDT- On the CPU side of matters, new Arm Cortex cores, higher clockspeeds, and more04:42PM EDT- 20% faster perf and 20% greater power efficiency04:42PM EDT- Adreno GPU is 25% faster04:42PM EDT- 40% faster in ray tracing04:42PM EDT- 25% greater power efficiency as well04:43PM EDT- And emphasizing heterogeneous computing, and being able to use different blocks for different tasks that they excel at (and accelerate)04:44PM EDT- 10% improved power savings overall04:45PM EDT- And that's the quick recap on 8 Gen 3's architecture04:46PM EDT- Now for a deeper dive on AI for the 8G304:46PM EDT- Every generation has done more and been more efficient04:47PM EDT- Generative AI is giving phones the ability to new new task they could not before04:48PM EDT- QC wants to offer \"best in class experiences\" across all of their products04:49PM EDT- The smartphone with reap the benefits of on-device AI, particularly when it comes to privacy04:49PM EDT- Demo time. An on-device chatbot04:49PM EDT- Asking the AI to plan a customized trip04:50PM EDT- This is live back-and-forth with the phone04:51PM EDT- 8G3 can support models up to 10B parameters04:51PM EDT- Up to 20 tokens per second for LLMs04:51PM EDT- And meanwhile smaller models are getting more capable04:52PM EDT- Qualcomm is quantizing Llama 2 for INT404:52PM EDT- Scalar, vector, and tensor accelerators all working together04:53PM EDT- QC is also doing speculative decoding04:54PM EDT- First time it's been used on an edge device04:54PM EDT- Allows for a much smaller graph model. Start in the CPU and then send it out to the NPU04:55PM EDT- And QC isn't stopping there04:55PM EDT- They want to bring more models over to run on 8G304:55PM EDT- Now running a demo against a chatbot model that speaks Mandarin, asking it how to surf04:56PM EDT- The demo has failed. The phone application stalled04:56PM EDT- (It's a live demo; it happens)04:57PM EDT- Now on to another demo with an avatar-based chatbot04:57PM EDT- And a Knight Rider reference04:58PM EDT- The QC AI software stack has seen some major upgrades04:58PM EDT- QC is releasing a suite of optimized models for various tasks04:58PM EDT- Qualcomm AI Stack Models04:58PM EDT- 30+ models at launch, with many more coming04:59PM EDT- Adding PyTorch ExecuTorch(?) support04:59PM EDT- Stable Diffusion can run in under 0.6 seconds05:00PM EDT- Now for another demo, fast Stable Diffusion05:01PM EDT- Make AI generation as quick as taking a photo so that it can seamlessly be integrated into camera apps05:01PM EDT- And working with Snap for new Snapchat AI features05:02PM EDT- Introducing new tech called \"on device personalization\"05:03PM EDT- Essentially how to integrate information from the sensing hub to improve an on-device model's knowledge05:04PM EDT- Having the model plan the presenter's weekend in Maui05:04PM EDT- \"You could not do this in the cloud\"05:05PM EDT- On-device AI can offer better experiences than the cloud05:06PM EDT- Now transitioning to camera technology and AI05:07PM EDT- 8 Gen 3 will deliver two big changes. A big boost to image quality in any image quality. And turning the camera into a powerful handheld imaging sensor05:09PM EDT- Using the cognative ISP for depth separation05:09PM EDT- And using AI to quickly remove unwanted objects from full resolution videos05:10PM EDT- (Family gatherings will never be the same)05:10PM EDT- There's a new framerate conversion engine to interpolate frames from 30fps to 60fps05:11PM EDT- 8G3's can consume the output of 200MP sensors05:11PM EDT- And QC is working with Samsung to deliver such a sensor05:12PM EDT- And using dual conversion gain sensors to improve low-light sensitivity05:13PM EDT- 8G3 will also introduce Dolby HDR photo capture technology05:13PM EDT- 10bpc color, Rec.2020 color gamu05:13PM EDT- Photos fully backwards compatible with JPEG05:14PM EDT- And some of QCs partners are working on their own smartphone-sized sensors to detect additional information05:15PM EDT- 8G3 has added a second sensing ISP for a second always-sensing camera05:16PM EDT- Now on to gaming on 8 Gen 305:17PM EDT- Latest Snapdragon display controllers can support 240Hz displays05:18PM EDT- QC's Adreno Frame Motion interpolation engine has also been updated to improve frame generation quality and performance. A new optical flow engine, among other changes05:19PM EDT- QC also has their own spatial upscaling tech, Game Super Resolution (GSR)05:19PM EDT- Quick demo upscaling from 4K to 8K (again, this is spatial-only)05:20PM EDT- And talking about the game devs QC has been working with, and who is launching cross-platform games on day one05:20PM EDT- 8 Gen 3 is capable of running Unreal Engine 5 Lumen at fully interactive framerates05:21PM EDT- This is the first for a smartphone SoC05:22PM EDT- Now rolling a game trailer05:23PM EDT- That was Justice Mobile05:23PM EDT- And that's Snapdragon 8 Gen 3 gaming05:24PM EDT- Now wrapping up Snapdragon 8 Gen 3 in general05:25PM EDT- New CPU cores, NPU, faster GPU, and more (and a darn good modem)05:25PM EDT- That's Snapdragon 8 Gen 305:25PM EDT- 8G3 will be in flagship devices \"in just a few weeks\"05:26PM EDT- (Or days, in the case of Xiamoi)05:26PM EDT- And that's a wrap for the 8 Gen 3 session, and for us05:26PM EDT- Thank you for joining us. I'm off to check out some demos\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/21109/qualcomm-snapdragon-tech-summit-live-blog-compute-spotlight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qualcomm Previews Snapdragon X Elite SoC: Oryon CPU Starts in Laptops\n",
      "Author: Ryan Smith\n",
      "Date Published: 2023-10-24T19:00:00Z\n",
      "URL: https://www.anandtech.com/show/21105/qualcomm-previews-snapdragon-x-elite-soc-oryon-cpu-starts-in-laptops-\n",
      "Content: While Qualcomm has become wildly successful in the Arm SoC market for Android smartphones, their efforts to parlay that into success in other markets has eluded them so far. The company has produced several generations of chips for Windows-on-Arm laptops, and while each has incrementally improved on matters, it’s not been enough to dislodge a highly dominant Intel. And while the lack of success of Windows-on-Arm is far from solely being Qualcomm’s fault – there’s a lot to be said for the OS and software – silicon has certainly played a part. To make serious inroads on the market, it’s not enough to produce incrementally better chips – Qualcomm needs to make a major leap in performance.Now, after nearly three years of hard work, Qualcomm is getting ready to do just that. This morning, the company is previewing their upcoming Snapdragon X Elite SoC, their next-generation Arm SoC designed for Windows devices. Based on a brand-new Arm CPU core design from their Nuvia subsidiary dubbed “Oryon”, the Snapdragon X Elite is to be the tip of the iceberg for a new generation of Qualcom SoC designs. Not only is it the heart and soul of Qualcomm’s most important Windows-on-Arm SoC to date, but it will eventually be in smartphones and a whole lot more.But we’re getting ahead of ourselves. For now let’s focus on the Snapdragon X Elite SoC and the Oryon cores underpinning it.While this morning’s announcement from Qualcomm is far from a deep dive on the hardware, it’s our first look at what will be Qualcomm’s flagship SoC, and the new CPU cores within it. With a projected launch date of mid-2024, the first laptops based on the SoC are still several months away from hitting retail shelves – andabout a year delayed overall. None the less, Qualcomm has finished their silicon development work, and with the chip’s specifications locked down, the company is now on to polishing things for a launch next year.The Oryon CPU cores within the Snapdragon X Elite are the culmination ofQualcomm’s Nuvia acquisition from early 2021, and an even longer period of work for the Nuvia team. The ambition of the team, and the importance of the custom Arm architecture CPU cores, cannot be overstated. So the Snapdragon X Elite is going to be an interesting chip on multiple levels, as it sets the pace for the next generation of Qualcomm chip designs.Snapdragon Compute (Windows-on-Arm) SiliconAnandTechSnapdragon X EliteSnapdragon 8cxGen 3Snapdragon 8cxGen 2Snapdragon 8cxGen 1Prime Cores12x Oryon3.80 GHz2C Turbo: 4.3GHz4x C-X13.00 GHz4 x C-A763.15 GHz4 x C-A762.84 GHzEfficiency CoresN/A4x C-A782.40 GHz4 x C-A551.80 GHz4 x C-A551.80 GHzGPUAdrenoSD X Elite4.6 TFLOPSAdreno8cx Gen 3Adreno 690Adreno 680NPUHexagon45 TOPS (INT8)Hexagon 8cx Gen 315 TOPSHexagon 6909 TOPSHexagon 6909 TOPSMemory8 x 16-bitLPDDR5x-8533136GB/sec8 x 16-bitLPDDR4x-426686.3 GB/sec8 x 16-bitLPDDR4x-426686.3 GB/sec8 x 16-bitLPDDR4x-426686.3 GB.secWi-FiWi-FI 7 + BE 5.4(Discrete)Wi-Fi 6E + BT 5.1Wi-Fi 6 + BT 5.1Wi-Fi 5 + BT 5.0ModemSnapdragon X65(Discrete)Snapdragon X55/X62/X65(Discrete)Snapdragon X55/X24(Discrete)Snapdragon X24(Discrete)Process4nmSamsung 5LPETSMC N7TSMC N7Starting with a high-level look at the chip, the Snapdragon X Elite is a high-performance SoC designed to power Windows-on-Arm laptops. Qualcomm isn’t listing any official TDPs, but the company has told us that the Elite is designed to scale across a “broad range” of thermal designs. Active cooling will be needed to get the most out of the Elite, but according to Qualcomm, passive/fanless designs are possible as well, and we should expect to see some retail devices designed as such.Qualcomm is fabbing the chip on an unspecified 4nm process. Given their previous performance issues with Samsung’s 4nm line, it’s a very safe bet that they’re building this chip at TSMC – possibly using the N4P line. The silicon itself is a traditional monolithic die, so there is no use of chiplets or other advanced packaging here (though the wireless radios are discrete).CPU: Oryon By The DozenThe star of the show (if you’ll forgive the pun) is Oryon, Qualcomm’s new custom-designed Arm CPU core. Designed by the Nuvia team that Qualcomm acquired in 2021, Oryon is the first high-performance, fully-custom Arm CPU core created by Qualcomm in several years. And following multiple generations of lackluster Snapdragon Compute SoCs built out of Arm Cortex-A/X designs and functionally bigger versions of Qualcomm’s mobile SoCs, Oryon marks a major change in direction for Qualcomm.Being that this is a preview, there are no significant architectural details to share on Oryon at this time. We don’t know the width, or various buffer sizes, execution ports, etc. But what we do know is that Qualcomm didn’t aim low with this SoC – the Nuvia team was working on a server-grade CPU core prior to their acquisition, and that kind of aggressive design has carried over into Oryon as well. Which, after all, was one of the major goals of Qualcomm’s acquisition, as they have desired a high performance CPU core to push them ahead of the other laptop (and eventually mobile) chip makers.The Snapdragon X Elite SoC ships with 12 Oryon CPU cores – and that’s it. Unlike Qualcomm’s 8cx family of designs, there are no distinct “efficiency” and “performance” cores based on different microarchitectures; this is a homogenous CPU design, more akin to traditional PC processors. This means that Oryon needs to pull double duty, excelling in performance in heavy workloads without chewing up a bunch of power in light workloads.The Oryon CPU cores are broken up into three clusters of 4 cores each. We’re still waiting on further technical details, of course, but it’s a safe assumption that each cluster is on its own power rail, so that unneeded clusters can be powered down when only a handful of cores are called for.Just on this basis alone, Snapdragon X Elite looks like a far more potent performer than the 8CX chips it replaces. The 8cx Gen 3 offered just 4 performance cores (Cortex-X1) and another 4 eficiency cores (Cortex-A78), so Snapdragon X Elite will hit the streets with 50% more CPU cores never mind the higher performance of those cores. For a laptop chip, Qualcomm is throwing a lot of CPU cores at the matter.With regards to clockspeeds, in an all-core turbo workload, all 12 Oryon CPU cores can hit run at up to 3.8GHz, power and thermal headroom permitting. Meanwhile in lighter workloads, the chip supports turboing up to 4.3GHz on 2 cores. Qualcomm’s slide on this matter shows a core from each cluster, but it’s unclear whether this is some kind of prime/favored core in action (where only certain cores are designed/validated for those speeds) or if it’s simply a stylistic choice.Either way, Qualcomm is aiming to turbo to relatively high clockspeeds for their laptop chip, a notable distinction from their much more modestly clocked 8CX chips. While high clockspeeds alone do not make for a fast chip, one of the performance bottlenecks the 8CX chips were their pokey clockspeeds, so if Oryon offers as high an IPC rate as we suspect it will, then this would go a long way towards boosting Qualcomm’s CPU performance to compete with the industry’s strongest players.Memory: 128-bit LPDDR5xFeeding the beastly Oryon CPU cores (as well as the rest of the chip) is a 128-bit LPDDR5x memory bus. This is less remarkable than the CPU side of the chip, but it’s important to note all the same. With the previous 8CX chips only supporting LPDDR4x, this brings Qualcomm back to parity with the latest PC chips in terms of memory technology support. And with supported data rates as high as LPDDR5x-8533, this will give Qualcomm one of the fastest memory controllers on the market.Qualcomm is also quoting a total of 42MB of cache in the system sitting between the various processor blocks and system memory. Given the explicit mention of “total cache”, this is almost certainly L2 + L3. Previous Qualcomm designs have offered a 6MB shared L3 (last level) cache. If that’s the case again here, then that would mean there’s 3MB of L2 cache available for each CPU core – or some permutation thereof.GPU: Latest Generation AdrenoOn the graphics side of matters, Snapdragon X Elite incorporates Qualcomm’s latest generation Adreno GPU. As is typical for Qualcomm in these matters, the company is saying virtually nothing about the architecture employed here, though it goes without saying that this is the latest and greatest iteration of Qualcomm’s in-house GPU design.From a feature perspective, this is a DirectX 12-class GPU with ray tracing support, mirroring the capabilities Qualcomm introduced with last year’s Snapdragon 8 Gen 2 mobile SoC. Within the Windows ecosystem, it will almost certainly qualify as a DirectX 12 Ultimate (feature level 12_2) design.Qualcomm is quoting a single throughput figure for the design: 4.6 TFLOPS at an unspecified bit depth/format (we’d guess FP32). Qualcomm has not previously disclosed similar figures for the 8CX chips, so it’s hard to say how this will compare. Or even how it will compare to other integrated GPUs, since there’s a lot more to real-world GPU performance than pure FLOPS.The display controller portion of the GPU offers support for up to 4 DisplayPort displays. Besides an internal display for the laptop, it can drive a further 3 external displays (all DP 1.4), with one output being 5K capable, while the rest are 4K.Finally, the SoC is getting Qualcomm’s latest video processing block (VPU) as well. This latest design not only support AV1 decoding, but in a first for a Qualcomm SoC, AV1 encoding as well.NPU: Hitting Hard with HexagonNext to the use of Oryon CPU cores, Qualcomm’s other big bet with the Snapdragon X Elite SoC is on the AI/neural processing unit side of things with their latest generation Hexagon NPU. Qualcomm is expecting that AI use will continue to rapidly grow over the next few years, and that the next big push is going to be AI models running locally on users’ systems. So they have invested a significant amount of resources in bulking up their Hexagon NPU for this generation of chips (X Elite and 8 Gen 3).The end result is a heavily revised NPU, which should greatly exceed the 8CX Gen 3’s NPU performance. Qualcomm is quoting 45 TOPS of performance here for modest precision INT8, whereas 8CX Gen 3 was previously quoted at 15 TOPS for an unspecified data format.Unlike their CPU and GPU, Qualcomm is sharing some architectural details here about the NPU, and what they’ve done to boost its performance. The tensor accelerator block, used in the densest matrix math, is outright 2.5x faster than before. Backing that (and the rest of the NPU) is a 2x larger shared memory/cache (though Qualcomm is not disclosing the actual size). Qualcomm is targeting large language models (LLMs) in particular with this change, as these are notoriously memory bound; according to the company, the chip will have enough resources to run a 13 billion parameter Llama 2 model locally.Qualcomm has also made some power delivery changes to help drive more performance/efficiency out of the NPU. The power-hungry tensor block is now on its own power rail, with the rest of the NPU sitting on a separate shared rail. The company has also made some further undisclosed improvements to how they handle micro-tiling of inferencing workloads, which directly impacts how well they can split up workloads to keep the various sub-blocks of the NPU as busy as possible while minimizing intermediate memory operations.I/O: USB4, PCIe 4, & Discrete Wi-Fi 7Rounding out the Snapdragon X Elite, let’s talk I/O.For internal I/O, the SoC offers PCIe 4.0 connectivity for NVMe storage. Elsewhere, the company is using PCIe 3 to supply connectivity to their modem and Wi-Fi solutions. No mention has been made of whether there are any free PCIe lanes for further peripherals.For external I/O, the SoC supports USB4. According to Qualcomm, it can drive up to 3 such Type-C ports, and there are also a pair of USB 3.2 Gen2 outputs, and a single USB 2.0 output for internal use.As noted earlier, both Wi-Fi and the modem are discrete for this product. The chip is intended to be paired with Qualcomm’s FastConnect 7800 silicon in the form of an M.2 card. The 7800 is their latest-generation Wi-Fi 7 solution, with support for 4 spatial streams as well as Bluetooth 5.4. The modem pairing is the Snapdragon X65, a high-performance 5G modem which was also available for the 8CX Gen 3.The fact that neither wireless system is integrated into the SoC is unusual for Qualcomm, but perhaps not too surprising since they want to bring the Elite to market ASAP. Integrating these modules would take further time, and as a laptop SoC, Qualcomm doesn’t need to be as space efficient. In any case, the official line from Qualcomm is that the discrete modem is for OEM flexibility – to give OEMs the option to either include a modem or not – though Qualcomm of course will be strongly encouraging OEMs to include one as a major feature differentiator of the platform.Performance ClaimsAs we don’t have enough architectural details to make any meaningful performance projections, the best thing we have for now are Qualcomm’s vague comparisons to their competitors. This is also the closest thing Qualcomm has provided to energy efficiency data for the chip (though, as always, target clockspeeds for a SKU play a massive part there).With 12 performance cores, Qualcomm is pushing hard on multi-threaded performance. In fact, multi-threaded performance is the only CPU performance comparisons Qualcomm makes, as there are no single-threaded comparisons to speak of. Make of that what you will.Against what is implied to be an Intel 12 core mobile CPU design, Qualcomm is reporting that Snapdragon X Elite delivers 2x the multi-threaded performance in Geekbench 6. Or at iso-performance, they hit the same mark at one-third the power consumption.Even against Intel’s best 14-core (H-class) chips, Qualcomm still reports that they lead by 60% in performance, and again are consuming one-third the power at iso-performance. Undoubtedly, a lot of this is down to the process node used, as TSMC N4 should be delivering a significant advantage over the Intel 7 process used on Intel’s current chips. This is also why the “moving target” aspect is so critical, as Snapdragon X Elite should be competing with the Intel 4 based Meteor Lake lineup by the time it launches next year.More interesting, perhaps, is that Qualcomm is reporting a 50% multi-threaded performance advantage over an unspecified \"Arm-based competitor,” This is meant to imply Apple, but depending on just how vague Qualcomm wishes to be, MediaTek does offer some Windows-on-Arm chips as well.Qualcomm also expects to lead in GPU performance in 3DMark Wildlife Extreme. Which again, with a process node advantage and a tendency to build bigger iGPUs overall, is not surprising.As always, these claims should be taken with a large grain of salt, especially for a platform that is still several months away from launching.Snapdragon X Elite: Coming Mid-2024Wrapping things up, Qualcomm is at this point putting the final touches on the Snapdragon X Elite. The company has deemed it one of their “most pivotal platform announcements in the company's recent history”, and for good reason. The Oryon CPU core being introduced here will eventually be at the heart of a good deal more products, so how competitive Oryon is will make or break Qualcomm’s next few generations of designs.Devices based on the Snapdragon X Elite should be available in mid-2024. Which on that schedule, should see the Snapdragon X Elite competing against Intel’s Meteor Lake (Core Ultra) chips, AMD’s Phoenix chips (Ryzen Mobile 7000), and whatever the latest available iteration is of Apple’s M-series chips.Gallery:Snapdragon X Elite Press Deck\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/21105/qualcomm-previews-snapdragon-x-elite-soc-oryon-cpu-starts-in-laptops-\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Intel Innovation 2023 Keynote Live Blog (8:30am PT, 15:30 UTC)\n",
      "Author: Ryan Smith\n",
      "Date Published: 2023-09-19T14:30:00Z\n",
      "URL: https://www.anandtech.com/show/20060/intel-innovation-2023-keynote-live-blog-830am-pt-1530-utc\n",
      "Content: Kicking off this morning is Intel's annual technology conference, Innovation. The second year of the revived show once again has Intel's CEO Pat Gelsinger leading things off, with what's scheduled to be a 90 minute keynote.Intel has four major themes for this year's show:Accelerating the AI EraTransformative Innovation for the Future of ComputingHype vs. Reality: Bringing Modern Applications to Market Faster (Edge to Cloud)Building and Scaling Industry-Leading Next-Generation Systems and PlatformsExpect to see AI everywhere - both in regards to silicon and promotional efforts. AI is the golden goose of the tech industry at the moment, and everyone is either making massive profits off of AI harfdware (NVIDIA), or wants to be (more or less everyone else, including Intel).Meanwhile, on the product front, this is the year that Intel's Meteor Lake client platform is scheduled to launch. The first Intel CPU built on their EUV-enabled Intel 4 process, Meteor Lake is also Intel's first disaggregated CPU, breaking up what would be a monolithic CPU into several chiplets. Coupled with a new VPU/NPU for AI, Meteor Lake will be a chip of several firsts for Intel when it arrives.11:28AM EDT- And things should be kicking off here in a couple of minutes11:28AM EDT- The keynote is scheduled to begin at the bottom of the hour, but people are still coming in11:29AM EDT- Once again, Intel is at the San Jose Convention Center for this year's conference, which is Intel's front yard, relatively speaking11:30AM EDT- Also joining me on live blog duty today is a very tired Gavin Bonshor11:31AM EDT- And here we go11:31AM EDT- Opening wth a trailer11:31AM EDT- Pat Gelsinger is writing on a whiteboard11:32AM EDT- And now it's a Gelsinger training montage for the Innovation keynote11:33AM EDT- And here's Pat11:34AM EDT- He's always very energetic. This year is no different11:34AM EDT- \"We have exciting achievements to share with you today\"11:34AM EDT- \"We have a lot to cover today\"11:35AM EDT- Now on stage, Rich Felton, the Director of Sporst Science and COO at ai.io11:35AM EDT- Also, the embargo on something called \"Meteor Lake\" has just dropped:https://www.anandtech.com/show/20046/intel-unveils-meteor-lake-architecture-intel-4-heralds-the-disaggregated-future-of-mobile-cpus11:36AM EDT- All about using Intel's technologies within their product stack11:37AM EDT- ai.io benchmarked Gelsinger in relation to soccer players11:38AM EDT- Report date: 27 May 2022?11:38AM EDT- Cognative metrics are quite high, however11:40AM EDT- Now on to the subject of the economy and the size of the silicon industry11:40AM EDT- $574B trillion silicon industry11:41AM EDT- The \"siliconomy\"11:41AM EDT- 4x increase in managed devices over the past 5 years11:42AM EDT- Computers in everything11:42AM EDT- AI is representing a generational shift in how computing is used11:42AM EDT- A simple rule: Developers rule11:44AM EDT- Discussing the Intel Dev Cloud11:44AM EDT- Intel's cloud hardware service for software development11:44AM EDT- Now in general availability11:45AM EDT- 3 tiers of service: freemium, commercial premium, and enterprise11:45AM EDT- (Freemium isn't free)11:46AM EDT- \"You're not my best friends\"11:46AM EDT- Giving everyone here a week of free access to the dev cloud11:46AM EDT- Now Intel is going to have a bit of a \"Shark Tank\" event11:47AM EDT- 3 companies. They will pick a winner at the end of the morning11:47AM EDT- Bachelor #1: Deep Render11:47AM EDT- AI-based compression for video11:48AM EDT- Now rolling a trailer11:48AM EDT- Deep Render wants to do an AI-only compression pipeline11:49AM EDT- At 5x smaller file sizes right now (not sure compared to what)11:49AM EDT- Now on to Intel's Gaudi AI accelerators11:50AM EDT- \"The last 10 years of AI have been incredible\"11:51AM EDT- Intel has formed a new partnership with Stability.ai. They'll be building a large AI supercomputer, expected to be one of the top 15 AI supercomputers in the world11:51AM EDT- Dell is now delivering systems with Gaudi accelerators as well11:52AM EDT- Now on to Xeon11:52AM EDT- Recapping 4th generation Xeon (Sapphire Rapids) progress11:53AM EDT- Sapphire Rapids was released in volume earlier this year11:53AM EDT- Now for another partner spot: Alibaba, who is using Xeon for their AI workloads11:53AM EDT- Rolling trailer11:55AM EDT- (Keep in mind that SPR introduced Intel's AMX matrix units, so they're not doing AI solely on traditional serial cores)11:55AM EDT- Showing the latest Gaudi roadmap11:55AM EDT- Gaudi 3 in the works on 5nm (presumably TSMC)11:55AM EDT- Silicon is just out of fab11:56AM EDT- Falcon Shores in 202511:56AM EDT- Gelsinger: Moore's Law is Alive and Well11:57AM EDT- Bachelor #2: Scala Biodesign11:57AM EDT- Rolling trailer11:58AM EDT- Computational biology12:00PM EDT- Now back to hardware with AI in the PC12:00PM EDT- \"What if we put it in the hands of every human on Earth?\"12:02PM EDT- \"We see the AI PC as a sea change moment\"12:02PM EDT- What's the killer app? Intel needs developers to create it12:03PM EDT- Demo time12:03PM EDT- Taking a look at a couple of new AI PC applications12:03PM EDT- Running Audacity with a plugin for Riffusion12:04PM EDT- Looks like it's going to be a music generation demo12:04PM EDT- A song in the style of Taylor Swift12:05PM EDT- Meanwhile, another machine with GIMP and an OpenVINO plugin for Stable Diffusion12:05PM EDT- Text to image demo12:06PM EDT- Now playing the generated song. It's not bad, it's not great. But it's a lot better than what could be done on a local system 5 years ago12:06PM EDT- As for the Stable Diffusion demo, a giraffe in a cowboy hat12:06PM EDT- World's first showing of a Lunar Lake system12:07PM EDT- Stable enough and running to showcase AI demos12:07PM EDT- Lunar Lake is Intel's next next gen platform12:08PM EDT- The successor to Arrow Lake (2024)12:08PM EDT- Now on to the immediate future: Meteor Lake12:08PM EDT- Launching December 14th12:08PM EDT- (So just in time to count for 2023)12:08PM EDT- Carrying the Core Ultra branding12:09PM EDT- Gelsinger is discussing Microsoft's Co-Pilot software and how it runs on Meteor Lake12:09PM EDT- Meteor Lake is a whole bunch of firsts for Intel. Intel 4 process (with EUV), chiplet client CPU, using Foveros packaging, first CPU with a high performance NPU block12:10PM EDT- Now bringing out some ecosystem partners to talk about upcoming systems12:10PM EDT- Starting with Jerry Kao of Acer12:11PM EDT- (If Lunar Lake is working, that means Intel has 18A process chips working)12:12PM EDT- Jerry is running a Meteor Lake laptop demo12:12PM EDT- Another Stable Diffusion demo12:13PM EDT- \"Please press the button\"12:13PM EDT- And done. Quite quickly12:13PM EDT- Acer's demo includes a parallax view ability12:14PM EDT- And that was Acer's upcoming Swift laptop12:14PM EDT- \"But this is just the beginning\"12:14PM EDT- Wafers!12:15PM EDT- Arrow Lake wafers. Intel's 20A process node12:15PM EDT- RibbonFETs and PowerVias12:15PM EDT- 2025 offering: Panther Lake12:15PM EDT- Sending it to fabs in Q1 of 202412:15PM EDT- Using Intel 18A process12:16PM EDT- So Meteor -> Arrow -> Lunar -> Panther by the end of 202512:16PM EDT- Now back to Xeons12:16PM EDT- Emerald Rapids is finally confirmed as the 5th generation Xeon12:17PM EDT- Also launching December 14th12:17PM EDT- One of the last Intel 7 products12:17PM EDT- EMR should bring \"huge PPW improvements\"12:17PM EDT- Same platform as Sapphire Rapids, so allowing rapid adoption12:18PM EDT- Meanwhile, the 2024 Xeons will be a next-gen platform with Granite Rapids and Sierra Forest12:18PM EDT- P-cores and E-cores respectively. Using a shared platform, and the same I/O die12:18PM EDT- Built on Intel 3 process technology12:19PM EDT- Pat has a wafer of this as well12:19PM EDT- Platform name: Birch Stream12:19PM EDT- Following that will be Clearwater Forest in 2025, a second-gen E-core part using the same platform12:20PM EDT- Sierra Forest will offer up to 288 cores on a single chip. using 2 dies12:20PM EDT- (A core for every business day of the year)12:20PM EDT- Now pivoting to the subject of security12:22PM EDT- There will be a second keynote tomorrow, with a focus on software. So more details to come then12:24PM EDT- Now another partner: Rewind AI12:24PM EDT- Talking about hearing aids12:24PM EDT- Rewind records you screen and keeps a record of it. Then you can ask Rewind about anything you've seen12:24PM EDT- Demo time as well12:25PM EDT- Ahh, anything seen, said, or heard12:26PM EDT- Asking Rewind about Innovation sessions that were previously viewed12:26PM EDT- That was apparently server-hosted GPT-412:27PM EDT- Now switching to a local demo, Wi-Fi off12:27PM EDT- Running on a Core Ultra12:27PM EDT- It knows that Pat's favorite sound is his granddaughter calling him papa (Pat had mentioned this off-hand earlier in the keynote)12:29PM EDT- Now on to hearing aids again. How AI can improve the hearing aid experience12:29PM EDT- Live demo with Starkey labs12:31PM EDT- Head tracking AI, switching the mode of the hearing aids between ambient and focus modes12:31PM EDT- Running on the NPU in the Core Ultra12:32PM EDT- Pat's chat partner was talking in French while Pat was off doing something else12:32PM EDT- The demo app translated it to English12:33PM EDT- Showing off an AR visor as well12:33PM EDT- A gimpse into sensing in the future12:33PM EDT- Bachelor #3: Antaris (Software for Space)12:34PM EDT- Antaris staff talking about their company and their vision12:35PM EDT- Software-driven engineering for satellites and other space hardware12:35PM EDT- Make all your mistakes in the cloud before you ever make a real satellite12:36PM EDT- Making it easier to get more satellites in orbit providing services12:37PM EDT- Now on to Intel + Arm12:37PM EDT- Arm is supporting the OpenVino platform12:38PM EDT- Models are going to run on everything from CPUs to NPUs and FPGAs12:39PM EDT- Hybrid AI12:39PM EDT- Intel will be releasing a hybrid AI SDK early next year12:39PM EDT- (Where's oneAPI?)12:40PM EDT- Next decade or two of development will not be cloud-native; it will be edge-native12:41PM EDT- A lot of work to make the edge more accessible and supportable. Enter Project Strata12:41PM EDT- Launching in early 202412:42PM EDT- Now for another partner demo. Fit Match.ai12:43PM EDT- Fit Match concierge solution12:44PM EDT- Tools to help with the issue in retail of clothes that don't fit12:44PM EDT- Give customers a full body scan and then show a curated selection of clothing that will fit their shape12:45PM EDT- Using a scan of Pat to show a selection of activewear12:46PM EDT- (There's a Windows activation message in the background)12:46PM EDT- And now back to the Dating Game (Intel Ignite)12:47PM EDT- \"They're all amazing\"12:47PM EDT- 2023 Ignite award winner: Deep Render12:47PM EDT- 2023 Intel Startup Innovator award12:48PM EDT- Pat previously worked on video conferencing (there's a CNET Central clip of him showing it off somewhere in my archives...)12:48PM EDT- So this is near and dear to his heart12:49PM EDT- Now for a live demo of Deep Render technology12:49PM EDT- Using a Core Ultra demo laptop12:49PM EDT- Using the NPU12:49PM EDT- Deep Render can now bring their tech to hundreds of millions of users thanks to Core Ultra's NPU12:50PM EDT- Looks like they're doing AI upscaling and denoising?12:52PM EDT- Now on to fab stuff12:52PM EDT- A look at Intel's current fab roadmap and what comes next12:52PM EDT- Meteor Lake ramping in Oregon, working on getting it transfered to Ireland12:52PM EDT- Looking beyond 5 nodes in 4 years12:52PM EDT- Intel 18A: 5th node in roadmap12:53PM EDT- Almost done with 0.9 PDK12:53PM EDT- 18A silicon on track to go into the fab in Q1'202412:53PM EDT- High-NA EUV update: development with 18A. Production will now be with post-18A \"Intel Next\"12:54PM EDT- (Does this mean Lunar Lake is in fact 20A?)12:54PM EDT- Working on the next version of PowerVia as well12:54PM EDT- Dr. Ann Kelleher's Christmas present: first High-NA EUV machines12:56PM EDT- Recapping yesterday's glass core substrate anouncement12:56PM EDT- Glass core substrates in HPC chips by the end of the decade12:57PM EDT- Now switching to chiplets and the UCIe standard for an open chiplet connectivity standard12:57PM EDT- Still early in UCIe progression12:57PM EDT- But Intel has their first test chips12:58PM EDT- Intel 3 die combined with EMIB and a TSMC N3E die12:58PM EDT- Intel is looking to push the envelope on 3D silicon12:59PM EDT- Now a word on neuromorphic computing12:59PM EDT- Loihi 2 on Intel 4, and the stackable 8 chip Kapoho Point01:00PM EDT- And Intel is working on quantum computing as well01:00PM EDT- Probably post-2030 for quantum supremacy01:01PM EDT- Intel is doing silicon qbits01:01PM EDT- \"If we get this working, we can do it at scale\"01:01PM EDT- Intel Tunnel FAlls, a 12 qubit device, 50x50nm. 95% yield rate across the wafer01:02PM EDT- And a Quantum SDK stack is in development, as well01:02PM EDT- And now for the Intel Innovation Lifetime Achievement Award01:02PM EDT- Last year Linus Torvalds won it01:03PM EDT- For 2023: Dr. Fei-Fei Li, Inaugural Sequoia Profession in the CompSci department at Standford University01:03PM EDT- And the co-director for Stanford's Human-Centered AI01:04PM EDT- (Who, coincidentally, is giving an industry luminary session coming up next)01:04PM EDT- Pat is now recapping and wrapping things up01:07PM EDT- And that wraps up the Intel Innovation 2023 opening keynote with Pat Gelsinger, Intel's CEO.01:08PM EDT- As always, thanks for joining us Don't forget, our piece detailing Intel's Meteor Lake disclosures can be found here -https://www.anandtech.com/show/20046/intel-unveils-meteor-lake-architecture-intel-4-heralds-the-disaggregated-future-of-mobile-cpus\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/20060/intel-innovation-2023-keynote-live-blog-830am-pt-1530-utc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Khadas Mind Premium Review: Raptor Lake-P in a Modular Portable Workstation\n",
      "Author: Ganesh T S\n",
      "Date Published: 2023-09-14T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/20038/khadas-mind-portable-modular-workstation-review-raptor-lakep-in-a-different-avatar\n",
      "Content: Khadas is a well-known vendor in the ARM-based single-board computer (SBC) circles with boards based on Rockchip and Amlogic SoCs. Recently, thecompanymade its first foray into the small form-factor x86 PC space with a rather unique product family - theKhadas Mind. Unlike regular mini-PCs, the Mind is actually an ecosystem of products created with the aim of bringing a portable modular workstation to the market. Instead of relying on Thunderbolt to achieve this vision, the company has created a proprietary Mind Link connector and interface with an intent to produce a more close-knit set of products.The main processing unit in the ecosystem is the Khadas Mind, which comes with either an Intel Core i7-1360P (Mind Premium) or an Intel Core i5-1340P (Mind Standard). Equipped with the Mind Link Connector, it can interface to a variety of peripherals such as the Mind Dock and Mind Graphics. While the former provides additional I/Os, the latter is a full-featured eGPU enclosure with a variety of ports. The main unit also includes a small battery that allows the unit to be seamlessly 'hot-plugged' into different peripherals, similar to a notebook.Khadas sent across the Mind Premium along with the Mind Dock to provide us with an introduction to the Mind ecosystem. This review explores the performance and product experience delivered by Raptor Lake-P in an unusual package.Introduction and Product ImpressionsAdvancements in processor architecture and semiconductor manufacturing have resulted in successful replacement of bulky tower desktops with miniaturized computing systems over the last decade. In parallel, notebooks have also become thinner, lighter, and very powerful. On the former front, many vendors are enjoying success with ultra-compact form-factor (UCFF) machines pioneered by Intel with theNUCproduct line. A few years back, Intel also started to introduce Compute Elements and even notebook platforms under the NUC tag in order to diversify beyond the original UCFF vision.These compact machines have their limitations in terms of platform power dissipation. Addition of discrete GPUs is usually not possible with the UCFF form-factor, resulting in many use-cases still requiring bulky desktops. eGPU enclosures connected via Thunderbolt / USB4 have started to address this aspect. Notebook platforms have traditionally been limited in terms of supported screen sizes. For many business use-cases in particular, the keyboard and built-in screen lie unused, as the notebook is either docked to a bigger monitor or kept in storage. Some users adopt a thin-client model for their notebook, but multiple devices and the compulsory need for network connectivity may result in pain points associated with file syncing. Khadas has created the Mind product family as a solution to these problems.The ecosystem consists of the main computing unit and a host of peripherals that interface using a proprietary Mind Link connector. The core computing unit is similar to that of an ultrabook's main board - compact, and limited in terms of I/O. However, the leeway over the unit's thickness (compared to a regular ultrabook) has enabled Khadas to equip the board with a vapor chamber-based cooling system. While the LPDDR5 RAM is soldered, the system comes with two M.2 2230 Gen4 x4 NVMe slots. Khadas doesn't sell barebones versions, but markets two different SKUs that come pre-installed with Windows 11 Home:Mind Premium: Core i7-1360P / 32GB of LPDDR5-5200 RAM / 1TB Gen4 x4 NVMe SSDMind Standard: Core i5-1340P / 16GB of LPDDR5-5200 RAM / 512GB Gen4 x4 NVMe SSDKhadas has plans for a large number of peripherals:Mind Dock: I/O expansion with additional network ports, video outputs, audio subsystem, and USB ports.Mind Graphics: eGPU enclosure with the NVIDIA RTX 4060Ti and additional I/O ports.for the gaming market.Mind Studio: Large touchscreen monitor with additional GPU for creative professionals.Mind xPlay: 2-in-1 tablet / notebook conversion kit (portable monitor with keyboard)Mind Talk: Conference room conversion kitCurrently, the company already has the Mind Dock ready to ship (it came in as part of our review package along with the Mind Premium), and the Mind Graphics is expected to ship in June 2024. The other peripherals are still in the planning stages.The Mind Premium package comes with the main unit and USB-PD compliant Type-C adapter (up to 65W) and a braided Type-C cable along with it. A warranty card and a quick start guide are also included.The gallery below presents the external industrial design of the main unit as well as the Mind Dock.Gallery:Khadas Mind Premium and Mind Dock Industrial DesignThe main unit is a compact rectangular slab measuring 146mm x 105mm x 20mm, with a weight of 450g. The rounded edges and metal enclosure lend it a solid look and feel. Unfortunately, the core unit is rather anemic in terms of I/O. There are three display outputs - a HDMI port, and two Type-C ports. Either of the Type-C ports can be used to connect the power adapter, but they also support data transfer as detailed below. There are two USB 3.2 Gen 2 Type-A ports. The absence of USB4 / Thunderbolt 4 is a major letdown, though.The Mind Dock provides wired networking support, a 3.5mm headphone jack, a SDXC UHS-II card slot and multiple USB 3.2 Gen 1 Type-A ports. Two HDMI 2.0 ports are also included. The Mind Dock also has a fingerprint reader and a volume control knob. Speakers are also built into the dock.Khadas provided a full-blown teardown picture of the Mind Standard along with their press package.The main unit is equipped with a Western Digital SN740 NVMe SSD, while the Samsung LPDDR5-5200 RAM chips are soldered. A 5.55Wh battery is also included - similar to a notebook platform. However, the battery is not meant to be used for regular operation in the absence of power input. Rather, it is meant to aid in hot-plugging and allowing the system to go to sleep without having to shut down in the absence of external power. Khadas promises up to 25 hours of standby in such scenarios.The full specifications of the review sample (as tested) are summarized in the table below. We evaluate two configurations - one with just the Mind Premium, and another with the Mind Dock.Systems Specifications(as tested)Khadas Mind PremiumKhadas Mind Premium + Mind DockProcessorIntel Core i7-1360PRaptor Lake 4P + 8E / 16T, up to 5.0 GHz (P) up to 3.7 GHz (E)Intel 7, 18MB L2, Min / Max / Base TDP: 20W / 64W / 28WPL1 = 28W, PL2 = 64WIntel Core i7-1360PRaptor Lake 4P + 8E / 16T, up to 5.0 GHz (P) up to 3.7 GHz (E)Intel 7, 18MB L2, Min / Max / Base TDP: 20W / 64W / 28WPL1 = 28W, PL2 = 64WMemory32GB LPDDR5-5200 Soldered64-48-48-112 @ 5200 MHz4x8 GB32GB LPDDR5-5200 Soldered64-48-48-112 @ 5200 MHz4x8 GBGraphicsIntel Iris Xe Graphics(96EU @ 1.50 GHz)Intel Iris Xe Graphics(96EU @ 1.50 GHz)Disk Drive(s)Western Digital PC SN740 SDDQTQD-1T00(1 TB; M.2 2230 PCIe 4.0 x4 NVMe;)(Western Digital 112L BiCS5 3D TLC NAND; SanDisk 20-82-10081 DRAM-less Controller)Western Digital PC SN740 SDDQTQD-1T00(1 TB; M.2 2230 PCIe 4.0 x4 NVMe;)(Western Digital 112L BiCS5 3D TLC NAND; SanDisk 20-82-10081 DRAM-less Controller)NetworkingIntel Wi-Fi 6E AX211 (2x2 802.11ax - 2.4 Gbps)Realtek RTL8156 2.5 GbE Controller (via Mind Link / USB)Intel Wi-Fi 6E AX211 (2x2 802.11ax - 2.4 Gbps)AudioDigital audio with bitstreaming support over HDMIUSB audio codec (via Mind Link / USB) (3.5mm audio jack in front)Digital audio with bitstreaming support over HDMIVideo1x HDMI 2.0 (Rear)2x Display Port 1.4a over Type-C Alt-Mode (Rear)1x HDMI 2.0 (Rear on Main Unit, HDR Supported)2x HDMI 2.0 (Rear on Dock, No HDR)2x Display Port 1.4a over Type-C Alt-Mode (Rear on Main Unit) (max. 4 displays)Miscellaneous I/O Ports2x USB 3.2 Gen 2 Type-A (Rear)1x USB 3.2 Gen 2 Type-C (Rear, with DP Alt Mode and PD Input)1x USB 2.0 Type-C (Rear, with DP Alt Mode and PD Input)2x USB 3.2 Gen 2 Type-A (Rear on Main Unit)1x USB 3.2 Gen 2 Type-C (Rear on Main Unit, with DP Alt Mode and PD Input)1x USB 2.0 Type-C (Rear on Main Unit, with DP Alt Mode and PD Input)1x SDXC UHS-II Card Reader (Front on Dock)1x USB 3.2 Gen 1 Type-A (Front on Dock)2x USB 3.2 Gen 1 Type-A (Rear on Dock)1x USB-C Power Input (Rear on Dock)Operating SystemWindows 11 Enterprise (22000.2360)Windows 11 Enterprise (22000.2360)PricingUS $1099 (as configured, with Windows 11 Home OS)US $1278 (as configured, with Windows 11 Home OS)Full SpecificationsKhadas Mnd SpecificationsKhadas Mnd SpecificationsIn the next section, we take a look at the system setup and follow it up with a detailed platform analysis.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/20038/khadas-mind-portable-modular-workstation-review-raptor-lakep-in-a-different-avatar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Apple WWDC 2023 Keynote Live Blog (Starts at 10am PT/17:00 UTC)\n",
      "Author: Ryan Smith\n",
      "Date Published: 2023-06-05T16:30:00Z\n",
      "URL: https://www.anandtech.com/show/18895/the-apple-wwdc-2023-keynote-live-blog-starts-at-10am-pt1700-utc\n",
      "Content: 12:55PM EDT- As we round the corner after Computex and transition into June, it's time once more for Apple's annual World Wide Developers Conference. As always, Apple kicks off WWDC with their big keynote event, which though aimed first and foremost at developers, is also used as a venue to announce new products and ecosystem strategies. The keynote starts at 10am Pacific (17:00 UTC) today, and AnandTech will be offering live blog coverage of Apple's event.12:55PM EDT- With WWDC going virtual once again this year, we're expecting another rapid-fire, two-hour run through of Apple's ecosystem. WWDC keynotes have historically covered everything from macOS and iOS to individual Apple applications and more. On the hardware side of matters, in previous years we've seen things like the official announcement Apple's M2 SoC; and as of 2023, we're still missing the legendary Mac Pro from the Apple Silicon lineup. All the while, the rumor mill intensely churns with the idea of an Apple augmented reality/virtual reality (AR/VR) headset. And of course, there's always the chance of the periodic product refresh.12:56PM EDT- The Apple Store, as is usually is the case, is down right now12:57PM EDT- So if you're looking to order any Apple products in the middle of the WWDC keynote, you'll be out of luck01:00PM EDT- Reading through the tea leaves this year, it's not sounding like this year will be as big of a year for Mac hardware as last year, which is an admittedly hard act to follow since we got the M2 SoC that year01:00PM EDT- But for consumers, any new/updated Macs will be a big deal in and of itself01:00PM EDT- And here we go01:02PM EDT- And here's Tim Cook, from a recording in the yards of Apple Park01:02PM EDT- This is the 15th anniversary of the iOS App Store01:03PM EDT- \"Today we're going to make some of our biggest announcements ever at WWDC\"01:03PM EDT- \"Introducing some exciting new products\"01:04PM EDT- And we're going right to the Mac to start things off01:04PM EDT- Recapping Apple Silicon and the performance/design benefits of it01:05PM EDT- Especially the MacBook Air01:05PM EDT- Rolling the announcement video for a new Mac01:05PM EDT- Introducing the 15-inch MacBook Air01:05PM EDT- \"The world's best 15-inch laptop\"01:06PM EDT- 11.5mm thick, \"world's thinnest 15-inch laptop\"01:06PM EDT- 2 TB4 ports + MagSafe, so the same as the 13-inch01:06PM EDT- 15.3-inch liquid retina screen, 500 nits brightness and 10-bit color01:07PM EDT- Powered by the M2 chip01:07PM EDT- This sounds like it's almost entirely just an enlarged MBA 1301:07PM EDT- Apple claims 2x the perf of a i7 Core-based laptop01:07PM EDT- And 50% more battery life01:08PM EDT- And this is fanless, just like the MBA1301:08PM EDT- 3.3 pounds01:09PM EDT- Pricing starts at $1299, ordering starts today for availability next week01:09PM EDT- The MBA13 is getting a $100 cut to $109901:09PM EDT- And that's the 15-inch MacBook Air01:09PM EDT- Now on to Pro products. Recapping the recent Pro laptops01:09PM EDT- Mac Studio update time01:10PM EDT- The Mac Studio is getting an update with the M2 Max SoC01:11PM EDT- And because it's the Studio, introducing the M2 Ultra SoC01:11PM EDT- Just like the M1, M2 connects two Max SoCs together using the UltraFusion interface01:11PM EDT- 76 GPU cores in total01:12PM EDT- M2 Ultra supports 192GB of unified RAM, 64GB more than the M1 Ultra01:12PM EDT- AI! (Take a shot)01:13PM EDT- Apple is touting neural network training as one of the use cases for the chip and its 192GB of memory01:13PM EDT- Mac Studio has \"higher bandwidth\" HDMI01:13PM EDT- Up to six Pro Displays off an M2 Ultra01:14PM EDT- But what about people that need PCIe expansion?!01:14PM EDT- And there it is, the Apple Silicon Mac Pro01:14PM EDT- M2 Ultra-based01:14PM EDT- 3x faster than the old Intel Mac Pro01:15PM EDT- Same 192GB memory limit as the M2U Mac Studio01:15PM EDT- And touting the advantage of the M2 Ultra over the Afterburner video processing card used in the Intel Mac Pro01:15PM EDT- 8 Thunderbolt 4 ports01:16PM EDT- 6 PCIe 4.0 expansion slots01:16PM EDT- Physically x16, from the looks of it, but probably not all electrically x1601:16PM EDT- Tower and Rackmount configurations01:17PM EDT- Mac Pro starts at $6999. Orders start today, available next week01:17PM EDT- And this completes the transition to Apple Silicon and the Arm ISA. Apple no longer has any leading systems using Intel x86 processors01:18PM EDT- That took a bit longer than the two years Apple previously talked about, but they got here01:18PM EDT- The lack of memory expanability with the Mac Pro may prove a wedge issue, but we'll see how things go01:18PM EDT- Now on to software (this is a dev conference, after all), starting with iOS01:19PM EDT- Previewing iOS 1701:19PM EDT- The Phone app is getting updated01:19PM EDT- Personalized contact posters01:20PM EDT- Customize the image, font, and color01:20PM EDT- This info is also available via CallKit01:20PM EDT- New feature: Live Voicemail01:20PM EDT- Live transcription of your voicemail as it's being left/recorded, so that you can better screen phone calls01:21PM EDT- Transcriptions are handled on-device for privacy reasons01:21PM EDT- And now you can leave recorded messages with Facetime01:21PM EDT- Now on to the Messages app01:22PM EDT- Search filters for more precise searching01:22PM EDT- Easier inline replies by swipping on a chat bubble01:22PM EDT- New feature: check-in01:23PM EDT- Let a contact know you've arrived home safely01:23PM EDT- Or if you've not reached your destination, if that happens to be the case01:24PM EDT- And some Sticker updates. All emoji available as stickers01:26PM EDT- New AirDrop functionality01:26PM EDT- New feature: Name Drop01:26PM EDT- Bring two phones close together to exchange contact information01:26PM EDT- Can also swap with an Apple Watch01:27PM EDT- You can also use this to quickly setup AirDrop between two devices to transfer content and other activities01:28PM EDT- Autocorrect feature updates01:28PM EDT- Apple has continued to advance the machine learning model for autocorrect01:28PM EDT- And the latest generation is based on transformer models (the same underlying tech used in GPT and the like)01:29PM EDT- \"Just want to type a ducking word\"01:30PM EDT- (The stream dropped out on my end. That's the first for an Apple event stream)01:30PM EDT- New app for iOS: Journal01:30PM EDT- It's a journaling application01:32PM EDT- New feature: Standby01:33PM EDT- Turn your iPhone into a nightstand clock01:33PM EDT- (It just doesn't have the charm of a seven segment display)01:34PM EDT- Along with the time, it can show calendar reminders, timers, and other information01:34PM EDT- And it can remember the user's view preferences for each location it's used with MagSafe01:35PM EDT- Siri is apparently getting rid of the \"Hey\" keyword?01:35PM EDT- So invoking Siri is now just \"Siri\"01:35PM EDT- Now on to iPadOS01:36PM EDT- New features, starting with widgets and the lock screen01:37PM EDT- The customized lock screen is coming to iPadOS01:38PM EDT- Optimized popular wallpapers (like astronomy) for the larger screen01:39PM EDT- Live activities on the lock screen as well01:39PM EDT- iPadOS is getting the Health app as well01:40PM EDT- And it's been redesigned to make good use of the larger iPad screen01:41PM EDT- The Notes app is getting updated as well, with improved PDF handling01:42PM EDT- PDF annotation within Notes when using an Apple Pencil01:43PM EDT- Also on Apple's list: updates to Stage Manager, and external camera support as part of an external display01:43PM EDT- And that's iPadOS 1701:44PM EDT- Now in to macOS01:45PM EDT- macOS Sonoma01:45PM EDT- A lot of those previously-announced iOS/iPadOS features are also coming to macOS01:45PM EDT- (Barring any surprises, this should be macOS 14)01:45PM EDT- More personalization through widgets01:46PM EDT- Widgets can go on the desktop; they're no longer limited to the notification center01:47PM EDT- (This seems like reinventing the wheel a bit. Didn't we have a lot of this with Dashboard?)01:47PM EDT- Moving widgets from the iPhone to the Mac01:48PM EDT- Now on to Mac Gaming01:48PM EDT- Apple Silicon brings a lot more GPU performance, which is what Apple has always wanted01:49PM EDT- Introducing Game Mode01:49PM EDT- Games get the highest priority on the CPU and GPU; pushes background tasks farther into the background01:49PM EDT- Focus on reducing input latency and BT audio latency01:49PM EDT- According to Apple, they're doubling the BT sampling rate in game mode01:50PM EDT- (A bit power hungry, but when you're going to be making heavy use of the CPU/GPU anyhow...)01:50PM EDT- Apple is also improving their tools for porting games from other platforms to the Mac01:50PM EDT- And now Apple has Hideo Kojima sharing a few words01:51PM EDT- Death Stranding Director's Cut is coming to the Mac later this year01:51PM EDT- (This was originally released in 2019)01:51PM EDT- Implements Metal Upscaling, among other features01:52PM EDT- And his studio is actively working to bring future titles to the platform01:52PM EDT- Now shifting gears to productivity01:52PM EDT- Video conferencing updates01:52PM EDT- Presenter Overlay01:53PM EDT- Get your face on top of your slides01:54PM EDT- Including a faux-presentation mode where your slide deck is behind you. The presenter's body is cut out and placed on top. It actually looks rather neat01:54PM EDT- Now on to Safari01:54PM EDT- Adding even more features to WebKit01:54PM EDT- Private browsing mode is being updated to block trackers and remove URL trackers01:55PM EDT- Passwords and passkeys can be shared among a group using iCloud Keychain (E2E encrypted)01:55PM EDT- Multiple browser profiles (think Firefox account containers)01:56PM EDT- So different tabs can belong to different profiles, rather than sharing a single profile and its cookies/logins01:56PM EDT- Web apps. They can be created from any website01:57PM EDT- And that's macOS Sonoma01:57PM EDT- Now on to Audio & Home01:57PM EDT- Starting with what's new for AirPods01:58PM EDT- New feature: adaptive audio01:58PM EDT- Dynamically blending transparency and active noise cancellation modes01:59PM EDT- Distracting noises are reduced. Useful noises are not01:59PM EDT- (So you can literally have selective hearing)02:00PM EDT- Adaptive Audio will supress noise when you're on a call as well02:00PM EDT- Also improving the automatic switching experience between Apple devices02:01PM EDT- AirPlay updates as well02:02PM EDT- AirPlay is coming to hotels02:02PM EDT- And SharePlay is coming to cars02:02PM EDT- Lets all the passengers control the sound system02:03PM EDT- A few words on tvOS 17 as well02:03PM EDT- The control center has been redesigned02:03PM EDT- Memories is now available as a screensaver02:03PM EDT- And Facetime is coming to Apple TV02:04PM EDT- Uses Apple's Continuity Camera functionality. Using the camera on an iPhone or iPad02:04PM EDT- With Continuity to provide framing02:05PM EDT- And devs will get access to the Continuity Camera APIs, allowing other video conferencing apps to access the functionality02:05PM EDT- Now on to watchOS02:06PM EDT- watchOS 1002:06PM EDT- Rolling a video02:07PM EDT- Can reveal widgets from any home screen using the crown to bring up a stack of widgets02:08PM EDT- Long press to add a widget to the stack02:08PM EDT- Redesigned apps across watchOS 1002:09PM EDT- World Clock, the Activity app, and more02:11PM EDT- New Apple Watch features aimed at cyclists02:11PM EDT- Apple Watch can now connect to BT-enabled bike sensors02:12PM EDT- Apple Watch can use this to estimate power output02:13PM EDT- Compass and Maps being updated for hiking usage as well02:13PM EDT- Keeps track of the last place you has cellular connectivity, plots it as a waypoint02:13PM EDT- New topographic map for Maps in the US02:14PM EDT- And new workout APIs for developers02:15PM EDT- And some new health-related features for Apple Watch, as well02:16PM EDT- Mindfulness app02:16PM EDT- Log your feelings02:16PM EDT- And the iPhone/iPad Health apps get the same functionality02:17PM EDT- Standardized health assessments within the Health app02:17PM EDT- (Mental health assessments)02:17PM EDT- Vision health tools as well02:18PM EDT- Now Apple Watch can measure how much time is spent in the daylight02:19PM EDT- This is being pitched as a tool for using with kids and making sure they get enough outside time02:19PM EDT- And screen distance reminders02:19PM EDT- Underscoring the on-device storage and encryption of Health data02:20PM EDT- And that's watchOS02:20PM EDT- And that's Apple's slate of hardware and software updates02:21PM EDT- All in 80 minutes. That's fast for a WWDC keynote, albeit dense02:21PM EDT- Developer beta OSes releases available today, and public betas next month02:21PM EDT- With the final versions shipping to the public in the fall02:21PM EDT- And back to Tim Cook02:21PM EDT- For the obligatory one more thing02:21PM EDT- Augmented reality02:22PM EDT- Announcing an entirely new AR platform and a new product02:22PM EDT- Rolling a video02:22PM EDT- An Apple AR headset02:23PM EDT- Apple Vision Pro02:23PM EDT- \"Seemlessly blending the real world with the digital word\"02:23PM EDT- Emphasizing looking through Vision Pro, and not at it02:23PM EDT- Eye and hand tracking. As well as voice controls02:24PM EDT- Cabled headset?02:24PM EDT- Tim is running through the use cases for AR and content consumption02:24PM EDT- \"Apple Vision Pro will introduce us to spatial computing\"02:25PM EDT- Designed a fully three-dimensional interface02:25PM EDT- Home view is the base interface02:26PM EDT- Apps wrap around to the sides when switching between them02:27PM EDT- And environments, for swtiching from AR to a more VR-like experience02:27PM EDT- Environments are volumetic02:27PM EDT- Controls are eyes, hands, and voice02:28PM EDT- Tapping fingers together to select something, and flick to scroll02:28PM EDT- Siri support to drive those voice commands02:29PM EDT- A user's eyes are visible from outside of the headset02:29PM EDT- Apple calls it EyeSight02:29PM EDT- (Sounds like this is an OLED screen, rather than actually seeing through the headset)02:30PM EDT- Lets users see if someone is approaching them - and the approacher know once they've been seen02:30PM EDT- (The home view reminds me of the Sony PS Vita Live Area home screen)02:32PM EDT- Virtual keyboard for typing02:32PM EDT- Works with popular Bluetooth accessories like the Magic Keyboard02:33PM EDT- 4K display?02:33PM EDT- Can project the screen from a Mac into Apple Vision Pro02:34PM EDT- Facetime support02:34PM EDT- Though it sounds like they can't see you, since there's not a camera in a suitable location02:35PM EDT- At this point Apple is talking less about technical features, and more about use cases02:36PM EDT- Which makes sense. It's a new product category for them, so they need to sell potential customers on the usefulness of the product02:36PM EDT- Which is often \"things you like to do, but better\"02:36PM EDT- The headset features a 3D camera setup and can record/playback 3D videos02:37PM EDT- Apple calls them spatial videos02:37PM EDT- Spatial cinema mode for watching video content02:38PM EDT- Content from Apple TV+ and other content services will be available on Apple Vision Pro02:39PM EDT- And there will be formal support for 3D movies02:39PM EDT- Apple Arcade games can only be used on the headset. Though they're a 2D application from the looks of it02:40PM EDT- Tim is bringing out Disney's CEO, Bob Iger02:41PM EDT- \"We believe Apple Vision Pro is a revolutionary platform\"02:42PM EDT- Now rolling a sneek peak video of content and expereinces Disney has been development for the headset02:44PM EDT- Disney+ will be available on Day One02:44PM EDT- And now on to the design of the headset itself02:45PM EDT- Apple Vision Pro is the culmination of decades of experience02:45PM EDT- Entire front of the headset is a single piece of deformed and laminated glass02:45PM EDT- Extremely lightweight frame02:45PM EDT- There'a button on one side, and a digital crown on the other side02:46PM EDT- Actively cooled thermal design. There's a processor in the headset, and no doubt the screen itself would like some cooling02:47PM EDT- Audio pods built into the headset band02:47PM EDT- Fabric band with an adjustment dial02:47PM EDT- For users who need glasses, created custom optical inserts that magnetically attach to the display02:47PM EDT- The battery is external02:48PM EDT- Which is the cable we've seen earlier02:48PM EDT- The battery is designed to be pocketed, tethered via the cable rather than weighing down on the head02:48PM EDT- Now for more on the tech02:49PM EDT- micro-OLED silicon backplane02:49PM EDT- 23M pixels across 2 panels02:49PM EDT- 64x the density of an iPhone display02:49PM EDT- 3 element lens02:49PM EDT- Looks like a pancake lens setup02:50PM EDT- \"Fine text looks super-sharp from any angle\"02:50PM EDT- The display is RGB, but not RGB stripe02:51PM EDT- Using audio raytracing for sound02:51PM EDT- Lidar, TrueDepth, hand tracking cameras, and more02:51PM EDT- Internal IR cameras for eye tracking as well (doing foveated rendering, I'm sure)02:51PM EDT- Dual chip processor setup02:52PM EDT- The first chip is an Apple M2 SoC02:52PM EDT- The second chip is a new chip: R102:52PM EDT- A dedicated sensor input and fusion processor02:52PM EDT- \"R1 virtually eliminates lag\"02:52PM EDT- Gets new images to the display within 12ms02:53PM EDT- Confirmed that EyeSight is using an external, curved OLED panel to fake transparency02:54PM EDT- For FaceTime, Apple recreates and renders the user's face, since an actual camera isn't possible02:54PM EDT- (It looks fairly fake, but not uncanny)02:54PM EDT- The OS: visionOS02:54PM EDT- Foveated rendering confirmed02:54PM EDT- Multi-app 3D engine02:55PM EDT- \"First OS designed from the ground up for spatial computing\"02:55PM EDT- And since this is WWDC, what does this mean for developers?02:56PM EDT- Showing off some apps that developers given early access have put together02:57PM EDT- Microsoft app support. Excel, Word, Teams02:57PM EDT- Reality Composer Pro, for assembling scenes with realistic objects02:57PM EDT- iPhone and iPad apps will run on the headset02:58PM EDT- Apple has been working with Unity to get the engine and apps based on it on the headset02:58PM EDT- Vision Pro will have a new App Store02:58PM EDT- More info for developers this afternoon with the Platform State of the Union02:59PM EDT- Now recapping privacy and security02:59PM EDT- User authentication through a new system, Optic ID03:00PM EDT- Based onretinaliris scans03:00PM EDT- Password auto fill and other major features available03:00PM EDT- Eye input is isolated to a separate background process. Apps and websites cannot tell where is a user is looking. Only where they tap03:01PM EDT- And that's Apple Vision Pro03:01PM EDT- \"The most advanced personal electronics device ever\"03:01PM EDT- Over 5,000 patents filed03:02PM EDT- Starts at $349903:02PM EDT- Available \"early next year\"03:03PM EDT- (So, not for at least more than another half a year)03:03PM EDT- And now back to Tim03:04PM EDT- (As a writer, I appreciate the vision puns)03:04PM EDT- Tim believes this is something only Apple can deliver03:04PM EDT- And now rolling an ad for Apple Vision Pro03:07PM EDT- And that's a wrap for the WWDC keynote. Apple will have plenty more for developers throughout the week\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18895/the-apple-wwdc-2023-keynote-live-blog-starts-at-10am-pt1700-utc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Unveils 2023 Mobile CPU Core Designs: Cortex-X4, A720, and A520 - the Armv9.2 Family\n",
      "Author: Gavin Bonshor\n",
      "Date Published: 2023-05-29T00:30:00Z\n",
      "URL: https://www.anandtech.com/show/18871/arm-unveils-armv92-mobile-architecture-cortex-x4-a720-and-a520-64bit-exclusive\n",
      "Content: Throughout the world, if there's one universal constant in the smartphone and mobile device market, it's Arm. Whether it's mobile chip makers basing their SoCs on Arm's fully synthesized CPU cores, or just relying on the Arm ISA and designing their own chips, at the end of the day, Arm underlies virtually all of it. That kind of market saturation and relevance is a testament to all of the hard work that Arm has done in the last few decades getting to this point, but it's also a grave responsibility – for most mobile SoCs, their performance only moves forward as quickly as Arm's own CPU core designs and associated IP do.Consequently, we've seen Arm settle into a yearly cadence for their client IP, and this year is no exception. Timed to align with this year's Computex trade show in Taiwan, Arm is showing off a new set of Cortex-A and Cortex-X series CPU cores – as well as a new generation of GPU designs – which we'll see carrying the torch for Arm starting later this year and into 2024. These include the flagship Cortex-X4 core, as well as Arm's mid-core Cortex-A720. and the new little-core Cortex-A520.Arm's latest CPU cores build upon the foundation of Armv9 and their previous Total Compute Solution (TCS21/22) ecosystem. For their 2023 IP, Arm is rolling out a wave of minor microarchitectural improvements through its Cortex line of cores with subtle changes designed to push efficiency and performance throughout, all the while moving entirely to the AArch64 64-bit instruction set. The latest CPU designs from Arm are also designed to align with the ongoing industry-wide drive towards improved security, and while these features aren't strictly end-user facing, it does underscore how Arm's generational improvements are to more than just performance and power efficiency.In addition to refining its CPU cores, Arm has undertaken a comprehensive upgrade of its DynamIQ Shared Unit core complex block, with the DSU-120. Although the modifications introduced are subtle, they hold substantial significance in terms of improving the efficiency of the fabric holding Arm CPU cores together, along with extending Arm's reach even further in terms of performance scalability with support for up to 14 CPU cores in a single block – a move designed to make Cortex-A/X even better suited for laptops.With three new CPU cores and a new core complex, there's a lot to cover. So let's dive right in.Arm TCS23 at a High Level: Pushing Efficiency & Going Pure 64-bitExpanding on the enhancements introduced in the Armv9.1 architecture last year, Arm is progressing through its scheduled development cycle with the latest Armv9.2 architecture. The primary objective of this cycle is to eliminate support for 32-bit applications and transition to a comprehensive 64-bit platform. Underpinning this transition is Arm's strategic framework, \"Total Compute Solutions\" (TCS), which revolves around three core principles: compute performance, security, and developer access. This approach forms the foundation for Arm's methodology and guides its efforts in delivering optimal performance, robust security measures, and streamlined developer capabilities.Arm's focus on phasing out the 32-bit instruction set has been one it has been working towards for several years. For their latest TCS23, they have finally created a fully 64-bit cluster to capitalize on the benefit of a complete 64-bit mobile ecosystem, excising AArch32 (32-bit instruction) support entirely.. So whether it's a big, mid, or little core, for Arm's latest generation of IP there is only AArch64.Developing a dynamic system-on-a-chip (SoC) that caters to a broad spectrum of mobile devices, ranging from cutting-edge flagship smartphones to entry-level models, necessitates a meticulous and consistent approach to maintaining competitiveness in a rapidly expanding market. In the realm of flagship devices, for instance, Qualcomm's Snapdragon 8 Gen2 SoC stands out, leveraging a cluster of Arm's Cortex-X3, Cortex A715/710, and Cortex-A510 cores. The upcoming iteration of Qualcomm's Snapdragon 8 Gen3 and other SoC manufacturers are poised to harness the power of Arm's TSC23 core cluster and intellectual property to further enhance performance in the subsequent generation of flagship mobile devices.Arm's latest DynamIQ Shared Unit, DSU-120, offers support for up to 14 CPU cores in a cluster, which opens the door to a significant number of different CPU core combinations. We'll see what SoC vendors have opted for later this year, but one probably configuration is a 1+5+2 (X4+720+520), which is likely a configuration for a high-end smartphone. Compared to a last-generation 1+3+4 cluster (X3+715+510), Arm is claiming an uplift of 27% in compute performance within GeekBench 6 MT and a more considerable uplift of between 33% and 64% in the Speedometer 2.1 benchmark depending on software optimizations implemented.Focusing more on the approach to 64-bit migration, last year Arm announced their first AArch64-only CPU core, the Cortex-A715. Consequently, last year saw the release of the first 64-bit only products, such as MediaTek's Dimensity 9200 SoC, as well as Google's Pixel 7 – which was 64-bit only as a platform choice rather than an architectural restriction.That said, actual AArch64 adoption/use within the larger software ecosystem has been slower than expected, primarily due to the Chinese market being slow to make the switch from 32-bit to 64-bit. Google has actually been key with its application storage (Google Play) by requiring its developers to submit 64-bit apps as far back as 2019, while also allowing the use of 32-bit applications on devices without native 64-bit support. Other markets haven't been as quick in doing so, but Arm claims that it is 'nudging' companies such as OPPO, Vivi, and Xiaomi to adopt AArch64 faster, which is believed to have the desired effect.With the initial Armv9 architecture, Arm made improvements to security through the use of its Memory Tagging Extension (MTE) (Armv8.5), which is a hardware-based implementation that uses Pointer Authentication (PA) extensions to help protect from memory vulnerabilities. Memory-based vulnerabilities have been a consistent threat to hardware-based security for many years, and it is something Arm is continually developing within its IP to help mitigate these types of attacks. For reference, Google's Chromium Project claimed that around 70% of high-severity bugs are from memory.One of the related security features of the latest Armv9.2 architecture is the introduction of a new QARMA3 Pointer Authentication Code (PAC) algorithm. Arm claims the newer algorithm reduces the CPU overhead of PAC to less than 1%, even on their little cores, giving developers and handset vendors even less of a reason to not enable the security feature. Most of these improvements revolve around hardware integrity and security, with a combination of MTE and native benefits through the 64-bit instruction and architecture, all designed to make devices even more secure going into 2023 and beyond. This fits with Arm's ethos to encourage a full switch to 64-bit over a hybrid 64 and 32-bit marketplace.Finally, looking at performance, Arm claims that their latest generation CPU and core complex architecture has made solid gains in power efficiency. At iso-performance, Cortex-X4 offers upwards of a 40% reduction in power consumption versus Cortex-X3, while Cortex-A720 and A520 save 20-22% over their respective predecessors. On the DSU-120 hub itself, Arm claims an 18% improvement in power efficiency.Of course, most of these power savings are going to instead be invested in additional performance. But it goes to show what SoC and handset vendors can aim for in this generation if they focus singularly on power efficiency and battery life.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18871/arm-unveils-armv92-mobile-architecture-cortex-x4-a720-and-a520-64bit-exclusive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NVIDIA Reports Q1 FY2024 Earnings: Bigger Things to Come as NV Approaches $1T Market Cap\n",
      "Author: Ryan Smith\n",
      "Date Published: 2023-05-25T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/18874/nvidia-reports-q1-fy2024-earnings-bigger-things-to-come-as-nv-approaches-1t-market-cap\n",
      "Content: Closing out the most recent earnings season for the PC industry is, as always, NVIDIA. The company’s unusual, nearly year-ahead fiscal calendar means that they get the benefit of being casually late in reporting their results. And in this case, they’ve ended up being the proverbial case of saving the best for last.For the first quarter of their 2024 fiscal year, NVIDIA booked $7.2 billion in revenue, which is a 13% drop over the year-ago quarter. Like the rest of the chip industry, NVIDIA has been weathering a significant slump in demand for computing products over the past few quarters, which in turn has dented NVIDIA’s revenue and profitability. However, while NVIDIA’s consumer-focused gaming division has continued to take matters on the chin, the strong performance of NVIDIA’s data center group has kept the company as a whole fairly profitable, with the most recent quarter setting a segment record and helping NVIDIA to avoid the tough financial situations faced by rivals AMD and Intel.NVIDIA Q1 FY2024 Financial Results (GAAP)Q1 FY2024Q4 FY2023Q1 FY2023Q/QY/YRevenue$7.2B$6.1B$8.3B+19%-13%Gross Margin64.6%63.3%65.5%+1.3ppt-0.9pptOperating Income$2.1B$1.3B$1.9B+70%+15%Net Income$2.0B$1.4B$1.6B+44%+26%EPS$0.82$0.57$0.64+44%+28%To that end, while Q1’FY24 was not by any means a record quarter for NVIDIA, it was still a relatively strong one for the company. NVIDIA’s net income of $2 billion makes for one of their better quarters in that regard, and it’s actually up 26% year-over-year despite the revenue drop. That said, reading between the lines will find that NVIDIA paid their Arm acquisition breakup fee last year (Q1’FY23), so NVIDIA’s GAAP net income looks a bit better than it otherwise would; while non-GAAP net income would be down 21%. Meanwhile, NVIDIA’s gross margins have held strong in the most recent quarter, with NVIDIA posting a GAAP gross margin of 64.6%.But even a solid quarter during an industry slump is arguably not the biggest news to come out of NVIDIA’s most recent earnings report. Rather, it’s the company’s projections for Q2’FY24. In short, NVIDIA is expecting revenue to explode in Q2, with the company forecasting $11 billion in sales. Should it come to fruition, such a quarter would blow well past NVIDIA’s previous revenue records – and shattering Wall Street expectations. As a result, NVIDIA’s stock has already taken off in overnight trading, and by the time the market opens a bit later this morning, NVIDIA is expected to be a $930B+ company, knocking on the door of crossing a market capitalization of a trillion dollars.NVIDIA Reporting Segment ResultsNVIDIA Segment Results, Q1 FY2024 (GAAP)Q1 FY2024Q4 FY2023Q1 FY2023Q/QY/YData Center$4,284M$3,616M$3,750M+18%+14%Gaming$2,240M$1,831M$3,620M+22%-38%Professional Visualization$295M$226M$622M+31%-53%Automotive$296M$294M$138M+1%+114%OEM & IP$77M$84M$158M-8%-51%But first things first, let’s take a look at the performance of NVIDIA’s individual segments. The bellwether of NVIDIA’s product portfolio over the most recent quarter was unambiguously the company’s data center segment, which booked $4.3B in revenue. The data center segment is doing most of the heavy lifting for NVIDIA’s revenue right now, as the other major segment, gaming, and most of the minor segments are all down year-over-year. In comparison to those other segments, data center revenue wasn’t just up 14% year-over-year, but it set a new record for the company.This also marked the first quarter where NVIDIA’s data center revenue eclipsed Intel’s data center revenue – though it may very well have been a fluke based on an unusually weak quarter from Intel ahead of higher volume shipments of Sapphire Rapids CPUs. Either way, quarters like these underscore why all three of the big PC chip makers are chasing after the data center market, as the profitability significantly eclipses the consumer market.Encompassing both NVIDIA’s data center compute products (GPUs, CPUs, etc) as well as NVIDIA’s ex-Mellanox networking products, NVIDIA is attributing most of the growth of this segment to growing demand for GPUs for use with large language models (LLMs) and other types of generative AI. As hinted at by the explosion in public interest in ChatGPT and other products late last year – and the subsequent knock-on effect it’s had on NVIDIA’s data center GPU sales – major technology companies seem to be investing significantly in snapping up GPUs for AI training and inference. NVIDIA is reporting that cloud service providers and consumer internet companies were the big drivers of growth, leaving enterprise sales as more consistent, and networking sales were down versus the year-ago quarter.NVIDIA, in turn, is expecting the demand for their data center products to remain strong, even as they continue to ramp up the production of H100 HPC accelerators, L-series server cards, and the first Grace CPU-based products. As a result, the expectations for NVIDIA’s data center segment are very high, as NVIDIA is in an extremely favorable position given the demand for server and data center GPUs – perhaps even more so than the peak of the most recent cryptocurrency boom.NVIDIA’s consumer-focused gaming division, on the other hand, was more of a mixed bag. At $2.2B in revenue, sales of GeForce and other cards were down significantly over what was largely the final quarter of the cryptocurrency boom and the overall pandemic-boosted rush on compute products in the consumer space. The 38% YoY drop comes as NVIDIA’s direct customers are still drawing down their product inventories (particularly now last-gen RTX 30 series parts), and RTX 40 series shipments are still picking up with the launch of larger parts of the product stack for desktops and laptops.Still, $2.2B in gaming revenue actually beat some analyst expectations for the segment. So while NVIDIA’s gaming sales are down significantly, they’re apparently down a bit less than industry watchers were expecting.Moving down the list, NVIDIA’s professional visualization segment largely follows their gaming segment in both good times and bad. So with revenues down 53% to $295M on a year-over-year basis, the most recent quarter was an especially rough one. Partners are still doing inventory draw-downs, though the introduction of new products is helping to turn things around.The automotive segment, on the other hand, was NVIDIA’s other growth segment for the quarter, with revenues jumping 114% for the quarter to $296M. While this segment has still yet to become a break-out segment for NVIDIA, sales have been looking consistently better since the launch of their Orin platform and the associated jump in overall sales.Finally, NVIDIA’s OEM & Other segment was another that saw significant declines, dropping 51% to $77M. According to the company, this was primarily driven by lower sales of entry-level GeForce MX GPUs.Looking Forward: Aiming to Beat NVIDIA’s FY2020 Revenue in a Single QuarterBut for as solid as NVIDIA’s Q1 report was in an otherwise tepid technology market, the other half of the story relating to their latest earnings release comes from what will happen next. Or rather, what NVIDIA is projecting.For the second quarter of their 2024 fiscal year, NVIDIA is projecting $11 billion (plus or minus 2%) in revenue. This would be a massive, 64% year-over-year jump in total revenue for the company, and a nearly as large 53% increase over Q1. And, as NVIDIA tells it, it’s not going to be a fluke.Driving this massive jump in revenue is expected to be a boom in NVIDIA data center product sales, especially as production of NVIDIA’s high-end data center products continues to ramp. Business interest in AI has already created significant demand for the H100 and other accelerators, and that demand isn’t expected to abate any time soon as NVIDIA assembles an ever-larger number of accelerators. In order to keep up, the comapny has already ordered \"substantially more\" GPUs for the second half of the year, based on that initial boom in demand.All pithiness aside, artificial intelligence is clearly the growth driver for the data center market across the entire industry right now, and NVIDIA’s control over the lion’s share of that market has them standing to benefit the most from the demand.If NVIDIA’s $11 billion quarter comes to pass, then it will lead to NVIDIA booking as much revenue as in all of FY2020 – or if you want to go to pre-pandemic times, FY 2018. All of which is significant growth for what was already a very large company before the pandemic.That $11 billion quarter projection has also blown past analysts’ expectations for the quarter, which prior to the announcement were on the order of $7.2 billion.As a result, NVIDIA’s stock price spiked almost the moment they made their earnings release – and has stayed that high overnight – as investors adapt to new revenue expectations for NVIDIA. At about an hour before the stock market opens, NVIDIA’s stock is up $78 to $385, a 26% jump, and one with very few precedents even within the wild tech industry.NVIDIA 10 Year Market Capitalization (StockAnalysis.com)The large jump in NVIDIA’s stock price is also driving up NVIDIA’s market capitalization. When the markets open, NVIDIA is expected to open as a $930+ billion company, $175B+ higher than its market capitalization the night before. To put things in perspective, that is an entire AMD ($174B) in market capitalization growth, or a whole Intel ($121B) with change to spare.This will also put NVIDIA on the doorstep of becoming the next trillion dollar company, a very elite club that, according to Bloomberg, only eight companies have hit before (and only 5 companies are members of now). NVIDIA is already the most valuable chipmaker (fabless or otherwise) by leaps and bounds, and this jump in market capitalization will further grow that gap.But regardless of whether NVIDIA hits the $1 trillion mark or not, the company’s latest earnings report and subsequent stock price rally underscore the value of AI infrastructure – perceived or otherwise. The rest of the industry is eager to make sure that the story of artificial intelligence is not the story of NVIDIA, and to that end we should expect plenty of AI-related news and hardware developments to come.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18874/nvidia-reports-q1-fy2024-earnings-bigger-things-to-come-as-nv-approaches-1t-market-cap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD Launches Zen 2-based Ryzen and Athlon 7020C Series For Chromebooks\n",
      "Author: Gavin Bonshor\n",
      "Date Published: 2023-05-23T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/18868/amd-launches-zen-2-based-ryzen-and-athlon-7020c-series-for-chromebooks\n",
      "Content: Last year, AMD unveiled theirentry-level 'Mendicino' mobile partsto the market, which combine their 2019 Zen 2 cores and their RDNA 2.0 integrated graphics to create an affordable selection of configurations for mainstream mobile devices. Although much of the discussion over the last few months has been about their Ryzen 7040 mobile parts, AMD has launched four new SKUs explicitly designed for the Chromebook space, the Ryzen and Athlon 7020C series.Some of the most notable features of AMD's Ryzen/Athlon 7020C series processors for Chromebooks include three different configurations of cores and threads, ranging from entry-level 2C/2T up to 4C/8T, all with AMD's RDNA 2-based Radeon 610M mobile integrated graphics. Designed for a wide variety of tasks and users, including and not limited to consumers, education, and businesses, AMD's Ryzen 7020C series looks to offer similar specifications and features to their regular 7020 series mobile parts but expands things to the broader Chromebook and ChromeOS ecosystem too.Quickly recapping last year's AMD Mendocino mobile parts, AMD launched four SKUs with Zen 2 cores and RDNA 2 integrated graphics. Much like the AMD Mendocino series for Chromebooks that AMD has announced, their generic Ryzen/Athlon 7020 parts for mobile devices range from two entry-level 2C/2T and 2C/4T parts up to two beefier 4C/8T parts, all with AMD's Radeon 610M 'Mendocino' integrated graphics.The top tier Ryzen 7020 series CPU, the Ryzen 5 7520U, has 4C/8T, with a base frequency of 2.8 GHz and a 1T boost frequency of up to 4.3 GHz. Similar in specification but slightly slower, the Ryzen 3 7320U has the same 4C/8T but with a 2.4 GHz base and 4.1 GHz 1T boost core clock speed, while both share the same 4 MB of L3 cache.The other two chips, Althon Gold 7220U and Athlon Silver 7120U were again similar, but the Athlon Gold 7220U has Simultaneous Multithreaded (SMT) enabled, giving it 2C/4T instead of 2C/2T. All of AMD's Ryzen/Athlon 7020 series processors also have a TDP of 15 W, making them more suitable for less intensive workloads and lower-powered solutions where things like extending battery life on the go are more important than raw compute power.AMD Ryzen/Athlon 7020C Series: Mendocino For ChromebooksInterestingly, the Ryzen/Athlon 7020C series is also based on AMD's Mendocino graphics and TSMC's 6 nm node, and the specifications between the 7020C and 7020 series are virtually identical. The only real difference between both sets of Mendocino chips is that the 7020C SKUs are designed specifically for ChromeOS. At the same time, the regular 7020 series caters to Microsoft Windows-based mobile devices such as notebooks.AMD Ryzen 7020 C-Series LineupSKUCores/ThreadsCPU Frequency(Base)CPU Frequency(1T Boost)CacheiGPUTDPRyzen 5 7520C4C / 8T2.8 GHz4.3 GHz2MB L2 + 4MB L3Radeon 610M15 WRyzen 3 7320C4C / 8T2.4 GHz4.1 GHz2MB L2 + 4MB L3Radeon 610M15 WAthlon Gold 7220C2C / 4T2.4 GHz3.7 GHz1MB L2 + 4MB L3Radeon 610M15 WAthlon Silver 7120C2C / 2T2.4 GHz3.5 GHz1MB L2 + 2MB L3Radeon 610M15 WStarting at the top of the stack, the AMD Ryzen 5 7520C is a 4C/8T part with a 2.8 GHz base and a 4.3 GHz 1T boost clock frequency. The Ryzen 3 7320C is very similar to the Ryzen 5 7520C, but it's slightly slower, with a base core frequency of 2.4 GHz (400 MHz less) and a 1T boost frequency of 4.1 GHz (200 MHz less). Each core has a dedicated 512 KB of L1 cache, sothe 4C/8T parts have a total L2 cache of 2 MB and a shared 4 MB pool of L3 cache.Moving down the stack, we have the Athlon Gold 7220C, a 2C/4T part with a combined total of 1 MB of L2 cache, and this specific SKU has the same 4 MB pool of L3 cache available to it. It has a 2.4 GHz base frequency, with a 1T boost frequency of up to 3.7 GHz. Last but not least, the entry-level Athlon Silver 7120C doesn't benefit from SMT, so it's essentially a 2C/2T chip with the same base frequency of 2.4 GHz as the Athlon Gold variant but with a slightly slower 1T boost frequency of 3.5 GHz (200 MHz less).While TDP is an enigmatic term, it's always a good indication of where power will likely sit from a manufacturer's standpoint. All four of AMD's Ryzen/Athlon 7020C series processors for ChromeOS use their RDNA 2.0-based Radeon 610M integrated graphics, which has 2 x CUs with a total of 128 shader cores that are clocked to 1.9 GHz. Although the Radeon 610M isn't primarily designed for gaming due to the low number of shader cores.For reference, the Ryzen 7020/7020C series features the same integrated graphics found on the higher-end Zen 4 based Ryzen 9 7945HX 16C/32T mobile processor, which requires discrete graphics to unleash the additional bandwidth benefits from PCIe 5.0 and PCIe 4.0 graphics chips.In the case of the 7020C series, all four SKUs have a TDP of 15 W set by AMD, which means these chips are focused on delivering more efficiency, which is critical in things such as extending battery life, which is vital in a mobile device.Despite both sets of the Mendocino-based Ryzen and Athlon 7020/C series processors featuring the exact specifications, there's one key and critical difference. This comes via Security features, and as it's to be expected, a device designed for Microsoft's Windows operating system has different requirements and needs from something such as ChromeOS. We reached out to AMD to find out what differences there are in integrated security, with AMD stating the following:\"These (7020C series) are the ChromeOS versions of the Mendocino processors launched for Windows last year. They do share the same specs. However, the 7020C Series is optimized for ChromeOS, including different fusing specifically for ChromeOS security features. Hence the 7020U series cannot be used for Chromebooks\".Other features of AMD's Mendocino-based 7020C series processors include support for up to three external 4K60 displays, with the capability to support Wi-Fi 6 and Bluetooth 5.2 devices; this is something the Chromebook vendors will implement through controllers. Like the Ryzen/Athlon 7020 series, the 7020C series also supports the lower-powered LPDDR5 memory, which concerning JEDEC, is LPDDR5-5500 in dual channel mode for the Mendocino-based chips. We asked for further clarification from AMD on what this fusing of security features entails, but AMD declined to comment or share additional details.As is typical with a new launch, AMD did provide some in-house performance figures, but we usually take these with a pinch of salt without emphasizing them too much. Within AMD's slide deck, the AMD to AMD performance comparisons were comparing the Ryzen 3 7320C against the Ryzen 3 3250C, which is, in fact, two generations old; the previous generation designed for Chromebooks was the 5025C series. AMD also compared performance against Arm-based MediaTek Kompanio chips and an Intel Core i3-N305 processor.Regarding the broader claims made, AMD claims a 1.8x uplift in performance on average compared with Arm's IP, with up to 15% 'better' performance on average against the competition on the x86 CISC. AMD also claims leadership in battery life. However, as it is widely down to notebook and Chromebook vendors to optimize through its batteries and power-saving mechanisms, it's a much harder metric to judge from a platform standpoint.In addition to their announcement of the Ryzen and Athon 7020C series for Chromebooks, AMD has also announced two new Chromebooks with its partners based on the 7020C series. One of these is the Dell Lattitude 3445 Chromebook, which is powered by the AMD 7020C series processors, with support for up to 256 GB of SSD, 16 GB (2 x 8 GB) of LPDDR5 memory, and with integrated support for Wi-Fi 6 and BT 5.2 devices.The second Chromebook to be announced is the ASUS Chromebook CM34 Flip (CM3401), which has a 14\" 16:10 aspect ratio touchscreen display with an optional one-touch fingerprint sensor and is powered by up to and including the Ryzen 5 7520C processor. The ASUS CM34 Flip also has an integrated FHD webcam with a privacy shutter and supports Wi-Fi 6 wireless connectivity and BT 5.2 devices.Both the Dell Lattitude 3445 and ASUS CM34 Flip (CM3401) are expected to release sometime in Q2 2023, with other devices from AMD's partners expected to launch new Chromebooks with Ryzen/Athlon 7020C series processors around the same time.At the time of writing, neither AMD nor their Chromebook partners have provided any indications on prices or other model specifications or configurations. As we've mentioned, AMD's Ryzen and Athlon 7020C series for Chromebooks are expected to launch and hit retail shelves sometime in Q2 2023.Gallery:AMD Ryzen and Athlon 7020 C-Series Slide Deck Gallery\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18868/amd-launches-zen-2-based-ryzen-and-athlon-7020c-series-for-chromebooks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Intel HPC Updates For ISC 2023: Aurora Nearly Done, More Falcon Shores, and the Future of XPUs\n",
      "Author: Ryan Smith\n",
      "Date Published: 2023-05-22T16:45:00Z\n",
      "URL: https://www.anandtech.com/show/18869/intel-hpc-update-isc-2023-falcon-shores-details-future-xpu-aurora-nearly-done\n",
      "Content: With the annual ISC High Performance supercomputing conference kicking off this week, Intel is one of several vendors making announcements timed with the show. As the crown jewels of the company’s HPC product portfolio have launched in the last several months, the company doesn’t have any major new silicon announcements to make alongside this year’s show – and unfortunately Aurora isn’t yet up and running to take a shot at the Top 500 list. So, following a tumultuous year thus far that has seen significant shifts in Intel’s GPU roadmap in particular, the company is using ISC to recompose itself and use the backdrop of the show to lay out a fresh roadmap for HPC customers.Most notably, Intel is using this opportunity to better explain some of the hardware development decisions the company has made this year. That includes Intel’s pivot on Falcon Shores, transforming it from XPU into a pure GPU design, as well to a few more high-level details of what will eventually become Intel’s next HPC-class GPU. Although Intel would clearly be perfectly happy to keep selling CPUs, the company has (and continues to) realign for a diversified market where their high-performance customers need more than just CPUs.CPU Roadmap: Emerald Rapids and Granite Rapids Xeons in the WorksAs noted earlier, Intel isn’t announcing any new silicon today across any part of their HPC portfolio. So Intel’s latest HPC roadmap is essentially a condensed version of theirlatest data center roadmap, which was first laid out to investors towards the end of March. HPC is, after all, a subset of the data center market, so the HPC roadmap reflects this.I won’t go into Intel’s CPU roadmap too much here, since we just covered it a couple of months ago, but the company is once again reiterating the rapid-fire run they intend to make through their Xeon products over the next 18 months. Sapphire Rapids is only a few months into shipping, but Intel intends to have its same-platform successor, Emerald Rapids, ready for delivery in Q4. Meanwhile Granite Rapids, Intel’s first P-Core Xeon on the Intel 3 process, will launch with its new platform in 2024. Granite will also be Intel’s first product to support higher bandwidthMCR DIMM memory, which was similarly demonstrated back in March.Notably here, despite the HPC audience of ISC, Intel still hasn’t announced a successor to the current-generation HBM-equipped Sapphire Rapids Xeon with HBM – which the company brands as the Xeon Max Series. Intel’s rather proud of the part – pointing out that it’s the only x86 processor with HBM whenever they get the chance – and it’s a core part of the Aurora supercomputer. We had been expecting its successor to fall into place with Falcon Shores back when it was an XPU, but since Falcon pivoted to being a GPU, there’s been no further sign of where another HBM Xeon will land on Intel’s roadmap.In the meantime, Intel is eager to demonstrate to the ISC audience the performance benefits of having so much high bandwidth memory on-package with the CPU cores – and especially before AMD launches their EPYC Genoa-X processors with their supersized, 1GB+ L3 caches. To that end Intel has published several fresh benchmarks comparing Xeon Max Series processors to EPYC 7000 and 9000 series chips, which as they’re vendor benchmarks I won’t get into here, but you can find in the gallery below.Gallery:Intel ISC 2023 XeonGPU Roadmap Today: Ponte Vecchio Now Shipping, Additional SKUs To Launch in Coming MonthsThe GPU counterpart to Sapphire Rapids with HBM for the HPC crowd is Intel’s Data Center GPU Max series, otherwise known as Ponte Vecchio. The massively tiled chip is still unlike any other GPU on the market, and Intel’s IFS foundry arm is quite proud to point out to potential customers that they’re able to reliably assemble one of the most advanced chips on the market, with nearly four dozen chiplets to perfectly place to bring the whole thing together.Ponte Vecchio has had a long and exhausting development cycle for Intel and its customers alike, so they’re taking a bit of a victory lap at ISC to celebrate that accomplishment. Of course, Ponte Vecchio is just the beginning of Intel’s HPC GPU efforts, and not the end. So they are still in the process of building up the OneAPI software and tool ecosystem to support the hardware – all while being mindful of the fact that they need a strong software ecosystem to match rival NVIDIA, and to capitalize on AMD’s current shortcomings.Despite being nearly a generation late, Intel surprisingly has some benchmarks comparing Ponte Vecchio to NVIDIA’s new Hopper architecture-based H100 accelerators. With that said, these are for Intel’s top-end OAM-based modules against H100 PCIe cards; so cherry picking aside, it remains to be seen just how well things would look with a more apples-to-apples hardware comparison.Gallery:Intel ISC 2023 Data Center GPU MaxSpeaking of OAM modules, Intel is using the show to announce a new 8-way Universal Baseboard (UBB) for Ponte Vecchio. Joining Intel’s existing 4-way UBB, the x8 UBB will allow for 8 Data Center Max GPU modules to be placed on a single server board, similar to what NVIDIA does with their HGX carrier boards. If Intel is to go toe-to-toe with NVIDIA and to capture part of the HPC GPU market, then this is one more area where they’re going to need to match NVIDIA’s hardware offerings. Thus far Supermicro and Inspur are signed up to distribute servers using the new x8 UBB, and if things go their way, these shouldn’t be Intel’s only customers.Along with the UBB announcement, Intel is also providing for the first time a detailed, month-by-month roadmap for Data Center Max GPU product availability. Now that Intel has nearly satisfied their Aurora order, the first parts have been vaguely available to select customers, but now we get to see where things stand in a bit more detail. Per that roadmap, OEMs should be ready to begin shipping 4-way GPU systems in June, while 8-way systems will be a month behind that in July. Meanwhile OEM systems using the PCIe version of Ponte Vecchio, the Data Center GPU Max 1100, will be available in July. Finally, a detuned version of Ponte Vecchio for “different markets” (read: China) will be available in Q4 of this year. Details on this part are still slim, but it will have reduced I/O bandwidth to meet US export requirements.GPU Roadmap Tomorrow: All Roads Lead to Falcon ShoresLooking past the current iteration of the Data Center GPU Max series and Ponte Vecchio, the next GPU in the pipeline for Intel’s HPC customers is Falcon Shores. As wedetailed back in March, Falcon Shores will be taking on a significantly different role in life than Intel first intended, following the cancellation of Rialto Bridge, Ponte Vecchio’s direct descendent. Instead of being Intel’s first combined CPU + GPU product – a flexible XPU that can use a mix of CPU and GPU tiles – Falcon is now going to be a purely GPU product. Unfortunately, it’s also picking up a year’s delay in the process, pushing it to 2025, meaning that Intel’s HPC GPU lineup is purely Ponte based for the next couple of years.The cancellation of Rialto Bridge and the de-XPUing of Falcon Shores created a good deal of consternation within the media and HPC community, so Intel is using this moment to get their messaging in order, both in terms of why they pivoted on Falcon Shores, and just what it will entail.The long and short of the story there is that Intel has decided that they mistimed the market for their first XPU, and that Falcon Shores as an XPU would have wound up being premature. In Intel’s collective mind, because these products offer a fixed ratio of CPU cores to GPU cores (vis a vie the number of tiles used), they are best suited for workloads that closely match those hardware allocations.And what workloads are those? Well, that ends up being the 100B transistor question. Intel was expecting the market to be more settled than it actually has been – that is to say, it’s been more dynamic than Intel was expecting – which Intel believes makes an XPU with its fixed ratios harder to match to workloads, and harder to sell to customers. As a result, Intel has backed off on their integration plans, leading to the all-GPU Falcon Shores.Now with that said, Intel is making it clear that they’re not aborting the idea of an XPU entirely; only that Falcon Shores in 2024/2025 is not the right time for it. So, Intel is also confirming that they will be developing a tile-based XPU as a future, post-Falcon Shores product (possibly as Falcon Shores’ successor?). There are no further details on that future XPU than this, but for now, Intel still wants to get to CPU/GPU integration once they deem the workloads and the market are ready. This also means that Intel is effectively ceding the mixed CPU-GPU accelerator market to AMD (and to a lesser extent, NVIDIA) for at least a few more years, so make of that what you will with Intel’s official rationale for delaying their own XPU.As for the all-GPU Falcon Shores, Intel is sharing just a hair more about the design and capabilities of their next-generation PC GPU. As you’d expect from a design that started as a tiled product, Falcon remains a chiplet-based design. Though it’s unclear just what kinds of chiplets Intel will use (if they’ll be homogenous GPU blocks or not), they will be paired with HBM3 memory, and what Intel terms as “I/O designed to scale.” In light of Intel’s decision to delay XPUs, this will be how they deliver a flexible CPU-to-GPU ratio for their HPC customers via the tried and true way: add as many GPUs to your system as you need.Falcon Shores will also support Ethernet switching as a standard feature, which will be a major component in supporting the kind of very large meshes that customers are building with their supercomputers today. And since these parts will be discrete GPUs, Intel will be embracing CXL to deliver additional functionality to system designers and programmers. Given the timing,CXL 3.0functionality is a safe bet, with things like P2P DMA and advanced fabric support going hand-in-hand with what the HPC market has been building towards.And with a few years of experience behind them at that point, Intel expects to be able to leverage OneAPI even harder. Especially as they’ll need the help of software to abstract the CPU-GPU I/O gap that Falcon Shores the XPU was otherwise going to be able to close in hardware.Gallery:Intel ISC 2023 Falcon ShoresAurora Update: 10K+ Blades Delivered, Additional Specifications DisclosedFinally, Intel is also offering an update on Aurora, their Sapphire Rapids with HBM + Ponte Vecchio based supercomputer for Argonne National Laboratory. A product of two delayed processors, Aurora is itself a delayed system that Intel has been working to catch up on. In terms of the hardware itself, the light is in sight at the end of the tunnel, as Intel is wrapping up delivery of Aurora’s compute blades.As of today, Intel has delivered over 10,000 blades for Aurora, very close to the final expected tally for the system of 10,624 nodes. Unfortunately, delivered and installed are not quite the same things here; so while Argonne has much of the hardware in hand, Aurora isn’t ready to make a run at the Top500 supercomputer list, leaving the AMD-based Frontier system to hold the top spot for another 6 months.On the plus side, with Aurora’s hardware shipments nearly complete, Intel is finally disclosing a more detailed summary of Aurora’s hardware specifications. This includes not only the number of nodes and the CPUs and GPUs within them, but also the various amounts of memory and storage available to the supercomputer.With 2 CPUs and 6 GPUs in each node, the fully assembled Aurora will be comprised of 21,248 Sapphire Rapids CPUs and 63,744 Ponte Vecchio GPUs, and as previously disclosed, the peak performance of the system is expected to be in excess of 2 ExaFLOPS of FP64 compute. Besides the 128GB of HBM on each GPU and 64GB of HBM on each CPU, there’s an additional 1 TB of DDR5 memory installed on each node. Peak bandwidth will come from the HBM for the GPUs, at 208.9PB/second, though even the “slow” DDR5 is still an aggregate 5.95PB/second.And since no supercomputer announcement would be complete without some mention of AI, Intel and Argonne are developing a generative/large-language-model AI for use on Aurora, which they are calling for now the Generative AI for Science. The model will be developed specifically for scientific use, and Intel expects it to be a 1 trillion parameter model (which would place it between GPT-3 and GPT-4 in size). The expectation is that they’ll use Aurora for both the training and inference of this model, though in the case of the latter, that would presumably be just a fraction of the system given the much lower system requirements for inference.At this point Aurora remains on schedule for a launch this year. Besides beginning production use, Intel expects that Aurora will be able to place on the Top500 list for its November update, at which point it is expected to become the most powerful supercomputer in the world.Gallery:Intel ISC 2023 AuroraGallery:Intel ISC 2023 Press Deck\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18869/intel-hpc-update-isc-2023-falcon-shores-details-future-xpu-aurora-nearly-done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ASRock Industrial 4X4 BOX-7735U UCFF PC Review: Zen 3+, RDNA2, and USB4 in a Potent Platform\n",
      "Author: Ganesh T S\n",
      "Date Published: 2023-04-06T14:30:00Z\n",
      "URL: https://www.anandtech.com/show/18794/asrock-industrial-4x4-box7735ud5-review\n",
      "Content: ASRock Industrial is one of the few vendors with a lineup of ultra-compact form-factor (UCFF) computing systems based on the latest Intel and AMD platforms. Their lineup ofNUC clones- the NUC BOX series with Intel, and the 4X4 BOX series with AMD - has enjoyed significant market success, mainly due to their commitment to leading edge processors. The companyintroduced the 4X4 BOX-7000 seriesUCFF PCs based on AMD's Rembrandt-R SoCs in early February, and sent across their flagship SKU - the 4X4 BOX-7735U/D5 - to be put through our rigorous evaluation routine.The 4X4 BOX-7735U/D5 is one of the first UCFF mini-PCs to rely on DDR5 SODIMMs, and it is the first AMD-based mini-PC in our labs to include USB4 functionality with PCIe tunneling. The process improvements in moving to Zen 3+ and a re-architected integrated GPU (RDNA2) should theoretically deliver significantly better performance and power efficiency for a range of workloads over the company's Cezanne-based flagship (4X4 BOX-5800U) from last year.The Ryzen 7 7735U, despite being a Zen 3+ part, was actuallyintroduced only in January 2023. With a TDP of 28W, this U series SKU is meant primarily for notebook platforms. However, its deployment in an actively cooled UCFF chassis has allowed ASRock Industrial to provide the end user with a bit of flexibility in terms of power consumption - and, as an extension, performance.ASRock Industrial ships the 4X4 BOX-7735U/D5 in the 'Normal Mode' with AMD's suggested TDP of 28W. However, a toggle in the BIOS can push the system into a 'Performance Mode' with a souped-up TDP of 42W. This review explores the performance profile of the PC in both modes, and provides detailed insights into the differentiating aspects of AMD's Rembrandt-R in a UCFF mini-PC platform.Introduction and Product ImpressionsUltra-compact form-factor (UCFF) systems have managed to successfully replace bulky tower desktops for many use-cases over the last decade. The category continues to experience growth in both home consumer and industrial settings. The B2B / industrial computing arms of many motherboard vendors have also started providing more attention to such systems. ASRock Industrial (spun out of ASRock's business unit in 2018) has been creating UCFF systems based on both AMD and Intel platforms since 2019. We have reviewed a number of systems from the company in the last couple of years, ranging from the4X4 BOX-V1000M(based on the AMD Ryzen Embedded V1605B Zen SoC) to theNUCS BOX-1360P/D4(based on the Intel Core i7-1360P Raptor Lake-P processor).This review delves in detail into the company's flagship AMD UCFF offering for 2023 - the4X4 BOX-7735U/D5. Based on AMD's high-end Rembrandt-R 28W offering (Ryzen 7 7735U), the new system is meant to be a follow-up to theCezanne-based 4X4 BOX-5800Ureleased last year. The Ryzen 7 7735U continues with the same 8C/16T configuration of the Ryzen 7 5800U. However, the fabrication process has moved from TSMC's 7nm to 6nm FinFET, allowing the Zen 3 microarchitecture to get rebranded as Zen 3+. The process change has resulted in better voltage-frequency curves, with the net result being higher clocks for better performance and improvements in energy efficiency. More importantly, AMD has re-architected the integrated GPU - moving from the Vega-based one in Cezanne to a RDNA2-based Radeon 680M in Rembrandt-R. This has allowed the company to reclaim ground lost to Intel when the latter introduces the new Xe architecture into Tiger Lake and later processors.ASRock Industrial's UCFF systems are non-descript machines that do not opt for a fancy industrial design. The functional casing used in previous 4X4 BOX systems is retained for the 4X4 BOX-7735U/D5 also. While the Intel-based NUC(S) BOX lineup made the thankful move to matte polycarbonate for the chassis, the 4X4 BOX series continues to retain the glossy fingerprint magnet casing. The I/O port locations are exactly the same as in the previous generation, but the changes in Rembrandt-R has resulted in major updates to the internal board.The company's 4X4 BOX-7000 series has only two members - one based on the Ryzen 7 7735U and the other based on the Ryzen 5 7535U. Some of the key relevant aspects are brought out in AMD's introductory slide to the product family back at the 2023 CES.Unlike some of the other rebadges like Barcelo-R, Rembrandt-R supports only DDR5 and LPDDR5. For systems with user-replaceable memory like the 4X4 BOX-7000 series, DDR5 SODIMMs are the only option. The move to PCIe 4.0 means that the M.2 SSD slot on the board becomes capable of supporting Gen 4 SSDs. AMD also promises USB4 in the platform for fast external devices - and, as we shall see later on in this review, ASRock Industrial has configured the board components appropriately to enable this on both Type-C ports in the front panel.ASRock Industrial offers both barebones version of the system as well as themotherboardalone. The former is typically sold in the retail, while the latter is meant for the B2B channel. The barebones version package comes with a 120W DC power adapter (19V @ 6.32A), VESA mount (and associated screws), a geo-specific power cord, the main unit, and a product overview / user setup guide.The barebones version needs to DDR5 SODIMMs and a M.2 2280 SSD to complete the build. Kingston offered a DDR5-4800 FURY kit (2x8GB) for the build, and we complemented that with a Samsung PM9A1 512GB Gen 4 NVMe SSD (OEM version of the 980 PRO).Access to the SODIMM and M.2 slots is via the underside. Removal of the four screws at the bottom allows the panel to be popped off.While it is possible to install a 2.5\" SATA drive in the system, ASRock Industrial strongly recommends not doing it in order to aid with proper airflow. The installation process is otherwise similar to the older 4X4 BOX systems, and we were up and running with a freshly installed OS in no time. Windows online updates resolve almost all of the unknown devices in the device manager, but a few do need the AMD Chipset Driver package from ASRock Industrial'sproduct support page.The full specifications of the review sample (as tested) are summarized in the table below. As we will note in the next section, the BIOS allows the system to be configured in either of two modes with different TDPs, as specified in the Processor entry.Systems Specifications(as tested)ASRock 4X4 BOX-7735U (Performance)ASRock 4X4 BOX-7735U (Normal)ProcessorAMD Ryzen 7 7735UZen 3+ (Rembrandt R) 8C/16T, 2.7 - 4.75 GHzTSMC 6nm, 16MB L3, 28WMax / Target TDP : 50W / 42WAMD Ryzen 7 7735UZen 3+ (Rembrandt R) 8C/16T, 2.7 - 4.75 GHzTSMC 6nm, 16MB L3, 28WMax / Target TDP : 34W / 28WMemoryKingston Fury KF548S38-8 DDR5-4800 SODIMM38-38-38-70 @ 4800 MHz2x8 GBKingston Fury KF548S38-8 DDR5-4800 SODIMM38-38-38-70 @ 4800 MHz2x8 GBGraphicsAMD Radeon 680M (Rembrandt) - Integrated(12 CUs @ 2.2 GHz)AMD Radeon 680M (Rembrandt) - Integrated(12 CUs @ 2.2 GHz)Disk Drive(s)Samsung PM9A1 MZVL2512HCJQ(512 GB; M.2 2280 PCIe 4.0 x4 NVMe;)(Samsung 6thGen. V-NAND 128L (136T) 3D TLC; Samsung Elpis S4LV003 Controller; OEM version of 980 PRO)Samsung PM9A1 MZVL2512HCJQ(512 GB; M.2 2280 PCIe 4.0 x4 NVMe;)(Samsung 6thGen. V-NAND 128L (136T) 3D TLC; Samsung Elpis S4LV003 Controller; OEM version of 980 PRO)Networking1x 2.5 GbE RJ-45 (Realtek RTL8125)1x GbE RJ-45 (Realtek RTL8111EPV)Mediatek MT7922 (RZ616) Wi-Fi 6E (2x2 802.11ax - 1.9 Gbps)1x 2.5 GbE RJ-45 (Realtek RTL8125)1x GbE RJ-45 (Realtek RTL8111EPV)Mediatek MT7922 (RZ616) Wi-Fi 6E (2x2 802.11ax - 1.9 Gbps)AudioRealtek ALC233 (3.5mm Audio Jack in Front)Digital Audio with Bitstreaming Support over HDMI and Display PortRealtek ALC233 (3.5mm Audio Jack in Front)Digital Audio with Bitstreaming Support over HDMI and Display PortVideo1x HDMI 2.11x Display Port 1.4a2x Display Port 1.4a over USB4 Type-C1x HDMI 2.11x Display Port 1.4a2x Display Port 1.4a over USB4 Type-CMiscellaneous I/O Ports2x USB 2.0 (Rear)2x USB4 Type-C (Front)1x USB 3.2 Gen 2 Type-A (Front)2x USB 2.0 (Rear)2x USB4 Type-C (Front)1x USB 3.2 Gen 2 Type-A (Front)Operating SystemWindows 11 Enterprise (22000.1696)Windows 11 Enterprise (22000.1696)Pricing(Street Pricing on April 17th, 2023)US$630(barebones)USD 781 (as configured, no OS)(Street Pricing on April 17th, 2023)US$630(barebones)USD 781 (as configured, no OS)Full SpecificationsASRock Industrial 4X4 BOX-7735U/D5 SpecificationsASRock Industrial 4X4 BOX-7735U/D5 SpecificationsIn the next section, we take a look at the various BIOS options and follow it up with a detailed platform analysis.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18794/asrock-industrial-4x4-box7735ud5-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD Announces Alveo MA35D Media Accelerator: AV1 Video Encode at 1W Per Stream\n",
      "Author: Ryan Smith\n",
      "Date Published: 2023-04-06T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/18805/amd-announces-alveo-ma35d-media-accelerator-av1-video-encode-at-1w-per-stream\n",
      "Content: AMD this morning is launching a new dedicated media accelerator and video encode card for data centers – and the first to be released under the AMD brand – the Alveo MA35D. The card is a successor to an earlier line of Xilinx cards that AMD picked up as part of their Xilinx acquisition, vaulting them into the market for dedicated video encode cards. The latest generation Alveo media accelerator card, in turn, promises significant performance benefits over its predecessor, quadrupling the maximum number of simultaneous video streams while also adding AV1 and 8K resolution encode support.Like its predecessor, the Alveo U30, the MA35D is a pure video encode card designed for data centers. That is to say that its ASICs are designed solely for real-time/interactive video encoding, with Xilinx looking to do one thing and do it very well. This design strategy is in notable contrast to competing products from Intel (GPU Flex Series) and NVIDIA (T4 & L4), which are GPU-based products and leverage the flexibility of their GPUs along with their integrated video encoders in order to function as video encode cards, gaming cards, or other roles assigned to them. The MA35D, by comparison, is a relatively straightforward product that is designed to more optimally and efficiently do video encoding by focusing on just that.As this is a product line inherited by AMD as part of their Xilinx acquisition and developed by the resulting Adaptive & Imbedded Computing Group, the Alveo MA35D is both new for AMD and familiar at the same. Previous data center video encode products released by AMD were based on their GPU lineup, so while this is the latest such video encode card for the ex-Xilinx team, this is the first time AMD proper has launched a dedicated video encode card in this fashion – and making it a prime example of the kind of new market opportunities AMD was looking for in acquiring Xilinx.The target market for the card is, like its predecessor, the data center market. AMD’s principle clients are live streaming services and other interactive video services (think Twitch, cloud gaming, video conferencing, etc), all of whom need to encode large numbers of video streams in real-time in a server environment. So like AMD’s EPYC processors, this is very much a server part aimed at a select group of businesses.Diving into the Alveo MA35D hardware itself, AMD is touting a significant generational upgrade over its predecessor. Whereas the Alveo U30 was an H.264 and H.265 encode card that could encode up to 8 1080p streams, the Alveo MA35D expands this substantially to 32 1080p streams. Meanwhile, support for the latest-generation AV1 codec has been added – joining the existing H.264 and H.265 options – and the maximum stream resolution has been increased from 4K to 8K – itself another quadrupling.At the heart of the card is AMD’s unnamed video encode ASIC, which they are calling their Video Processing Unit (VPU). The MA35D contains two VPUs, each with their own 8GB pool of LPDDR5 memory and a PCIe 5.0 x4 connection back to the host processor. The VPU is being built on a 5nm process, through strangely AMD is not disclosing the fab being used, which makes us think it’s a Samsung 5nm process (ed: at this point, if someone is using TSMC, they’re usually bragging about it).Under the hood, each VPU contains 4 video encode blocks, augmented with the various accessory blocks needed to make it a fully functional chip. Two of the encode blocks are full-featured, supporting H.264, H.265, and AV1, while the other two blocks are solely for AV1 – underscoring the additional computational complexity of the new codec. Other blocks on the VPU include video decoder blocks for transcoding, memory controllers, management controllers, a bitrate scaler, composition engines, and a 22 TOPS throughput AI processor to further improve the card’s video encode quality.With the video encode blocks themselves, AMD’s engineers were quick to note that, despite the overlapping similarities between this part and AMD’s GPU efforts, the VPU’s video encode blocks are a unique design, and not pulled from AMD’s GPU video encode blocks. While I wouldn’t be surprised to see AMD eventually merge encoder IP across the product lines, for the current generation product the Alveo MA35D’s VPUs were in development before the Xilinx acquisition ever closed, so the former Xilinx team finished what they started. This means that the VPUs are bound to come with their own set of quirks, but also, there’s a certain degree of pride from the Alveo team that they’ve built the better video encoder.The VPU also marks the transition of the Alveo video encoder family to a fully ASIC-based product. Xilinx, of course, is best known for their programmable FPGAs, and while the previous Alveo U30’s processors used hard logic for their video encode blocks, that was combined with a FPGA fabric network. So that product was still a mix of ASIC and FPGA design. MA35D’s VPUs, on the other hand, are tried and true ASICs with no FPGA elements, allowing the company to fully exploit the power efficiency benefits of using fixed function logic for a dedicated product.And energy efficiency is the other major gain over the older U30 card – and what AMD considers a significant edge over their competition, as well. The formal TDP of the card is 50 Watts, but in practice AMD is finding that the typical power consumption of the card is closer to about 35 Watts, or a hair over 1W per stream for 1080p60. The U30, by comparison, had a formal TDP of 25 Watts, putting its worst-case power consumption at a bit over 3W per stream. AMD doesn't provide a similar \"typical\" power consumption figure for the U30, but at least under a maximum load, the UA35D should consume half as much energy per stream as its predecessor.Meanwhile, new to the Alveo MA35D and its VPU is an AI acceleration block. Unlike GPU-based products, this isn’t for quasi-related AI tasks like image recognition; rather AMD is using the AI accelerator to feed additional data into their video encoder to further improve their encoding quality. Rated for 22 TOPS of performance, the AI processor exists to evaluate streams on a frame-by-frame basis, and then use that analysis to adjust the encode parameters used by the rest of the chip.Using both region-of-interest encoding and artifact detection, the AI processor essentially allows the MA35D to get away with lower bitrates than a more naïve video encode strategy. Region-of-interest encoding allows for portions of a video to receive higher quality encoding (text, faces, etc), while artifact detection can catch when the encoder is being fed blocky or otherwise degraded images – which are actually harder to encode – and removing/correcting them before a frame is sent off for encoding.All told, AMD is making some fairly aggressive image quality claims with the Alveo MA35D; H.264 and H.265 image quality should be similar to x264 Medium and x265 Medium presets respectively, while the card’s AV1 encoding quality should be comparable to x265 slow. These comparisons are based on VMAF scores, and what settings it takes to achieve similar scores. Or to frame things in a bitrate basis, using AV1 AMD says the MA35D can deliver the same image quality as the Alveo U30 in H.264 mode at 55% of the bitrate (a 1.8x efficiency improvement).Finally, although secondary to the video encode capabilities of the MA35D, it’s interesting to note that the management processors in the VPU have shifted from Arm to RISC-V. Whereas the U30’s processors used quad core Cortex-A53 cores, the MA35D VPU uses a pair of quad core RISC-V cores – though AMD doesn’t specify whose. The RISC-V architecture has been quietly pushing out Arm for management controllers such as these, and this is another example of that transition in action.With two VPUs, the complete Alveo MA35D card is still small enough that it comes in a single slot half-height half-length form factor. Meanwhile a 50W TDP means that the card is entirely powered via the PCIe slot, attached via a PCIe x8 connector (which gets bifurcated down to x4 for each VPU). And, as is typical for data center accelerator cards, the MA35D is passively cooled.According to AMD, the Alveo is sampling to partners now. The company expects to begin production shipments in the third quarter of the year, with a suggested retail price of $1595.Gallery:Alveo MA35D Press Deck\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18805/amd-announces-alveo-ma35d-media-accelerator-av1-video-encode-at-1w-per-stream\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Samsung and AMD Renew GPU Architecture Licensing Agreement: More RDNA Exynos Chips to Come\n",
      "Author: Ryan Smith\n",
      "Date Published: 2023-04-06T04:45:00Z\n",
      "URL: https://www.anandtech.com/show/18804/samsung-and-amd-renew-gpu-architecture-licensing-agreement-more-rdna-exynos-chips-to-come\n",
      "Content: In a joint press release released this evening, AMD and Samsung have announced that the two companies are renewing their GPU architecture licensing agreement for Samsung’s Exynos SoCs. The latest multi-year deal between AMD and Samsung will see Samsung continuing to license AMD’s Radeon graphics architectures for use in the company’s Arm-based Exynos SoCs, with the two companies committing to work together over “multiple generations” of GPU IP.The extension of the licensing agreement comes just shy of 4 years afterSamsung and AMD announced their initial licensing agreement in June of 2019. The then-groundbreaking agreement would see Samsung license Radeon GPU IP for use in their flagship Exynos SoCs in an effort to get a jump on the mobile SoC market, tapping AMD’s superior Radeon graphics IP to get access to newer features and more efficient designs sooner than Samsung otherwise might have with their own internal efforts.That initial licensing agreement came to fruition with what’s (so far) a single product: the Exynos 2200, and its RDNA2-based Xclipse 920 integrated GPU. The switch to AMD’s GPU designs allowed Samsung to ship cutting-edge, PC-level features such as hardware ray tracing and variable rate shading (VRS) in its Galaxy S22 phones, several months before its competition.Unfortunately, the Exynos 2200 as a whole wasa poorly received SoC. There are numerous reasons for this (more than we can get in to in a simple news piece), but above all else, Samsung’s 5nm-class lithography processes have proven to be a mess, with Samsung experiencing yield issues, and working chips underperforming chips based on rival TSMC’s 5nm-class nodes. The fab issues alone were bad enough to cause Qualcomm to jump ship from Samsung to TSMC mid-generation with theSnapdragon 8+ Gen 1, and for their latest generation Galaxy S23 phones, Samsung didn’t even bother to ship an Exynos variant.But while Samsung is essentially taking a gap year on the high-end SoC front right now, they’ve previously made it clear that they would continue producing Exynos SoCs, and thus, they are still in the market for licensing GPU IP. Which brings us back to today’s extension of Samsung’s licensing agreement with AMD. For whatever problems Samsung had with the Exynos 2200, this extension indicates that Samsung is remaining committed to using AMD’s GPU architecture over the longer haul – so the Xclipse 920 will not be a one-off product.The extension announcement from the two firms does not come with much in the way of public details; AMD will be licensing Samsung “multiple generations” of Radeon GPU IP, and Samsung in turn will be using that IP in an “expanded portfolio of Samsung Exynos SoCs”. No specific architectures or timelines are mentioned, or even how many SoCs Samsung may be looking at. Even 1 new Exynos SoC using AMD’s GPU IP would be an expanded portfolio, so the announcement on the whole is very generic in that regard.Looking at AMD’s GPU architecture roadmap, the company has currently plotted out GPU architectures through the end of 2024, with theforthcoming RDNA 4 architecture. Given AMD’s typical two-year cadence, that is an architecture we’d expect in desktop products in late 2024. So if Samsung is looking at releasing a new AMD-powered SoC in the next 18 months, it’s far more likely we’d see something based on the current RDNA 3 architecture, used in AMD’s latest mobile SoCs and Navi 3x GPUs. Otherwise, anything farther out than 18 months would be a good candidate for RDNA 4, which for the moment, we know very little about.Meanwhile, as this latest deal is an extension of Samsung and AMD’s initial agreement from 2019, this strongly implies that the product restrictions from the initial agreement remain in place. In that agreement, Samsung was prohibited from using AMD’s GPU IP to compete with AMD, restricting Samsung’s use of the IP to SoCs for smartphones and tablets. Larger and more powerful devices, such as laptops, were off the table. The Windows on Arm market is still nascent at best, but if this restriction is still in place, that means we won’t be seeing Samsung participate using any of their AMD-derived designs.An equally important question for the partnership going forward is whether Samsung and AMD are going to maintain their collaborative design approach, or if Samsung is reverting to a more traditional licensing model. One of the most noteworthy aspects of the original agreement was that Samsung and AMD were working together to design the Xclipse GPU used in Samsung’s SoCs. 4 years and 1 product later, it’s hard to tell from the outside how well this has truly worked out for both parties. But in theory, all of the benefits from collaboration still stand – so it would not be surprising to find out that both parties are continuing with that approach.Finally, this announcement is the latest piece of evidence that AMD is happy to continue with their GPU IP licensing business. Between their semi-custom game console deals and the Samsung deal, AMD’s GPU architectures have ended up far more widely used than their PC market share numbers alone would indicate. And while designing and refining GPU IP for other chipmakers does come with some opportunity cost trade-offs, it also comes with a guaranteed revenue stream for the company that they are happy to take.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18804/samsung-and-amd-renew-gpu-architecture-licensing-agreement-more-rdna-exynos-chips-to-come\n",
      "Title: Intel Updates Data Center Roadmap: Xeons On Track - Emerald in Q4'23, Sierra Forest in H1'24\n",
      "Author: Ryan Smith\n",
      "Date Published: 2023-03-29T15:30:00Z\n",
      "URL: https://www.anandtech.com/show/18797/intel-updates-data-center-roadmap-xeons-on-track-emerald-in-q423-sierra-forest-in-h124\n",
      "Content: Coming to the end of the first quarter of 2023, Intel’s Data Center and AI group is finding itself at an interesting inflection point – for reasons both good and bad. After repeated delays, Intel is finally shipping their Sapphire Rapids CPUs in high volumes this quarter as part of the 4thGeneration Xeon Scalable lineup, all the while its successors are coming up very quickly. On the other hand, the GPU side of the business has hit a rough spot, with theunexpected cancelation of Rialto Bridge– what would have been Intel’s next Data Center GPU Max product. It hasn’t all been good news in the past few months for Intel’s beleaguered data center group, but it’s not all bad news, either.It’s been just over a year since Intel last delivered a wholesale update on its DCAI product roadmaps, which were last refreshed at their 2022 investors meeting. So, given the sheer importance of the high margin group, as well as everything that has been going on in the past year – and will be going on over the next year – Intel is holding an investor webinar today to update investors (and the public at large) at the state of its DCAI product lineups. The event is being treated as a chance to recap what Intel has accomplished over recent months, as well as to lay out an updated roadmap for the DCAI group covering the next couple of years.The high-level message Intel is looking to project is that the company is finally turning a corner in their critical data center business segment after some notable stumbles in 2021/2022. In the CPU space, despite the repeated Sapphire Rapids delays, Intel’s successive CPU projects remain on track, including their first all E-core Xeon Scalable processor. Meanwhile Intel’s FPGA and dedicated AI silicon (Gaudi) are similarly coming along, with new products hitting the market this year while others are taping-in.Sapphire Rapids: 4thGeneration Xeon Scalable Shipping in VolumeFollowing what can only be described as a prolonged development process for Intel’s next generation Xeon Scalable processors, Sapphire Rapids finally began shipping in volume over the past few months. The Q1’23 (ed: or is that Q5’22?) launch of the product has come later than Intel would have ever liked, but the company is finally able to put the development process behind them and enjoy the fruits of shipping the massive chips in high volumes.At this point Intel isn’t quoting precise shipment numbers – back at launch, the company said it expected to make it to a million units in record time – but the company is doubling-down on their claims that they’ll be able to product the large, complex chips in high enough volumes to meet customer demand. Built on the Intel 7 process, the final iteration of what started as Intel’s 10nm line, Intel is benefitting from that well-tuned process. At the same time, however, the 4thGeneration Xeon Scalable lineup includes Intel’s first chiplet-based Xeon design, so it is still not the easiest launch.Besides meeting customer demand, Intel’s main point is that all of their major customers are adopting the long-awaited chips. This is largely unsurprising given that Intel still holds the majority of the data center CPU market, but given the investor audience for today’s announcements, it’s also unsurprising to see Intel explicitly calling attention to this. Besides a generational improvement in CPU core architecture, Sapphire Rapids also delivers everything from DDR5 to PCIe 5/CXL support, so there is no shortage of interest in replacing older Ice lake and Cascade Lake (3rd& 2ndGen Xeon Scalable) hardware with something newer and more efficient.Intel, of course, is looking to fend off arch-rival AMD from taking even more market share in this space with their EPYC processors, which are now on to their 4thgeneration (9004 series) Genoa parts. There are a few demos slated to be run this morning showcasing performance comparisons; Intel is keen to show investors that they’re shipping the superior silicon, especially as AMD has the advantage in terms of core counts. So expect Intel to focus on things like their AI accelerator blocks, as well as comparisons that pitch an equal number of Sapphire Rapids (Golden Cove) and Genoa (Zen 4) CPU cores against each other.Emerald Rapids: On Track for Q4’23, Will Be 5thGeneration Xeon ScalableDiving into the future of Intel’s product roadmap, the first disclosure from today’s event is an update on the status of Emerald Rapids, the architectural successor to Sapphire Rapids. Intel’s previous roadmap had chips based on the architecture slated to arrive in 2023, a launch cycle that has been increasingly called into question given Sapphire Rapids’ delay to 2023. But sure enough, Intel still expects to deliver the next generation of Xeon processors later this year, in Q4.According to Intel, Emerald Rapids chips are already sampling to customers. At the same time, volume validation is already underway as well. As Emerald Rapids is a relatively straightforward successor to Sapphire Rapids, Intel is looking to avoid the long validation period that Sapphire Rapids required, which will be critical for making up for lost time and getting the next Xeon parts out by the end of this year.Given that this is an investor meeting, Intel isn’t offering much in the way of technical specifications for the next-generation chips. But the company is confirming that Emerald Rapids will operate in the same power envelope as Sapphire Rapids – improving on the platform’s overall performance-per-watt efficiency. In fact, the fact that Emerald Rapids will use the same LGA 4677 platform as Sapphire is being treated as a major selling point for Intel, who will be fully leveraging the drop-in compatibility that will afford. Customers will be able to swap out Sapphire for Emerald in their existing designs, allowing for easy upgrades of already-deployed systems, or in the case of OEMs, quickly bringing Emerald Rapids systems to the market.Intel has previously disclosed that Emerald Rapids will be built on the Intel 7 process. This means that the bulk of any performance/efficiency gains will have to come from architectural improvements. That said, Intel is also touting “increased core density”, so it sounds like Emerald will also offer higher core counts than Sapphire, which topped out at 60.As part of the webinar, Intel also showed off an uncapped Emerald Rapids chip. Based on the sheer amount of silicon on the package and the multi-tile configuration (each tile is easily over 700mm2), we believe this is likely the highest-end XCC configuration. Which at two tiles, is a significant design change from Sapphire Rapids, which used four smaller tiles for its XCC configuration. Which goes to show that even though Sapphire and Emerald are socket-compatible and using the same platform, Intel isn't restraining itself from making changes under the hood (or in this case, under the IHS).Finally, following in the footsteps of the product naming scheme they’ve used for the last several years now, Intel is officially naming Emerald Rapids as the 5thGeneration Xeon scalable family. So expect to see the official name used in place of the code name for the bulk of Intel’s announcements and disclosures going forward.Granite Rapids: Already Sampling, to Ship In 2024 With MCR DIMM SupportFollowing Emerald Rapids, in 2024 Intel will be shipping Granite Rapids. This will be Intel’s next-generation P-core based product. Like Emerald, Granite has been previously disclosed by Intel, so today’s announcement is an update on their progress there.According to Intel, Granite Rapids remains on track for its previously announced 2024 launch. The part is expected to launch “closely following” Sierra Forest, Intel’s first E-core Xeon Scalable processor, which is due in H1’24. Despite being at least a year out, Granite Rapids is already to the point where the first stepping is up and running, and it’s already sampling to some Intel customers.As noted in previous disclosures, Granite Rapids is a tile-based architecture, with separate compute and I/O tiles – an evolution from Sapphire Rapids, which even in its tiled form is essentially a complete SoC in each tile. Granite Rapids’ compute tiles are being built on the Intel 3 process, Intel’s second-generation EUV node, having been pulled in from Intel 4 in its earliest incarnation. Meanwhile we still don’t have significant official information on the I/O tiles.Along with upgrades to its CPU architecture, Intel is also disclosing for the first time that Granite Rapids will also come with a notable new memory feature: MCR DIMM support.First revealed by SK hynix late last year, Multiplexer Combined Ranks (MCR) DIMMs essentially gang up two sets/ranks of memory chips in order to double the effective bandwidth to and from the DIMM. With MCR, Intel and SK hynix are aiming to get data rates equivalent to DDR5-8800 (or higher) speeds, which would be a significant boon to memory bandwidth and throughput, as that's often in short supply with today's many-core chips.As part of today’s presentation, Intel is showing off an early Granite Rapids system using MCR DIMMs to achieve 1.5TB/second of memory bandwidth on a dual socket system. Based on Intel’s presentation, we believe this to be an812 channel memory configuration with each MCR DIMM running at the equivalent of DDR5-8800 speeds.As an aside, it’s worth noting that as the farthest-out P-core Xeon in Intel’s roadmap, there’s a notable lack of mention of High Bandwidth Memory (HBM) parts. HBM on Sapphire Rapids was used as the basis of Intel’s offerings for the HPC market, and while that wasn’t quite a one-off product, it’s close. Future HPC-focused CPUs were being developed as part of the Falcon Shores project, which was upended with the change to Intel’s GPU schedule. So at this time, there is not a new HBM-equipped Xeon on Intel’s schedule – or at least, not one they want to talk about today.Sierra Forest: The First E-Core Xeon and Intel 3 Lead Product, Shipping H1’24Shifting gears, we have Intel’s forthcoming lineup of E-core Xeons. These are chips that will be using density-optimized “efficiency” cores, which were introduced by Intel in late 2021 and have yet to make it to a server product.Sierra Forest is another previous Intel disclosure that the company is updating investors on, and is perhaps the most important of them. The use of E cores in a Xeon processor will significantly boost the number of CPU cores Intel can offer in a single CPU socket, which the company believes will be extremely important for the market going forward. Not only will the E core design improve overall compute efficiency per socket (for massively threaded workloads, at least), but it will afford cloud service providers the ability to consolidate even more virtual machine instances on to a single physical system.Like Granite Rapids, Sierra Forest is already up and running at Intel. The company completed the power-on process earlier in the quarter, getting a full operating system up and running within 18 hours. And even though it’s the first E-core Xeon, it’s already stable enough that Intel has it sampling to at least one customer.As previously disclosed, despite the E-Core/P-Core split, Sierra Forest and Granite Rapids will be sharing a platform. In fact, they’re sharing a whole lot more, as Sierra will also use the same I/O tiles as Granite. This allows Intel to develop a single set of I/O tiles and then essentially swap in E-core or P-core tiles as needed, making for Sierra Forest or Granite Rapids.And for the first time, we have confirmation of how many E-cores that Sierra will offer. The Xeon will ship with up to 144 E-cores, over twice as many cores as found on today’s P-core based Sapphire Rapids processors. There are no further architectural disclosures on the E-cores themselves – it was previously confirmed that it’s a post-Gracemont architecture – so more details are to come on that front. Gracemont placed its E-cores in quads, which if that holds for the CPU architecture used in Sierra Forest, would mean we’d be looking at 36 E-core clusters across the entire chip.With Sierra Forest up and running, this also means that Intel has wafers to show off. As part of her portion of the presentation, Lisa Spelman, Intel's CVP and GM of the Xeon product lineup, held up a finished Sierra Forest compute tile wafer to underscore Intel's progress in manufacturing their first E-core Xeon CPU.Speaking of manufacturing, Intel has also confirmed that Sierra Forest is now the lead product for the Intel 3 node across the entire company. Which means Intel is looking to make a massive leap in a very short period of time with respect to its Xeon product lineup, moving from Intel 7 on Emerald Rapids in Q4’23 to their second-generation EUV process no later than Q2’24. Sierra does get the benefit of products based on Intel 4 (the company’s first-generation EUV process) coming first, but this still makes Sierra’s progress very important, as Intel 3 is the first “full service” EUV process for Intel,offering support for Intel’s complete range of cell libraries.Of all of the Xeon processor architectures outlined today, Sierra is arguably the most important for Intel. Intel’s competitors in the Arm space have been offering high density core designs based on the Neoverse architecture family for a few years now, and arch-rival AMD is going the same direction this year with the planned launch of its Zen 4c architecture and associated EPYC “Bergamo” processors. Intel expects an important subset of their customers to focus on maximizing the number of CPU cores over growing their overall socket counts – thus making data center CPU revenue more closely track core counts than socket counts – so Intel needs to meet those demands while fending off any competitors wanting to do the same.Clearwater Forest: Second-Gen E-core Xeon In 2025 on Intel 18A ProcessFinally, in an all-new disclosure for Intel, we have our first details on the part that will succeed Sierra Forest as Intel’s second-generation E-core Xeon processor. Codenamed Clearwater Forest, the follow-up E-core part is scheduled to be delivered in 2025, placing it no more than 18 months after Sierra Forest.Similar to how Sierra is Intel’s first Intel 3 part, Clearwater Forest is slated to be the first Xeon produced on Intel’s 18A process – their second-generation RibbonFET process, which last yearwas moved up in Intel’s scheduleand will be going into production in the second half of 2024.At two years out, Intel isn’t disclosing anything else about the chip. But its announcement today is to confirm to investors that Intel is committed to the E-core lineup for the long-haul, as well as to underscore how, on the back of the 18A process, this is the point where Intel expects to re-attain process leadership. Meanwhile, Intel has also confirmed that there won’t be any Xeons made on their early 20A process, so Clearwater Forest will be Intel’s first RibbonFET-based Xeon, period.Finally, it’s worth noting that with the latest extension to Intel’s CPU roadmap, P-core and E-core Xeons are remaining distinct product lines. Intel has previously commented that their customers either want one core or the other on a CPU – but not both at the same time – and Clearwater Forest maintains this distinction.Xeon Scalable GenerationsDateAnandTechCodenameAbbr.MaxCoresNodeSocketQ3 20171stSkylakeSKL2814nmLGA 3647Q2 20192ndCascade LakeCXL2814nmLGA 3647Q2 20203rdCooper LakeCPL2814nmLGA 4189Q2 2021Ice LakeICL4010nmLGA 4189Q5 20224thSapphire RapidsSPR60 PIntel 7LGA 4677Q4 20235thEmerald RapidsEMR>60 PIntel 7LGA 4677H1'20246th?Sierra ForestSRF144 EIntel 3?2024Granite RapidsGNR? PIntel 320257th?Clearwater ForestCWF? EIntel 18A??Next-Gen P?? P?AI Accelerators & FPGAs: Capturing Market Share At All EndsWhile the bulk of today’s presentation from Intel is focused on their CPU roadmap, the company is also briefly touching on the roadmaps for their FPGA and dedicated AI accelerator products.First and foremost, Intel is expecting to qualify (PRQ) 15 new FPGAs across the Stratix, eASIC, and Agilex product lines this year. There are no further technical details on these, but the products, and their successors, are in the works.Meanwhile, for Intel’s dedicated AI acceleration ASICs, the company’s Habana Labs division has recently tapped-in their next-generation Gaudi3 deep learning accelerator. Gaudi3 is a process shrink of Gaudi2, which was first released back in the spring of 2022, moving from TSMC’s 7nm process to a 5nm process. Intel isn’t attaching a delivery date to the chip for its investor crowd, but more details will be coming later this year.All told, Intel is projecting the market for AI accelerators to be at least a $40 billion market opportunity by 2027. And the company intends to tackle the market from all sides. That means CPUs for AI workloads that are still best served by CPUs (general computer), GPUs and dedicated accelerators for tasks that are best served by highly parallel processors (accelerated computer), and then FPGAs bridging the middle as specialist hardware.It’s interesting to see that, despite the fact that GPUs and other highly parallel accelerators deliver the best performance on large AI models, Intel doesn’t see the total addressable market for AI silicon being dominated by GPUs. Rather they expect the 2027 market to be a 60/40 split in favor of CPUs, which given Intel’s much stronger position in CPUs than GPUs, would certainly be to their advantage. Certainly, CPUs aren’t going anywhere even for AI workloads (if nothing else, something needs to prepare the data for those GPUs), but it will be interesting to see if Intel’s TAM predictions hold true in 4 years, especially given the eye-watering prices that GPU vendors have been able to charge in recent years.Gallery:Intel DCAI Investor Webinar 2023 Slide Deck\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18797/intel-updates-data-center-roadmap-xeons-on-track-emerald-in-q423-sierra-forest-in-h124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The NVIDIA GTC Spring 2023 Keynote Live Blog (8:00am PT/15:00 UTC)\n",
      "Author: Ryan Smith\n",
      "Date Published: 2023-03-21T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/18782/the-nvidia-gtc-spring-2023-keynote-live-blog-800am-pt1500-utc\n",
      "Content: 10:57AM EDT- Welcome to our live blog coverage of NVIDIA’s Spring GTC 2023 keynote address10:58AM EDT- The traditional kick-off to the show – be it physical or virtual – NVIDIA’s annual spring keynote is showcase for NVIDIA’s vision for the next 12 to 24 months across all of their segments, from graphics to AI to automotive. Along with a slew of product announcements, the presentation, delivered by CEO Jensen Huang always contains a few surprises10:58AM EDT- Looking at NVIDIA's sizable product stack, NVIDIA is coming off of the launch of their new Hopper and Ada Lovelace GPU architectures for servers and clients respectively. But there are plenty of spots for individual products that remain to be filled. Meanwhile, NVIDIA expects to release their long-awaited Grace CPU this year, and while many of the technical details of that Armv9-based core have since been released, we should hopefully get some launch details for that. As well as its combined CPU+GPU counterpart, Grace Hopper, which places the Grace CPU and Hopper GPU on the same package.10:58AM EDT- Meanwhile, we're expecting NVIDIA to take a small victory lap at this year's GTC for having the uncanny timing in launching Hopper and its large language model-friendly Transformer Engines right as the market for GPT and other LLMs has exploded. Now it will be interesting to see how NVIDIA intends to further grow (and profit from) those businesses. The company has all but promissed investors that a cloud service play of some kind will be announced at this GTC.10:59AM EDT- At this point we're just waiting for the keynote stream to kick off, which should be promptly at 8am11:00AM EDT- NVIDIA's GTC conference is, bucking the trends, remaining a virtual conference this year11:01AM EDT- And here we go11:01AM EDT- Jensen is immediate diving into the subjects of accelerated computing and AI11:02AM EDT- New advances in NVIDIA's full stack of services11:02AM EDT- \"Welcome to GTC\"11:02AM EDT- Welcoming more than 250K people to this year's virtual conference11:02AM EDT- 4 years ago the last in-person conference had 8K attendees11:03AM EDT- \"650 amazing talks\"11:04AM EDT- And quickly covering a list of subjects covered in this year's talks11:05AM EDT- \"The purpose of GTC is to inspire the world on the art of what's possible with accelerated computing\"11:05AM EDT- Now rolling NVIDIA's latest \"I am AI\" video11:08AM EDT- And, of course, NVIDIA has used AI to put together parts of this video (as they have done for a couple of years now)11:08AM EDT- \"Accelerated computing is not easy\"11:09AM EDT- Accelerated applications can enjoy speed ups and scale ups across many systems11:09AM EDT- Giving a 1 million-fold increase in performance over the last decade in certain tasks11:10AM EDT- Using then-and-now comparison of AlexNet and GPT-3 as a comparison of the number of FP operations required to train the respective neural nets11:10AM EDT- And of course, GPT is all the rage now11:11AM EDT- \"Several thousand applications aare now NVIDIA accelerated\"11:11AM EDT- And NVIDIA has established a cycle of users, applications, and developers to make an active ecosystem11:12AM EDT- A big part of laying this groundwork has been NVIDIA providing so many libraries for different tasks11:12AM EDT- Jensen is going through some of those libraries now11:13AM EDT- NVIDIA is continuing their work with quantum computing and their cuQuantum library11:13AM EDT- Which is being used to help simulate quantum computers11:14AM EDT- Today NVIDIA is announcing a quantum control link which allows connecting NV GPUs to quantum computers for error correction (of the quantum computer)11:15AM EDT- Now on to Spark RAPIDS and vector databases11:15AM EDT- Introducing a new library: RAFT11:16AM EDT- For further accelerating vector databases11:17AM EDT- (As a general reminder, NVIDIA has more software engineers than it does hardware engineers. So new software is a huge part of their total body of work. It also means that software is a huge part of GTC presentations these days)11:18AM EDT- Jensen is highlighting NVIDIA's partnership with AT&T, which is using NV tech for everything from 5G planning to Riva for voice synthesis11:18AM EDT- Now talking about NVIDIA's inference platform, recapping TensorRT, Triton, and TMS11:19AM EDT- New features include multi-GPU, multi-node inference for GPT large language models11:20AM EDT- Now on to GPU video processing. CV-CUDA and VPF11:20AM EDT- Microsoft, Tencent, and others are using these libraries to process hundreds of thousands of videos per day11:21AM EDT- Video processing is a major processing consumer, as a result. Making it a good target for optimization and acceleration11:21AM EDT- Next up: Genomics11:21AM EDT- Including NVIDIA Parabricks11:21AM EDT- Announcing Parabricks 4.1 today11:22AM EDT- And Holoscan, NV's library for real time medical image processing11:22AM EDT- And NVIDIA is partnering with Medtronic to develop a common AI platform11:23AM EDT- Now on to chip manufacturing11:23AM EDT- Jensen is talking about the extremely small scale of silicon lithography today11:23AM EDT- Litho is an imaging problem at the edge of physics11:24AM EDT- Recapping how EUV litho works. And how much it costs - over $250M11:24AM EDT- As well as how interference patterns within the EUV light is used to create patterns smaller than the light11:24AM EDT- Computational lithography11:25AM EDT- Simulating Maxwell's equations to develop and refine litho masks11:25AM EDT- Computational lithography use is growing fast11:26AM EDT- New library: cuLitho, to accelerate computational lithography by over 40x11:26AM EDT- A single reticle currently takes 2 weeks to process. cuLitho can do it in an 8 hour shift11:26AM EDT- cuLitho can also reduce power consumption by reducing the number of systems required11:27AM EDT- TSMC will be qualifying cuLitho for production starting in June11:27AM EDT- Now on to cloud computing11:28AM EDT- Increasing computing needs are being capped by data center physical power limits, not to mention a desire to cut back on power consumption for environmental reasons11:28AM EDT- Looks like this will be about NV's Grace CPU11:28AM EDT- Grace excels where GPUs do not - single threaded serial processing11:28AM EDT- 72 Arm cores with a 3.2TB/sec fabric11:29AM EDT- Grace superchip is 2 Graces on a single board11:29AM EDT- Grace Superchip module11:29AM EDT- 5 x 8 inches11:30AM EDT- 2 Grace Superchip modules can fit in a single air-cooled 1U server rack11:30AM EDT- Claiming 2x the perf at iso-power11:31AM EDT- Grace is sampling now11:31AM EDT- And NVIDIA's partners are working to assemble systems11:31AM EDT- Now on to NVIDIA's networking hardware business11:31AM EDT- BlueField-3 is in production11:32AM EDT- That's NVIDIA's latest-generation DPU11:32AM EDT- Now on to NVIDIA DGX11:32AM EDT- Half of Fortune 100 companies have installed DGX11:33AM EDT- Recapping DGX topology and features11:33AM EDT- DGX H100 is now in full production (now that Intel is finally shipping Sapphire Rapids in volume)11:34AM EDT- And public cloud providers, including Microsoft's Azure, are quickly adopting DGX for their services11:34AM EDT- \"DGX supercomputers are modern AI factories\"11:34AM EDT- \"Generative AI has triggered a sense of urgency to develop AI strategies\"11:34AM EDT- Announcing NVIDIA DGX Cloud11:35AM EDT- So here's NVIDIA's big cloud services announcement11:35AM EDT- NVIDIA's ecosystem available via DGX systems hosting cloud instances at the public cloud providers11:35AM EDT- \"Cloud extension of our business model\"11:36AM EDT- Oracle Cloud Infrastructure will be the first DGX public cloud service11:36AM EDT- 50 early access customers across several industries11:36AM EDT- Now on to generative AI and its recent explosion11:37AM EDT- (GPT is going to sell a massive number of H100s at this rate...)11:38AM EDT- Jensen is recapping large language models and the many things that can be done with GPT and other LLMs, such as generating text and images11:38AM EDT- \"Generative AI is a new kind of computer, one we program in human language\"11:38AM EDT- \"Now, everyone is a programmer\"11:39AM EDT- Comparing generative AI to whole platforms such as the PC11:39AM EDT- Now quickly talking about the many services using generative AI in some form or another11:39AM EDT- Even accelerated drug design11:40AM EDT- \"The industry needs a foundry. A TSMC for large language models\"11:40AM EDT- Announcing NVIDIA AI Foundations11:40AM EDT- Language, visual, and biology model making services11:41AM EDT- Using NVIDIA NeMo, Picasso, and BioNeMo respectively11:41AM EDT- Customers can create their own models or start with one of NVIDIA's pre-trained models and customize from there11:42AM EDT- Now rolling a video about AI Foundations and how it works11:42AM EDT- (This is moving far too quickly to recap it all)11:43AM EDT- Reinforcement learning used in NeMo to further improve its performance and accuracy11:43AM EDT- \"A personalized, AI model that you control\"11:43AM EDT- That was a video on NeMo. Now we're on to a video about Picasso11:44AM EDT- Picasso is a service for generating images, video, and models11:45AM EDT- Getty Images will be using the Picasso service, trained on their library of legally licensed images11:45AM EDT- Shutterstock will be doing something similar11:46AM EDT- Announcing a significant expansion of the Adobe partnership to build a set of next-gen AI capabilities into Adobe's software11:46AM EDT- Adobe Generative Images11:47AM EDT- And Adobe's Content Authenticity Initiative11:47AM EDT- Now on to BioNeMo - generative AI for biology11:48AM EDT- BioNeMo provides models for drug discovery11:48AM EDT- Protien folding, molecule generation, etc11:49AM EDT- Accurately predict the structure of a protein in seconds11:50AM EDT- And that's NVIDIA AI Foundations11:50AM EDT- Now on to automotive11:51AM EDT- No, my bad. Now on to talking about data center construction11:51AM EDT- \"No one accelerator can optimally process\" the wide variety of models11:51AM EDT- New inference platform: 4 configurations, 1 architecture, 1 software stack11:51AM EDT- New product: L4 acceleraetor card. Replaces T411:52AM EDT- Want to use L4 to replace CPU servers for AI video processing11:52AM EDT- Google is offering L4 on Google Cloud11:53AM EDT- Google GCP is now a premiere NVIDIA AI cloud11:53AM EDT- More info on that to come later on11:54AM EDT- L40 accelerator card. This is more of a recap, as the product was released last year11:54AM EDT- The L series being NVIDIA's designation for server cards based on the Ada Lovelace architecture11:55AM EDT- L40 is more aimed at image processing/generation, and is the backbone of NVIDIA's Omniverse/OVX hardware11:55AM EDT- Large language models live up to the name. GPT can get extremely large11:55AM EDT- Announcing H100 NVL11:56AM EDT- Dual card/quad slot PCIe product. 2 GH100s with 94GB of memory each11:56AM EDT- And then Grace Hopper11:57AM EDT- Grace CPU + Hopper GPU on a single board, for tasks that need both types of processing11:57AM EDT- CPU/GPU interface 7x faster than PCIe11:58AM EDT- A bit aspirational at this second, as Grace Hopper is not yet shipping11:58AM EDT- And that's NVIDIA's AI hardware11:59AM EDT- Now on to Omniverse11:59AM EDT- Rolling a video of how Amazon's robotics arm is using Omniverse12:00PM EDT- Using Isaac Sim to develop the technology12:01PM EDT- And using simulations to train their models faster12:02PM EDT- Once again promoting Omniverse and its use of the USD file format12:02PM EDT- NV has made significant updates to Omniverse in every area12:02PM EDT- Now rolling a highlight video12:03PM EDT- DRIVE Sim, Replicator, PhysX Flow, Warp, multi-GPU/multi-node support, Isaac Sim, SimReady Assets, Replicator, Audio2Face, Neural Materials, and more12:04PM EDT- \"Nearly 300K creators and designers have downloaded Omniverse\"12:05PM EDT- Lising numerous new CAD/CAM applications that are now plugged in to Omniverse12:07PM EDT- And listing off the many companies using Omniverse in some fashion and how they're using it. A lot of manufacturers, to say the least12:07PM EDT- BMW is building a factory virtually first, 2 years before they build the actual thing12:09PM EDT- Demoing how a virtual planning session goes12:09PM EDT- Using Microsoft Teams and Omniverse12:10PM EDT- Announcing 3 systems to run Omniverse12:11PM EDT- New generation of workstations powered12:11PM EDT- New OVX 3.0 servers12:11PM EDT- And #3: NVIDIA Omniverse Cloud12:12PM EDT- Omniverse running on cloud services12:13PM EDT- A fuily-managed cloud service. Partnering with Microsoft, to be hosted on Azure12:14PM EDT- Connecting Omniverse Cloud to Microsoft's 365 services12:14PM EDT- Bringing Omniverse to millions of 365 and Azure users12:15PM EDT- Now recapping the keynote12:15PM EDT- New hardware, new libraries, and more12:16PM EDT- Extending business model with NVIDIA DGX Cloud12:16PM EDT- Best of NVIDIA at best of world's leading CSPs12:17PM EDT- And NVIDIA AI Foundations for model making services12:17PM EDT- Plus numerous Omniverse upgrades, and Omniverse cloud services12:18PM EDT- Jensen is now thanking NVIDIA's partners and employees12:18PM EDT- And that's a wrap! Please check out our individual articles on NVIDIA's new hardware announcements12:18PM EDT-https://www.anandtech.com/show/18780/nvidia-announces-h100-nvl-max-memory-server-card-for-large-language-models12:19PM EDT-https://www.anandtech.com/show/18781/nvidia-unveils-rtx-ada-lovelace-gpus-for-laptops-desktop-rtx-4000-sff\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18782/the-nvidia-gtc-spring-2023-keynote-live-blog-800am-pt1500-utc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qualcomm Announces Snapdragon 7+ Gen 2: Premium Segment SoC Gets a Cortex-X CPU Core\n",
      "Author: Ryan Smith\n",
      "Date Published: 2023-03-17T06:00:00Z\n",
      "URL: https://www.anandtech.com/show/18775/qualcomm-announces-snapdragon-7-gen-2-premium-segment-gets-a-cortexx-core\n",
      "Content: After a 2021/2022 product cycle that was a bit more interestingthan Qualcomm perhaps would have liked, 2023 has been a far more straightforward year for the prolific SoC and cellular modem vendor. After releasing the first of their Gen 2 family of parts earlier this year with the flagship-class Snapdragon 8 Gen 2, the company is preparing to iterate through the next step of its product stack with the Snapdragon 7+ Gen 2. Aimed at what’s become Qualcomm’s traditional $400 to $600 “premium” market segment, which focuses on flagship-level features with more modest performance and costs, for the Snapdragon 7+ Gen 2, Qualcomm is aiming to deliver a sizable performance boost to the platform.Positioned as the successor to last year’s Snapdragon 7 Gen 1, this year’s iteration of the Snapdragon 7 is, broadly speaking, more focused on improving performance than adding features. Whereas last year’s Gen 1 part added mmWave support and new CPU and GPU architectures – particularly Armv9 architecture CPU cores – this year there’s only a handful of new features. In place of that, however is what Qualcomm is touting as one of their biggest performance boosts ever for the Snapdragon 7 family. This is being enabled in large part by a much-welcomed pivot from Samsung’s beleaguered 4nm process to TSMC’s 4nm process, mirroring the switch Qualcomm made last year for the well-received mid-cycleSnapdragon 8+ Gen 1part.Also new this year, Qualcomm is dropping hints that this will not be the only Snapdragon 7 Gen 2 part we see this year, vis-a-vie the decision to launch their first Gen 2 part as the 7+ rather than the 7. In a nutshell, launching as a Snapdragon 7+ part leaves Qualcomm room to launch a vanilla Snapdragon 7 part later on. To be sure, Qualcomm isn’t explicitly announcing any such part now, but there’s little reason to launch a 7+ first unless they had plans for something below it; otherwise they could have launched it as 7 part ala the Snapdragon 7 Gen 1, which was always a one-chip stack.Qualcomm Snapdragon 7-Class SoCsSoCSnapdragon 7+ Gen 2(SM7475-AB)Snapdragon 7 Gen 1(SM7450-AB)CPU1xCortex-X2@ 2.91GHz3xCortex-A710@ 2.49GHz4xCortex-A510@ 1.8GHz1xCortex-A710@ 2.4GHz3xCortex-A710@ 2.36GHz4xCortex-A510@ 1.8GHzGPUAdrenoAdrenoDSP / NPUHexagonHexagonMemoryController2x 16-bit CH@ 3200MHz LPDDR5 / 25.6GB/s2x 16-bit CH@ 3200MHz LPDDR5 / 25.6GB/sISP/CameraTriple 18-bit Spectra ISP1x 200MP or 108MP with ZSLor64+36MP with ZSLor3x 32MP with ZSL4K HDR video & 64MP burst captureTriple 14-bit Spectra ISP1x 200MP or 84MP with ZSLor64+20MP with ZSLor3x 25MP with ZSL4K HDR video & 64MP burst captureEncode/Decode4K60 10-bit H.265Dolby Vision, HDR10+, HDR10, HLG1080p240 Slow Motion Recording4K30 10-bit H.265Dolby Vision, HDR10+, HDR10, HLG720p480 Slow Motion RecordingIntegrated ModemX62 Integrated(5G NR Sub-6 + mmWave)DL = 4400 Mbps5G/4G Dual Active SIM (DSDA)X62 Integrated(5G NR Sub-6 + mmWave)DL = 4400 MbpsMfc. ProcessTSMC 4nmSamsung 4nmIn terms of CPU organization, the Snapdragon 7+ Gen 2 retains the same 1+3+4 CPU core configuration that we’ve seen for the past few generations of the Snapdragon 7 family. The big news here is that the top-performing Prime core is getting a significant performance improvement, as Qualcomm makes the switch from using a slightly higher clocked mid-core to using a more performant CPU architecture altogether.So, for the first time ever for a Snapdragon 7 part, Qualcomm is tapping one of Arm’s Cortex-X cores for the Prime core. The Cortex-X2 used here is technically Arm’s previous-generation design, so it won’t be stepping on the toes of the Snapdragon 8 Gen 2 and its Cortex-X3 core. But compared to the A710 core used for the 7 Gen 1’s Prime core (and 7+ Gen 2’s mid-cores), the Cortex-X2 represents a significant improvement in both IPC and clockspeeds. As a result, the peak clockspeed for the Prime core has gone from 2.4GHz to 2.91GHz, which compounds further with the IPC gains of the more complex core.All told, Qualcomm is touting an “up to” 50% CPU performance improvement for 7+ Gen 2 over the 7 Gen 1; virtually all of this is coming from the new Prime core.The trade-off is that such a large performance boost is really only accessible for single-threaded workloads, since there’s only the one Cortex-X2 core. The three mid (performance) cores are once again Cortex-A710 based, and are clocked all of 2% higher than before. As such, the 7+ Gen 1 isn’t going to see huge gains on heavily multithreaded workloads. The improved power efficiency of TSMC’s 4nm process should pay some dividends there, but some of those gains have been invested into making that power-hungry Cortex-X2 viable from a battery life perspective.Meanwhile, the 7+ Gen 2 also incorporates a faster Adreno GPU. As has been the case with Qualcomm’s integrated GPUs for a couple of generations now, the company isn’t assigning a product number to it – let alone disclosing significant architectural details – so there’s a limited amount of detail we can share. Based on the feature summary, it doesn’t look like this is using the newer GPU architecture from the 8 Gen 2; so it would seem Qualcomm has incorporated a bigger version of their existing GPU and almost certainly given it a healthy clockspeed boost.Whatever is the case, the GPU performance expectations for the new SoC are significant: Qualcomm is boasting a massive 2x performance improvement over the 7 Gen 1 – a platform that only delivered 20% more than its own predecessor. Even though these aren’t flagship-class SoCs, Qualcomm still likes to position the Snapdragon 7 series as being a good match for gaming smartphones, especially in China, so it’s not too surprising to see Qualcomm investing so much into GPU performance.All told, Qualcomm is touting a 13% improvement in power efficiency over the 7 Gen 1, at least on an “extended daily use” basis. The switch to TSMC’s 4nm process should pay significant dividends, as evidenced by last year’s 8+ Gen 1 part, but at the same time it’s clear that Qualcomm has been investing a good portion of those gains into improving overall performance.Feeding the dragon is a 32-bit (dual 16-bit) LPDDR5 memory controller. Unlike the Snapdragon 8 Gen 2, the 7+ Gen 2 is not getting support for faster LPDDR5X memory, which means status quo reigns for the Snapdragon 7 family. In this case, this means support for memory speeds up to LPDDR5-6400, which works out to 25.6GB/second of memory bandwidth. Contrasted with the significant CPU and GPU performance increases, there’s going to be a lot more pressure placed on Qualcomm’s cache and memory subsystem to keep the various processing blocks fed.Speaking of which, it’s not just the CPU and GPU blocks that have seen major performance increases. Qualcomm’s Hexagon DSP/AI engine block has also received a significant performance tune-up, rivaling the 2x increase to the GPU. Qualcomm was light on the technical details here, however in our briefing there was no mention made of features such as INT4 or micro-tiling – two major features of the next-gen Hexagon block on the 8 Gen 2 – so it seems likely that this is a heavily beefed up version of the Hexagon block used on the previous 7 Gen 1.One piece of Snapdragon 8 technology that is making its way down to the Snapdragon 7, however, is its triple 18-bit Spectra ISP. Replacing the 14-bit unit featured in earlier generations of the platform, the 18-bit unit on the 7+ Gen 2 will bring support for triple exposure computational HDR video capture, as well as improved low light photography, which Qualcomm calls their Mega Low Light feature. The end result is that the 7+ Gen 2 can capture at higher resolutions when using zero shutter lag functionality, and combined with the updated GPU, can now record 4K video at up to 60fps, doubling the 7 Gen 1’s 4K30 limit.Finally, rounding out the package is a reprise of Qualcomm’s Snapdragon X62 integrated modem. Like last year’s SoC, this is a mmWave + Sub-6 Release 16 design that can achieve a theoretical maximum download rate of 4.4Gbps. This year’s design does come with a twist, however: dual SIM dual active (DSDA) support, which is another first for the Snapdragon 7 platform. Both active radios on 7+ Gen 2 support 5G and 4G communications, allowing dual SIM users to use essentially whatever network they’d like on either radio. This is another premium feature, that until now, had been limited to Qualcomm’s Snapdragon 8 platform.As for non-cellular connectivity, the 7+ Gen 2 uses a FastConnect 6900 radio system. This is a relatively modest update over the earlier 6700 radio, bumping up Bluetooth support to version 5.3 of the protocol, and increasing the peak bandwidth of the 2x2 stream Wi-Fi 6E radio to 3.6Gbps thanks to dual-band simultaneous (DBS) support.Wrapping things up, the Snapdragon 7+ Gen 2 will be coming to market very quickly. According to Qualcomm, handsets using the SoC will be available later this month, with Redmi and Realme among the OEMs slated to release phones based around the new chip.Gallery:Snapdragon 7+ Gen 2 Press Deck\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18775/qualcomm-announces-snapdragon-7-gen-2-premium-segment-gets-a-cortexx-core\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Intel Scraps Rialto Bridge GPU, Next Server GPU Will Be Falcon Shores In 2025\n",
      "Author: Ryan Smith\n",
      "Date Published: 2023-03-04T21:30:00Z\n",
      "URL: https://www.anandtech.com/show/18756/intel-scraps-rialto-bridge-gpu-next-server-gpu-will-be-falcon-shores-in-2025\n",
      "Content: On Friday afternoon,Intel published a letter by Jeff McVeigh, the company’s interim GM of their Accelerated Computing Systems and Graphics group (AXG). In it, McVeigh offered a brief update on the state of Intel’s server GPU product lineups and customer adoption. But, more importantly, his letter offered an update to Intel’s server GPU roadmap – and it’s a bit of a bombshell. In short, Intel is canceling multiple server GPU products that were planned to be released over the next year and a half – including their HPC-class Rialto Bridge GPU – and going all-in on Falcon Shores, whose trajectory has also been altered, delaying it until 2025. There’s a lot to digest here, so let’s dive right in.Intel’s most recent public server GPU product roadmap, released back during SC22 in May of 2022, outlined that Intel intended to release server GPU products at a fairly rapid clip. This was (in part) to make up for lost time from the multiple delays involving Ponte Vecchio, Intel’s first HPC-class GPU that is now sold under the Data Center GPU Max family. At that event, Intel announced Ponte Vecchio’s successor, Rialto Bridge, which was to be an evolution of Ponte and was set to sample in mid-2023. Following up on Rialto would be Falcon Shores in 2024, which along with employing an even newer version of Intel’s Xe GPU architecture, would be Intel’s first combined CPU+GPU product (XPU) and was slated to be the ultimate evolution of both Intel’s HPC CPU and GPU lines.But, citing “a goal of maximizing return on investment for customers”, Intel has significantly refactored their server GPU roadmap, cancelling their previously planned 2023/2024 products. Rialto Bridge, announced less than a year ago, has been scrapped. Instead, Intel will be moving straight on to Falcon Shores for their HPC GPU needs.These cancellations also impact Intel’s Flex line of server GPUs for cloud gaming and media encoding, with Intel axing Lancaster Sound (aka “Next Sound”) in favor of moving on to its successor, Melville Sound. Melville Sound’s development will be accelerated, in turn, though Intel hasn’t attached a hard date to it. Previously, it was expected at around the same time as Falcon Shores.Intel did not provide any new visuals for the new roadmap, but I’ve gone ahead and kitbashed something together based on their SC22 slide in order to show where things stand, and what products have been cancelledAccording to Intel, these product changes are designed to allow the company to align itself to a two-year product release cadence for server GPUs. Rivals NVIDIA and AMD have been operating at a similar cadence for the past several years, so this would bring Intel’s product cycles roughly in line with the competition. Which, as Intel puts it “matches customer expectations on new product introductions and allows time to develop their ecosystems.”Reading between the lines, the implication is that Intel wasn’t confident on their ability to sell Rialto and Lancaster to their core customer base. Whether that’ssolelybecause of the rapid product release cycles, or if there was something more to it, is a detail that will remain inside the halls of Intel for now. But regardless, the company is essentially scrubbing their plans to release new server GPU products this year in order to focus on their successors later on.And in case there’s any doubt about whether this is a good or bad development for Intel, I’ll note that this information was released after 5pm Eastern on a Friday. So Intel is very clearly trying to bury bad news here by releasing it at the end of the week.Falcon Shores Goes from an XPU in 2024 to a GPU in 2025Besides cancelling Rialto Bridge and Lancaster Sound, the other major update to come from Intel’s letter is the revelation that Intel has significantly refactored their plans for Falcon Shore. What was (and may yet still be) Intel’s first combined CPU+GPU product for HPC has now been retasked to serve as Intel’s next-generation HPC GPU. Which in turn has significant repercussions for Intel’s product lineup, and their competitive positioning.Falcon Shores was another 2022 Intel announcement, which was revealed back at the company’s 2022 Investor Meeting in February. At a high level, Falcon Shores was designed to be Intel’s first XPU product – a chip using a variety of compute architectures in order to best meet the execution needs of a single workload. This would be accomplished by using separate processor tiles, and while in practice this primarily meant putting Xe GPU tiles and x86 CPU tiles together on a single chip, the tiled approach left the door open to adding other types of accelerators as well. Intel is already doing tiled server chips with products like Ponte Vecchio and the recently launched Sapphire Rapids XCC, so Falcon Shores was slated to be the next step by placing more than just one type of accelerator on a chip.At the time, Falcon Shores was slated for release in 2024, to be built on an “angstrom era process”. As planned, Falcon Shores would have been Intel’s power play for the HPC industry, allowing them to deliver a combined GPU+CPU product based on their latest architectures and built on their cutting-edge 20A/18A process node, givng them an edge in the HPC market and putting them ahead of traditional GPU-based accelerators. And while Falcon Shores may yet live up to those goals, it won’t be doing so in 2024. Or even 2025, for that matter.Instead, Falcon Shores has been retasked to serve as an HPC GPU part. Intel’s brief letter doesn’t go into the technical changes, but the implications are that rather than mixing CPU and GPU tiles on a single chip, Intel is going to devote itself to building a product out of just GPU tiles, reducing what was to be intel’s XPU into a more straightforward GPU.With the cancellation of Rialto Bridge, it goes without saying that Intel needs a new HPC-class GPU product if they want to remain in the market. And while there’s currently no reason to think that Falcon Shores won’t be a good product, the fact that Intel has delayed it until 2025 is not a promising sign. Ponte Vecchio is already an older design than its age lets on – it was initially intended to launch in 2021 and compete with the HPC GPUs of that era – so by not having anything newer to offer until 2025, Intel is essentiallycedingthe HPC GPU market for the next two years, outside of their supercomputer wins. Intel’s long-term plans still call for the company to take a sizable chunk of the highly profitable server GPU market – to swipe a piece of what’s largely been NVIDIA’s pie – so these developments essentially put those plans on hold for another two years.And what may prove worse, this puts Intel even farther behind its competitors in shipping an XPU/APU-like product. As confirmed byServe The Home, while Intel hasn’t given up on shipping a Falcon Shores XPU, the prioritization of GPUs means that such a product wouldn’t launch in 2025. That means an Intel server-class XPU is now a 2026 product at best – and longer still at worst.Meanwhile AMD will be shipping a similar server APU later this year with theInstinct MI300. That part will employ multiple chiplets/tiles on a single chip in order to offer HPC-class CPU and GPU performance on a single chip, accomplishing a long-awaited goal for AMD. And while NVIDIA is a bit farther behind in integration with theirGrace Hopper superchip– it’s essentially two tightly-coupled chips on a single board, rather than being chiplets on a single chip – it’s still ahead of where Intel is today. And, worryingly for Intel, the next-gen successor to that part is almost certain to launch before any potential Falcon Shores XPU hits the market.In other words, by pushing back their server GPU schedule, all of Intel’s products that relied on it are now similarly delayed. That leaves Intel’s server CPUs to hold the line on their own for the next few years.A History of Server GPU Cancellations as Intel Keeps TryingUltimately, the cancellation of Rialto Bridge makes it the latest in what has become a surprisingly long line of cancelled GPUs and GPU-like accelerators over at Intel. The company has, in one form or another, been attempting to break into the accelerator market since the late ‘00s, with projects such asLarrabee. And now, 15 years later, Intel still isn’t as far along as it wants to be, and is still cancelling chips to get there.As the latest in Intel server GPU casualties, Rialto Bridge is joining not only Larrabee, but Intel’s ill-fated Xeon Phi lineup as well. And even then, Rialto Bridge can’t claim to be the first Xe architecture part to get canned – that honor goes to Xe-HP, which wascancelled in favor of using Intel’s mixed compute and graphics Xe-HPG silicon.The silver lining to this situation, at least, is that Intel isn’t cancelling a whole architecture here. The Xe architecture has firmly established itself as Intel’s in-house GPU architecture, and the fact that it’s shipping in everything from SoCs to video cards to HPC accelerators underscores its viability and importance at Intel. The cancellation of Rialto Bridge coupled with the delay of Falcon Shores is still a significant setback for Intel, but at the end of the day it’s just shelving one iteration of the broader Xe architecture for another.In the meantime, Intel will keep working to expand their GPU offerings, and to break into the HPC GPU market in a bigger way. It’s a market that is simply too big and too profitable for Intel to ignore – least it eats even more into their server CPU sales – so they will have to keep at it until they find the success they seek.Source:Intel\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18756/intel-scraps-rialto-bridge-gpu-next-server-gpu-will-be-falcon-shores-in-2025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The AMD Ryzen 9 7950X3D Review: AMD's Fastest Gaming Processor\n",
      "Author: Gavin Bonshor\n",
      "Date Published: 2023-02-27T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/18747/the-amd-ryzen-9-7950x3d-review-amd-s-fastest-gaming-processor\n",
      "Content: Last year,AMD released its Ryzen 7 5800X3D to the market with 96 MB of L3 V-Cache. The consumer implementation of AMD's then-new 3D stacked V-Cache technology, which allowed for greatly expanding the total L3 cache available on a CPU, the 5800X3D was primarily aimed at the gaming market, where the additional 64MB of L3 cache could be uniquely useful to improving performance in CPU-bound gaming workloads. While hit-and-miss depending on the specific game at hand, in the right games and the right scenarios, the additional cache could provide a performance boost that even the highest-clocked CPUs couldn't match.AMD's initial implementations of V-Cache in the 5800X3D and its server counterpart,Milan-X, were just the tip of the iceberg for the company. Though the benefits of this stacked cache manufacturing strategy were limited with the initial generation of products, AMD's eye has been on the long game: V-Cache is forward-looking design that will pay off with bigger dividends for AMD over time, as they'll eventually be able to produce the V-Cache dies using older and cheaper manufacturing processes, optimizing their manufacturing costs while still being able to offer what is by CPU standards an absurd amount of L3 cache. Ultimately, this multi-generational strategy for V-Cache was laid out in greater detail inAMD's 2023-2024desktop CPU roadmap, which outlined their intention to bring it to AMD's recently launched Ryzen 7000 series chips.That brings us to today, and the impending launch of AMD's second generation of V-Cache equipped consumer chips, the Ryzen 7000X3D family. Tomorrow morning AMD will be releasing a pair of their latest-generation Ryzen 7000 chips with the extra cache stacked on, including the Ryzen 9 7950X3D (16C/32T) and the Ryzen 7 7900X3D (12C/24T). Both chips build upon their X-series predecessors by adding a further 64MB of L3 cache, bringing them to an impressive total of 128 MB of L3 cache.Meanwhile, a third SKU, the AMD Ryzen 7 7800X3D, is in the works for April 6th. That part will offer 8 CPU cores and 96 MB of L3 cache, making it the most direct successor to theRyzen 7 5800X3D.Ultimately, all three chips will serve to update AMD's product stack by combining the strengths of the Zen 4 CPU architecture with the performance benefits of the extra L3 cache, which during the overlapping period of the last several months, has been split between the Ryzen 5000 and Ryzen 7000 families. In short, PC gamers will finally be able to have their cake and eat it too, gaining access to AMD's Zen 4 microarchitecture and its myriad of benefits (higher IPC, higher clockspeeds, DDR5, PCIe 5) with a nice helping of additional L3 cache slathered on top.From that stack, today we're reviewing the new flagship Ryzen 9 7950X3D. The 7950X3D offers 16 Zen 4 cores spread over two CCDs (8C/16T per CCD). AMD had to elect one of the CCDs to stack the additional L3 cache onto, resulting in a new-to-AMD heterogeneous CPU design, but they do have some special sauce as a garnish to make it work. We aim to determine if the Ryzen 9 7950X3D is the chip gamers have been yearning for and how it stacks up against other Ryzen 7000 chips (and Intel's 13th Gen) in our test suite.The Ryzen 9 7950X3D: 128 MB of L3 Cache, 16-Cores, Designed For GamersUp until now, we've had a wave of Ryzen 7000 series SKUs launch, including the flagship Ryzen 9 7950X and other X-series SKUs such as the Ryzen 9 7900X, Ryzen 7 7700X, and Ryzen 5 7600X, as well as their 65 W SKUs which have filled out the product stack nicely. Offering different configurations of core counts, frequencies, and ultimately price, the Ryzen 7000 stack is loaded with chips for users of all levels.AMD's 3D V-Cache packaging slide (Zen 3): AMD uses the same method for Zen 4These newest chips from AMD, in turn, expand the Ryzen 7000 stack further by adding a trio of parts incorporating AMD's 3D V-Cache technology. The basic idea behind 3D V-Cache is to stack additional cache memory on top of the existing processor die. The cache memory is then connected to the die using through-silicon vias (TSVs), which are vertical interconnects that pass through the silicon substrate. This allows for a much larger amount of L3 cache memory to be added to the processor without requiring that AMD mint a separate, larger CCD.The 3D V-Cache packaging process worked well with theRyzen 7 5800X3D, and we did see gains in frame rates across some of the games we tested; some games are memory/cache-bound, and thus can make good use of the additional L3 cache for a performance boost, while other games are unfazed by the additional cache as they are more outright compute-bound. Ultimately, the benefits of the additional cache do not make for a one-size-fits-all situation, as it varies from title to title, sometimes significantly so. As things stand, there unfortunately isn't anything quite like a whitelist of games that can benefit from the additional cache, which means the best guidance we can offer is that gamers will want to take the time to do a little extra research to see if the specific games they care about are any faster on these X3D chips.AMD Ryzen 7000 Series Line-Up (as of 02/27)AnandTechCoresThreadsBaseFreqTurboFreqMemorySupportL3CacheTDPPPTPrice(Street)Ryzen 9 7950X3D16C / 32T4.2 GHz5.7 GHzDDR5-5200128 MB120W162W$699Ryzen 9 7950X16C / 32T4.5 GHz5.7 GHzDDR5-520064 MB170W230W$589Ryzen 9 7900X3D12C / 24T4.4 GHz5.6 GHzDDR5-5200128 MB120W162W$599Ryzen 9 7900X12C / 24T4.7 GHz5.6 GHzDDR5-520064 MB170W230W$448Ryzen 9 790012C / 24T3.6 GHz5.4 GHzDDR5-520064 MB65W88W$460Ryzen 7 7800X3D(04/06/2023)8C / 16T4.2 GHz5.0 GHzDDR5-520096 MB120W162W$449Ryzen 7 7700X8C / 16T4.5 GHz5.4 GHzDDR5-520032 MB105W142W$341Ryzen 7 77008C / 16T3.6 GHz5.3 GHzDDR5-520032 MB65W88W$329Ryzen 5 7600X6C / 12T4.7 GHz5.3 GHzDDR5-520032 MB105W142W$241Ryzen 5 76006C / 12T3.8 GHz5.1 GHzDDR5-520032 MB65W88W$230Looking at the main specifications of the Ryzen 9 7950X3D, it has 16 Zen 4 CPUs cores and operates at a TDP rating of 120 W; this is some 50 W lower than the regular Ryzen 9 7950X. According to AMD, the more relevant Package Power Tracking (PPT) rating is 1.35x the TDP, which puts the socket power output at up to 162 W. The Ryzen 9 7950X3D also shares the same turbo frequency as the Ryzen 9 7950X, which is 5.7 GHz, but AMD has lowered the base frequency by 300 MHz, making it 4.2 GHz on the 7950X3D. Memory support remains unchanged from the rest of the Ryzen 7000 family, with JEDEC-compliant speeds topping out at DDR5-5200, while overclocking (AMD EXPO, etc) can take it over DDR5-6000.Although we're only reviewing the Ryzen 9 7950X3D today, the two other Ryzen 7000 X3D SKUs are worth mentioning. The other SKU launching today is the Ryzen 9 7900X3D, a 12-core/24-thread part with the same 128 MB of 3D stacked L3 V-Cache. It also features a 120 W TDP/162 W PPT, and has the same 5.6 GHz turbo clock speed as the Ryzen 9 7900X. Like the differences between the Ryzen 9 7950X3D and 7950X, the base core frequency is lower on the Ryzen 9 7900X3D than its counterpart, with a base frequency of 4.4 GHz, a 200 MHz drop over the Ryzen 9 7900X.The third SKU with 3D V-Cache is the Ryzen 7 7800X3D, which isn't available until the 6th of April, but it's still a very interesting part. Not only is it the only chip to use the 7800 model number, but it's very similar in specifications to the original Ryzen 7 5800X3D, a very successful SKU that is currently (still) the number 5 best-selling CPU on Amazon at the time of writing. Like the Ryzen 7 5800X3D, the soon-to-be-released Ryzen 7 7800X3D shares a turbo frequency of 5 GHz. The Ryzen 7 7800X3D has a 4.2 GHz base frequency, but it benefits from AMD's latest Zen 4 architecture, built upon TSMC's N5 5 nm manufacturing process. As we saw in our Ryzen 9 7950X and Ryzen 5 7600X review, AMD has boosted performance per watt efficiency and improved IPC performance nicely.A Tale of Two CCDs: One with 3D V-Cache, One WithoutThe Ryzen 7000 and 5000 families are built upon a chiplet-based design, with top-end chips such as the Ryzen 9 7950X featuring two core complex dies (CCD) as well as a singular I/O die (IOD) Chips in the family with lower core counts, such as the Ryzen 7 and 5 families, typically bring this down to one active CCD (and potentially one fused-off CCD if the chip wassalvaged).With their first-generation Ryzen 7 5800X3D, AMD avoided any complexities involving mixing V-Cache with their multi-CCD designs by only ever producing X3D parts with a single CCD. However for this generation, AMD is taking the next step by making multi-CCD X3D parts available – and they aren't putting V-Cache on both CCDs.This quandary over using two CCDs with V-Cache has raised many questions with users since AMD initially revealed their new Ryzen 7000X3D parts. The long and short of matters is that for this generation, AMD has taken steps to have the best of both worlds – to have a V-Cache-equipped CCD while also having another, unencumbered CCD in order to offer the highest compute performance.Touching on the design of the Ryzen 9 7950X3D, AMD is employing an asymmetric chiplet design, where one of the CCDs is a vanilla Zen 4 CCD, and the other is a Zen 4 CCD with 64 MB of V-Cache stacked on. This means that the Ryzen 9 7950X3D has one CCD with 96 MB of L3 cache, while the second \"vanilla\" CCD still has the usual 32 MB available. AMD's Infinity Fabric Interconnect keeps everything flowing, which connects both CCDs to the centralized IOD in a die-to-die connect flow.Notably, just as the two CCDs are unequal in cache, they're also unequal in peak clockspeeds. Attaching a V-Cache die to a CCD limits the highest clockspeeds it can attain, capping it at around 5.25GHz. Thus, compared to the 5.7GHz max clockspeed of a plain Zen 4 CCD (as used on the 7950X/7950X3D), this represents a roughly 8% frequency handicap for adding V-Cache. Which is why AMD is including both types of CCDs on their Ryzen 9 7000X3D parts, ensuring that applications have access to whatever type of CCD better fits their processing needs.Ultimately, AMD states that having one CCD with V-Cache and one standard CCD provides the best balance for maximizing performance in both gaming workloads and general heavy workload compute tasks. Applications and games that can benefit from the larger pool of L3 cache can be loaded up on to the V-Cache CCD, while everything else is loaded up on the standard CCD (and growing out to the second CCD as needed).Drivers & OS Scheduling: PPM Provisioning & V-Cache Performance OptimizerThis additional layer of complexity means that, for the first time on a consumer AMD platform, AMD's CPUs are now heterogeneous. Depending on which CCD they're on, the CPU cores within a 7950X3D are dissimilar, and that's something that needs to be taken into account when scheduling which CPU core/CCD a thread will go on. To that end, AMD has developed a pair of Windows drivers for the 7000X3D series to help the Windows scheduler more intelligently place threads on the best CCD for the task.Under the hood, AMD is essentially hooking into Windows' Game Mode (and Mixed Reality Mode) to let the OS determine when a game is active, and then use that hint to change how Windows thread scheduling works. The resulting layers of BIOS features, Xbox Game Bar software (which controls Game Mode), and AMD's drivers add up to a complete control mechanism for allocating work on the 7950X3D and 7900X3D.Diving into AMD's software stack, the first of AMD's drivers is the AMD PPM Provisioning Driver. This comes included within the 5.01.03.005 version (or newer) of AMD's chipset driver. This driver steers threads by parking what AMD determines is the least performant CCD, leaving only the more performant CCD initially active. This keeps all of the threads for a game on the same CCD, reducing (if not eliminating) the need to reach across the IOD to access the L3 cache on the other CCD, and thus improving the cache hit rate and resulting performance.Technically, the PPM driver can park either CCD. But in practice it's going to virtually always be the vanilla CCD, pushing games on to the V-Cache CCD. Though should a game ask for more CPU cores (and technically, threads) than a single CCD can provide, then the PPM will allow the other CCD to go active. Parking the CCD doesn't prevent its use – so all 16 CPU cores are available – it just discourages using more than 1 CCD (8 cores) when at all possible.Meanwhile, users can manually override this feature in their motherboard's firmware under the AMD CBS section of the Ryzen overclocking settings. While each motherboard vendor's firmware is different in design, features, and styling, changing the setting to auto can be switched to either 'cache' for workloads or games benefiting from the 3D V-Cache, or set it to 'frequency' in which it will allow the faster CCD to do the heavy lifting.Moving on, the second of AMD's new X3D drivers is the AMD 3D V-Cache Performance Optimizer Driver. Again, this comes bundled into the chipset driver, and it operates as the counterpart to the PPM driver, changing how Windows ranks the performance of the various CPU cores.Typically, Windows will rank CPU cores by clockspeeds, which is usually the correct way to go for physically homogenous CPU cores. Especially in the case of things like favored/preferred cores and \"prime\" CPU cores (e.g. Windows on Arm), putting heavy workloads on the fastest CPU core will net you the best performance. However having heterogeneous cores with different amounts of L3 cache changes this calculus – it's possible (and even likely) for a slower CPU core to be faster thanks to its access to a larger local L3 cache pool.The AMD 3D V-Cache Performance Optimizer Driver working in the backgroundThe net result, then, is that the 3D V-Cache Performance Optimizer Driver will change Windows' performance rankings when Game Mode is active so that Windows will try to use the CPU cores on the V-Cache CCD first. Coupled with the PPM driver, these two techniques are how AMD will try to ensure that games and their threads are sent to the V-Cache CCD first, while everything else continues to favor the traditional, higher-clocked CCD.Ryzen 9 7950X3D automatically detecting Total War: Warhammer 3 as a game in Xbox Game BarFinally it's worth highlighting that Game Mode within Windows 11/10 must be enabled for the AMD PPM and Performance Optimizer drivers to work seamlessly. AMD states that the Ryzen 7000X3D chips and their associated software stack will work under both Windows 10 (1903 or later) and Windows 11 (21H2 or later). But AMD pretty strongly recommends users go with Windows 11, as if nothing else, there are some potential cosmetic reporting bugs that remain on Windows 10 when Virtualization Based Security (VBS) is enabled. Meanwhile VBS is an outright requirement on Windows 11.Ryzen 9 7950X3D: No Unlocked Multiplier, But Supports Curve Optimizer, Precision Boost Overdrive, and AMD EXPO MemoryAMD, with its Ryzen 7000 65 W processors, showed impressive power-to-performance efficiency compared with its existing Ryzen 7000 chips. AMD is looking to show off its capabilities further while drawing less power. Of course,Unlike AMD's regular X-series SKUs, The Ryzen 9 7950X3D doesnothave an unlocked multiplier; meaning manually overclocking the cores is disabled. But the chip does still benefit from some of AMD's overclocking technology. While the Ryzen 7 5800X3D was limited to memory overclocking, the Ryzen 9 7950X3D has AMD's Curve Optimizer and Precision Boost Overdrive enabled. The Curve Optimizer allows users an easy one-click option to at least, in theory, deliver a boost to core performance. This can be enabled in AMD's Ryzen Master overclocking utility, while AMD's Precision Boost Overdrive can also be enabled similarly.Memory overclocking is also still available on these X3D SKUs. This goes for both manual overclocking, and using EXPO memory profiles on memory kits that come with those overclocking profiles. There are numerous EXPO-enabled kits on the market right now, especially from Corsair and G.Skill. And even XMP kits work to an extent, though their timings may not be ideal for AMD's Ryzen 7000 series processors.For a more detailed analysis of our AMD Ryzen 7000 (Zen 4) coverage, here are a few links with a more in-depth look at the Zen 4 core architecture, as well as AMD's AM5 platform:AMD Zen 4 Ryzen 9 7950X and Ryzen 5 7600X Review: Retaking The High-EndAMD's Desktop CPU Roadmap: 2024 Brings Zen 5-based \"Granite Ridge\"AM5 Chipsets: X670 and B650, Built by ASMediaRyzen 7000 I/O Die: TSMC & Integrated Graphics at LastFrom a marketing point of view, the AMD Ryzen 9 7950X3D is primarily aimed at gamers, particularly those who play games that can make good use of the additional L3 cache. When we reviewed the Ryzen 7 5800X3D, we found it lacking a little in compute tasks when directly compared to the regular Ryzen 7 5800X due to lower core frequencies and a lock on turbo frequencies. The flip side is that we saw some of the benefits of AMD's 3D V-Cache stacking technology in gaming, making it a purchase better suited for gamers in particular, rather than the broader performance enthusiast market.AMD is looking to change that perception somewhat with its Ryzen 7000X3D SKUs, thanks to higher overall clockspeeds and the use of heterogeneous (asymmetrical) CPU cores. Thus we expect less of a hit on compute performance with these due to still combining decent levels of frequencies, and specifications, albeit, at a lower PPT power rating of 162 W.From a pricing point of view, AMD has priced the Ryzen 9 7950X3D at what was the original MSRP of the Ryzen 9 7950X:$699. This makes the flagship X3D chip around $110 more expensive than the 7950X, which these days can be snagged for$589. For the other chips, the Ryzen 9 7900X3D ($599) is $150 more expensive than the Ryzen 9 7900X (currently$449), while the Ryzen 7 7800X3D will cost $449 when it releases in April.The Ryzen 9 7950X3D in retail packaging: Ready to cook upFinally, taking a look at the competitive landscape, AMD is aiming the Ryzen 9 7950X3D more at the Core i9-13900K in gaming than its Ryzen 9 7950X counterpart. So it's time to put it on our test bench, put it through its paces in our test suite, and see how well the Ryzen 9 7950X3D fares against the competition. Compute performance enthusiasts will likely find that the Ryzen 9 7950X is still an overall better chip; whereas the Ryzen 9 7950X3D is looking to leverage its 128 MB of L3 cache in games to make it the best gaming chip on the market. Let's see what we're dealing with.The Current CPU Test SuiteFor our AMD Ryzen 9 7950X3D testing, we are using the following test system:AMD Ryzen 7000 Series System (DDR5)CPURyzen 9 7950X3D ($699)16 Cores, 32 Threads120 W TDPMotherboardGIGABYTE X670E Aorus Master (BIOS 813b)MemoryG.Skill Trident Z5 Neo2x16 GBDDR5-5200 CL44CoolingEK-AIO Elite 360 D-RGB 360 mm AIOStorageSK Hynix 2TB Platinum P41 PCIe 4.0 x4 NMvePower SupplyCorsair HX1000GPUsAMD Radeon RX 6950 XT, Driver 31.0.12019Operating SystemsWindows 11 22H2Our updated CPU suite for 2023 includes various benchmarks, tests, and workloads designed to show variance in performance between different processors and architectures. These include UL's latest Procyon suite with both office and photo editing workloads simulated to measure performance in these tasks, CineBench R23, Dwarf Fortress, Blender 3.3, and C-Ray 1.1.Meanwhile, we've also carried over some older (but still relevant/enlightening) benchmarks from our CPU 2021 suite. This includes benchmarks such as Dwarf Fortress, Factorio, and Dr. Ian Cutress's 3DPMv2 benchmark.We have also updated our pool of games going forward into 2023 and beyond, including the latest F1 2022 racing game, the CPU-intensive RTS Total War: Warhammer 3, and the popular Hitman 3.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18747/the-amd-ryzen-9-7950x3d-review-amd-s-fastest-gaming-processor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Intel NUC 12 Pro Wall Street Canyon Kits Review: Alder Lake in UCFF Avatar\n",
      "Author: Ganesh T S\n",
      "Date Published: 2023-01-26T16:30:00Z\n",
      "URL: https://www.anandtech.com/show/18729/intel-nuc-12-pro-wall-street-canyon-kits-review-alder-lake-in-ucff-avatar\n",
      "Content: The early 2010s saw the PC market stuck in the doldrums, with the nettop craze having died down, and smartphones / tablets increasingly taking over day-to-day computing use-cases. While the traditional business PCs and workstation market was stable, the moribund consumer market needed a shot in the arm. Intel introduced the Next Unit of Computing - a 4\" x 4\" motherboard - in2012and started marketing mini-PCs based on the boards with their own branding. Low-power processors became powerful enough to deliver the processing power of bulky desktops in the palm of one's hand. The NUC quickly caught the market's fancy, triggering clones such as the GIGABYTE BRIX and ASRock Beebox from Intel's partners. Over the last decade, this ultra-compact form-factor (UCFF) and other SFF machines, along with systems targeting the gaming market have emerged as bright spots. The NUC family has since expanded to target all of those market segments.2022 was a milestone in the decade-long journey for the original 4\"x4\" form-factor. Intel introduced their first UCFF product family with hybrid processors. The NUC 12 Pro - codenamed Wall Street Canyon - was made available in a variety of flavors. In order to bring out the evolution of the UCFF ecosystem over the last ten years, Intel sampled us three different Wall Street Canyon NUCs targeting different market segments.The NUC12WSKi7 targets mainstream business users and home consumers, while the vPro-enabled NUC12WSKv7 is geared towards IT departments for business and enterprise deployments. The Wall Street Canyon family also includes bare boards such as the NUC12WSBi70Z (a Lite version of the board inside the NUC12WSKi7) that can be taken by OEMs and customized for specific applications. Our sample set included the Bleu Jour Meta 12 - a rugged fanless mini-PC based on the NUC12WSBi70Z and optimized for industrial use-cases. The review below takes a detailed look at the performance profile and feature set of the three NUCs.Introduction and Product ImpressionsIntel's Alder Lake processors brought the era of heterogeneous computing with hybrid processors to the mainstream market. The mixture of performance and efficiency cores was first marketed for desktop platforms before making the move to the low-power market inearly 2022. Fabricated in Intel 7, the processor family brought a multi-tasking focus to computing, providing hints to the OS on where different tasks need to be run and translating to a better user experience. The Alder Lake-P series is supposed to deliver all that within a 28W power envelop. The first UCFF systems based on Alder Lake-P processors were actually from ASRock Industrial - The NUC BOX-1200 series was introduced within a couple of weeks of the launch of Alder Lake-P. While being the first to market with the new platform, ourreview of the NUC BOX-1260Pfound that the company had not spent much time on optimizing the BIOS for optimal power consumption and performance. Our impressions of Alder Lake-P from that system was not favorable from a performance per watt perspective. The final verdict on that would undoubtedly be decided based on Intel's own Alder Lake-P offering - the Wall Street Canyon NUCs (NUC 12 Pro).In order to celebrate the ten-year long journey of the mainstream UCFF NUC form-factor, Intel sampled us three different Wall Street Canyon NUCs:Intel NUC12WSKi7, based on the Core i7-1260PIntel NUC12WSKv7, based on the Core i7-1270P (with a slightly different case design, apparently intended for future mainstream NUCs)Bleu Jour Meta 12, an as-yet unreleased rugged fanless NUC based on the NUC12WSBi70Z - a Lite version of the board in the NUC12WSKi7, without the Thunderbolt 4 / USB4 Type-C portsOne of the primary performance drivers over the last few generations of the Intel NUCs has been fine-grained power control. At the high-end, in particular, Intel has started setting the PL1 limits beyond the rated TDP. As late as the Comet Lake-based Frost Canyon NUC (the NUC10i7FN), the PL1 limit was set to 28W. Coming to the Tiger Lake family, the Tiger Canyon NUC that we evaluated - the NUC11TNBi5 also had its PL1 set to 28W. However, the Panther Canyon NUC (NUC11PAQi7) upped the ante and configured the PL1 to 40W by default. While we didn't publish a standalone review of the Panther Canyon NUC, readers can find all the benchmark numbers for that system in this review. The Alder Lake NUCs build upon the Tiger Lake ones by keeping the default PL1 and PL2 values in the BIOS to 40W / 64W. As we shall see further down in this review, this poses a challenge for fanless system vendors, while also exacerbating the fan noise issue in the standard kits.The NUC12WSKi7 sample came with a Kingston KC2500 PCIe 3.0 x4 500GB NVMe SSD and 2x Crucial CT16G4SFRA32A 16GB DDR4-3200 SODIMMs pre-installed. We performed our evaluation of the three systems in sequence, allowing the reuse of the same RAM and SSD for all three systems. While the overall packaging of the systems was geared towards unboxing videos, the contents of each box reflected the components in the retail packaging - VESA or industrial mounts, as applicable, a 120W (20V @ 6A) power adapter, a geo-specific power cord, screws for mount installation and M.2 SSD installation, as well as an assortment of quick start guides and regulatory information pamphlets.The NUC12WSKi7 continues with the same ultra-compact form-factor design seen in the previous NUCs. Doing away with the 2.5\" drive support allows the system to have a height of just 37mm. The front ports are all Type-A, with the two Thunderbolt 4 Type-C ports both relegated to the rear. Surprisingly, the rear I/O includes a USB 2.0 Type-A port too. The SDXC slot seen in previous generation NUCs is not seen here, but that is made up for by the presence of two Thunderbolt 4 ports.NUC12WSKi7 - Chassis Design and I/OThe retail version of the NUC12WSKv7 is externally identical to the NUC12WSKi7 shown above. However, our review sample set opted to put it in a redesigned casing.NUC12WSKv7 - An Updated Chassis Color SchemeThe new case design above is a preview of the plans for future 4x4 NUC models. The perforated case bottom can be removed without the aid of any tools to get access to the underside of the system (for the installation of the RAM and SSD).NUC12WSKv7 - 4x4 Designer Version PreviewThe third NUC model is the Bleu Jour Meta 12. The Meta series of fanless systems from Bleu Jour is geared towards industrial applications. The Meta 12 uses the Lite board - the NUC12WSBi70Z - which does away with the Type-C ports. A key difference is the presence of a DC power input connector in addition to the regular power adapter connection.Bleu Jour Meta 12 (NUC12WSBi70Z) Fanless Rugged Industrial PCThe gallery below presents additional photographs of the internals of the above three systems.Gallery:Intel Wall Street Canyon NUC KitsThe full specifications of the normal and vPro review samples are provided in the table below. The Bleu Jour Meta 12 is essentially the same as that of the NUC12WSKi7, except for the absence of the two Thunderbolt 4 Type-C ports.Systems Specifications(as tested)Intel NUC12WSKi7 (Wall Street Canyon)Intel NUC12WSKv7 (Wall Street Canyon vPro)ProcessorIntel Core i7-1260PAlder Lake 4P + 8e / 16T, up to 4.7 GHz (P) / 3.4 GHz (e)Intel 7, 18MB L2, 28W(PL1 = 40W, PL2 = 64W)Intel Core i7-1270PAlder Lake 4P + 8e / 16T, up to 4.8 GHz (P) / 3.5 GHz (e)Intel 7, 18MB L2, 28W(PL1 = 40W, PL2 = 64W)MemoryCrucial CT16G4SFRA32A.C16FR DDR4-3200 SODIMM22-22-22-52 @ 3200 MHz2x16 GBCrucial CT16G4SFRA32A.C16FR DDR4-3200 SODIMM22-22-22-52 @ 3200 MHz2x16 GBGraphicsIntel Iris Xe Graphics(96EU @ 1.40 GHz)Intel Iris Xe Graphics(96EU @ 1.40 GHz)Disk Drive(s)Kingston KC2500 SKC2500M8500G(500 GB; M.2 2280 PCIe 3.0 x4 NVMe;)(Kioxia BiCS4 96L 3D TLC; Silicon Motion SMI 2262EN Controller)Kingston KC2500 SKC2500M8500G(500 GB; M.2 2280 PCIe 3.0 x4 NVMe;)(Kioxia BiCS4 96L 3D TLC; Silicon Motion SMI 2262EN Controller)Networking1x 2.5 GbE RJ-45 (Intel I225-V)Intel Wi-Fi 6 AX211 (2x2 802.11ax - 2.4 Gbps)1x 2.5 GbE RJ-45 (Intel I225-LM)Intel Wi-Fi 6 AX211 (2x2 802.11ax - 2.4 Gbps)AudioDigital Audio with Bitstreaming Support over HDMI Ports3.5mm stereo headset jack (Realtek audio codec)Digital Audio with Bitstreaming Support over HDMI Ports3.5mm stereo headset jack (Realtek audio codec)Video2x HDMI 2.0b2x Display Port 1.4a with HBR3 over Thunderbolt 42x HDMI 2.0b2x Display Port 1.4a with HBR3 over Thunderbolt 4Miscellaneous I/O Ports2x USB 3.2 Gen 2 Type-A (Front)1x USB 3.2 Gen 2 Type-A (Rear)1x USB 2.0 Type-A (Rear)2x Thunderbolt 4 (Rear) (Type-C)2x USB 3.2 Gen 2 Type-A (Front)1x USB 3.2 Gen 2 Type-A (Rear)1x USB 2.0 Type-A (Rear)2x Thunderbolt 4 (Rear) (Type-C)Operating SystemWindows 11 Enterprise (22000.1455)Windows 11 Enterprise (22000.1516)Pricing(Street Pricing on January 25th, 2023)US$660(barebones)$843 (as configured, no OS)(Street Pricing on January 25th, 2023)US$827(barebones)$1010 (as configured, no OS)Full SpecificationsIntel NUC12WSKi7 SpecificationsIntel NUC12WSKv7 SpecificationsIn the next section, we take a look at the BIOS options along with an analysis of the motherboard platform. Following that, we have a number of sections focusing on various performance aspects before concluding with an analysis of the value proposition of the systems.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18729/intel-nuc-12-pro-wall-street-canyon-kits-review-alder-lake-in-ucff-avatar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Ubiquiti Diaries: A Site-to-Site VPN Story\n",
      "Author: Ganesh T S\n",
      "Date Published: 2022-12-21T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/18692/the-ubiquiti-diaries-a-sitetosite-vpn-story\n",
      "Content: Ubiquiti Networks is a popular vendor of networking-related equipment in the SMB / SME space. Their gear is immensely popular among prosumers too, thanks to the combination of ease of use and the ability to customize for specific requirements. I have been running an Ubiquiti UniFi installation at home for the last five years or so, and recently had the opportunity to create a new deployment in another country. There were two main reasons to go with Ubiquiti for the new location - a single management plane for both sites, and the ability to easily create a site-to-site VPN.The new installation was fairly smooth and the site-to-site VPN was up and running in a stable manner until the ISP at the remote site moved the gateway from a public-facing WAN IP to one behind a carrier-grade NAT (CGNAT). That started a deeper investigation into various options available for site-to-site VPNs with Ubiquiti's gear for different scenarios. In this process, I ended up encountering a host of issues worthy of documentation to help folks who might encounter them in their own installations. This article provides a recount of my trip down the rabbit hole - including a step-by-step guide detailing my attempts to work around the various pitfalls.IntroductionUbiquiti Networks offers a range of products targeting the networking market. While wireless ISPs are a key market segment for the company (serviced by the airFiber line), today's piece is focused on their UniFi product line - a range of managed software-defined networking equipment for SMBs, SMEs, and prosumers. There are a number of reasons for UniFi's popularity products among tech-savvy consumers. The company had a first-mover advantage in offering a cost-effective managed SDN solution. Isolating functionality into different devices (security gateways, routers, switches, and wireless access points) allowed users to pick and choose different equipment based on their custom needs. The unified management plane for all the UniFi products enables easy maintenance while retaining deployment flexibility. Network scaling in response to requirement changes is also straightforward. The company started out with a local management controller, which has now been augmented with a cloud-based offering.My first brush with Ubiquiti was their mFi product line (which has since been unfortunately EOL-ed). Their lineup of network-connected power outlets with energy and power monitoring, as well as remote relay control was (and continues to be) more flexible than anything else in the market - and this was without even taking the low pricing into account. I had purchased a few of their units for my home / AnandTech testing lab use, and written a short review after a couple of months of use (those units are still in deployment).After I published the mFi review, Ubiquiti's PR department approached me with an offer to review their UniFi product line. Around that time back in 2017, I had the opportunity to lay out a wired Cat 6 backbone for all the rooms in my house here in California. I took up the offer to spec out a UniFi system for testing out. The USG Pro 4 gateway took up the routing duties with a UniFi Cloud Key (first generation) performing controller duties. Access points with varying capabilities were mounted around the house to avoid wireless dead-spots. A number of switches were placed in the media center and different lab locations. I ended up augmenting the system with additional PoE switches and in-wall APs on my own.The system was configured with the usual guest wireless network, and a bunch of different VLANs (serving the IoT devices in the house, the home lab equipment, and another for devices such as the common family desktop, phones, etc.). On the whole, it was an overkill for a residential installation. That said, the deployment has held its own over five years of stressful usage (and still going strong). The only hiccup I had was when the CloudKey controller became inaccessible on the network a couple of years back. It turned out that a power interruption had ended up corrupting the database - nothing that a few SSH commands (thanks to the helpful community) couldn't resolve. Since then, I ended up investing in a UPS for the rack holding the UniFi equipment to avoid the recurrence of such scenarios.Such issues are also the reason why I recommend Ubiquiti equipment only to tech-savvy users. In almost all cases, calling up the company's support line and creating a ticket ends up being a waste of time. There are innumerable resources online (both the company's ownusers forum, as well as countless prosumer bloggers such asScott HanselmanandTroy Hunt. In light of reviews from such sources, there is not much for readers to gain from posting yet another review of the Ubiquiti UniFi lineup. Instead, I am hoping to take up specific use-cases and figure out how Ubiquiti's product lineup can address those in these series of articles.Earlier this year, my parents back in India decided to downsize their home. I took the opportunity to revamp their home network from the ground-up. I had been intending to add features to the home network of my parents, but had never had the opportunity because my visits were becoming infrequent. However, with my first visit post-pandemic, I wanted to get a few things set up as part of their move:Easier remote management and troubleshooting of network issues without the need for port forwarding.Ability to seamlessly use their Indian home network during travel / visits over here to CaliforniaAbility to perform secure remote offsite backups for my data without relying on an external cloud storage providerAbility to seamlessly utilize Indian OTT service subscriptions irrespective of user location either in California or in IndiaWhen I initially set up the Cloud Key back in 2017, there was no requirement to use a cloud account. Unfortunately, the UniFi Network mobile application user experience became quite onerous without a ui.com ID a couple of years back. I caved in and ended up associating my installation with a cloud ID just for this purpose. Since I was already managing my network through this ID, it became a straightforward decision to go with Ubiquiti for the deployment back in India.The key to fulfilling the above requirements was a secure VPN tunnel between my home network here in California and my parents' network in India. Prior to traveling, I arranged for a Ubiquiti Dream Machine to be delivered to the new home. The Ubiquiti UniFi Dream Machine is an all-in-one solution / UniFi starter kit. It integrates a 4-port switch, a 4x4 802.11ac access point, a security gateway, and an integrated controller. The Annapurna Labs AL314-based solution comes with a single WAN port, and is an acceptable solution for most home networks in the the 1000 sq. ft - 1200 sq. ft range.From my use-case perspective, I wanted a solution that would support simple VPN tunnel configuration and easy app-based access for both the US and Indian networks via a single interface.The Evolution of UniFi - A Short RecapUbiquiti's UniFi lineup was launched after their lineup of edge-focused products for WISPs started gaining traction in other markets. These EdgeRouters and EdgeSwitches were based on Vyatta OS, and the UniFi products initially started out with the same EdgeOS firmware base. The UniFi Security Gateway Pro 4 in my primary deployment runs EdgeOS to date.The USG Pro 4 is based on Cavium's OCTEON II networking SoC, with a MIPS64 application processor. However, Ubiquiti's latest gateways / routers / switches in the UniFi lineup now run a custom Debian-based Linux distribution. The UniFi Dream Machine uses the Annapurna Labs AL314, and runs a distribution meant for the AArch64 platform. The UniFi OS itself runs as a container using podman.The end result is that there are quite a number of disconnects between the features available on EdgeOS and UbiOS / UniFi OS. Migration from the EdgeOS line to UniFi OS is not straightforward enough for heavily customized installs. With focus shifting to UbiOS / UniFi OS, the updates for the older equipment have become few and far apart. While that might not be a concern for stable networks, it has unfortunately not kept up to date with evolving network security practices. For example, Android's recent releases have completely dropped support for L2TP VPNs, while EdgeOS has L2TP as the recommended VPN server type. This brings us to the topic of VPNs.VPN Server Options in Ubiquiti's StackUbiquiti offers a range of VPN options depending on the gateway being used. At home here in California with the USG Pro 4, I have been running a L2TP VPN server (allowing me to connect to it from public coffee shops and airports for secure browsing purposes) for several years now. I hadminimal troublesetting it up for access from a Windows notebook. However, as mentioned in the previous sub-section, this VPN server is of no use for my mobile phone running Android 12. The USG Pro 4 also supports PPTP VPN, but it is not recommended even by Ubiquiti themselves.The primary option for a VPN server in the UniFi Dream Machine running UbiOS / UniFi OS is quite different.Here, Teleport (Ubiquiti's customized Wireguard implementation) takes precedence. This is a one-click VPN more in tune with today's mobile-first ecosystem. Clients are authorized via invites that can be generated either from the configuration page (on the unifi.ui.com cloud, or via the machine's local IP) or the UniFi Network mobile app. The invites can be opened on the client device using the Wifiman mobile application. The unfortunate aspect here is that Windows users are out of luck. While MacOS, Android, and iOS are covered, Windows users are left in the lurch. This is a hugely disappointing situation given that the L2TP option in EdgeOS works with Windows clients, but not Android and the Teleport option in UbiOS / UniFi OS works with Android clients, but not Windows. It must be noted that the UDM still supports L2TP for Windows clients.Under the Teleport & VPN section, Ubiquiti also provides an option to create site-to-site VPNs, which is where our story starts.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/18692/the-ubiquiti-diaries-a-sitetosite-vpn-story\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD Reveals Radeon RX 7900 XTX and 7900 XT: First RDNA 3 Parts To Hit Shelves in December\n",
      "Author: Ryan Smith\n",
      "Date Published: 2022-11-03T20:46:00Z\n",
      "URL: https://www.anandtech.com/show/17638/amd-reveals-radeon-rx-7900-xtx-and-7900-xt-first-rdna-3-parts-to-hit-shelves-in-december\n",
      "Content: With AMD’s first-wave of Zen 4 CPUs now in the books with the Ryzen 7000 series, the consumer arm of AMD is now shifting its attention over to its graphics business. In a presentation that ended moments ago dubbed “together we advance_gaming”, Dr. Lisa Su and other AMD leaders laid out the future of AMD’s graphics products. And that future is the RDNA 3 architecture, which will be the basis of the new Radeon RX 7900 XTX and Radeon RX 7900 XT video cards.The two cards, set to be released on December 13th, will be the first products released using the RDNA 3 architecture. According to AMD, the new flagship 7900 XTX will deliver up to 70% more performance at 4K than their previous flagship, the 6950 XT. This performance boost comes curtesy of several architectural improvements in RDNA that cumulatively offer 54% higher performance per watt than RDNA 2, as well as higher clockspeeds courtesy of TSMC’s 5nm (and 6nm) processes, and higher overall power consumption.The full-fledged RX 7900 XTX will be hitting the streets at $999. Meanwhile the second-tier RX 7900 XT will run for $899.AMD Radeon RX 7000 Series Specification ComparisonAMD Radeon RX 7900 XTXAMD Radeon RX 7900 XTAMD Radeon RX 6950 XTAMD Radeon RX 6900 XTStream Processors12288(96 CUs)10752(84 CUs)5120(80 CUs)5120(80 CUs)ROPs192192128128Game Clock2.3GHz2.0GHz2100MHz2015MHzBoost Clock2.5GHz2.4GHz2310MHz2250MHzThroughput (FP32)56.5 TFLOPS43 TFLOPS21.5 TFLOPS20.6 TFLOPSMemory Clock20 Gbps GDDR620 Gbps GDDR618 Gbps GDDR616 Gbps GDDR6Memory Bus Width384-bit320-bit256-bit256-bitVRAM24GB20GB16GB16GBInfinity Cache96MB80MB128MB128MBTotal Board Power355W300W335W300WManufacturing ProcessGCD: TSMC 5nmMCD: TSMC 6nmGCD: TSMC 5nmMCD: TSMC 6nmTSMC 7nmTSMC 7nmTransistor Count58B58B - (1 MCD)26.8B26.8BArchitectureRDNA3RDNA3RDNA2RDNA2GPUBig Navi 3xBig Navi 3xNavi 21Navi 21Launch Date12/13/202212/13/202205/10/202212/08/2020Launch Price$999$899$1099$999AMD’s eagerly anticipated update to their GPU architecture comes as the company has been firing on all cylinders for the last few years. On the CPU side of matters the Zen 3 and Zen 4 architectures in particular have proven very performant, and meanwhile AMD has been able to claw its way back from its graphics slump with the RDNA family of GPU architectures. RDNA 2, the basis of the Radeon RX 6000 series, exceeded expectations and proved to be a very strong competitor, and now AMD is seemingly setup to exceed expectations once again, with RDNA 3’s 54% performance-per-watt coming in ahead of AMD’s earliest promises of a 50% gain.AMD Goes Chiplets For GPUsWhile today’s reveal from AMD was a more closely guarded event than the Ryzen 7000 unveil a couple of months ago, AMD has still given us a quite a bit of detail on the RDNA 3 architecture and the cards – more than we have time to cover here – so let’s get started from the top, with the construction of the first RDNA 3 GPU.The Navi 3x GPU (AMD is not confirming the specific GPU name at this time) breaks new ground for AMD not only on the performance front, but in terms of its construction. For the first time from any of the big 3 GPU manufacturers, AMD is utilizing chiplets in the construction of the GPU.Gallery:Big Navi 3x GPUChiplets are in some respects the holy grail of GPU construction, because they give GPU designers options for breaking apart complex monolithic GPU designs into multiple smaller parts – allowing for new options for scaling, as well as mixing and matching the process node used in manufacturing. That said, it’s also a holy grail because the immense amount of data that must be passed between different parts of a GPU (on the order of terabytes per second) is very hard to do – and very necessary to do if you want a multi-chip GPU to be able to present itself as a single device.For their big Navi 3x chip, AMD has assembled two types of chiplets, essentially breaking off the memory functions from a classic GPU into their own chiplets. This means that the core functions of the GPU are housed in what AMD is calling the Graphics Compute Die (GCD), which houses all of the ALU/compute hardware, the graphics hardware, as well as ancillary blocks like the display and media engines.Because the GCD houses the performance-critical aspects of the overall GPU, it’s being built on TSMC’s 5nm process. This gives AMD the best density, power consumption, and clockspeeds for these parts, though obviously at a higher manufacturing cost. The GCD die size measures 300mm2.Meanwhile the new Memory Cache Die (MCD) houses AMD’s infinity cache (L3 cache), as well as a 64-bit (technically 2x32-bit) GDDR6 memory controllers. The MCD is one of the scalable aspects of the chiplet design, as Big Navi 3x GPU SKUs can be configured by paring them with more or fewer MCDs. A full configuration in this case is 6 active MCDs, which is what we see in the 7900 XTX. Meanwhile the 7900 XT will have 5 active MCDs, with a 6thdefective/spacer MCD present for salvaging purposes and physical package stability.An individual MCD is 37mm2in die size, and is built on TSMC’s 6nm process. This is an example of AMD’s process node flexibility, putting the less critical GDDR6 memory controllers and Infinity Cache on a cheaper process node. GDDR6 controllers are one of those classic examples of a technology that doesn’t scale very well with smaller process geometries (like most forms of I/O), so it’s easy to see why AMD would want to avoid building it on 5nm for minimal benefits.In the full 6 MCD configuration (7900 XTX), Big Navi 3x offers a 384-bit GDDR6 memory bus, along with 96MB of L3 cache. Meanwhile a 5 MCD (7900 XT) offers a 320-bit GDDR6 memory bus and 80MB of L3 cache.For the purposes of today’s announcement, AMD has not gone into great depth onhowthey managed to make a chiplet-based GPU work, but they have confirmed a few important details. First and foremost, in order to offer the die-to-die bandwidth needed have the memory subsystem located off-chip, AMD is using an unspecified fanout bridge technology.their Elevated Fanout Bridge (EFB) packaging technology, which AMD first used for their Instinct MI200 series accelerators (CDNA2). On those accelerator parts it was used to hook up the monolithic GPUs to each other, as well as HBM2e memory. On RDNA 3, it’s being used to hook up the MCDs to the GCD.Notably, fanout bridges are a non-organic packaging technology, which is to say it’s complex. That AMD is able to get 5.3TB/second of die-to-die bandwidth via it underscores its utility, but it also means that AMD is undoubtedly paying a good deal more for packaging on Big Navi 3x than they were on Navi 21 (or Ryzen 7000).Internally, AMD is calling this memory-to-graphics link Infinity Link. Which, as the name implies, is responsible for (transparently) routing AMD’s Infinity Fabric between dies.As mentioned before, the cumulative bandwidth here between the MCDs and GCD is 5.3TB/second. It’s unclear if the constraining factor is the bandwidth of the Infinity Link, or that the combined Infinity Cache + GDDR6 memory controllers cannot move enough data to fully saturate the link. But regardless, it means there’s essentially just shy of 900GB/second of bandwidth between an individual MCD and GCD – more than all of the combined off-die memory bandwidth of the last-generation Radeon RX 6950 XT (and 2.7x more than Navi 21’s on-die bandwidth).While we’re on the subject of AMD’s L3 Infinity Cache, it’s notable here that it’s actually a bit smaller on Big Navi 3x than it was on Navi 21, with a maximum capacity of 96MB versus 128MB on the former. According to AMD they’ve made further improvements to improve data reuse on the Infinity Cache to offset this drop in capacity. At this point it’s not clear if the change is a function of software algorithms, or if they’ve made more fundamental hardware changes.Finally, while AMD is quoting die sizes for the GCD and MCD, they aren’t quoting individual transistor counts. So while we know that a complete 6 MCD Big Navi 3x configuration is comprised of 58 billion transistors (2.16x more than Navi 21), we don’t know how much of that is the GCD versus the MCDs.AMD RDNA 3 Compute & Graphics Architecture: Bringing Back ILP & Improving RTDiving down a level, let’s take a look at the actual graphics and compute architecture backing RDNA 3 and Big Navi 3x.While still clearly sharing many of the core design elements of AMD’s overarching RDNA architecture, RDNA 3 is in some respects a much bigger shift in architectural design than RDNA 2 was. Whereas RDNA 2’s compute core was essentially unchanged from RDNA (1)’s, RDNA 3 makes a few big changes.The biggest impact is how AMD is organizing their ALUs. In short, AMD has doubled the number of ALUs (Stream Processors) within a CU, going from 64 ALUs in a single Dual Compute Unit to 128 inside the same unit. AMD is accomplishing this not by doubling up on the Dual Compute Units, but instead by giving the Dual Compute Units the ability to dual-issue instructions. In short, each SIMD lane can now execute up to two instructions per cycle.But, as with all dual-issue configurations, there is a trade-off involved. The SIMDs can only issue a second instruction when AMD’s hardware and software can extract a second instruction from the current wavefront. This means that RDNA 3 is now explicitly reliant on extracting Instruction Level Parallelism (ILP) from wavefronts in order to hit maximum utilization. If the next instruction in a wavefront cannot be executed in parallel with the current instruction, then those additional ALUs will go unfilled.This is a notable change because AMD developed RDNA (1) in part to get away from a reliance on ILP, which was identified as a weakness of GCN – which was why AMD’s real-world throughput was not as fast as their on-paper FLOPS numbers would indicated. So AMD has, in some respects, walked backwards on that change by re-introducing an ILP dependence.We’re still waiting on more information from AMD outlining why they made this change. But dual-issue is typically a cheap way to add more throughput to a processor design (you don’t have to do all the instruction tracking required for a fully separate Dual Compute Unit), and it can be worthwhile tradeoff if you can ensure you’ll be able to dual-issue most of the time. But it means that AMD’s real-world ALU utilization rate is likely lower on RDNA 3 than RDNA 2, due to the bubbles from not being able to dual-issue.Which to bring things back to gaming and the products at hand, it means that the FLOPS numbers between RDNA 3 and RDNA 2 parts are not going to beentirelycomparable. 7900 XTX may push 2.6x as many FP32 FLOPs as 6950 XTX on paper, but the real world advantage on anything less than ideal code is going to be less. Which is one of the reasons why AMD is only promoting a real-world performance uplift of 1.7x for the 7900 XTX.In any case, SIMDs aren’t the only changes to the core compute architecture of RDNA 3. Feeding the beast, AMD has made the Vector General Purpose Register (VGPR) bank 50% larger than on RDNA 2.More significant than that is that AMD is finally integrating dedicated silicon for AI processing on their consumer GPUs. This is an area where both of AMD’s competitors (NVIDIA and Intel) have already made the investment on their consumer parts, and as the use of GPU inference in workloads continues to grow, it’s not something AMD can ignore any longer.Given the gaming-centric focus of today’s presentation, AMD did not spend much time talking about the new AI units. Each RDNA 3 CU will have 2 of these units, and they will support new AI instructions (some kind of INT8 tensor operation seems like a given). All told, AMD is saying that the new AI units give the Radeon RX 7900 XTX 2.7x the AI performance, which AMD is measuring as bfloat16 performance versus the RX 6950 XT.Overall, the importance of this to gamers is something that remains to be seen. AMD isn’t currently using AI units for FSR 2 (unlike NVIDIA’s DLSS 2). But that could change for future projects. Otherwise, for more professional users (or anyone who likes to mess with Stable Diffusion), this is an addition that’s good news.Moving on, AMD has also updated their raytracing hardware for RDNA 3. The second-generation RT accelerator, as AMD calls it, can handle 1.5x more rays in flight. There are also new hardware box sorting and traversal features that weren’t present in RDNA 2’s initial RT functionality. AMD’s presentation gave the technical details a light treatment, but it certainly looks like AMD is moving to doing a bigger part of the ray tracing process in dedicated hardware. Which in turn would help improve their performance, and keep performance steadier by not stealing quite so many resources from the rest of the CU.AMD’s own performance slides tout anywhere between a 47% and 84% increase in RT performance. Though it should be noted that AMD’s numbers are with FSR enabled; so we cannot divorce these gains from any changes that improve FSR performance on the 7900 XTX.Last, but not least, AMD has made an interesting decision with clockspeeds on the RDNA 3. In short, AMD has decoupled their clocks; rather than running the entire GCD at the same clockspeed, AMD will be running the shaders and front-end at different clockspeeds. In the case of the 7900 XTX, this will see the shaders running at 2.3GHz (the advertised game clock speed), while the front-end will run at a slightly speedier 2.5GHz (about 9% faster).AMD did not go into great detail on why they’ve made this change, but at a high level it’s all about balancing performance versus power consumption. The shaders could run at 2.5GHz as well (indeed, the 7900 XTX’s rated boost clock is 2.5GHz), but as we’ve seen time and time again, those final clocks are the most expensive in terms of power as you go up the v/f curve. So AMD has made the choice to give up a bit of potential performance to save a lot on power, as 96 CUs/12288 ALUs is a lot of silicon to light up. Conversely, the front-end is relatively small, and with AMD having beefed up their CUs by so much, spending a bit more power on the front-end is presumably worth it to keep them from bottlenecking the rest of the GPU.RDNA 3 Display & Media Engines: The Latest and the GreatestAMD’s core compute/graphics architecture was not the only part of the RDNA 3 architecture to get an update in this generation. AMD has also used the opportunity to upgrade their display and media engines to support new features and new formats.On the display engine front, AMD’s display engine, which they are now calling the “AMD Radiance Display Engine” has been upgraded to support DisplayPort 2.1. Specifically, AMD has added support for theDisplayPort 2.x feature suiteas well as the UHBR 10 and UHBR 13.5 data rates. This means that RDNA 3 cards can offer 2x the DisplayPort bandwidth of their DisplayPort 1.4-enabled predecessors, which in turn allows for higher resolutions and higher refresh rates. Notably, this ever so slightly exceeds HDMI 2.1’s bandwidth as well, putting DisplayPort back into the lead, at least on AMD cards.Unsurprisingly, AMD is using this functionality to push forthcoming higher resolution and higher refresh rate gaming monitors, including a Samsung ultrawide display set to launch in 2023 with a horizonal 8K resolution. So it’s not just for showing off specs, and AMD and its partners are intending to put it to good use.AMD has not said anything about the total number of supported displays. So at this point I expect it’s still a maximum of 4 displays.Meanwhile on the media engine front, AMD has given RDNA 3 support for the latest and greatest video codecs. Along with the usual H.264 and H.265 support, RDNA 3’s media engines also add full AV1 encode and decode support, making this the latest GPU family to roll out support for the next-generation open format codec. RDNA 3 will be able to encode and decode AV1 at up to 8Kp60.The overall performance of the media engine has been increased significantly. According to AMD the media engine runs 80% faster than it did on RDNA 2 parts, allowing for simultaneous encoding (or decoding) of up to two H.264/H.265 streams. Though it’s unclear if that also applies to AV1.Finally on the subject of AMD’s GPU uncore, while not explicitly called out in AMD’s presentation, it’s worth noting that AMD hasnotupdated their PCIe controller. So RDNA 3 still maxes out at PCIe 4.0 speeds, with Big Navi 3x offering the usual 16 lanes. This means that even though AMD’s latest Ryzen platform supports PCIe 5.0 for graphics (and other PCIe cards), their video cards won’t be reciprocating in this generation. In fact, this means that no one will have a PCIe 5.0 consumer video card.Radeon RX 7900 XTX & Radeon RX 7900 XT: Launching December 13thBringing today’s reveal full circle, let’s turn back to the cards themselves, the Radeon RX 7900 XTX and RX 7900 XT.AMD’s flagship card will be the Radeon RX 7900 XTX. While we’re still waiting on confirmation of this, this would seem to be a fully-enabled Big Navi 3x part, with all of the blocks in both the GCDs and the individual MCDs themselves enabled. As mentioned previously, AMD is touting a broad performance uplift of up to 70% versus the previous-generation flagship, the RX 6950 XT.Internally, this means 96CUs and 96MB of L3 Infinity Cache will be available on the card. The game clock (average clockspeed) will be 2.3GHz, while based on other AMD figures, we can infer that the boost (maximum) clockspeed will be 2.5GHz. The game clock in particular is a ~10% improvement over the 6950 XT, so AMD is enjoying a modest frequency uplift generation-over-generation, but nothing too massive. Most of the heavy lifting will come courtesy of the architecture and memory changes.Gallery:AMD Radeon RX 7900 XTXSpeaking of memory, the RX 7900 XTX will be paired with 24GB of GDDR6 memory running at (at least) 20Gbps. Apparently, AMD’s partners have the headroom to go even higher than this with factory overclocking, but the floor value for the part will be 20Gbps overall. This is a modest increase in memory clockspeeds versus the 6950 XT (11%). Instead, the bulk of the VRAM bandwidth gains will come from the 50% larger memory bus, with the 7900 XTX moving to a sizable 384-bit bus. In total, this means the 7900 XTX will have 960GB/sec of memory bandwidth, 66% more than its predecessor. AMD got their “free” memory subsystem performance boost in the last generation with Infinity Cache, so for this time around, they are back to needing to add more physical memory bandwidth to keep the ever-growing beast properly fed.Meanwhile, the 7900 XT will be a chip off the block, with fewer CUs, less VRAM, and lower clockspeeds. All told we’re looking at 84CUs paired with 20GB of 20Gbps GDDR6, and backed by an 80MB infinity cache. The card’s game clockspeed rating is 2.0GHz, and we do not have any information on the boost clockspeed. The combination of a 13% drop in clockspeeds and 13% drop in CUs adds up to what is, on paper, a 24% deficit in compute/shading performance. That said, AMD’s pricing indicates that the real-world performance gap shouldn’t be this high, and we’re still missing some important details such as ROP counts. So for better or worse, we don’t have a good frame of reference fright now for how the 7900 XT will perform relative to anything else, current-generation or last.Gallery:AMD Radeon RX 7900 XTUnsurprisingly, power consumption at the high end will be going up. The 7900 XTX will be a 355W card, up 20W from the 335W 6950 XT (and 55W from the 300W 6900 XT). This is a more modest power requirement than on NVIDIA’s high-end RTX 4090 Ti (450W), but we’re still talking about a card well north of 300W. For gamers with a slightly smaller appetite for large power bills, the 7900 XT will be holding the line at 300W. Both cards will require 2 8-pin PCIe power connectors.AMD has also sent over pictures of both the reference 7900 XTX and 7900 XT. Of particular note, both cards will feature a USB-C port for display outputs. This is a feature that AMD introduced with the RX 6000 series and has opted to carry forward into the RX 7000 series. As with the previous-gen cards, the presence of the USB-C port is for directly hooking up monitors that rely on DisplayPort Alt Mode over USB-C. Meanwhile, rounding out the collection will be a paid of DisplayPorts (2.1) and an HDMI 2.1 port.Both cards are using a new triple fan blower design from AMD. We’re still waiting on further details here, but AMD has told us that the 7900 XTX measures 287mm long, and is 2.5 slots wide.Wrapping things up, both cards will be launching on December 13th, with AMD planning on having both reference and AIB partners’ cards on the shelf for launch day. The 7900 XTX will start at $999, meanwhile the 7900 XT will be right behind it at $899. AMD isn’t offering any performance comparisons versus NVIDIA cards, but at this juncture it seems like the wildcard is the soon-to-launch GeForce RTX 4080 16GB. By the time AMD launches in December, we should have a much better idea of where AMD and NVIDIA’s dueling lineups stand in comparison to each other.Gallery:AMD RDNA 3 Launch Slide Deck\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17638/amd-reveals-radeon-rx-7900-xtx-and-7900-xt-first-rdna-3-parts-to-hit-shelves-in-december\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD Zen 4 Ryzen 9 7950X and Ryzen 5 7600X Review: Retaking The High-End\n",
      "Author: Ryan Smith\n",
      "Date Published: 2022-09-26T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/17585/amd-zen-4-ryzen-9-7950x-and-ryzen-5-7600x-review-retaking-the-high-end\n",
      "Content: During AMD’s ‘together we advance_PCs’ event at the end of August, the company unveiled its Ryzen 7000 series of desktop processors, with four SKUs aimed at the mid-range and high-end market segments. After whetting the audience's appetites with that announcement, tomorrow AMD will be officially releasing their long-awaited next-generation CPUs.The launch of the Ryzen 7000 series brings a lot to digest, for casual fans and hardcore hardware enthusiasts alike. For their newest lineup of chips, AMD has given their desktop CPU platform a top-to-bottom update, not only releasing new CPUs, but releasing an entirely new platform, socket AM5 around it. As a result, for the first time in a few generations these chips are not drop-in compatible with existing AMD motherboards. But at the same time it has allowed AMD to deliver on a collection of platform improvements, ranging from DDR5 and PCIe 5.0 support to improved power management capabilities. AMD has even managed to sneak an entry-level Radeon RDNA2 architecture-based iGPU into the chip.The Ryzen 9 7950X: 16 Cores, 32 Threads, New 170 W TDP: $699We'll start, as always, with the CPUs themselves. AMD's flagship for this generation is the Ryzen 9 7950X, a 16 Zen 4 core CPU that AMD is looking to top the charts with for both single-threaded and multi-threaded workloads. The Ryzen 9 7950X has a base frequency of 4.5 GHz and a peak turbo clockspeed of 5.7 GHz, which makes it the highest clocked desktop x86 CPU to hit the market yet.But don't think AMD's Zen 4 architecture is just about clockspeeds. AMD has also improved the IPC of their CPU architecture by an average of 13% – primarily relying on the addition of AVX-512 instruction support and comfortably larger caches and buffers throughout the CPU – which means that the Ryzen 7000 chips can deliver some significant performance improvements in a variety of single-threaded workloads.As for multi-threaded workloads, AMD has been able to improve performance there as well, albeit with a reliance on both architecture improvements and higher TDPs to allow for higher sustained clockspeeds. One of the enabling factors here is that the AM5 platform allows for higher chip TDPs – up to 170W in the case of the 7950X – which is some 65W higher than the max TDPs on AMD's fastest 16 core Ryzen 5000 parts. As a result AMD is in a good position to deliver on the \"leadership\" class performance that the company is after, but not entirely for free.The Ryzen 9 7900X, Ryzen 7 7700X, and Ryzen 5 7600XMoving one down the stack is the Ryzen 9 7900X, which is a 12C/24T and 170W TDP part; it has a higher base frequency than the 7950X of 4.7 GHz, but with a slightly lower boost frequency of up to 5.6 GHz.Below that, AMD has launched one Ryzen 7 part designed for mid-range desktop computing, the Ryzen 7 7700X. This is an 8C/16T SKU, with a boost frequency on a single core of up to 5.4 GHz, and a base frequency of 4.5 GHz. Notably, unlike the Ryzen 9 parts, this part has a more typical-for-AMD TDP of 105W.Finally, also aimed at the mid-range market and the cheapest member of AMD's new product stack, we have the Ryzen 5 7600X. Offering 6C/12T with a TDP of 105W, the 7600X is Zen 4 at a more reasonable price point. The chip runs at a base frequency of 4.7 GHz, with a modest (compared to Ryzen 9) boost frequency on a single core of 5.3 GHz.AMD Ryzen 7000 versus Ryzen 5000AnandTechCoresThreadsBaseFreqTurboFreqMemorySupportL3CacheTDPMSRPRyzen 9 7950X16C / 32T4.5GHz5.7GHzDDR5-520064 MB170 W$699Ryzen 9 5950X16C / 32T3.4 GHz4.9 GHzDDR4-320064 MB105 W$799Ryzen 9 7900X12C / 24T4.7GHz5.6GHzDDR5-520064 MB170 W$549Ryzen 9 5900X12C / 24T3.7 GHz4.8 GHzDDR4-320064 MB105 W$549Ryzen 7 7700X8C / 16T4.5GHz5.4GHzDDR5-520032 MB105 W$399Ryzen 7 5800X8C / 16T3.8 GHz4.7 GHzDDR4-320032 MB105 W$449Ryzen 5 7600X6C / 12T4.7GHz5.3GHzDDR5-520032 MB105 W$299Ryzen 5 5600X6C / 12T3.7 GHz4.6 GHzDDR4-320032 MB65 W$299Comparing apples to apples, so to speak, between the new Ryzen 7000 series parts to the previous-generation Ryzen 5000 series parts, Ryzen 7000 has made some big overall improvements to the chips' capabilities. All of the Ryzen 7000 chips offer significant increases in both base and boost frequencies, which bodes well for overall performance. The worst we can say is that AMD hasn't increased their core counts at any price point/market segment, so all of the performance gains we'll see here today are entirely from architecture and clockspeeds, rather than the more immediate MT gains of throwing more silicon at the matter.AMD's performance gains have been made possible in part through the Zen 4 architecture's superior power efficiency. While the Zen 4 architecture is modest refinement of Zen 3, delivering a 13% IPC improvement, it also gets the big advantage of being produced on TSMC's 5 nm process node, a full node's shrink from the TSMC 7nm process that was used for Ryzen 5000/3000. This efficiency has allowed AMD to boost clockspeeds without breaking the power bank, with the 105W TDP 7700X seeing a 700MHz improvement for no change in TDP. And multi-threaded performance is not left out in the cold, either; by increasing their top TDP to 170W, AMD is able to keep the CPU cores on their 12C and 16C parts at higher sustained turbo clocks, delivering much better performance there as well.Of course one of the key arguments here is that more power calls for more cooling, which is very much true for the Ryzen 7000 series. Ryzen 7000’s TjMax for its Precision Boost Overdrive technology stands at 95°C, which means that the CPU will use all of the available thermal headroom right up to that point in order to maximize performance.Although this can be overridden when manually overclocking, none the less the top-end Ryzen 7000 chips call for better cooling than their Ryzen 5000 counterparts. Users will need to employ more premium and aggressive coolers to squeeze every last drop of performance from Zen 4, as most of us are wont to do. AMD for their part has accounted for all of this with their design choices and product marketing, clearly advising Ryzen 9 79x0 owners to use a liquid cooler with these chips. Still, this does mean that AMD is not bundling their own CPU coolers with their retail SKUs, instead directing buyers to fairly powerful third-party coolers.New AM5 Socket: AM4 Coolers will Support AM5 TooAMD has also transitioned to a new platform for Ryzen 7000, named AM5. Along with AM5 also comes a new socket, the titular socket AM5, a LGA-1718 socket that is AMD's first use of the LGA form factor for mainstream desktop CPUs. Now what’s interesting is AMD has specified that most AM4 coolers will support the new AM5 socket, which is great for keeping compatibility with existing coolers.This also means that AM4 is slowly on its way to becoming a thing of the past. While AMD is still (many) months away from replacing their complete Ryzen 4000/5000 stack with Ryzen 7000 parts, today is the first day and the first step to doing so. None the less, AM4 does offer some incredible deals right now (e.g. 5800X3D), as well as support for cheaper DDR4 memory. This sits in contrast to the AM5 platform, which is entirely DDR5-only. Though when it comes to memory AMD does have a small advantage over Intel; whereas Intel's 12th Gen Core chips only support a maximum (JEDEC) speed of DDR5-4800, the Ryzen 7000 chips are officially rated for DDR5-5200.To go with the new AM5 platform and provide motherboards for their new CPUs, AMD has unveiled four(ish) new chipsets. These are the B650 and X670, as well as their \"Extreme\" variations, the B650E and X670E. The top-end X670E series will feature both PCIe 5.0 lanes to the top PEG slot and support for PCIe 5.0 NVMe storage devices. The regular X670 chipset, on the other hand, foregoes the mandatory PCIe 5.0 speeds for the PEG slot in favor of easier-to-implement PCIe 4.0. In either case, both versions of X670 are intended to offer a plethora of I/O options, and in keeping with the general tiered structure of AMD's AM5 chipsets, X670 boards will generally offer better designs, better controllers, and better specifications.The B650 chipsets, meanwhile, are designed to be more affordable, doing away with some of the I/O lanes and overall I/O flexibility the X670 chipsets enjoy. Like the Extreme X670, the B650E is intended for boards that will offer PCIe 5.0 to the PEG slot and NVMe storage. Otherwise, the lowest-tier B650 chipset dials that back to PCIe 4.0 for the PEG slot as well.For this week's launch, only the X670/X670E boards will be available. Buyers looking for the cheaper B650/B650E boards will need to hold out until October.New I/O Die: TSMC 6nm For Ryzen 7000Last, as has been the case for the last couple of Ryzen desktop generations, for the Ryzen 7000 series AMD is constructing their CPUs out of chiplets. All Ryzen 7000 desktop chips are built from an I/O Die (IOD) as well as either one or two core complex dies (CCDs) depending on the SKU. The IOD hosts all of the PCIe 5.0 lanes, the DDR5 integrated memory controller (IMC), and new for Ryzen 7000, an integrated GPU based on AMD's Radeon RDNA2 GPU architecture. All things considered, the IOD used for the Ryzen 7000 is a pretty significant overhaul compared to AMD's previous IOD, with AMD implementing several new performance and power-saving features, as well as further cutting down on power consumption thanks to TSMC's 6nm process.It’s time to dive deep into all of AMD’s new improvements and changes for its Zen 4 microarchitecture. Over the following pages we’ll, be going over the following:Ryzen 7000 Overview: Comparing Ryzen 7000 to Ryzen 5000 specificationsSocket AM5: The New Platform For Consumer AMDMore I/O For AM5: PCIe 5, Additional PCIe Lanes, & More DisplaysAM5 Chipsets: X670 and B650, Built by ASMediaDDR5 & AMD EXPO Memory: Memory Overclocking, AMD’s WayRyzen 7000 I/O Die: TSMC & Integrated Graphics at LastZen 4 Architecture: Power Efficiency, Performance, & New InstructionsZen 4 Execution Pipeline: Familiar Pipes With More CachingTest Bed and SetupCore-to-Core LatencySPEC2017 Single-Threaded ResultsSPEC2017 Multi-Threaded ResultsCPU Benchmark Performance: Power, Web, & ScienceCPU Benchmark Performance: Simulation and EncodingCPU Benchmark Performance: RenderingCPU Benchmark Performance: Legacy TestsGaming Performance: 720p and LowerGaming Performance: 1080pGaming Performance: 4KConclusion\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17585/amd-zen-4-ryzen-9-7950x-and-ryzen-5-7600x-review-retaking-the-high-end\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NVIDIA Drops DRIVE Atlan SoC, Introduces 2 PFLOPS DRIVE Thor for 2025 Autos\n",
      "Author: Ryan Smith\n",
      "Date Published: 2022-09-20T15:45:00Z\n",
      "URL: https://www.anandtech.com/show/17582/nvidia-drops-drive-atlan-soc-introduces-2-pflops-drive-thor-for-2025-autos\n",
      "Content: Among the spate of announcements from NVIDIA today as part of their fall GTC 2022 event, the company is delivering a surprising shake-up to their DRIVE automotive SoC plans. Effective immediately, NVIDIA has cancelled Atlan, their planned post-Orin SoC for 2025 automobiles. In its place, NVIDIA is announcing Thor, an even more powerful SoC set to launch in the same 2025 timeframe.NVIDIA’s Atlan SoC was first revealed back at spring GTC 2021, where NVIDIA announced it as their next-generation automotive SoC to succeed the (now current) Orin SoC. At the time of its announcement, Atlan was planned to be a high-performance SoC offering 1000 TOPS of INT8 inference performance, using a next-generation (Lovelace) GPU design and next-generation Grace CPU design. The chip even integrated a BlueField DPU as a networking and security processor, with the intent of delivering a single SoC that could handle all of the compute functions required by a self-driving car.Atlan-that-wasBut whatever Atlan was meant to be, now it’s no more. As of NVIDIA new DRIVE SoC roadmap, Atlan has been scrapped. In its place is a new SoC, Thor, which is slated to be even more powerful than what NVIDIA had planned for Atlan.Like the Atlan announcement in 2021, NVIDIA is only releasing a handful of details about Thor this far ahead of its release. The high-level details include that, while not naming specific NVIDIA CPU and GPU architectures, the SoC is tapping features that were first introduced with the Grace CPU, Ampere GPU architecture, and Lovelace GPU architecture. Meanwhile, NVIDIA’s blog post on the matter does go one further, stating that the SoC uses an Automotive Enhanced (AE) version of Arm’s thus far secretive Poseidon CPU core. Of what little we know about Poseidon, it is a next-generation high-performance CPU core being developed by Arm that will be used in their next-generation Neoverse V-series platform, replacing the just-releasedNeoverse V2.NVIDIA ARM SoC Specification ComparisonThorAtlanOrinCPU CoresArm \"Poseidon AE\"Grace-Next12x Arm CA-78AE \"Hercules\"GPU CoresYou BetAmpere-Next iGPU(Lovelace)Ampere iGPU(2048 CUDA Cores64 Tensor Cores )FP8/INT8 DL OPS2000 TOPS (FP8)1000 TOPS (INT8)275 TOPS (INT8)Manufacturing Process??Samsung 8nmTransistors77B?17BTDP??60WFrom a performance standpoint, Thor is slated to deliver 2 PFLOPS (2000 TFLOPS) of floating point inference performance using thenewly-standardized FP8 data format. Which, although not apples-to-apples with Atlan’s 1000 TFLOPS INT8 figure, still represents twice the throughput for 8-bit precision computing. The SoC’s tensor cores will also feature NVIDIA’s transformer engines, allowing the SoC to further accelerate the processing of transformer networks.Notably, packing in all of this performance is going to make Thor a very bulky chip. While NVIDIA is not announcing the process node, they are already saying that it will use 77 billion transistors, which is all of 3 billion transistors less than their new flagship GH100 GPU. NVIDIA’s performance claims don’t indicate whether matrix sparsity is in use, but even if it is that will put Thor at half the FP8 performance as NVIDIA’s flagship GPU. All of which underscores NVIDIA’s extreme performance goals for the planned SoC.And while NVIDIA’s mockup for the chip shows it on an AGX board in a single chip configuration, today’s announcement also makes explicit mention of NVLink chip-to-chip (NVLink-C2C) chip interconnect technology. This is a curious mention, since NVIDIA’s key art doesn’t show Thor as being chiplet-based. This may mean that NVIDIA is instead going to use NVLink-C2C for even more powerful multi-chip DRIVE AGX boards (ala Pegasus), or it could very well be that Thor is a chiplet-based design and NVIDIA is being purposely generic with their art.Beyond that, NVIDIA is not offered any further technical details on the SoC. So details on the memory type used, the GPU architecture, and other functional blocks remains to be seen.At this point, NVIDIA has also not detailed why they cancelled Atlan in place of Thor. Thor is certainly a more powerful design and would seem to incorporate some newer features not found on (or at least never disclosed for) Atlan. Whether this means NVIDIA is somehow pulling in what would have been a post-Atlan chip or if they have tossed Atlan because their customers need better AI inference performance for their self-driving cars remains to be seen.Hardware upgrades aside, what is clear is that NVIDIA is designing Thor for the same market segment as Atlan. Which is to say it’s meant to be a high-performance single-chip design for handling all of the computational needs of a self-driving car, from infotainment systems and sensor fusion up to the actual self-driving algorithms themselves. As with Atlan, the goal is to replace what are currently separate computers within a car with a single computer that does it all, leveraging the functional safety design techniques with extensive isolation (including MIG) to keep the separate tasks from interfering with each other.Perhaps the most surprising thing of all, however, is that this change in SoCs is not expected to impact NVIDIA’s SoC delivery date. NVIDIA says that they’ll have Thor in automakers’ hands for their 2025 cars, the same timeframe that Atlan was intended to arrive in. So while the devil is in the details, at a high level NVIDIA is aiming to deliver Thor near the same time as they would have delivered Atlan. It is worth noting, however, that while NVIDIA had previously announced that Atlan would be sampling in 2023, no such announcement has been made about Thor. As such, Thor’s sampling date may end up being later than what would have been Atlan’s.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17582/nvidia-drops-drive-atlan-soc-introduces-2-pflops-drive-thor-for-2025-autos\n",
      "Title: Arm Announces Neoverse V2 and E2: The Next Generation of Arm Server CPU Cores\n",
      "Author: Ryan Smith\n",
      "Date Published: 2022-09-15T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/17575/arm-announces-neoverse-v2-and-e2-the-next-generation-of-arm-server-cpu-cores\n",
      "Content: Just under four years ago, Arm announced their Neoverse family of infrastructure CPU designs. Deciding to double-down on the server and edge computing markets by designing Arm CPU cores specifically for those markets – and not just recycling the consumer-focused Cortex-A designs – Arm set about tackling the infrastructure market in a far more aggressive manner. Those efforts, in turn, have increasingly paid off handsomely for Arm and its partners, whom thanks to the likes of products like Amazon’s Graviton and Ampere Altra CPUs have at long last been able take a meaningful piece of the server CPU market.But as Arm CPUs finally achieve the market penetration that eluded them in the previous decade, Arm needs to make sure it isn’t resting on its laurels. Of the company’s three lines of Neoverse core designs –the efficient E, flexible N, and high-performance V – the company is already on its second generation of N cores, aptly dubbed theN2. Now, the company is preparing to update the rest of the Neoverse lineup with the next generation of V and E cores as well, announcing today the Neoverse V2 and Neoverse E2 cores. Both of these designs are slated to bring the Armv9 architecture to HPC and other server customers, as well as significant performance improvements.Arm Neoverse V2: Armv9 Graces High-Performance ComputingLeading the charge for Arm’s new CPU core IP is the company’s second-generation V-series design, the Neoverse V2. The complete V2 platform, codenamed Demeter, marks Arm’s first iteration on their high-performanceV-series cores, as well as the transition of this core lineup from the Armv8.4 ISA to Armv9. And while this is only Arm’s second go at a dedicated high-performance core for servers, make no mistake: Arm aims to be ambitious. The company is claiming that Neoverse V2 CPUs will offer the highest single-threaded integer performance available in the market, eclipsing next-generation designs from both AMD and Intel.While this week’s announcement from Arm is not a full-on deep-dive of the new architecture – and, more annoyingly, the company is not talking about specific PPA metrics – Arm is offering a high-level look at some of the changes and features that will be coming with the V2 platform. To be sure, the V2 IP is already finished and shipping to customers today (most notably NVIDIA), but Arm is playing coy to some degree with what they’re saying about V2 before the first chips based on the IP ship in 2023.First and foremost, the bump toArmv9brings with it the full suite of features that come with the latest Arm architecture. That includes the security improvements that are a cornerstone feature of the architecture (and especially handy for cloud shared environments) along with Arm’s newer SVE2 vector extensions.On the latter, Arm is making an interesting change here by reconfiguring the width of their vector engines; whereas V1 implemented SVE(1) using a 2 pipeline 256-bit SIMD, V2 moves to 4 pipes of 128-bit SIMDs. The net result is that the cumulative SIMD width of the V2 is not any wider than V1, but the execution flow has changed to process a larger number of smaller vectors in parallel. This change makes the SIMD pipeline width identical to Arm’s Cortex parts (which are all 128-bit, the minimum size for SVE2), but it does mean that Arm is no longer taking full advantage of thescalablepart of SVE by using larger SIMDs. I expect we’ll find out why Arm is taking this route once they do a full V2 deep dive, as I’m curious whether this is purely an efficiency play or something more akin to homogenizing designs across the Arm ecosystem.Past that, it’s likely worth noting that while Arm’s presentation slides put bfloat16 and int8 matmul down as features, these are notnewfeatures. Still, Arm is promising that V2’s SIMD processing will provide microarchitecture efficiency improvements over the V1.More broadly, V2 will also be introducing larger L2 cache sizes. The V2 design supports up to 2MB of private L2 cache per core, double the maximum size of V1. V2 will also be introducing further improvements to Arm’s integer processing performance, though the company isn’t going into further detail at this point. From an architectural standpoint, the V1 borrowed a fair bit from the Cortex-X1 CPU design, and it wouldn’t be too surprising if that was once again the case for the V2, borrowing from the X2. In which case consumer chips like the Snapdragon 8 Gen1 and Dimensity 9000 should provide a loose reference on what to expect.For the Demeter platform Arm will be reusing theirCMN-700 mesh fabric, which was first introduced for the V1 generation. CMN-700 is still a modern mesh design with support for up to 144 nodes in a 12x12 configuration, and is suitable for interfacing with DDR5 memory as well as PCIe 5/CXL 2 for I/O. As a result, strictly speaking the V2 isn’t bringing anything new at the fabric level – even the 512MB of SLC could be done with a V1 + CMN-700 setup – but this does mean that the CMN-700 mesh and its features is now a baseline moving forward with V2.The Neoverse V2 core, in turn, is going to be the cornerstone of the upcoming generation of high-performance Arm server CPUs. The de facto flagship here will beNVIDIA’s Grace CPU, which will be one of the first (if not the first) V2 design to ship in 2023. NVIDIA had previously announced that Grace would be based on a Neoverse design, so this week’s announcement from Arm finally confirms the long-held suspicion that Grace would be based on the next-generation Neoverse V core.NVIDIA, for its part, has their fall GTC event scheduled to take place in just a few days. So it’s likely we’ll hear a bit more about Grace and its Neoverse V2 underpinnings as NVIDIA seeks to promote the chip ahead of its release next year.Neoverse E2: Cortex-A510 For Use With N2Alongside the Neoverse V2 announcement, Arm is also using this week’s briefing to announce the Neoverse E2 platform. Unlike the V2 reveal, this is a much smaller scale announcement, and Arm is only offering a handful of technical details. Ultimately, E2’s day in the sun will be coming a bit later on.That said, the E2 platform is being delivered to partners with an eye towards interoperability with the existing N2 platform. For this, Arm has paired the Cortex-A510 CPU, Arm’s little/high-efficiency Cortex CPU core, and paired that with the CMN-700 mesh. This is intended to give server operators/vendors further flexibility by providing an alternative CPU core to the N2, while still offering the modern I/O and memory features of Arm’s mesh. Underscoring this, the E2 system backplane is even compatible with the N2 backplane.Neoverse Next: Poseidon, N-Next, and E-NextFinally, Arm’s announcement this week provides a glimpse at the company’s future roadmap for all three Neoverse platforms, where, unsurprisingly, Arm is working on updated versions of each of the platforms.Notably, all three platforms call for adding PCIe 6 support as well asCXL 3.0support. This would come from the next iteration of Arm’s CMN mesh network, which as Arm already does today, is shared between all three platforms.Meanwhile, it’s interesting to see the Poseidon name once again pop up in Arm’s roadmaps. Going back to Arm’svery first Neoverse roadmap, Poseidon was the name attached to Arm’s 5mn/2021 platform, a spot since taken by N2 and V1/V2 in various forms. With V2 not landing in hardware until 2023, Poseidon/V3 is still years off, but there’s likely some significance to Arm keeping the codename (such as new microarchitecture).But first out of the gate will be the N-Next platform – the presumable Neoverse N3. With the Neoverse N platform a generation ahead of the rest (N2 was first announced in 2020), it’ll be the next platform due for a refresh. N3 is due to be available to partners in 2023, with Arm broadly touting generational performance and efficiency improvements.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17575/arm-announces-neoverse-v2-and-e2-the-next-generation-of-arm-server-cpu-cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qualcomm Unveils Snapdragon 6 Gen 1 and 4 Gen 1 SoCs: Updating Mid-Range and Entry-Level Phones\n",
      "Author: Ryan Smith\n",
      "Date Published: 2022-09-06T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/17560/qualcomm-unveils-snapdragon-6-gen-1-and-4-gen-1-socs\n",
      "Content: Qualcomm this morning is taking the wraps off a pair of new SoCs for the mid-range and entry-level smartphone markets. Refreshing the company’s longstanding 600 and 400 series of chips, Qualcomm is announcing the Snapdragon 6 Gen 1 and Snapdragon 4 Gen 1. Both SoCs are receiving similar spec bumps, incorporating newer and faster IP blocks from Qualcomm – such as Arm Cortex-A78 derived CPU cores – as well as moving to newer, more contemporary manufacturing processes.The Snapdragon 600/400 lineups were last updated in mid and early 2021 respectively, so as Qualcomm is already preparing for 2023, the time has finally come to update the bottom half of their product stack. Following Qualcomm’s broad cascading IP strategy, this generation of parts sees both SoC lineups align around Cortex-A78 CPUs for their main CPU cores, and in the case of the 6 Gen 1, doubling the number of high-performance CPU cores. Both SoCs also come with faster Adreno GPUs, though in traditional Qualcomm fashion, the company isn’t offering much in the way of details on the underlying hardware there.Notably, however, Qualcomm’s 2023 mid-range/low-end parts aren’t making the jump to the Armv9 architecture. Unlike the 8 Gen 1 and 7 Gen 1, which incorporated Arm’s new Armv9 cores, Qualcomm’s cascading development strategy means that the 6 and 4 series will remain a bit farther behind the curve. For end users this should have little significance for the moment, but for smartphone vendors and software developers, it does mean Qualcomm won’t complete the Armv9 transition for at least another generation.Meanwhile, coming up on nearly a year since Qualcomm announced their initialSnapdragon 8 Gen 1 SoC, today’s announcement from Qualcomm brings their remaining smartphone SoC families in alignment with their new product branding strategy. The 6 and 4 series pick up from where the 600 and 400 series left off, respectively, resetting the counted with the inaugural Gen 1 parts. Like the rest of simplified “Gen” series, this also means that Qualcomm is doing away with individual model numbers for its Kyro/Hexagon/Adreno/Spectra blocks, obfuscating a bit what generation of IP Qualcomm is using there.Snapdragon 6 Gen 1Starting things off, we have the Snapdragon 6 Gen 1. Built on an unidentified 4nm process (Qualcomm hasn’t specified whether it’s Samsung or TSMC, though the former is more likely), it replaces the outgoing Snapdragon 695. And in the process, delivers some major updates to the CPU, camera, memory, and modem blocks.Qualcomm Snapdragon 6-Series SoCsSoCSnapdragon 6 Gen 1Snapdragon 695Snapdragon 690Snapdragon 675CPU4x CA78@ 2.2GHz4x CA55@ 1.8GHz2x Kryo 660 (CA78)@ 2.2GHz6x Kryo 660 (CA55)@ 1.7GHz2x Kryo 560 (CA77)@ 2.0GHz6x Kryo 560 (CA55)@ 1.7GHz2x Kryo 460 (CA76)@ 2.0GHz6x Kryo 460 (CA55)@ 1.7GHzGPUAdrenoAdreno 619Adreno 619LAdreno 612DSPHexagonHexagon 686Hexagon 692Hexagon 685ISP/CameraSpectra(3x 12-bit)1x 108MP or 48MP with ZSLor25+16MP with ZSLor3x 13MP with ZSL4K HDR videoSpectra 346T(3x 12-bit)1x 108MP or 32MP with ZSLor25+13MP with ZSLor3x 13MP with ZSLSpectra 355L(2x 14-bit)1x 192MPor32+16MP with ZSLSpectra 250L(2x 14-bit)1x 192MPor16+16MP with ZSLEncode/Decode2160p30H.264, H.265, VP9 (Decode-Only)HLG1080p60H.264 & H.265Memory2x 16-bit @ 2750MHzLPDDR522.0GB/s2x 16-bit @ 2133MHzLPDDR4X17.1GB/s2x 16-bit @ 1866MHzLPDDR4X14.9GB/sIntegrated ModemX62 IntegratedLTE5G NRSub-6 + mmWave140MHz + 200MHzDL = 2900 MbpsX51 IntegratedLTEDL = 800 MbpsUL = 210 Mbps5G NRSub-6 + mmWaveDL = 2500 MbpsUL = 1500 MbpsX51 IntegratedLTEDL = 1200 MbpsUL = 210 Mbps5G NRSub-6DL = 2500 MbpsUL = 900 MbpsX12 LTE Integrated(Category 12/13)DL = 600Mbps3x20MHz CA, 256-QAMUL = 150Mbps2x20MHz CA, 64-QAMWi-Fi/BTWi-Fi 6E (2x2)Bluetooth 5.2Wi-Fi 5 (2x2)Bluetooth 5.2Wi-Fi 5 (2x2)Bluetooth 5.1Wi-Fi 5 (2x2)Bluetooth 5.0Mfc. Process4nmTSMC 6nm8nm LPP11nm LPPThe biggest news this year is arguably that Qualcomm has rearranged the CPU core configuration for their mid-range SoC lineup. Whereas previous 6-series used a 2+6 configuration with two high-performance (Cortex-A7x) cores and 6 high-efficiency (Cortex-A5x) cores, 6 Gen 1 shifts this over to a 4+4 configuration, very similar to what Qualcomm already does for the 8 and 7 series. The net result is that the 6 Gen 1 should offer significantly better multi-threaded CPU performance than its predecessor, with Qualcomm touting a 40% improvement. Meanwhile single-threaded performance should remain largely unchanged, as Qualcomm is using the same Cortex-A78s at the same 2.2GHz peak frequency as was found in the Snapdragon 965. Otherwise, it will be interesting to see what the larger number of performance cores means for the 6 Gen 1’s battery life, as these cores are going to be (relatively) more power hungry.On the GPU side of matters, Qualcomm isn’t saying much about the Adreno GPU block used here. With that said, the explicit mention of variable rate shading support does imply that it’s from a newer generation of Qualcomm’s GPU IP, as the feature was not available on the Snapdragon 695. Qualcomm’s claims of an (up to) 35% GPU performance improvement also hint at a more significant update to their GPU.Feeding the SoC is an LPDDR5-capable memory controller, bringing that down to a 6-series chip for the first time. The maximum frequency supported here is slightly lower than Qualcomm’s higher-end SoCs, at LPDDR5-5500, which combined with the 32-bit memory bus nets a total of 22GB/second of memory bandwidth. This works out to a roughly 29% increase in memory bandwidth.Qualcomm has also updated their Hexagon DSP/AI block for their latest 6-series chip. The unnamed version of the IP is one of Qualcomm’s “7thgeneration” designs, meaning that along with vector and scalar processing, it also includes tensor processing capabilities that are chiefly aimed at speeding up AI inference.As for the camera/ISP, Qualcomm is still using a triple 12-bit ISP here, with some modest performance improvements in terms of pixel throughput. The big news here is the (re)introduction of HDR support for the 6-series, with Qualcomm supporting computational HDR on this class of SoC for the first time. As a result the 6 Gen 1 can handle 4K HDR video recording at up to 30fps.Last but not least, Qualcomm has also significantly upgraded the wireless capabilities of the SoC, both for cellular and Wi-Fi communications. On the cellular side of matters, the chip integrates a Snapdragon X62-class modem, supporting both sub-6 and mmWave bands. Qualcomm isn’t providing a detailed breakout of transfer speeds, but on paper the modem can achieve downloads as fast as 2.9Gbps over 5G. Meanwhile the Wi-Fi radio has undergone a long-overdue upgrade, replacing Qualcomm’s dated Wi-Fi 5 FastConnect 6200 radio with a more modern Wi-Fi 6E radio based on their FastConnect 6700 IP.Of today’s two SoC announcements, the 6 Gen 1 is the more forward looking of the two. According to Qualcomm, we should see handsets using the new SoC show up in the first quarter of 2023, approximately four to six months from now.Snapdragon 4 Gen 1Meanwhile, bringing up the entry-level segment of Qualcomm’s SoCs is the new Snapdragon 4 Gen 1. Unlike its 6 Gen 1 counterpart, Qualcomm isn’t as extensively updating this chip, but the 4 Gen 1 should still deliver some modest improvements over its predecessor, the Snapdragon 480.Qualcomm Snapdragon 4-Series SoCsSoCSnapdragon 4 Gen 1Snapdragon 480Snapdragon 460CPU2x CA78@ 2.0GHz6x CA55@ 1.8GHz2x CA76@ 2.0GHz6x CA55@ 1.8GHz4x Kryo 240 (CA73)@ 1.8GHz4x Kryo 240 (CA53)@ 1.8GHzGPUAdrenoAdreno 619Adreno 610DSPHexagonHexagon 686Hexagon 683ISP/CameraSpectra(3x 12-bit)1x 108MP or 32MP with ZSLor25+13MP with ZSLor3x 13MP with ZSLSpectra 345(3x 12-bit)1x 64MPor25+13MPor3x 13MPSpectra 340(2x 14-bit)1x 48MPor2x 22MP dualEncode/Decode1080p60H.264, H.265, VP9 (Decode-Only)Memory2x 16-bit @ 2133MHzLPDDR4X17.0GB/s2x 16-bit @ 2133MHzLPDDR4X17.0GB/s2x 16-bit @ 1866MHzLPDDR4X14.9GB/sIntegrated ModemX51 IntegratedLTEDL = 800MbpsUL = 210Mbps5G NRSub-6 + mmWave(100MHz)DL = 2500MbpsUL = 900MbpsX51 IntegratedLTEDL = 800MbpsUL = 210Mbps5G NRSub-6 + mmWave(100 + 200MHz)DL = 2500MbpsUL = 660MbpsX11 LTE Integrated(Cat 12/13)DL = 390Mbps2x20MHz CA, 256-QAMUL = 150Mbps2x20MHz CA, 64-QAMWi-Fi/BTWi-Fi 5 (2x2)Bluetooth 5.2Wi-Fi 5 (2x2)Bluetooth 5.1Wi-Fi 5 (1x1)Bluetooth 5.1Mfc. ProcessTSMC 6nm8nm LPP11nm LPPOn the CPU front, the 4 Gen 1 is getting the same Cortex-A78 upgrade as the 6 Gen 1, pairing the cores with Cortex-A55 cores in a 2+6 configuration. In terms of architectural improvements this is actually a bigger step up here than it was for the 6 Gen 1, as the Snapdragon 4 series previous used Cortex-A76 cores here. Still, with CPU frequencies still capped at 2.0GHz, all of the performance gains will have to come sole from the CPU architectural improvements. Overall, Qualcomm is touting a 15% improvement in CPU performance.As for the GPU, the 4 Gen 1 gets an unnamed Adreno GPU block. This is again a more modest upgrade, with Qualcomm promoting an (up to) 10% increase in GPU performance. But as Qualcomm isn’t upgrading the memory controller – the 4 Gen 1 still uses LPDDR4X-4266 memory on a 32-bit bus – there aren’t any memory bandwidth improvements to feed a significantly faster GPU anyhow.The camera/ISP block for the new 4-series SoC is also quite similar to its predecessor, with Qualcomm employing a triple 12-bit ISP. The notable improvement here is that Qualcomm has significantly increased the maximum photo resolution to 108MP, almost doubling the number of pixels that can be captured in a single shot, and in the process matching what the more powerful 6-series SoCs can do.Meanwhile, in a sign that mmWave adoption in handsets isn’t as high as Qualcomm initially planned, the 4 Gen 1’s wireless suite ends up being a regression of sorts.The 5G Snapdragon X51 modem ends up dropping mmWave support, leaving just support for the sub-6 bands.Update (09/06):Qualcomm has sent a note over this afternoon stating that while mmWave does not appear in the spec sheets for Snapdragon 4 Gen 1, the hardware does still support the tech. So mmWave is available to any handset manufacturers that wished to support it. With that said, given the low prices expected by the intended market, handset manufacturers are likely to have little appetite for including the necessary mmWave antenna modules – and is likely why Qualcomm didn't bother to list it as a feature.But even without mmWave appearing in these budget phones, this is enough to drive download speeds of up to 2.5Gbps, and supported upload speeds are now as high as 900Mbps. The Wi-Fi radio remains similarly unchanged, with Qualcomm using a Wi-Fi 5 + Bluetooth 5.2 radio based on their FastConnect 6200 IP.Finally, the Snapdragon 4 Gen 1 is being fabbed on TSMC’s 6nm process, making it the latest Qualcomm SoC line to be shifted over to TSMC from rival Samsung. According to Qualcomm, handsets based on the SoC will be shipping imminently, with devices slated to become available in the remainder of this quarter.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17560/qualcomm-unveils-snapdragon-6-gen-1-and-4-gen-1-socs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Akasa AK-ENU3M2-07 USB 3.2 Gen 2x2 SSD Enclosure Review: 20Gbps with Excellent Thermals\n",
      "Author: Ganesh T S\n",
      "Date Published: 2022-08-01T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/17508/akasa-akenu3m207-usb-32-gen-2x2-ssd-enclosure-review-20gbps-with-excellent-thermals\n",
      "Content: Storage bridges have become an ubiquitous part of today's computing ecosystems. The bridges may be external or internal, with the former ones enabling a range of direct-attached storage (DAS) units. These may range from thumb drives using an UFD controller to full-blown RAID towers carrying Infiniband and Thunderbolt links. From a bus-powered DAS viewpoint, Thunderbolt has been restricted to premium devices, but the variants of USB 3.2 have emerged as mass-market high-performance alternatives. USB 3.2 Gen 2x2 enables the highest performance class (up to 20 Gbps) in USB devices without resorting to PCIe tunneling.The last couple of years have seen many vendors introduce new products in this 20 Gbps-performance class - including portable SSDs and M.2 NVMe SSD enclosures. Host support has also started to look up. The key challenges for enclosures and portable SSDs supporting USB 3.2 Gen 2x2 include handling power consumption and managing thermals. We have been analyzing these aspects (in addition to regular performance numbers) in our reviews of the 20Gbps-performance class PSSDs and enclosures. The review below presents our evaluation report of Akasa's AK-ENU3M2-07 - a USB 3.2 Gen 2x2 enclosure for M.2 NVMe SSDs.Introduction and Product ImpressionsSSD speeds and storage capacity have improved significantly in the last decade, thanks to rapid advancements in flash technology as well as high-speed interfaces / protocols. Starting from 2.5-inch disk drives barely able to saturate the SATA III (6 Gbps) interface in the early 2010s, we now have gumstick- and palm-sized drives with PCIe 4.0 support capable of sustaining more than 7000 MBps (56 Gbps).These SSDs have also formed the base platform for portable SSDs. Traditionally, such drives have fallen into one of the six categories below, depending on the performance profile and internal components. Recently, we have seen direct flash-to-USB controllers across all but the highest performance tier listed here.2.5GBps+ class: Thunderbolt SSDs with PCIe 3.0 x4 NVMe drives2GBps+ class: USB 3.2 Gen 2x2 SSDs with PCIe 3.0 x4 NVMe drives1GBps+ class: USB 3.2 Gen 2 SSDs with PCIe 3.0 (x4 or x2) NVMe drives500MBps+ class: USB 3.2 Gen 2 SSDs with SATA drives400MBps+ class: USB 3.2 Gen 1 SSDs with SATA drivesSub-400MBps+ class: USB 3.2 Gen 1 flash drives with direct flash-to-USB controllersIn addition to portable SSDs, this type of segmentation is also applicable to storage enclosures. Since the mid-2010s, we have seen a regular stream of SSD enclosures hit the market, catering to 2.5\", mSATA, and M.2 form-factors.Akasais a well-known manufacturer of thermal solutions for computing systems targeting industrial applications as well as home consumers. They have been maintaining a lineup of storage bridge products catering to different market segments since 2013. Werevieweda bunch of their M.2 SATA and NVMe enclosures last year, and came away impressed with their comprehensive lineup addressing different requirements. This review looks at theAK-ENU3M2-07, an aluminum enclosure sporting a USB 3.2 Gen 2x2 20Gbps Type-C upstream interface and a M.2 2230/42/60/80 NVMe downstream port internally.There are currently two shipping device solutions for USB 3.2 Gen 2x2 - the Silicon Motion SM2320 used in portable SSDs like the Kingston XS2000 is a native UFD controller, while the ASMedia ASM2364 is a bridge solution more suitable for use in enclosures. The AK-ENU3M2-07 uses the latter. The enclosure itself is made of solid aluminum with ridges to aid in heat dissipation. It comes with a single Type-C to Type-C cable rated for 20Gbps operation. A single thermal gap filler is supplied in the package along with a carrying pouch and a user manual.One of the attractive aspects of the AK-ENU3M2-07 is its tool-free nature. Accessing the internal board for SSD installation is a simple matter of loosening up the two screws on either side of the Type-C port. They are big enough to unscrew without the use of any tools. This allows the bottom panel to be slid out. The board itself is affixed to this panel, and doesn't need to be taken out for any purpose. A plastic tab to hold the M.2 SSD in place is affixed to the 2280 hole by default. Rotating this tab along the notch allows the SSD to be placed in and locked in place. Without the SSD in the picture, further rotation to make the longer arm of the tab parallel to the rear panel allows it to be completely taken out (and affixed to one of the other holes corresponding to 30mm, 40mm, or 60mm SSD lengths). After the installation of the SSD, the thermal gap filler can be placed on top. The gallery below provides pictures of the enclosure as well as the SSD installation steps.Gallery:Akasa AK-ENU3M2-07 : Case Design & SSD InstallationWe evaluate M.2 NVMe storage enclosures using the SK hynix Gold P31 1TB NVMe SSD. Since this SSD is used in all the relevant reviews, it makes for an apples-to-apples comparison across different products.The table below presents a comparative view of the specifications of the different storage bridges and PSSDs presented in this review.Comparative Storage Bridges ConfigurationAspectAkasa AK-ENU3M2-07Yottamaster HC2Seagate FireCuda 1TBWD_BLACK P50 1TBSilverstone MS12Kingston XS2000 1TBYottamaster HC2Seagate FireCuda 1TBWD_BLACK P50 1TBSilverstone MS12Kingston XS2000 1TBAkasa AK-ENU3M2-07Downstream Port1x PCIe 3.0 x4 (M.2 NVMe)1x PCIe 3.0 x4 (M.2 NVMe)Upstream PortUSB 3.2 Gen 2x2 Type-CUSB 3.2 Gen 2x2 Type-CBridge ChipASMedia ASM2364ASMedia ASM2364PowerBus PoweredBus PoweredUse CaseTool-free M.2 2230 / 2242 / 2260 / 2280 NVMe SSD enclosureDIY 2GBps-class, compact, and sturdy portable SSD with a gumstick form-factorM.2 2230 / 2242 / 2260 / 2280 NVMe SSD enclosureDIY 2GBps-class, compact, and sturdy portable SSD with a USB flash drive-like form-factorPhysical Dimensions122 mm x 46 mm x 15 mm105 mm x 40 mm x 12 mmWeight112 grams (without cable / SSD / thermal pad)60 grams (without cable / SSD ; with thermal pads)Cable29 cm USB 3.2 Gen 2x2 Type-C to Type-C16 cm USB 3.2 Gen 2x2 Type-C to Type-C16 cmd USB 3.2 Gen 2 Type-C to Type-AS.M.A.R.T PassthroughYesYesUASP SupportYesYesTRIM PassthroughYesYesHardware EncryptionSSD-dependentSSD-dependentEvaluated StorageSK hynix P31 PCIe 3.0 x4 NVMe SSDSK hynix 128L 3D TLCSK hynix P31 PCIe 3.0 x4 NVMe SSDSK hynix 128L 3D TLCPriceGBP 69USD 139Review LinkAkasa AK-ENU3M2-07 ReviewYottamaster HC2-C3 ReviewThe key aspect that stands out is how heavy the AK-ENU3M2-07 is, compared to other enclosures using the same bridge chip. While the Akasa enclosure is 112g, the Silverstone MS12 is just 53g, and the Yottamaster HC2 is 60g. This gives the enclosure a higher thermal mass to cool down the SSD inside, and should potentially result in better thermal performance. Prior to looking at the benchmark numbers, power consumption, and thermal solution effectiveness, a description of the testbed setup and evaluation methodology is provided.Testbed Setup and Evaluation MethodologyDirect-attached storage devices are evaluated using the Quartz Canyon NUC (essentially, the Xeon / ECC version of theGhost Canyon NUC) configured with2x 16GB DDR4-2667 ECC SODIMMsand a PCIe 3.0 x4 NVMe SSD - theIM2P33E8 1TBfrom ADATA.The most attractive aspect of the Quartz Canyon NUC is the presence of two PCIe slots (electrically, x16 and x4) for add-in cards. In the absence of a discrete GPU - for which there is no need in a DAS testbed - both slots are available. In fact, we also added a spare SanDisk Extreme PRO M.2 NVMe SSD to the CPU direct-attached M.2 22110 slot in the baseboard in order to avoid DMI bottlenecks when evaluating Thunderbolt 3 devices. This still allows for two add-in cards operating at x8 (x16 electrical) and x4 (x4 electrical). Since the Quartz Canyon NUC doesn't have a native USB 3.2 Gen 2x2 port, Silverstone'sSST-ECU06add-in card was installed in the x4 slot. All non-Thunderbolt devices are tested using the Type-C port enabled by the SST-ECU06.The specifications of the testbed are summarized in the table below:The 2021 AnandTech DAS Testbed ConfigurationSystemIntel Quartz Canyon NUC9vXQNXCPUIntel Xeon E-2286MMemoryADATA Industrial AD4B3200716G2232 GB (2x 16GB)DDR4-3200 ECC @ 22-22-22-52OS DriveADATA Industrial IM2P33E8 NVMe 1TBSecondary DriveSanDisk Extreme PRO M.2 NVMe 3D SSD 1TBAdd-on CardSilverStone Tek SST-ECU06 USB 3.2 Gen 2x2 Type-C HostOSWindows 10 Enterprise x64 (21H1)Thanks to ADATA, Intel, and SilverStone Tek for the build componentsThe testbed hardware is only one segment of the evaluation. Over the last few years, the typical direct-attached storage workloads for memory cards have also evolved. High bit-rate 4K videos at 60fps have become quite common, and 8K videos are starting to make an appearance. Game install sizes have also grown steadily even in portable game consoles, thanks to high resolution textures and artwork. Keeping these in mind, our evaluation scheme for direct-attached storage devices involves multiple workloads which are described in detail in the corresponding sections.Synthetic workloads using CrystalDiskMark and ATTOReal-world access traces using PCMark 10's storage benchmarkCustom robocopy workloads reflective of typical DAS usageSequential write stress testIn the next section, we have an overview of the performance of the Akasa AK-ENU3M2-07 enclosure in these benchmarks. Prior to providing concluding remarks, we have some observations on the device's power consumption numbers and thermal solution also.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17508/akasa-akenu3m207-usb-32-gen-2x2-ssd-enclosure-review-20gbps-with-excellent-thermals\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Intel Core i9-12900KS Review: The Best of Intel's Alder Lake, and the Hottest\n",
      "Author: Gavin Bonshor\n",
      "Date Published: 2022-07-29T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/17479/the-intel-core-i9-12900ks-review-the-best-of-intel-s-alder-lake-and-the-hottest\n",
      "Content: As far as top-tier CPU SKUs go, Intel's Core i9-12900KS processor sits in noticeably sharp In contrast to the launch of AMD's Ryzen 7 5800X3D processor with 96 MB of 3D V-Cache. Whereas AMD's over-the-top chip was positioned as the world's fastest gaming processor, for their fastest chip, Intel has kept their focus on trying to beat the competition across the board and across every workload.As the final 12th Generation Core (Alder Lake) desktop offering from Intel, the Core i9-12900KS is unambiguously designed to be the powerful one. It's a \"special edition\" processor, meaning that it's low-volume, high-priced chip aimed at customers who need or want the fastest thing possible, damn the price or the power consumption.It's a strategy that Intel has employed a couple of times now – most notably with theCoffee Lake-generation i9-9900KS– and which has been relatively successful for Intel. And to be sure, the market for such a top-end chip is rather small, but the overall mindshare impact of having the fastest chip on the market is huge. So, with Intel looking to put some distance between itself and AMD's successful Ryzen 5000 family of chips, Intel has put together what is meant to be the final (and fastest) word in Alder Lake CPU performance, shipping a chip with peak (turbo) clockspeeds ramped up to 5.5GHz for its all-important performance cores.For today's review we're putting Alder Lake's fastest to the test, both against Intel's other chips and AMD's flagships. Does this clockspeed-boosted 12900K stand out from the crowd? And are the tradeoffs involved in hitting 5.5GHz worth it for what Intel is positioning as the fastest processor in the world? Let's find out.Below is a list of our detailed Intel Alder Lake and Z690 coverage:Intel Announces Core i9-12900KS: 5.5 GHz Turbo, 5.2 GHz All-Core, Coming April 5thThe Intel 12th Gen Core i9-12900K Review: Hybrid Performance Brings Hybrid ComplexityThe Intel Core i7-12700K and Core i5-12600K Review: High Performance For the Mid-RangeThe Intel Core i3-12300 Review: Quad-Core Alder Lake ShinesIntel Architecture Day 2021: Alder Lake, Golden Cove, and Gracemont DetailedIntel Announces 12th Gen Core Alder Lake: 22 New Desktop-S CPUs, 8 New Laptop-H CPUsThe Intel Z690 Motherboard Overview (DDR5): Over 50+ New ModelsThe Intel Z690 Motherboard Overview (DDR4): Over 30+ New ModelsAs a quick recap, we've covered Alder Lake's dual architectural hybrid design inour Core i9-12900K review, including the differences betweenthe P (performance)andE (efficiency cores). The P-cores are based on Intel's high-performingGolden Cove architecture, which provides solid single-threaded performance. Meanwhile, theGracemont-based E-cores, although lower-performing on their own, are smaller and draw much less power, allowing Intel to pack them in to benefit multi-threaded workloads without blowing the chips' power and thermal budgets.The Intel Core i9-12900KS: The World's Fastest ProcessorNot just content with having a solid selection of premium 12th generation Core series processors, including the then flagship Core i9-12900K,Intel a few months ago unleashed the Core i9-12900KS to the market. Primarily targeted at gamers and enthusiasts who need the very best of performance, the Core i9-12900KS is essentially a high bin version of the Core i9-12900K, but with a few (not so subtle) differences.Based on the same Alder Lake die, both 12900Ks share the same core and thread count (8P+8E). Instead, the most significant difference comes in the speed: the Core i9-12900KS has a whopping 5.5 GHz boost core on its Performance (P) cores, which is 300 MHz higher than the regular K chip. Meanwhile the base frequencies are also improved by 200 MHz higher, leading to a chip that offers 3.4 GHz base and 5.5 GHz boost clockspeeds. That makes the Core i9-12900KS the fastest desktop processor hat Intel has ever created in terms of out-of-the-box core frequency. Even the Efficiency (E) cores have been clocked higher for this SKU, with a 100 MHz bump on both the base and boost frequencies, putting the E cores at 2.5 GHz base and a 4.0 GHz boost.Intel 12th Gen Core i9 SeriesAlder LakeAnandTechCoresP+EE-CoreBaseE-CoreTurboP-CoreBaseP-CoreTurboL3MBiGPU(UHD)BaseWTurboWPrice$1kui9-12900KS8+8250040003400550030770150241$739i9-12900K8+8240039003200520030770125241$589i9-12900KF8+8240039003200520030-125241$564i9-12900F8+8180038002400510030-65202$464i9-129008+818003800240051003077065202$489i9-12900T8+810003600140049003077035106$489The engineering tradeoff to the bump in core frequencies on both the Performance (P) and Efficiency (E) cores is that the Core i9-12900KS draws more power, with a base TDP of 150 W and a boost TDP of up to 241 W. This is an increase of 25 W for the base TDP versus the original Core i9-12900K processor, which from our experience, is already a hot running processor that can draw some serious power when overclocked. And there's room to go higher still – like other K-series chips, the 12900KS is multiplier unlocked, meaning users can attempt to overclock these chips even further.Focusing on price, the Intel Core i9-12900KS is officially priced at $739 in for 1000 chip orders. Street pricing, in turn, is almost spot-on, withAmazonandNeweggcharging $735 each. Officially this puts a $150 premium on the top-tier 12900KS, while comparing street prices it's closer to about a $175 premium right now. This also puts it $235 more expensive when directly compared to AMD's most expensive desktop chip, the Ryzen 9 5950X processor ($499), and just shy of $300 over the Ryzen 7 5800X3D ($439). Suffice it to say, when it comes to 'halo' products such as this, any notions or dreams of value typically go out of the window, and users that are looking for the fastest and greatest going to be expected to dig deep in to their wallets.The Core i9-12900KS hitting 5.5 GHz on P-core 4 and 5, rest of the P cores at 5.2 GHzAnd while there's a significant price difference between the two, make no mistake: the Core i9-12900KS was created to go directly up against AMD's impressive Ryzen 7 5800X3D and its 96 MB of 3D L3 V-Cache. Pitched as the ultimate gaming processor, the Ryzen 7 5800X3D,as we have seen, is very potent in gaming, often vaulting to the top in CPU-limited workloads. Even factoring in the 5.5 GHz performance core boost frequencies, the large pool of L3 cache on the 5800X3D will shine in games that can utilize that extra cache. In the titles where additional cache doesn't influence performance, the insane clock speeds of 5.5 GHz will shine, or so that is the hope for Intel here.The most significant benefit of Intel's Alder Lake special edition chip is that it isn't a one-trick pony; because this SKU is based on across-the-board clockspeed increases, it offers some serious horsepower for computational tasks. This is where we saw the AMD Ryzen 7 5800X3D lag behind the competition, as L3 cache typically doesn't influence the performance of rendering, encoding, and transcoding as pure core grunt does. It will be interesting to see how the Core i9-12900KS stacks up against the i9-12900K in our computational suite while it will be going metaphorically head to head with the Ryzen 7 5800X3D in our gaming suite.Test Bed and SetupAlthough there were some problems initially with the Intel Thread Director when using Windows 10 at the launch of Alder Lake, any core scheduling issues are entirely negated by using the latest Windows 11 operating system. The Intel Thread Director works in tandem with Alder Lake to assign the right P-cores and E-cores to different tasks based on the complexity and severity of the workload. We also test the Core i9-12900KS with DDR5 memory at JEDEC specifications (DDR5-4800 CL40). We are also using Windows 11, which we are using now for CPU and motherboard reviews as we advance into the rest of 2022 and beyond.For our testing, we are using the following:Alder Lake Test System (DDR5)CPUCore i9-12900KS ($735)8+8 Cores, 24 Threads150W Base, 241W TurboMotherboardMSI Z690 Carbon WI-FIMemorySK Hynix2x32 GBDDR5-4800 CL40CoolingASUS ROG Ryujin II 360mm AIOStorageCrucial MX300 1TBPower SupplyCorsair HX850GPUsNVIDIA RTX 2080 Ti, Driver 496.49Operating SystemsWindows 11 Up to DateAll other chips for comparison were run as tests listed in our benchmark database,Bench, on Windows 10.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17479/the-intel-core-i9-12900ks-review-the-best-of-intel-s-alder-lake-and-the-hottest\n",
      "Title: Intel NUC11TNBi5 and Akasa Newton TN Fanless Case Review: Silencing the Tiger\n",
      "Author: Ganesh T S\n",
      "Date Published: 2022-07-22T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/17502/intel-nuc11tnbi5-and-akasa-newton-tn-fanless-case-review-silencing-the-tiger\n",
      "Content: Intel's Tiger Lake-based NUCs have been shipping for well over a year now. Four product lines were planned initially - Panther Canyon for the mainstream market, Tiger Canyon for the professional / business market, Phantom Canyon for gaming enthusiasts, and the Elk Bay Compute Element for embedded applications. Supply chain challenges have been impacting availability of different models in different regions, but that has not prevented Intel's partners from delivering complementary products.Akasais a well-known manufacturer of thermal solutions for computing systems targeting industrial applications as well as home consumers. They have been maintaining a lineup of passively-cooled cases for Intel's NUCs since 2013. We hadreviewedtheir Turing case for the Bean Canyon NUC a couple of years back, and come away pleasantly surprised. Not often do we see a fanless case managing to keep the processor cooler than an actively-cooled solution for the same workload, but that is exactly what the Akasa Turing achieved. For Intel's Tiger Canyon NUCs,Akasa carriesthree main products - Newton TN, Plato TN, and Turing TN. The company sampled us the Newton TN for review with Intel's sample of the NUC11TNKi5.On the Tiger Lake UCFF front, we had reviewed only a couple of systems - theASRock Industrial NUC BOX-1165G7last year, and more recently, theSupermicro SYS-E100-12T-H. We took the Akasa Newton TN review opportunity to also perform a detailed evaluation of Intel's own offering - the NUC11TNKi5 Tiger Canyon NUC. The review below introduces the Tiger Canyon NUC hardware and details the build process for its fanless version using the Akasa Newton TN before going into the usual platform analysis. Following this, the benchmark numbers for the passively cooled configuration are compared against the original NUC11TNKi5 (along with a host of other systems). Finally, results from the thermal evaluation of the fanless system are presented. Together, these provide an idea of what Tiger Lake can deliver in a reasonably compact fanless system and also whether the Newton TN manages to replicate the success of the Turing.Introduction and Product ImpressionsIntel's Tiger Lake processors brought the Willow Cove microarchitecture fabricated in a reasonably mature 10nm process to the market last year. The focus was mainly on the mobile market, but the company did launch a suite of mini-PCs based on them last year. The company segments the Tiger Lake-based mini-PCs them into different categories - Performance, Pro, Enthusiast, and Extreme. The NUC11 Pro Kit NUC11TNKi5 (Tiger Canyon) we are looking at today is a UCFF solution that places a 100mm x 100mm motherboard inside a 117mm x 112mm x 37mm chassis. The board comes with a soldered processor - the Core i5-1135G7. Belonging to the Tiger Lake-U family, it can operate over a range of configurable TDPs - from 12W to 28W. The NUC's default BIOS settings set the PL1 (sustained) and PL2 (burst mode) levels to 28W and 64W respectively, with the PL1 Time Window set to 28 seconds.End-users have the flexibility to choose their own storage device and RAM. For best performance, a PCIe 4.0 x4 NVMe SSD can be used, and DDR4-3200 SODIMMs are supported. Our NUC11TNKi5 sample came with the following components pre-installed:Samsung SSD 980 PRO PCIe 4.0 x4 NVMe SSD2x Kingston ValueRAM KVR32S22D8/16 DDR4-3200 SODIMM for 32GB of DRAMThe system is actively cooled, with a blower fan on the underside, away from the side of the motherboard with the SODIMM and M.2 slots.The specifications of our Intel NUC11TNKi5 review configuration are summarized in the table below.Intel NUC11TNKi5 / NUC11TNBi5 + Akasa Newton TN Specifications(as tested)ProcessorIntel Core i5-1135G7Tiger Lake 4C/8T, 2.4 - 4.2 GHzIntel 10nm SuperFin, 8MB L2, 28W(PL1 = 28W, PL2 = 64W)MemoryKingston ValueRAM KVR32S22D8/16 DDR4-3200 SODIMM22-22-22-52 @ 3200 MHz2x16 GBGraphicsIntel Iris Xe Graphics(80EU @ 1.30 GHz)Disk Drive(s)Samsung 980 PRO MZ-V8P500B/AM(2500 GB; M.2 2280 PCIe 4.0 x4 NVMe;)(1xxL V-NAND Gen 6 3D TLC; Samsung Elpis S4LV003 Controller)Networking1x 2.5 GbE RJ-45 (Intel I225-LM)Intel Wi-Fi 6 AX201 (2x2 802.11ax - 2.4 Gbps)AudioDigital Audio with Bitstreaming Support over HDMI PortsVideo2x HDMI 2.0b2x Display Port 1.4a with HBR3 over ThunderboltMiscellaneous I/O Ports2x USB 3.2 Gen 2 Type-A (Front)1x USB 3.2 Gen 2 Type-A (Rear)1x USB 2.0 Type-A (Rear)1x Thunderbolt 4 + 1x Thunderbolt 3 (Type-C) (Rear)Operating SystemWindows 11 Enterprise (22000.739)Pricing(Street Pricing on July 21st, 2022)$420 (Board)+$150 (Case)+ $130 (RAM) + $95 (SSD) = $795 (as configured, no OS)Full SpecificationsIntel NUC11TNBi5 SpecificationsAkasa Newton TN SpecificationsThe NUC package includes the usual VESA mount and screws along with an integration guide. A region-specific power cord accompanies the 120W (19V @ 6.32A) adapter.The gallery below provides an overview of the kit's chassis and the I/O distribution. The key differences when compared to the Performance line of Tiger Lake NUCs (Panther Canyon) are the absence of a SDXC card reader, a HDMI 2.0b port instead of a mini-DP port, an extra USB 3.2 Gen 2 Type-A port in the front panel, and the restriction of Thunderbolt Type-C ports only to the rear panel.Gallery:Intel NUC11TNKi5 Chassis DesignCompared to the Performance line, the NUC11 Pro line adds official support for Linux and Window IoT, supports operation over a wider DC input voltage range, has an extra internal SSD slot (M.2 2242 SATA / PCIe 3.0 x1), vPro capabilities in select SKUs, and a longer product life cycle.The standard NUC11TNKi5 kit above can be used in most scenarios, but there may be use-cases that require the complete absence of any moving parts. In industrial deployments, the reason may be the need to avoid performance loss due to cooling efficiency degradation resulting from dust build-up. For professional creators, it may be due to the need to avoid extraneous noise affecting the work output. The average home consumer may also prefer a silent system to better focus on the work at hand. For HTPCs, multimedia content can be enjoyed without distractions - an aspect that may be of paramount importance to audiophiles.Traditionally, passively cooled computing systems have either been woefully underpowered for general purpose use, or carried a significant premium in terms of both cost and physical footprint. However, advancements in compute performance per watt and novel passive cooling chassis designs (that do not cost an arm and a leg to mass-produce) have combined to give consumers the ability to create powerful, yet affordable, fanless systems. This is where vendors like Akasa come in. For the NUC11TNBi5 (the board inside the NUC11TNKi5 kit), Akasa has three different cases, with dimensions that can satisfy virtually any deployment scenario:Newton TN [ 176.6mm x 200mm x 53.6mm ]Plato TN [ 247mm x 240mm x 38.5mm ]Turing TN [ 95mm x 113.5mm x 247.9mm ]We put the standard kit through our benchmarking process first. Following that, we disassembled the unit, and transferred the board to the Akasa Newton TN. The same benchmarks were processed again on the Newton build.Prior to analyzing the NUC11 platform and looking at the comparative specifications of the considered systems, we take a deep dive into the build process using the Akasa Newton TN.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article already exists: https://www.anandtech.com/show/17502/intel-nuc11tnbi5-and-akasa-newton-tn-fanless-case-review-silencing-the-tiger\n",
      "Title: Apple Announces M2 SoC: Apple Silicon for Macs Updated For 2022\n",
      "Author: Ryan Smith\n",
      "Date Published: 2022-06-06T21:30:00Z\n",
      "URL: https://www.anandtech.com/show/17431/apple-announces-m2-soc-apple-silicon-updated-for-2022\n",
      "Content: Though primarily a software-focused event, Apple’s WWDC keynotes are often stage for an interesting hardware announcement or two as well, and this year Apple did not disappoint. At the company’s biggest Mac-related keynote of the year, Apple unveiled the M2, their second-generation Apple Silicon SoC for the Mac (and iPad) platform. Touting modest performance gains over the original M1 SoC of around 18% for multithreaded CPU workloads and 35% in peak GPU workloads, the M2 is Apple’s first chance to iterate on their Mac SoC to incorporate updated technologies, as well as to refresh their lower-tier laptops in the face of recent updates from their competitors.With the king of the M1 SoCs, M1 Ultra, not even 3 months behind them, Apple hasn’t wasted any time in preparing their second generation of Apple Silicon SoCs. To that end, the company has prepared what is the first (and undoubtedly not the last) of a new family of SoCs with the Apple Silicon M2. Designed to replace the M1 within Apple’s product lineup, the M2 SoC is being initially rolled out in refreshes of the 13-inch MacBook Pro, as well as the MacBook Air – which is getting a pretty hefty redesign of its own in the process.The launch of the M2 also gives us our first real glimpse into how Apple is going to handle updates within the Apple Silicon ecosystem. With the iPhone family, Apple has kept to a yearly cadence for A-series SoC updates; conversely, the traditional PC ecosystem is on something closer to a 2-year cadence as of late. M2 seems to split this down the middle, coming about a year and a half afterthe original M1– though in terms of architecture it looks closer to a yearly A-series SoC update.From a high level, there has been a limited number of changes with the M2 – or at least as much as Apple wants to disclose at this time – with the focus being on a few critical areas, versus the bonanza that was the initial M1 SoC. While all of this is preliminary ahead of either further disclosures from Apple or getting hands-on time with the hardware itself, the M2 looks a lot like a derivate of the A15 SoC, similar to how the M1 was derived from A14. As a result, at first glance the M1 to M2 upgrade looks quite similar to theA14 to A15 upgrade.According to Apple, the new SoC is comprised of roughly 20 billion transistors, which is 4B (25%) more than the original M1 – and 5B more than the A15 SoC. The chip is being made on what Apple terms a “second generation 5nm” process, which we believe is likelyTSMC’s N5P line, the same line used for the A15 SoC. N5P offers improved performance characteristics versus N5, but not density improvements. So while Apple doesn’t disclose die sizes, the company’s side-by-side die shots are at least accurate in that M2 is going to be a bigger chip than M1.Apple Silicon SoCsSoCM2M1CPU4x High Performance (Avalanche?)16MB Shared L24x High Efficiency (Blizzard?)4MB Shared L24x High Performance (Firestorm)12MB Shared L24x High Efficiency (Icestorm)4MB Shared L2GPU\"Next Generation\"10-Core3.6 TFLOPS8-Core2.6 TFLOPSNeural Engine16-Core15.8 TOPS16-Core11 TOPSMemoryControllerLPDDR5-64008x 16-bit CH100GB/sec Total Bandwidth (Unified)LPDDR4-42668x 16 CH68GB/sec Total Bandwidth (Unified)Memory Capacity24GB16GBEncode/Decode8KH.264, H.265, ProRes, ProRes RAW4KH.264, H.265USBUSB4/Thunderbolt 32x PortsUSB4/Thunderbolt 32x PortsTransistors20 Billion16 BillionMfc. Process\"Second Generation 5nm\"TSMC N5P?TSMC N5Starting from the top, in terms of their Arm-architecture CPU cores, the M2 retains Apple’s 4 performance plus 4 efficiency core configuration. Apple is not disclosing what generation CPU cores they’re using here, but based on the performance expectations and timing, there’s every reason to believe that these are the Avalanche and Blizzard cores that were first introduced on the A15.With regards to performance, Apple is saying that the M2 offers 18% improved multi-threaded CPU performance versus the M1. The company does not offer a breakdown of clockspeeds versus IPC gains, but if our hunch about M2 being Avalanche/Blizzard is correct, then we already have a good idea of what the breakdown is. Relative to the Firestorm core in the A14/M1, Avalanche offers only modest performance gains, as Apple invested most of their improvements into improving overall energy efficiency. As a result, the bulk of the performance gains there come from increased clockspeeds rather than IPC improvements.The performance CPU cores on M2 also come with a larger pool of L2 cache, which also serves to improve performance. Whereas M1 had 12MB of L2 cache shared among the cores, M2 brings this up to 16MB, a 4MB increase over both the M1 and for that matter the A15.Based on what we’ve already seen with the A15, this bigger update in this generation is on the efficiency core side of matters. The Blizzard CPU cores are increasingly behaving like not-so-little cores, offering relatively high performance and a much wider backend design than what we see with other Arm efficiency cores. Among other things, Blizzard added a fourth Integer ALU, which combined with other changes gave A15 a significant (28%) performance increase in those cores. Carried over to M2, and it’s not unreasonable to expect similar gains, though the wildcard factor will be what clockspeeds Apple dials things to.This, in turn, is also seemingly why Apple has decided to focus on MT performance for their Apple-to-Apple comparison. With the largest performance gains coming courtesy of the efficiency cores, in performance-bound situations it’s MT workloads which get to tap the E cores alongside the P cores that would see the greatest performance improvements. On the whole, Avalanche/Blizzard made for a modest year on the CPU microarchitecture front, and that looks to be carrying over for the M2 SoC.Meanwhile on the GPU front, Apple is going bigger. Though reclusive as always about the underlying architecture – merely calling this a “next generation” GPU – M2 comes with 10 GPU cores baked in, up from 8 on the M1. Officially, this GPU is rated for 3.6 TFLOPS, which is a 1 TFLOPS more than the 8 core M1. As well, the new GPU comes with a larger shared L2 cache, though Apple isn’t disclosing the cache size there.With a combination of a larger core count and what would seem to be a 10% or so increase in GPU clockspeeds (based on TFLOPS), Apple is touting two performance figures for the M2’s GPU. At iso-power (~12W), the M2 should deliver 25% faster GPU performance than the M1. However the M2’s GPU can, for better or worse, also draw more power than the M1’s GPU. At its full power state of 15 Watts, according to Apple is can deliver 35% more performance.Overall this indicates that while Apple has been able to improve their energy efficiency – GPUs love running wide and slow – Apple’s peak GPU power consumption is going up. This should have minimal impact on light workloads, but it will be interesting to see what it means for relatively heavy and constant workloads, especially on the fanless MacBook Air. Meanwhile the GPU’s display controller remains seemingly unchanged, topping out at 6K for external monitors.Tangential to the GPU updates, M2 also comes with an updated video encode/decode block, which at first glance looks a lot like a pared-down version of the block used on the M1 Pro/Max. Those SoCs added support for Apple’s ProRes and ProRes RAW codecs, and that support has now filtered back down into the base M2 SoC. As well, Apple is now officially supporting 8K video decode on the M2, whereas the M1, though never having an official resolution designation, was essentially a 4K part.Finally, on the processing side of matters, the M2 is inheriting the A15’s updated neural engine. According to Apple, this is still a 16-core design, and it happens to have the same 15.8 trillion operations per second (TOPS) rating as the A15’s neural engine. Which, despite only being on par with the A15, still makes it 40% faster than the M1’s neural engine, which topped out at 11 TOPS.Altogether, Apple is projecting a great deal of confidence in the performance of their second-generation Apple Silicon chip, and even more so its competitiveness versus Intel. While we’ll have to wait to get our hands on the hardware to confirm its performance, the M1 certainly lived up to claims there. So the expectations for M2 are similarly high.Memory: LPDDR5-6400, Up To 24GBWhile the core logic of Apple’s latest SoC would seem to be largely an enhanced version of the A15, it does have one very notable feature advantage: LPDDR5 support.Whereas the vanilla M1 (and the A15) only supported LPDDR4x memory, the M2 supports the newer LPDDR5 memory standard. The biggest change of which is support for much higher memory clockspeeds; based on Apple’s figures, the M2 is running at 6400Mbps/pin (LPDDR5-6400), which is up significantly from the 4266Mbps/pin (LPDDR4x-4266) memory clockspeeds of the original M1. The net result is that, on the SoC’s 128-bit memory bus, the M2 has 100GB/second of memory bandwidth to play with, a 50% increase over the M1 (~68GB/sec).Apple’s unconventional use of memory technologies remains one of their key advantages versus their competitors in the laptop space, so a significant increase in memory bandwidth helps Apple to keep that position. Improvements in memory bandwidth further improve every aspect of the SoC, and that especially goes for GPU performance, where memory bandwidth is often a bottlenecking factor, making the addition of LPDDR5 a key enabler for the larger, 10-core GPU. Though in this case, it's the M2 playing catch-up in a sense: the M1 Pro/Max/Ultra all shipped with LPDDR5 support first, the M2 is actually the final M-series chip/tier to get the newer memory.Past that, Apple is once again placing their LPDDR5 memory packages on-chip with the processor die itself. So each M2 chip will need to be equipped with memory ahead of time, and the device supply is likely to fluctuate a bit based on memory capacity depending on what the most popular configurations are, especially early on.M2 devices are available with either, 8GB, 16GB, or 24GB of memory. Given that Apple is still using just two stacks of memory, it looks like the company is finally taking advantage of LPDDR’s support for non-power-of-two die sizes (e.g. 12Gb dies), which allows them to get 12GB of memory into a single package without any further shenanigans. And assuming Apple replicates this down the line for the obligatory Pro/Max/Ultra SoCs, we should see the top memory capacities of all of Apple’s SoCs increase by 50% over the previous generation.And the Rest: Updated ISP, Same USBRounding out today’s M2 announcement, there are a couple more items that warrant a quick call-out.First, the M2 is getting an updated ISP as well as an updated Secure Enclave. Like other aspects of M2, these are likely inherited from the A15, which received similar updates as well.Meanwhile, a look at the specs of the new MBA and MBP indicate that there haven’t been any notable changes in USB or other I/O support for the new SoC. M1 was already at the top of the curve in 2020 when it launched with USB4 support, so nothing has changed here. This does mean, however, that the SoC is seemingly still limited to Thunderbolt 3 support, despite the fact that Thunderbolt 4 has now been out for well over a year. Both the MBA and MBP are also shipping with two USB ports, so it would seem that’s still the native limit of the SoC.Apple also hasn’t talked at all about PCIe capabilities. We’ll know more once we have the hardware in-hand, but at least for now there’s no reason to believe that Apple has added PCIe 5 support or changed the number of lanes available. I/O has remained something of a constraining factor for the entire Apple Silicon family, so it does make me wonder about what this means for the eventual Apple Silicon Mac Pro.Available in JulyClosing out today’s announcement, the M2 will be shipping in the new 2022 MacBook Air, as well as the refresh 2022 12-inch MacBook Pro. According to Apple, those devices will be available in July, with pre-orders open today.In the meantime, the M1 isn’t going anywhere. Besides being at the heart of the Mac Mini – which didn’t receive an update today – Apple is keeping the 2020 M1-based MacBook Air around. So both versions of the entry-level M-series SoC will be sticking around for some time to come.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17431/apple-announces-m2-soc-apple-silicon-updated-for-2022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Apple WWDC 2022 Keynote Live Blog (Starts at 10am PT/17:00 UTC)\n",
      "Author: Ryan Smith\n",
      "Date Published: 2022-06-06T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/17429/the-apple-wwdc-2022-keynote-live-blog-starts-at-10am-pt1700-utc\n",
      "Content: As we round the corner after Computex and transition into June, it's time once more for Apple's annual World Wide Developers Conference. As always, Apple kicks off WWDC with their big keynote event, which though aimed first and foremost at developers, is also used as a venue to announce new products and ecosystem strategies. The keynote starts at 10am Pacific (17:00 UTC) today, and AnandTech will be offering live blog coverage of Apple's event.With WWDC going virtual once again this year, we're expecting another rapid-fire, two-hour run through of Apple's ecosystem. WWDC keynotes have historically covered everything from macOS and iOS to individual Apple applications and more. On the hardware side of matters, in previous years we've seen things like the official announcement of Apple's shift from x86 to Apple Silicon; and while 2021 was light on hardware, one never quite knows what Apple has in store. Apple has yet to launch an Arm-based Mac Pro, so there's still some big surprises left in their bag, and of course there's always the chance of the periodic product refresh.So join us at 10am Pacific to see just what Apple is working on for this year and beyond.01:00PM EDT- Thank you, as always, for joining us for this year's WWDC presentation01:00PM EDT- And Apple is kicking things off right on time01:01PM EDT- (I never realized this before, but Apple's HQ building looks like a stargate)01:01PM EDT- And here's Tim to kick things off01:02PM EDT- As a general reminder, this is first and foremost a developer-focused show. But still, the morning keynote is a chance for all audiences to get a look at Apple's plans for the future of its platforms01:02PM EDT- And with any luck, we get some new hardware as well01:03PM EDT- Apple of course never stops courting developers. Both because they're the minority platform in the PC space, but because the company wants to be on the cutting edge of software as well as hardware01:04PM EDT- Apple expects millions of developers to engage with WWDC this year01:04PM EDT- The total developer community is now 34 million devs01:04PM EDT- And all of WWDC is free for them01:05PM EDT- Now handing things off to Craig Federighi01:05PM EDT- Starting things with iOS01:05PM EDT- iOS 1601:05PM EDT- iOS is getting a new set of personalization features01:06PM EDT- The lock screen is getting updated with it's \"biggest update ever\"01:06PM EDT- Now rolling a promo video01:07PM EDT- Now showing off the new lock screen01:07PM EDT- A new filter mode for the lock screen wallpaper, as well as an editor for all of this01:08PM EDT- Change the filter, as well as changing fonts and colors for the clock01:08PM EDT- And placing widgets on the lock screen01:09PM EDT- Apple also provides suggested photos for the wallpaper, as well as the ability to have it rotate photos throughout the day01:09PM EDT- And specialty wallpapers/lock screens that show things like the current weather conditions01:09PM EDT- And that's the new lock screen01:09PM EDT- Now on to notifications01:10PM EDT- Notifications now roll in from the bottom of the lock screen instead of the top01:10PM EDT- And a new feature called Live Activities01:10PM EDT- Live Activities will come iwht its own API for developers01:11PM EDT- This allows a notification item to update itself, rather than updates having to be delivered as additional notifications (think: sports scores)01:11PM EDT- Now on to Focus01:11PM EDT- Focus is being extended to the lock screen, changing lock screens based on the current focus mode01:12PM EDT- And Focus Filters01:12PM EDT- Filters allow for Focus in apps, hiding elements/pages to keep the user focused01:12PM EDT- Now on to Messages01:12PM EDT- Adding three \"highly requested\" features01:13PM EDT- 1) Edit just-sent messages01:13PM EDT- 2) Undo send - immediately recall a message01:13PM EDT- 3) Mark any thread as unread01:13PM EDT- I'm curious how long the undo/edit window will be01:14PM EDT- And a new feature/API: Shared With You, for sharing inside Messages01:14PM EDT- Now on to talking about Apple's SharePlay feature01:14PM EDT- Apple is looking to make it easier to discover SharePlay01:15PM EDT- SharePlay is being extended beyond FaceTime. It will be available in Messages in iOS 1601:15PM EDT- And on the subject of Messages, Apple is updating Dictation as well01:16PM EDT- Dictation is used 18 billion times a month01:16PM EDT- Dictation is already happening on device01:16PM EDT- And Apple is introducing a new Dictation paradigm that allows for using both voice and touch together seemlessly01:17PM EDT- Automatic punctuation!01:17PM EDT- (Take it from a writer: punctuation is under-loved)01:17PM EDT- Now on to Live Text01:18PM EDT- Live Text will now be available alongside videos01:18PM EDT- Can also be used within the Translate app01:18PM EDT- And there will be a Live Text API01:19PM EDT- Apple is also updating Visual Lookup. Among other things, using their ML hardware to allow it to automatically cut-out items being analyzed01:19PM EDT- Now on to Wallet01:20PM EDT- Starting with drivers licenses01:20PM EDT- 2 states so far, 11 more in progress01:20PM EDT- And Apple is limiting the amount of information Wallet presents to what's necessary01:21PM EDT- And (virtual card) keys can be shared with other users via Messages01:21PM EDT- Apple Pay01:21PM EDT- Tap To Pay is coming this month (as previously announced)01:21PM EDT- So iPhones can be used to accept payments from other iPhones and other sources like RFID-enabled credit cards01:22PM EDT- And Apple is getting even deeper into the financial services market01:22PM EDT- Apple Pay Later01:22PM EDT- Split up a purchase into 4 payments, with no interest or fees01:22PM EDT- All of this is tracked in Wallet01:22PM EDT- Apple is also making it possible for merchants to send tracking/shipment notifications directly to Wallet01:23PM EDT- Now on to Maps01:23PM EDT- Apple has launched its redesigned maps to 10 countries/regions thus far01:23PM EDT- Another 11 are coming this y ear01:24PM EDT- Apple is adding Las Vegas and 6 more cities for highly detailed cities01:24PM EDT- New feature: multi-stop routing01:24PM EDT- Siri can add stops to a route01:25PM EDT- Transit mode is making it easier to see the cost01:25PM EDT- And transit cards for Wallet01:25PM EDT- Look Around is being added to MapKit, so that devs can add the view to their apps01:26PM EDT- And that's Maps01:26PM EDT- Now to sports01:26PM EDT- (Craig, of course, is on Team Swift)01:27PM EDT- Quickly recapping sports-related developments, such as Friday baseball on Apple TV+01:27PM EDT- Apple News is getting an update to make it even better for sports news01:27PM EDT- A new section: My Sports01:27PM EDT- League standings, highlight clips01:27PM EDT- Available in US, UK, Canada, and Australia01:28PM EDT- Now on to Family Sharing01:28PM EDT- Recapping Family Sharing: shared content with up to 5 users01:28PM EDT- This year Apple is making it easier to create accounts for kids01:29PM EDT- New UI/interface for setting up kids accounts and their restrictions01:29PM EDT- Screen Time requests can be responded to within Messages01:29PM EDT- On to Photos01:30PM EDT- Photos is getting some new sharing functionality01:30PM EDT- iCloud Shared Photo Library01:30PM EDT- Share photos seamlessly and automatically01:30PM EDT- A separate shared library that up to 5 other people can add photos to01:31PM EDT- Can select photos based on date, who's in them, etc01:31PM EDT- Camera can also send photos to the shared library when they're taken01:31PM EDT- And even using proximity of other users to determine if new photos should be uploaded to the shared library01:32PM EDT- And last: privacy01:32PM EDT- For personal safety, Apple is adding a tool to quickly turn off shared access01:32PM EDT- New feature: Safety Check01:33PM EDT- Safety Check allows the user to review and reset the access granted to others01:33PM EDT- To cut ties and get to safety01:33PM EDT- Stops sharing location via FindMy and resets privacy permissions01:34PM EDT- Recapping iOS 1601:35PM EDT- And now on to HomeKit01:36PM EDT- Apple is focusing on smarthome functionality and how to grow the ecosystem01:36PM EDT- Recapping Apple's Matter interconnect standard01:36PM EDT- And updating on how it's being adopted01:37PM EDT- \"All new\" home app01:37PM EDT- Redesigned how accessories are viewed and organized01:38PM EDT- Tiles redesigned as well to make them easier to see/notice01:38PM EDT- New Home app coming to i-devices and the Mac01:38PM EDT- Now on to CarPlay01:38PM EDT- Available on 98% of new cars in the US01:39PM EDT- Apple has been working with automakers to further enhance the use of CarPlay01:40PM EDT- Next-generation CarPlay offers content for all of a driver's screens01:40PM EDT- So even as much as the instrument panel01:40PM EDT- Widgets powered by the iPhone01:40PM EDT- CarPlay powers the entire instrument cluster01:41PM EDT- Different curated themes for gauges01:42PM EDT- Vehicles with the tech will start to be announced late next year01:42PM EDT- And that's iOS 1601:42PM EDT- Now on to Spatial Audio01:42PM EDT- Personalized spatial audio profiles01:42PM EDT- And Quick Note is coming to iOS01:43PM EDT- And, of course, more Memoji customizations01:43PM EDT- And now on to Apple Watch01:43PM EDT- With Kevin Lynch01:44PM EDT- New themed watch faces01:44PM EDT- Lunar calendars and more01:45PM EDT- WatchOS 9's Podcast app will feature search and listen now01:45PM EDT- WatchOS 9 is also getting new features for fitness01:46PM EDT- Adding three new running form metrics for tracking running performance01:46PM EDT- Using ML and accelerometers to determine how the user is moving01:48PM EDT- Power metric. How much power you're consuming (in Watts, no less!)01:48PM EDT- (Do you consume more power than an RTX 3080?)01:49PM EDT- The fitness app is coming to iOS 1601:49PM EDT- So basic fitness tracking running off of an iPhone01:50PM EDT- It only offers a subset of what the Watch can do01:50PM EDT- Now on to WatchOS's Sleep app01:50PM EDT- Sleep Stages01:50PM EDT- Using heart rate and accelerometer input to determine what sleep stage a wearer is in01:51PM EDT- Apple Heart and Movement Study participants will also be able to submit data01:51PM EDT- Atrial Fibrillation01:52PM EDT- WatchOS can now keep a history of AFib events01:52PM EDT- Expecting FDA clearance for AFib History soon01:52PM EDT- Medications01:53PM EDT- WatchOS 9's Medications app can be used to track what mediciations have been taken. And supply notifications for when a medication needs to be taken01:53PM EDT- This also works in the iOS Health app01:53PM EDT- The iPhone camera can be used to scan medications01:54PM EDT- And Medications can flag drug-drug interaction checking when new drugs are added01:54PM EDT- And health information can be shared01:55PM EDT- And when it's being shared, there will be periodic reminders of what's being shared and with whom01:55PM EDT- And that's WatchOS 901:55PM EDT- And now to the Mac01:55PM EDT- With John Ternus01:55PM EDT- Starting with chips01:56PM EDT- Nearly the entire Mac product line has been transitioned to Apple Silicon/M101:56PM EDT- The next generation of Apple Silicon01:56PM EDT- M201:56PM EDT- Takes the breakthrough perf and capabilities of M1 even further01:56PM EDT- Recapping current M1 family01:57PM EDT- Apple is staying focused on power-efficiency. Keep power consumption low when increasing performance; don't raise power to keep performance growing01:57PM EDT- 20B transitors, built on \"second-gen\" 5nm process01:57PM EDT- 100GB/sec of unified memory bandwidth01:57PM EDT- 50% more than M101:58PM EDT- M2 supports up to 24GB of memory01:58PM EDT- 128-bit LPDDR5 interface01:58PM EDT- 8 core CPU. Next generation01:58PM EDT- 4+4 configuration01:58PM EDT- Efficiency cores have been significantly enhanced01:58PM EDT- 18% greater MT performance over M101:59PM EDT- Touting 87% of the perf of a 12 core laptop chip at 25% of the power01:59PM EDT- 10 core GPU on M201:59PM EDT- Up to 25% higher graphics performance at the same power level01:59PM EDT- Up to 35% higher perf at highest power state02:00PM EDT- And, of course, Apple is well ahead of Intel's integrated GPUs02:00PM EDT- M2 gets the next-gen Secure Enclave and Neural Engine02:00PM EDT- 40% more neural ops than M102:00PM EDT- Media engine adds ProRes support. 4K and 8K video02:00PM EDT- (15.8T neural ops)02:01PM EDT- Taking the performance and efficiency of M1 even further02:01PM EDT- The first Mac to get the M2 chip02:01PM EDT- MacBook Air02:01PM EDT- \"World's best-selling laptop\"02:01PM EDT- All new, completely redesigned MacBook Air02:01PM EDT- Rolling promo video02:02PM EDT- The new MacBook Air, powered by M202:02PM EDT- The wedge shape is gone02:03PM EDT- Aluminum unibody enclosure02:03PM EDT- 11.3mm thick02:03PM EDT- 2.7 pounds02:03PM EDT- Available in 4 finishes02:03PM EDT- 2 USB/TB ports. Magsafe charging. Audio jack with support for high impedence headphones02:04PM EDT- Liquid Retina display02:04PM EDT- It has a notch02:04PM EDT- 13.6-inch display02:04PM EDT- 500 nits max brightness. 25% brighter than before02:04PM EDT- 1 billion color support. So 10bpc color02:04PM EDT- New 1080p FaceTime camera (2x the res of the previous)02:05PM EDT- The speakers and mics and located between the keyboard and display02:05PM EDT- Three mic array02:05PM EDT- 4 speaks altogether for audio02:05PM EDT- Full-height function row for the function keys02:05PM EDT- Up to 20% faster than M1 MBA in Photoshop filters02:06PM EDT- And still a fanless design02:06PM EDT- Up to 18 hours of video playback02:06PM EDT- MBA supports Fast Charging with a 67W adapter. 50% in 30 minutes02:06PM EDT- Apple is also introing a new compact charger with 2 USB ports02:07PM EDT- Now rolling another promo video02:07PM EDT- Body changes aside, this looks like a pretty straightforward and sensible update to the MBA. Nothing ground-breaking, but the MBA typically isn't where Apple rolls out big new features02:08PM EDT- I'm curious to see how the actual battery capacity compares to the M1 MBA. That'll give us a better idea of Apple's power efficiency expectations02:08PM EDT- M2 is also coming to a new 13-inch MacBook Pro02:09PM EDT- 39% faster than the M1 version in GPU-heavy workloads02:09PM EDT- Available with up to 24GB of memory02:09PM EDT- No body or feature changes, from the looks of things02:09PM EDT- 20 hours of battery life02:10PM EDT- MBA starts at $119902:10PM EDT- 13-inch MBP starts at $129902:10PM EDT- Available next month02:10PM EDT- The M1 MBA is staying in production. Will be $99902:11PM EDT- And that's the next generation of Apple Silicon and the new Macs02:11PM EDT- Now on to macOS02:12PM EDT- macOS 13 Ventura02:13PM EDT- New window organization feature: Stage Manager02:13PM EDT- Stage Manager automatically moves windows from inactive apps off to the side02:14PM EDT- So almost, but not quite automatically hiding inactive apps02:14PM EDT- Apps can also be grouped to be kept active together02:15PM EDT- Draging files from the desktop into a Stage Manager group02:15PM EDT- Spotlight searching is getting a big update02:15PM EDT- Quick Look for results02:16PM EDT- And searching text inside of images via Live Text02:16PM EDT- Results can be presented in a full window02:16PM EDT- Rich results are also coming to i-devices02:17PM EDT- Now on to Mail02:17PM EDT- Undo Send02:17PM EDT- And scheduled Send02:17PM EDT- Mail Search has been overhauled02:18PM EDT- Instant suggestions as you type02:18PM EDT- And using synonyms02:18PM EDT- Also coming to iOS/iPadOS02:18PM EDT- Now to Safari02:19PM EDT- Recapping Safari features and new dev features added in the last year02:19PM EDT- New feature: Shared Tab Groups02:19PM EDT- Share a whole tab group with another user02:20PM EDT- Apple is looking to replace passwords with Passkeys02:21PM EDT- Essentially using biometrics to unlock a key, which is used in what sounds like a challenge-response interaction02:21PM EDT- Been working with the FIDO alliance02:21PM EDT-https://arstechnica.com/information-technology/2022/05/how-apple-google-and-microsoft-will-kill-passwords-and-phishing-in-1-stroke/02:21PM EDT- Now on to Gaming02:22PM EDT- Starting with Metal02:22PM EDT- Rolling out Metal 302:22PM EDT- Unleashing the full potential of Apple Silicon for years to come02:22PM EDT- MetalFX Upscaling02:23PM EDT- So Apple is doing their own upscaling tech02:23PM EDT- Spatial upscaling + temporal AA02:23PM EDT- Which sounds like something a bit below XeSS/DLSS/FSR202:24PM EDT- Resident Evil Village is coming to the Mac02:25PM EDT- Showing off footage of the game running on macOS02:26PM EDT- Available later this year02:26PM EDT- Now on to Continuity02:26PM EDT- Handoff is being extended to FaceTime02:27PM EDT- Seamlessly moving a call from the iPhone to a Mac, and vice versa02:27PM EDT- And iPad as well02:27PM EDT- Continuity Camera. Use iPhone as your web cam02:27PM EDT- Since the iPhone has larger and more capable cameras than the small FaceTime cameras placed in a laptop lid02:28PM EDT- This all works wirelessly02:28PM EDT- Continuity Camera supports the Center Stage features02:28PM EDT- New feature: Desk View02:29PM EDT- Using the ultrawide camera on the iPhone to simulate a overhead view of a user's desk02:29PM EDT- Continuty Camera can be used with any video conferencing app02:29PM EDT- Apple is also working with Belkin for stands02:30PM EDT- Coming later this year02:30PM EDT- And that's macOS Ventura02:30PM EDT- Now to iPadOS02:31PM EDT- iPadOS of course gets most of iOS's new features, as well as a bunch of features announced for macOS02:31PM EDT- And a version of the Weather app for the iPad02:31PM EDT- And a new API: WeatherKit, for adding weather to third-party apps02:32PM EDT- Collaboration02:33PM EDT- Can select a Messages group to collaborate with02:33PM EDT- Faster collaboration with groups the user is already communicating with02:34PM EDT- More uses of shared tab groups - you can see who is on what tabs, and they can dynamically add tabs to the group02:35PM EDT- Collaboration features will also work with apps like Notes and Keynote02:35PM EDT- Also coming to iOS and macOS02:35PM EDT- New app coming later this year: Freeform02:36PM EDT- (Does this remind anyone else of Google Wave?)02:37PM EDT- Freeform is being built into the full suite of Apple OSes02:37PM EDT- And now iPad Gaming02:37PM EDT- Metal 3 is coming to iPadOS, with the same upscaling and loading features02:38PM EDT- Also rolling out a new API to download large assets in the background02:39PM EDT- The Game Center dashboard is being updated with activity tracking02:39PM EDT- Coming in an update to iPad 16 later this year02:39PM EDT- (So not in the initial launch version?)02:39PM EDT- \"Desktop-class apps\"02:39PM EDT- Bringing more Mac-like features to iPadOS02:40PM EDT- Undo/redo02:40PM EDT- A bunch of File features like changing file extensions and viewing folder sizes02:40PM EDT- Customizable toolbars in iPadOS02:40PM EDT- New APIs for developers to allow devs to build it into their own apps02:41PM EDT- New feature: Reference Color02:41PM EDT- Allowing it to be used as a reference, calibrated display as part of a workflow02:42PM EDT- New display scaling setting02:42PM EDT- Increase the pixel density of the iPad display to see more02:42PM EDT- iPadOS 16 adds support for virtual memory swap02:42PM EDT- So we're back to page files02:42PM EDT- Stage Manager coming to iPadOS as well02:43PM EDT- Demoing Stage Manager on the iPad02:44PM EDT- Same feature set as on the Mac. Groups, resizing windows, etc02:44PM EDT- And you can use Stage Manager with external displays02:44PM EDT- Up to 8 apps simultaneously02:45PM EDT- And that's iPadOS 1602:46PM EDT- And back to Tim02:46PM EDT- Developer betas today for OS releases02:46PM EDT- Public betas next month02:47PM EDT- Full releases this fall02:47PM EDT- Apple has a huge week ahead. The Platform State of the Union is this afternoon02:47PM EDT- 175 sessions02:48PM EDT- And that's a wrap! Off to look into the M2\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17429/the-apple-wwdc-2022-keynote-live-blog-starts-at-10am-pt1700-utc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Computex 2022: NVIDIA Keynote Live Blog (11pm ET/03:00 UTC)\n",
      "Author: Ryan Smith\n",
      "Date Published: 2022-05-24T02:20:00Z\n",
      "URL: https://www.anandtech.com/show/17404/computex-2022-nvidia-keynote-live-blog-11pm-et0300-utc\n",
      "Content: 10:58PM EDT- We're back for day two of our live blog coverage of Computex10:59PM EDT- Following AMD's big, CPU-centric showcase, NVIDIA is on deck this evening to deliver what promises to be a varied keynote touching upon several aspects of NVIDIA's business11:00PM EDT- The untitled keynote is slated to run for an hour and features a who's who of NVIDIA VPs, including Ian Buck (VP Accelerated Computing), Michael Kagan (CTO), and Jeff Fisher (SVP GeForce)11:00PM EDT- So expect the keynote to cover everything from datacenters to gaming11:01PM EDT- And here we go11:02PM EDT- Starting with a quick promo video of the NVIDIA and the growth of their GPU business from gaming to AI and much more11:03PM EDT- First up, Ian Buck11:03PM EDT- Starting with AI11:04PM EDT- This requires reimagining the datacenter itself11:04PM EDT- AI is also enabling a new market for digital twins11:04PM EDT- A potential $150B market11:04PM EDT- And a further $100B market for cloud game streaming11:05PM EDT- All of which NVIDIA's partners can tap into by using NVIDIA's hardware11:05PM EDT- These tasks consume a lot of computing power - so energy efficiency is a big deal11:06PM EDT- Of course, NVIDIA is also happy to argue that a lot of power can be saved by moving from using CPUs for compute tasks to GPUs11:07PM EDT- And NVIDIA has a sizable collection of other hardware, including Grace CPUs and ex-Mellanox networking hardware11:07PM EDT- Now recapping the H100 accelerator11:08PM EDT- NVLink 4, Multi-Instance GPU, Confidential Computing, transformer accelerator, etc11:08PM EDT- And NVLink switches for forming larger clusters of computers11:10PM EDT- And a shout-out to NVIDIA's Taiwanese partners for their part in building these new servers11:10PM EDT- Current NVIDIA roadmap11:11PM EDT- Two year rhythm on hardware. Interleaving between x86 and Arm11:11PM EDT- Now on to Grace11:11PM EDT- Grace is on track to ship next year11:11PM EDT- Grace and Grace Hopper11:12PM EDT- For CPU and CPU+GPU workloads respectively11:12PM EDT- The Grace Superchip (dual Graces) has 144 cores. It has 1TB of LPDDR5X memory and consumes 500 Watts11:13PM EDT- And reiterating NVIDIA's plans to put NVLink in all of their silicon, so that it can be linked up to other NVIDIA and third party hardware with a high speed cache coherent interconnect11:14PM EDT- NVIDIA has developed a large number of reference systems over the years, and they are now producing a new generation of them based on Grace11:15PM EDT- New CGX, OVX, and GFX refrence designs using Grace and/or Grace Hopper11:15PM EDT- Announcing HGX Grace and HGX Grace Hopper systems11:15PM EDT- Both are specifically designed for OEM 2U chassis11:16PM EDT- Can leverage their existing system architectures11:16PM EDT- Grace systems will start shipping in the first half of 202311:17PM EDT- Now on to talking about Bluefield-3 NPUs11:18PM EDT- NVIDIA is betting heavily on Bluefield and DPUs becoming essential for datacenters11:18PM EDT- It is silicon-heavy and will be able to leverage NVIDIA's GPUs for processing11:19PM EDT- Or as NVIDIA likes to put it, this networking gear will be the backbone of AI systems11:19PM EDT- Now on to NVIDIA's enterprise software stack11:20PM EDT- As a reminder, NVIDIA has more software engineers than hardware engineers. So at the enterprise level, software is even more of a differentiating factor for NV than hardware is11:20PM EDT- The next wave of AI: robotics11:21PM EDT- \"We see a clear demand for automation\"11:22PM EDT- Recapping NVIDIA's Isaac robotics platform11:23PM EDT- And how Isaac interfaces with omniverse for simulation and robotic training11:24PM EDT- A big part of enabling this are NVIDIA's pre-trained models available on NGC11:25PM EDT- Which is another example of where NVIDIA is using software to build up their ecosystem11:26PM EDT- Isaac Sim using Omniverse11:26PM EDT- NVIDIA wants to close the sim-to-real gap11:26PM EDT- Announcing Isaac Sim 2022.1 release11:27PM EDT- Now rolling a demo of Isaac Sim11:29PM EDT- Now on to NVIDIA's hardware for robotics: Jetson11:31PM EDT- NVIDIA's Orin SoC is reportedly performing well in MLPerf benchmarks11:31PM EDT- The Jetson AGX Orin dev kit is available now11:32PM EDT- Production systems with Jetson Orin are available now, and many are being announced at Computex this week11:33PM EDT- Isaac Nova Orin. A state-of-the-art architecture for autonomous mobile robots11:34PM EDT- One more thing: Drive Hyperion11:35PM EDT- Hyperion version 8 will start shipping in 202411:35PM EDT- And Hyperion 9 is in development for cars shipping starting in 202611:35PM EDT- Processes twice as much sensor data as Hyperion 811:36PM EDT- And NVIDIA has signed up additional supplier-partners for Hyperion11:36PM EDT- And now on to gaming with Jeff Fisher11:37PM EDT- \"Taiwan is the birthplace of the PC ecosystem\"11:37PM EDT- Recapping NVIDIA's current stack of hardware, including GeForce RTX series video cards, Max-Q laptops, and G-Sync monitors11:38PM EDT- PC game hardware is expected to be a $67B market this year, with double digit growth over coming years11:38PM EDT- Jeff is thanking NVIDIA's partners for their products11:39PM EDT- \"[Ampere] is the world's fastest GPU\"11:39PM EDT- Recapping Ampere family features such as RT cores, DLSS, and NVENC video encoder11:40PM EDT- Now rolling a video on NVIDIA's driver development process11:41PM EDT- This appears to be the same (or similar) Game Ready development video they released a couple of weeks back11:41PM EDT- \"Our mission with drivers is to be invisible\"11:42PM EDT- There are now over 250 RTX games and applications11:42PM EDT- Hitman 3 is getting an update with ray tracing and DLSS11:43PM EDT- Also announcing that F1 22, which is launching in July, will also have RT and DLSS11:44PM EDT- Now on to NVIDIA Reflex11:44PM EDT- \"Low system latency helps all gamers\"11:44PM EDT- NVIDIA recently conducted a study on how system latency impacts aim accuracy11:44PM EDT- Kovaak's system latency challenge11:45PM EDT- Running another promo video, this time on Reflex11:45PM EDT- Talking about reducing system latency with Reflex11:46PM EDT- Reflex is even a plugin in Unreal Engine these days11:46PM EDT- Currently up to 38 games and 22 monitors11:47PM EDT- Introducing a new Reflex-capable display11:47PM EDT- A new 500Hz 1080p 24-inch Asus monitor11:47PM EDT- TN panel11:47PM EDT- Includes the Reflex Analyzer11:48PM EDT- Now on to GeForce laptops with Max-Q11:48PM EDT- Recapping the Max-Q feature set11:49PM EDT- New gaming laptops at Computex from MSI, Asus, Gigabyte, and others11:49PM EDT- Now on to NVIDIA Studio11:49PM EDT- NVIDIA's creation-focused platform11:50PM EDT- Now over 200 Studio accelerated applications11:51PM EDT- And how Omniverse will also help game developers and content creators11:51PM EDT- Seen a 10x increase in Omniverse downloads11:51PM EDT- Omniverse is being updated with new features as well, such as Omniverse Cloud and Audio2Face11:53PM EDT- Now recapping the gaming segment of this keynote and the state of NVIDIA's gaming ecosystem11:53PM EDT- And Jeff thanks NVIDIA's partners one last time11:54PM EDT- And by signing off with Toy Jensen, that's a wrap. Thanks for joining us.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17404/computex-2022-nvidia-keynote-live-blog-11pm-et0300-utc\n",
      "Title: Qualcomm Announces Snapdragon 7 Gen 1: Bringing Armv9 To Premium Smartphones\n",
      "Author: Ryan Smith\n",
      "Date Published: 2022-05-20T14:30:00Z\n",
      "URL: https://www.anandtech.com/show/17397/qualcomm-announces-snapdragon-7-gen-1\n",
      "Content: Alongside the newSnapdragon 8+ Gen 1, as part of Qualcomm’s “Snapdragon Night” event in China this evening, the company is also rolling out a brand-new SoC for the premium phone market, the Snapdragon 7 Gen 1. Aimed downmarket of Qualcomm’s traditional flagship SoCs, the Snapdragon 7 Gen 1 is the first non-flagship SoC to be introduced by Qualcomm since they implemented their new platform naming and differentiation scheme. But, like the Snapdragon 7xx series before it, the latest Snapdragon 7 SoC follows the same design mantra of offering flagship-level features with more modest performance and costs.Within Qualcomm’s pantheon of smartphone SoCs, the Snapdragon 7 Gen 1 is the successor to last year’sSnapdragon 780G/778G parts. For the moment, Qualcomm is sticking with a single SKU, though last year the 780G and 778G were a few months apart – so Qualcomm may bifurcate things farther down the line.Like its predecessors, the 7 Gen 1 follows Qualcomm’s cascading development strategy, which sees new features first pioneered in their flagship SoC successively implemented in future, lower-tier chips. The big update on the Snapdragon 8 Gen 1 was of course the switch to the Armv9 architecture and Arm’s associated CPU cores, and now Arm’s premium tier SoC is getting the same treatment.Qualcomm Snapdragon 7-Class SoCsSoCSnapdragon 7 Gen 1Snapdragon 780GCPU1xCortex-A710@ 2.4GHz3xCortex-A710@ 2.36GHz4xCortex-A510@ 1.8GHz1x Cortex-A78@ 2.4GHz3x Cortex-A78@ 2.2GHz4x Cortex-A55@ 1.9GHzGPUAdrenoAdreno 642DSP / NPUHexagonHexagon 770MemoryController2x 16-bit CH@ 3200MHz LPDDR5 / 25.6GB/s2x 16-bit CH@ 2133MHz LPDDR4X / 17.0GB/sISP/CameraTriple 14-bit Spectra ISP1x 200MP or 84MP with ZSLor64+20MP with ZSLor3x 25MP with ZSL4K HDR video & 64MP burst captureTriple 14-bit Spectra 570 ISP1x 192MP or 84MP with ZSLor64+20MP with ZSLor3x 25MP with ZSLEncode/Decode4K30 10-bit H.265Dolby Vision, HDR10+, HDR10, HLG720p480 infinite recording4K30, 1080p120H.264 & H.26510-bit HDR pipelinesIntegrated ModemX62 Integrated(5G NR Sub-6 + mmWave)DL = 4400 MbpsX53 Integrated(5G NR Sub-6 4x4 100MHz)DL = 3300 MbpsMfc. ProcessSamsung 4nmSamsung 5LPEIn terms of organization, the new 7 Gen 1 is very similar to the 780G, implementing a 1+3+4 core configuration. Qualcomm is only using two core types here – the high-performance Cortex-A710 and high-efficiency Cortex-A510 – so rather than the prime core being a unique core type (ala the Cortex-X2 on the 8G1), it’s another Cortex-A710 here. Qualcomm is clocking this core at 2.4GHz, meanwhile the remaining 3 A710s are clocked a bit lower at 2.36GHz. Finally, the A510s come clocked at 1.8GHz, the same as on the 8G1.The shift in CPU cores also brings an update to Qualcomm’s memory controller. The 7 Gen 1 adds LPDDR5 capabilities, supporting up to the same LPDDR5-6400 data rates as the 8 Gen 1. So with a 2x16-bit channel memory bus, the 7 Gen 1 can access up to 25.6GB/second of memory bandwidth, a 50% improvement over the LPDDR4X-capable 780G.Unusual even for Qualcomm, the company isn’t talking about CPU performance with the new part. So absent benchmarking data, there’s little we can say here about performance expectations. The new Arm cores should allow for better performance, but there’s a lot of wiggle-room due to implementation details.Meanwhile Qualcomm is being a bit more talkative on the GPU front, though not immensely so. Like the 8G1, Qualcomm is dropping any kind of model number for the GPU on the 7 Gen 1, so it’s simply “Adreno” with nothing to differentiate it from the 8G1 or previous-generation parts. Technically, we can’t even infer whether it uses Qualcomm’s new GPU architecture – though given the feature set and cascading design strategy, it’s almost certainly yes. But what Qualcommissaying about the GPU performance of the new part is that it’s 20% faster than the Snapdragon 778G and it’s Adreno 642L GPU. By going with the weaker SKU, this is cherry picking by Qualcomm, but for now it at least confirms that the 7 Gen 1 will be somewhat faster than its predecessor.Which is a good thing, too, since gaming is one of the major focuses of the 7 Gen 1. It’s gaming/GPU performance that Qualcomm leads with in their 7 Gen 1 marketing materials, and it’s no coincidence that this part is being announced at an event in China. Mobile gaming is big across the globe, but nowhere is it bigger than the Chinese market. So Qualcomm is undoubtedly expecting their OEM partners to shift a significant number of gaming-focused handsets in that busy market.Meanwhile, since this isn’t a flagship part like the new 8+ Gen 1, Qualcomm isn’t being as aggressive on the fab front, either. The 7 Gen 1 is being produced on Samsung’s 4nm process, the same one as the original 8 Gen 1, and which Qualcomm is intentionally supplanting with TSMC for their flagship SoC. And while Samsung’s process is clearly the lower performing option at this juncture, with lines at TSMC running out the door and around the block, it’s not surprising to see Qualcomm opt for what is a less constrained (and undoubtedly cheaper) fab for their second-tier smartphone SoC.CPU and GPU matters aside, Qualcomm’s cascading design strategy also means a few other notable features from the 8G1 are moving down to their new 7-series SoC. That includes the newest Spectra ISP, as well as Qualcomm’s latest AI hardware and their trust management engine.In the case of the 7 Gen 1’s Spectra ISP, it has been scaled down accordingly from the 8G1. Qualcomm is still implementing a “triple” ISP here that can handle up to 3 simultaneous cameras, however total throughput is lower. Still, the ISP can handle up to a 200MP image now, though that drops significantly to 64MP if you need zero shutter lag. Past that, this is another 14-bit imaging pipeline ISP, which is on-par with the 780G, but lacks the additional dynamic range of the 8G1.Rounding out the package is an integrated modem, in the form of Qualcomm’s Snapdragon X62. Compared to last year’s 780G SoC and its X53 modem, the X62-equipped 7 Gen 1 offers both greater throughput and more bands. In particular, this SoC is mmWave capable, which was a notable omission from last year’s SoC. Altogether, the 7G1/X62 is capable of reaching download speeds up to 4.4Gbps, a 33% improvement over its predecessor.Wrapping things up, the Snapdragon 7 Gen 1 will be the quicker to ship of Qualcomm’s two new SoCs today. According to the company, handsets using the SoC will be available this quarter, with Honor, OPPO, and Xiaomi among the OEMs slated to release phones based around the new chip.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17397/qualcomm-announces-snapdragon-7-gen-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qualcomm Announces Snapdragon 8+ Gen 1: Moving to TSMC for More Speed, Lower Power\n",
      "Author: Ryan Smith\n",
      "Date Published: 2022-05-20T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/17395/qualcomm-announces-snapdragon-8-gen-1-moving-to-tsmc-for-more-speed-lower-power\n",
      "Content: As the dark of the night rolls into China this evening, Qualcomm is hosting a mobile-focused product launch event they’re calling “Snapdragon Night”. Headlining the event is the announcement of the company’s new flagship SoC, the Snapdragon 8+ Gen 1. A mid-generation update to their flagship smartphone SoC, the Snapdragon 8 Gen 1, the 8+ Gen 1 follows Qualcomm’s annual tradition of releasing a refresh product to boost performance and to give partners something new to work with for the second half of the year. And for this year in particular, we’re looking at a very notable change in chips from Qualcomm.Unlike previous generations where Qualcomm merely launched a faster speed bin of their existing silicon, for 2022 we have something more substantial to talk about. Qualcomm has switched up foundries entirely – moving from Samsung to TSMC – and as a result is rolling out a new die. Thanks to this, the Snapdragon 8+ Gen 1 Qualcomm is reaping something of a one-off manufacturing gain, allowing them to both dial up CPU and GPU performance while simultaneously cutting power consumption.Qualcomm Snapdragon 8 Gen 1 Flagship SoCsSoCSnapdragon 8+ Gen 1Snapdragon 8 Gen 1CPU1xCortex-X2@3.2GHz3xCortex-A710@2.8GHz4xCortex-A510@2.0GHz6MB sL31xCortex-X2@ 3.0GHz3xCortex-A710@ 2.5GHz4xCortex-A510@ 1.8GHz6MB sL3GPUAdreno(10% Higher Clockspeed)AdrenoDSP / NPUHexagonHexagonMemoryController4x 16-bit CH@ 3200MHz LPDDR5 / 51.2GB/s4MB system level cacheISP/CameraTriple 18-bit Spectra ISP1x 200MP or 108MP with ZSLor64+36MP with ZSLor3x 36MP with ZSL8K HDR video & 64MP burst captureEncode/Decode8K30 / 4K120 10-bit H.265Dolby Vision, HDR10+, HDR10, HLG720p960 infinite recordingIntegrated ModemX65integrated(5G NR Sub-6 + mmWave)DL = 10000 MbpsUL = 3000 MbpsMfc. ProcessTSMC 4nmSamsung 4nmQuickly diving into the specifications, the new Snapdragon 8+ Gen 1 is essentially the original Snapdragon 8 Gen 1 ported over from Samsung’s 4nm line over to one of TSMC’s 4nm lines. Under more normal circumstances, this kind of a shift would likely be unremarkable – or at most, an amusing exercise in looking for edge cases – however for Qualcomm’s flagship SoC, the matter is more significant.While official sources and statements on the quality of Samsung’s 4nm process are few and far between, unofficially, it’s become clear that Samsung’s 4nm process hasn’t lived up to expectations. This has caused a cascading impact on the chips made on the process node, leading to the original Snapdragon 8 Gen 1 developing an affinity for power consumption, and Samsung’s own Exynos 2200 not faring any better. Conversely, by all accountsTSMC’s N4 processis looking great, with the optically shrunk node building off of TSMC’s already successful and very performant 5nm technologies.As a result of this performance gap between Samsung and TSMC’s 4nm nodes, Qualcomm is taking the unusual step of (essentially) porting their high-end SoC over to TSMC’s fab. Which, although not strictly necessary – Qualcomm carries a lot of momentum and the 8 Gen 1 has been selling well – is certainly a prudent move for the company. Qualcomm is facing especially stiff competition this generation from MediaTek, whose flagship-levelDimensity 9000 SoCwas the lead product for TSMC’s 4nm node. And that leaves MediaTek with a distinct advantage against the original 8 Gen 1, one that Qualcomm would be very happy to nullify.Ultimately, the switch in fabs is giving Qualcomm a chance to improve upon the original 8 Gen 1 from both ends of the spectrum, resulting in the Snapdragon 8+ Gen 1. On the performance front, TSMC’s node affords them an easy opportunity to increase CPU and GPU clockspeeds for more performance. The prime Cortex-X2 core is now clocked 7% higher, at 3.2GHz, and meanwhile the A710 and A510 clusters have seen their clockspeeds dialed up even more significantly, by around 12% each. Now even the slowest A510 cores can run at 2GHz. GPU clockspeeds have also been similarly increased, and while Qualcomm does not disclose specific clockspeeds there, they have confirmed that the 8+ Gen 1’s Adreno GPU block is clocked 10% higher than the original 8 Gen 1.But, if anything, the bulk of Qualcomm’s gains from the switch in manufacturing nodes are being invested in cutting power consumption. Something of a sore point with the 8 Gen 1, TSMC’s better 4nm process means that Qualcomm is seeing much lower power consumption across their SoC at iso-frequency.Officially, Qualcomm is claiming a 30% improvement in both GPU and CPU power efficiency. Though as mentioned before, this is at iso-frequency and doesn’t take into account the higher peak clockspeeds of the 8+ Gen 1. Consequently, the real-world power savings aren’t going to be quite as great on a peak-to-peak basis, but according to Qualcomm the power savings are still significant. Overall, the company is touting a 15% reduction in SoC power usage under “practical usage patterns” versus the original 8 Gen 1, which in turn should lead to improved battery lifetimes in handsets that adopt the new SoC.Past that, the official specifications for the 8+ Gen 1 do not reflect any material changes to the SoC’s configuration versus the original chip. So we’re still looking at the same integrated X65 5G modem, the same Spectra ISP, and the same video encode/decode blocks (sorry, gang, still no AV1 support!). So, despite assembling a new die for their mid-generation refresh product this year, there aren’t any new features to speak of with the 8+ Gen 1.As for SoC performance, officially Qualcomm is claiming 10% improved GPU and CPU performance, owing to those aforementioned clockspeed increases. While we weren’t able to attend a benchmarking session that Qualcomm held last week, the Performance Mode figures being released by the company are roughly in-line with these claims. Qualcomm’s Geekbench 5 results are several percent higher than what we benchmarked back in December at the8 Gen 1 launch event, though it’s notable that they didn’t achieve meaningfully higher scores in PCMark. GPU performance figures are similarly mixed, with some of Qualcomm’s official results coming very close to our original 8 Gen 1 results, but I’m reluctant to read too much into them given how much if a difference peak versus sustained testing makes in GPU results. As always, the final word will need to come down to independent, third-party testing, though at face value nothing Qualcomm claims is unreasonable given the clockspeed improvements and the thermal headroom gained from the switch to TSMC.Finally, as for consumers, they’ll be able to get their first look at Snapdragon 8+ Gen 1 devices in Q3 of this year. According to Qualcomm, many of the usual suspects have signed on to release phones based on the new SoC, including Asus, Motorola, OnePlus, Honor, and Xiaomi.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17395/qualcomm-announces-snapdragon-8-gen-1-moving-to-tsmc-for-more-speed-lower-power\n",
      "Title: The Apple \"Peek Performance\" Event Live Blog (Starts at 10am PT/18:00 UTC)\n",
      "Author: Ryan Smith\n",
      "Date Published: 2022-03-08T17:00:00Z\n",
      "URL: https://www.anandtech.com/show/17302/the-apple-peek-performance-event-live-blog\n",
      "Content: 12:58PM EST- Thanks for joining us for this year's Apple spring product event12:59PM EST- Today's event is entitled \"Peek Performance\"; emphasis of course on the use of peek instead of peak12:59PM EST- So we're expecting some high performance hardware out of Apple12:59PM EST- These virtual Apple events tend to be packed with a barrage of product announcements, and this year is no different. In previous years these events have covered new Macs, iPads, and even iPhones, and this year should be much the same. So it should be interesting to see what Apple has in store, especially as the company continues its multi-year transition in the Mac from x86 CPUs to their own Arm-based Apple Silicon chips01:00PM EST- And here we go01:00PM EST- On stage: Tim Cook01:00PM EST- Jumping straight away into Apple's AppleTV+ service01:01PM EST- Cook is recapping various series/works and awards they've received01:02PM EST- And now rolling a trailer to preview upcoming original films for the service01:04PM EST- Now rolling another, baseball-themed trailer01:05PM EST- \"Friday Night Baseball\" on AppleTV+01:05PM EST- It sounds like Apple has an exclusive agreement for two baseball games on Friday nights01:05PM EST- Though it's a shame MLB is on hold due to the strike...01:05PM EST- Now moving on to the iPhone01:06PM EST- Two new colors/finishes for the iPhone 1301:06PM EST- Green for iPhone 13 and Alpine Green for the iPhone 13 Pro01:07PM EST- Pre-orders this Friday, available March 18th01:07PM EST- Now on to Apple Silicon01:07PM EST- Recapping Apple Silicon's capabilities and touting Apple's performance advantages thus far01:08PM EST- A15 Bionic is coming to another iPhone01:08PM EST- Okay, so this is another iPhone announcement in disguise01:08PM EST- New iPhone SE01:08PM EST- This makes for the 3rd SE01:08PM EST- Cook is noting the continuing importance of a cheaper iPhone option01:09PM EST- As well as its ability to help bring over new users01:09PM EST- The SE is getting a 6 core version of the A1501:10PM EST- And the 4 core GPU01:11PM EST- 4.8-inch Retina HD display01:11PM EST- Glass on the front and back. The same glass that's on the back of the iPhone 13 series01:11PM EST- Home button with Touch ID01:11PM EST- Apple is also touting better battery life, though isn't saying by how much01:11PM EST- Adding 5G support as well01:12PM EST- Still a single rear camera. 12MP sensor01:12PM EST- With access to all of Apple's computational photography capabilities01:13PM EST- Pre-loaded with iOS 15, of course01:13PM EST- Will get the latest iOS updates \"for years to come\"01:14PM EST- Pricing starting at $429. Pre-orders this Friday, available March 18th01:14PM EST- Overall this sounds like a straightforward update to the iPhone SE. Which is exactly what iPhone SE users like01:14PM EST- This also keeps Touch ID around for another generation01:15PM EST- Now on to iPad01:15PM EST- Today's focus: iPad Air01:15PM EST- Rolling an iPad trailer01:15PM EST- The new \"even more amazing\" iPad Air01:16PM EST- Jumping right in to performance01:16PM EST- Apple is bringing the M1 to the iPad Air!01:16PM EST- Looks like X series SoCs for the iPads are very much dead, then01:17PM EST- Apple is comparing performance to Windows laptops01:18PM EST- 500 nit display01:18PM EST- Front camera has been upgraded to 12MP. Ultrawide01:18PM EST- iPad Air is also getting a 5G modem (presumably as an option)01:19PM EST- The USB-C port is also getting a performance boost. Now \"twice as fast\"01:19PM EST- Presumably USB 3.x Gen 2?01:19PM EST- The iPad Air also supports the latest Apple Pencil01:20PM EST- Comes with iPadOS 15. And Apple's suite of content creation apps01:20PM EST- Now rolling another trailer about the new iPad Air01:22PM EST- New iPad will come in 5 colors, including a \"stunning\" new blue01:22PM EST- Prices start at $599 for a 64GB Wi-Fi configuration. Pre-orders Friday, available March 18th01:23PM EST- So that's iPhone and iPad down in just 20 minutes01:23PM EST- Now on to Mac01:23PM EST- Cook is recapping the switch to Apple Silicon Arm processors on the Mac01:23PM EST- And how successful Macs based on these new processors have been01:24PM EST- \"It simply has no equal\"01:24PM EST- Apple's been setting records every quarter since they started shipping Arm-based Macs01:25PM EST- Now to John Ternus01:25PM EST- One more M1 chip?!01:25PM EST- M1 Ultra01:25PM EST- \"A monster of a chip\"01:25PM EST- Mac desktop focused01:26PM EST- Discussing the benefits of a homogeneous architecture for SoCs across their lineup01:26PM EST- Chip design: M1 Max is basically as big of a chip as Apple can fab01:26PM EST- So how does Apple one-up the Max?01:27PM EST- M1 Max has a not so secret secret: a die-to-die connector01:27PM EST- Apple's going chiplets01:27PM EST- Apple calls their die-to-die connector UltraFusion01:27PM EST- 2.5TB/sec interposer bandwidth01:28PM EST- Looks like they're using a silicon bridge style interposer01:28PM EST- 114B transistors!01:28PM EST- Die shots: M1 Ultra is basically two M1 Maxes bonded together01:28PM EST- Doubled the memory channels. Total of 800GB/sec of memory01:28PM EST- And up to 128GB of unified memory01:29PM EST- 20 CPU cores: 16 perf, 4 efficiency01:29PM EST- 64 core GPU01:29PM EST- 8x faster than M101:29PM EST- 32 neural engine cores in total01:29PM EST- And of course, twice as many video engines01:30PM EST- 65% less power used than a 10 core x86 chip01:30PM EST- 1/3rd the power usage than a high-end GPU01:31PM EST- \"Game changing chip for our pro users\"01:31PM EST- Now talking about the synergy effects of designing the silicon and the OS. Apple can prepare for everything Apple needs to make a chiplet-style M1 work01:32PM EST- And the number of Arm applications continues to increase on the Mac Store01:32PM EST- Now rolling a video01:33PM EST- Re-checking slides, the CPU perf comparisons have been against the 12600K and 12900K for 10 and 16 core processors respectively01:34PM EST- I'll need to go back to the stream later to check the footnotes on the GPU perf comparisons. But it looked like a RTX 3070 Ti?01:34PM EST- All of this implies Apple has a transparent multi-chip GPU solution that works. That is huge01:34PM EST- But we'll need to get confirmation on this, of course01:35PM EST- Recapping current Apple desktop designs: iMac and Mac Mini01:36PM EST- Neither has quite enough connectivity or modularity (owing in part to the limits of the base M1)01:36PM EST- So Apple is adding another desktop computer design01:36PM EST- Mac Studio01:36PM EST- It looks like a bigger Mac Mini (Mac Less-Mini?)01:36PM EST- Apple is also rolling out a new Apple Studio Display01:36PM EST- Diving into Mac Studio01:38PM EST- 7.7-inches square footprint. 3-inch height01:38PM EST- Designed to fit under displays01:38PM EST- Showing the cooling system. Uses a dual fan blower01:38PM EST- Rear exhaust01:38PM EST- With a focus on keeping down the acoustics01:39PM EST- I/O: 4 TB4 ports on the back, 10GigE port, 2 USB-A ports, HDMI port, and an audio jack01:39PM EST- Wi-Fi 6 and BT 5 for wireless comms01:39PM EST- 2 USB-C ports on the front for M1 Max systems. TB4 ports for M1 Ultra01:40PM EST- And a front-facing SDXC slot01:40PM EST- Can drive up to 4 Pro displays and a 4K TV01:41PM EST- M1 Max version is up to 2.5x faster than the 27-inch iMac. 50% faster than the Mac Pro (16C)01:41PM EST- And outperforms a Mac Pro with a 6700XT01:41PM EST- M1 Ultra performance: CPU perf up to 90% fsater than the 16C Mac Pro01:41PM EST- Still 60% faster than the 28C Mac Pro01:42PM EST- GPU perf: 4.5x faster than the 6700XT iMac, and up to 80% faster than the 6800XT Mac Pro01:42PM EST- Up to 64GB memory for the Max, 128GB for the Ultra01:43PM EST- Can play up to 18 streams of 8K 422 ProRes video01:43PM EST- M1 Max version is 3.4x faster than 27-inch iMac01:44PM EST- And 80% faster than the top Mac Pro01:44PM EST- Now talking about some of the use cases/workloads for the Mac Studio01:46PM EST- Uses 1000kWh energy than a high-end PC desktop over a year01:46PM EST- SSD: up to 8TB01:47PM EST- Designed the Mac Studio alongside the new Studio Display01:47PM EST- Thin bezel display01:48PM EST- Multiple stand option; one offers height adjustment, one doesn't01:48PM EST- And a VESA mount option01:48PM EST- 27-inch, 14.7M pixels. A 5K Retina display01:48PM EST- 600 nits brightness and P3 color01:48PM EST- Also offers Apple's nano-texture anti-glare coating as an option01:49PM EST- The monitor has an A13 chip inside01:49PM EST- Used to drive an integrated 12MP camera (from the iPad) and audio01:49PM EST- This also enables Center Stage support01:49PM EST- Three-mic array01:49PM EST- And a 6 speaker sound system01:50PM EST- 4 woofers, 2 tweeters01:50PM EST- Spatial audio is supported as well01:50PM EST- 3 USB-C ports (USB 3.x Gen 2, 10Gbps)01:50PM EST- And a Thunderbolt port as well01:51PM EST- Can deliver up to 96W of power over the TB port to charge a host device01:51PM EST- A MBP can drive up to 3 Studio Displays01:51PM EST- It's interesting that HDR hasn't been mentioned here at all01:52PM EST- Can be paired with any Mac01:52PM EST- Now rolling a trailer of the new Mac and monitor01:54PM EST- Apple is very picky about HDR. I suspect they don't want to release something less capable than the XDR display. Which implies the Studio Display has been designed as more of the ultimate SDR display, but we'll find out more once Apple posts the product pages01:54PM EST- Mac Studio pricing starts at $1999. $3999 for the lowest M1 Utra SKU01:54PM EST- Studio display is $159901:54PM EST- Pre-orders start today. Available March 18th01:55PM EST- Apple's Arm transition is nearly complete. Just 1 more product to go: Mac Pro. But Apple will talk about that another day01:55PM EST- And this confirms that the Mac Pro is sticking around01:56PM EST- Now back to Tim Cook, who is recapping today's announcements01:57PM EST- And that's a wrap. Thank you for joining us\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17302/the-apple-peek-performance-event-live-blog\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Universal Chiplet Interconnect Express (UCIe) Announced: Setting Standards For The Chiplet Ecosystem\n",
      "Author: Ryan Smith\n",
      "Date Published: 2022-03-02T13:30:00Z\n",
      "URL: https://www.anandtech.com/show/17288/universal-chiplet-interconnect-express-ucie-announced-setting-standards-for-the-chiplet-ecosystem\n",
      "Content: If there has been one prominent, industry-wide trend in chip design over the past half-decade or so, it has been the growing use of chiplets. The tiny dies have become an increasingly common feature as chip makers look to them to address everything from chip manufacturing costs to the overall scalability of a design. Be it simplysplitting up a formerly monolithic CPUin to a few pieces, or going to the extreme with47 chiplets on a single package, chiplets are already playing a big part in chip design today, and chip makers have made it clear that it’s only going to grow in the future.In the meantime, after over 5 years of serious, high-volume use, chiplets and the technologies underpinning them seem to finally be reaching an inflection point in terms of design. Chip makers have developed a much better idea of what chiplets are (and are not) good for, packaging suppliers have refined their ultra-precise methods needed to place chiplets, and engineering teams have ironed out the communications protocols used to have chiplets talk amongst each other. In short, chiplets are no longer experimental designs that need to be proven, but instead have become proven designs that chip makers can rely on. And with that increasing reliance on chiplet technology comes the need for design roadmaps and stability – the need for design standards.To that end, today Intel, AMD, Arm, and all three leading-edge foundries are coming together to announce that they are forming a new and open standard for chiplet interconnects, which is aptly being namedUniversal Chiplet Interconnect Express, orUCIe. Taking significant inspiration from the very successful PCI-Express playbook, with UCIe the involved firms are creating a standard for connecting chiplets, with the goal of having a single set of standards that not only simplify the process for all involved, but lead the way towards full interoperability between chiplets from different manufacturers, allowing chips to mix-and-match chiplets as chip makers see fit. In other words, to make a complete and compatible ecosystem out of chiplets, much like today’s ecosystem for PCIe-based expansion cards.Chiplets in a Consumer Processor - Both Dense and SparseThe comparisons to PCIe are apt on multiple levels, and this is perhaps the best way to quickly understand the UCIe group’s goals. Not only is the new standard being made available in an open fashion, but the companies involved will be establishing a formal consortium group later this year to administer UCIe and further develop it. Meanwhile from a general technology perspective, the use of chiplets is the latest step in the continual consolidation of integrated circuits, as smaller and smaller transistors have allowed more and more functionality to be brought on-chip. In essence, features that have been on an expansion card or separate chip up until now are starting to make their way on to the chip/SoC itself. So like PCIe moderates how these parts work together as expansion cards, a new standard has become needed to moderate how these parts should work together as chiplets.Ultimately, the stated goal of the groups behind UCIe is to establish an open and ubiquitous ecosystem for chiplets. Whether that means simply standardizing some of the physical aspects for easier manufacturing, or enabling a true mix-and-match setup where clients can request a chip built with chiplets from multiple chip(let) makers, a strong underlying standard is needed to make this happen. And the major players in the chipmaking industry are throwing their weight behind UCIe to make this happen.Why Chiplets?The underlying rationale for all of this, in turn, is the increasing use of – and in some cases, outright need for – chiplets. Chiplets are already being used tomix dies from multiple chipmakersor from multiple process nodes, and they’re being used to build large chips that otherwise wouldn’t be possible due to reticle limits. All of which is being driven by either economics in some fashion (not using an expensive, bleeding-edge node for every part of a chip), or a desire to combine IP from disparate manufacturers in a more expedient fashion than spending years taping out a monolithic chip. To be sure, monolithic chips as a whole aren’t going away entirely (moving data remains expensive), but the economics of chip design are inexorably driving the use of chiplets in more cases.Meanwhile there is also a push for performance and efficiency that is driving the ongoing interest in chiplets. Or to be more specific, driving a desire to integrate more functions on to a single chip package. PCIe, for as quick as it is, is still slow by chip standards; the long trace lengths from a CPU to a controller (and back again) add up to a lot of latency, and pushing data around that far is relatively expensive in terms of power. So chip makers are increasingly wanting to bring those functions on chip, to bring down latencies and cut down on power consumption. Which in the case of chiplets (and thus UCIe) means being able to improve performance by upwards of 20x and reducing power consumption by roughly the same amount.UCIe 1.0: New Die-To-Die Spec with PCIe & CXL Layered on Top – Available TodayDiving into the first revision of the UCIe specification, we find something that’s pretty straightforward, and something that’s very clearly designed around the capabilities of today’s packaging technologies. What UCIe is bringing to the table today isn’t so much new technologies as it is standardization between the different implementations of current technologies, so that everyone has a common ground to work with.Perhaps not too surprisingly, this initial version of UCIe comes from Intel, who is donating the specification wholesale to the industry and what will become the UCIe consortium. Intel has been responsible for the initial development of several high-profile open interconnect technologies over the decades – not the least of which has been USB, PCIe, and Thunderbolt 3 – so it’s not too shocking to see them donating another interconnect technology to help kickstart what they (and the rest of the industry) believe to be the next wave of computing. Make no mistake, though, this isn’t an Intel-only initiative, as evidenced by the companies backing the new standard and who will be forming the upcoming consortium. (Whenever you have senior fellows from both Intel and AMD on the same briefing call, you know something big is going on)Under the hood, UCIe borrows from Intel's earlierAdvanced Interface Bus (AIB) technology. Intel previously donated that technology to the CHIPS alliance in 2020, so this is not the first time Intel has released a version of this technology in an open fashion. But UCIe is the largest (and most chiplet-focused) effort yet, as evidenced by the backing of Intel's fab rivals, as well as CPU design rivals.As for the UCIe specification itself, let’s talk about what it does and doesn’t cover. The specification covers the physical layer, laying out the electrical signaling standards that chiplets will use to talk to each other, as well as the number of physical lanes and the supported bump pitches. And the specification covers the protocol layer, defining the higher-level protocols overlaid on those signals to make sense of everything and to provide the necessary feature set.What the specification doesn’t cover, however, is the packaging/bridging technology used to provide the physical link between chiplets. This isn’t Intel giving away EMIB or Foveros, for example. Rather, UCIe is bridge-agnostic; chiplets can be linked via fanout bridge, silicon interposers, EMIB, or even just a plain old organic substrate in the case of lower bandwidth devices. UCIe is meant to work with all of these, as the bridge itself is essentially a dumb pipe to carry the electrical signals between chiplets. So long as a chiplet adheres to the standard (including bump pitch), then it will be able to talk to another UCIe chiplet.On that note, UCIe 1.0 comes with essentially two performance/complexity standard levels. The specifications for the aptly-named “standard package” level are designed for lower bandwidth devices that use traditional organic substrates. These parts will use up to 16 lanes of data, 100μm + bump pitches, and extended channel lengths. At a high level, it’s like hooking up two devices over a contemporary PCIe link, but placing them much, much closer.Meanwhile a second set of specifications covers what the UCIe group deems “advanced package”, and this covers all of the high-density silicon bridge-based technologies like EMIB and InFO. The advanced package specification calls for smaller bump pitches – on the order of 25μm to 55μm – as well as 4x as many lanes per cluster owing to the greater density, and very short channel reaches of under 2mm. Which taken to its fullest configuration, the UCIe promoters believe that an advanced package setup using today’s 45μm bump pitch technology would be able to deliver up to 1.3TB/s/mm of shoreline (linear) bandwidth. That is to say, 1.3TB per second of data would be able to pass through a 1mm edge of a chip.I won’t rattle off every single figure here, but all of this is meant to underscore how UCIe is being setup to serve chiplet needs at both ends of the performance spectrum. For chip makers who just need to get two chiplets together on a single package in a cost-effective manner, there’s the standard package approach. And for chipmakers who need to make two chiplets behave as close as possible to a single monolithic chip, the advanced packaging specifications allow for a lot of lanes – and thus a lot of bandwidth.Meanwhile, it’s interesting to note just what the promoters are expecting in terms of latency and energy efficiency. For all package types, latency is expected to be under 2ns, which is especially critical in chiplet designs that are splitting up what would previously have been a monolithic chip design. Meanwhile power efficiency ranges from a low 0.5 pJ/bit for standard packaging, to an even lower 0.25 pJ/b for advanced packaging. This helps to illustrate just why some chipmakers are eager to get on board with chiplets, as the improvements over discrete PCIe/CXL cards could be significant.The physical layer linking up chiplets, in turn, is new for UCIe. Intel and the other promoters aren’t going too deep into how this works (even in their whitepaper), but at a high level the physical layer standard provides for electrical signaling, clocking, link training, and sideband signaling. A 256 byte Flow Control Unit (FLIT) in turn handles the actual data transfer.Above this is something of a half-way layer, which the group calls the Die-to-Die Adapter. The D2D provides the basis for link state management and parameter negotiation between chiplets. The D2D is also responsible for providing optional support for additional data reliability safeguards via CRCs and link-level retries.Finally, at the protocol layer, chiplet makers have a few different options. The official standardized protocols for UCIe are PCI-Express and its cache-coherent cousin, Compute Express Link, which is itself built on top of PCIe. In discussing their choice here, the UCIe promoters opted to take the pragmatic approach: there is already industry-wide support for PCIe and CXL, so rather than reinvent the wheel themselves, they are going to leverage the existing ecosystems at the protocol layer. This means that UCIe is hitting the ground running with a fully fleshed out and well proven protocol layer that can provide for reliable data transfer and link management, as well as extra bespoke features like cache coherency. And perhaps equally as important, it means customers and chipmarkers alike can leverage their existing software investments in PCIe/CXL, further simplifying the development process and getting UCIe-compliant chiplets out the door that much sooner.In practice, I would have been far more surprised if UCIedidn’tleverage PCIe/CXL in this fashion. PCIe technologies have become the backbone of various other technologies, and the industry as a whole has moved past trying to out-invent PCIe when it comes to basic device interconnect needs.That said, the promoters have made it clear that UCIe isn’t locked to just PCIe/CXL. Future versions of the standard may add other protocols if something comes along and the owner is willing to donate it to the standard.Finally, chipmakers are also free to use their own custom/bespoke protocols as well; they are not restricted to usingjustPCIe/CXL. UCIe supports a raw/streaming protocol option that allows any other protocol to be used. Both chiplets would need to support this custom protocol to make a connection, of course, but even in this case, this would allow a chipmaker to leverage the physical aspects of the UCIe standard to simplify their own design/production.That also means that existing interconnect protocols, such as AMD’s Infinity Fabric, aren’t likely to be going anywhere, even with the introduction of UCIe. Protocols like IF are still far more complex and specialized than what PCIe/CXL are capable of, which makes sense given the very specific coherency requirements for linking up CPU cores and I/O dies. Put another way, the very cutting edge of chiplet design remains ahead of where UCIe 1.0 is starting things off.Going Long: UCIe For External Interconnects, As Well?!And though UCIe is first and foremost focused on providing an on-chip interconnect for chiplets, the standard actually includes provisions for going off-chip. Way off-chip.If a chip/system builder desires to, the specification allows for retimers to be used to transfer UCIe at the protocol level over much longer distances. This does, admittedly, sacrifice the latency and power benefits by virtue of adding (a lot of) distance. But the UCIe promoters envision server customers using this to provide UCIe connections at the rack or pod level, offering a direct chiplet-to-chiplet connection even over long distances.The most interesting and most obvious use case for this kind of setup is silicon photonics. Extending the chiplet concept there, a chip maker could build a co-packaged optical transceiver on to the edge of a package, and then use UCIe to connect it to another chiplet. This would allow optical interconnection directly from the chip, bypassing the need (and power cost) of going to an off-chip transceiver.The promoters are also showing off concepts based around external memory/storage devices. As well as external racks/draws with more SoCs on them.UCIe 1.0 Is Just the BeginningWhile the UCIe 1.0 specification is being released today, the promoters behind the standard are already turning their eye to the future of the technology, and of the consortium itself.UCIe 1.0 is very much a “starting point” standard, which comes from being originally developed in a solo fashion inside of Intel. As previously mentioned, the consortium will be looking at other possible protocols to add to the standard. And thus far, the standard is only defining two of what they consider to be four aspects of chiplet design: the physical layer and the communication protocols. The group would like to go further in making a mix-and-match chiplet ecosystem a reality by also defining standardized chiplet form factors, and even managing other chiplets.This is on top of ongoing changes in chip packaging technologies, which are still progressing. The UCIe 1.0 standard is essentially only defined for 2D and 2.5D chip packaging, but not 3D direct die-to-die technologies like the forthcoming Foveros Direct. As 3D chip packing becomes available, the standard will need to be updated to account for the new capabilities offered, as well as the even greater densities.But for that, UCIe will need a proper consortium behind it, which is why today’s announcement is as much a revelation of the new standard as it is a pitch to other companies to join up and help develop future iterations of the standard. The UCIe promoters group is already a very sizable list, featuring backing from chip/IP designers AMD, Arm, Intel, and Qualcomm, chip fabs TSMC and Samsung (and Intel), chip packaging firm Advanced Semiconductor Engineering, and cloud computing providers Google, Microsoft, and Meta.In short, it’s a who’s who of many of the big players in the chip industry (be them producers or consumers), but the promoters are looking for more members. Pragmatically, the more buy-in the standard gets the more effective and widely adopted it will be, but the group also benefits from the opinions of additional firms, and understanding what their compute needs are.Full details on the UCIe specification can be found on thegroup’s new website, including the UCIe whitepaper. Meanwhile interested companies can also find more information on how to join the group as the formal consortium gets created a bit later this year.Gallery:UCIe Presentation\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17288/universal-chiplet-interconnect-express-ucie-announced-setting-standards-for-the-chiplet-ecosystem\n",
      "Title: AMD's Ryzen 9 6900HS Rembrandt Benchmarked: Zen3+ Power and Performance Scaling\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2022-03-01T14:30:00Z\n",
      "URL: https://www.anandtech.com/show/17276/amd-ryzen-9-6900hs-rembrandt-benchmark-zen3-plus-scaling\n",
      "Content: Earlier this year, AMD announced an update to its mobile processor line that we weren’t expecting quite so soon. The company updated its Ryzen 5000 Mobile processors, which are based around Zen3 and Vega cores, toRyzen 6000 Mobile, which use Zen3+ and RDNA2 cores. The jump from Vega to RDNA2 on the graphics side was an element we had been expecting at some point, but the emergence of a Zen3+ core was very intriguing. AMD gave us a small pre-brief, saying that the core is very similar to Zen3, but with ~50 new power management features and techniques inside. With the first laptops based on these chips now shipping, we were sent one of the flagship models for a quick test.AMD Ryzen 6000 MobileZen3+ and RDNA2 Equals RembrandtEveryone loves a good codename, and the silicon behind these new mobile processors is called Rembrandt, following AMD’s cadence of naming its mobile processors after painters. Built on the TSMC N6 process node, Rembrandt is one of the first products to use this node enhancement and get some additional voltage/frequency benefits of the updated process. Featuring 8 Zen3+ compute cores and up to 12 RDNA2 compute units for graphics, the monolithic Rembrandt die is designed to scale all the way across AMD’s notebook portfolio, from thin-and-light notebooks down at 15 W all the way up to mobile workstation-level performance at 65 W.AMD Ryzen 6000 Mobile CPUs'Rembrandt' on 6nmAnandTechC/TBaseFreqTurboFreqGPUCUsGPUMHzTDPH-Series 35W+Ryzen 9 6980HX8/163300500012240045W+Ryzen 9 6980HS8/163300500012240035WRyzen 9 6900HX8/163300490012240045W+Ryzen 9 6900HS8/163300490012240035WRyzen 7 6800H8/163200470012220045WRyzen 7 6800HS8/163200470012220035WRyzen 5 6600H6/12330045006190045WRyzen 5 6600HS6/12330045006190035WU-Series 15W-28WRyzen 7 6800U8/162700470012220015-28WRyzen 5 6600U6/12290045006190015-28W12 CU RDNA2 Graphics is marketed as Radeon 680M6 CU RDNA2 Graphics is marketed as Radeon 660MFor our testing today, we have the Ryzen 9 6900HS, which is in the top tier product line but designed to be a power-optimized product that AMD uses with select partners based on a collaborative design approach. Anything with the HS at the end means that AMD has been involved in the planning, design, and optimization – the goal here is that AMD wants the HS parts, which have been selected from production with the best performance-to-power ratio, as showcasing the Ryzen brand at its best.New FeaturesFor this new core to move from 7nm Zen3 to 6nm Zen3+, a number of new additions to the microarchitecture have been made. Normally we consider this to be either a simple manufacturing optimization due to the process node change, or something more fundamental to the core when there’s a microarchitectural change. In this case, AMD hasn’t really discussed any specific improvements coming from the smaller process node, instead focusing on improvements made to the SoC as a whole. At the announcement of the hardware, the headline was ’50 improvements relating to power’, and with the hardware launched we now have insight into what a number of these are.Fundamentally, the base CPU core is the same Zen3 microarchitecture as the previous generation. Clock for clock, AMD is expecting Zen3+ to behave the same as Zen3 in raw performance output/IPC, with the changes being solely at the power level. Fundamentally AMD is saying that a number of the libraries used in the design were power-optimized, while still keeping a high-frequency capability. Normally a power-optimized design kit offers low power and low area at the expense of frequency, so in reality AMD is finding what it considers to be a more optimal point in that spectrum.AMD highlighted the following as ‘microarchitecture’ enhancements:Per-Thread Power/Clock Control: Rather than being per core, each thread can carry requirementsLeakage: Optimized process and design elements updated for better efficiencyDelayed L3 Initialization: Removes the need to wait for L3 to fully wake from an idle state, making it asynchronousPeak Current Control: Better control of power ramp from idle to peak to reduce stress and save powerCache Dirtiness Counter: If cache misses are high (workload is bigger than L3), stay in a high power state even when low power is requested to reduce overall power useCPPC Per Thread: Previously the OS was only aware of workloads per core, now is aware per-thread for finer controlPC6 Restore: Hardware-assisted wake-from-sleep for quick responseSelective SCFCTP Save: Before waking up cores, refer to utilization before PC6 sleepEnhanced CC1 State: Better sleep control when core is idleWith this being a mobile chip, a lot of context here is on power saving and responsiveness when in-and-out of sleep. The concept of keeping cores at a low idle power, or moving to sleep when idle, is all in aid of enabling a device with a long(er) battery life. For example, if a core is idle for a few seconds, would it be better to put in a sleep state? This isn’t just idle frequency, but actually turning parts of the core off in a specific order – and then how and when those parts are turned back on, which has a power cost all of its own – ultimately leading to working smarter in order to conserve power usage.On the SoC side of power matters, AMD is showcasing that Rembrandt has better control over the internal Infinity Fabric power states, better global ‘almost off’ power states, support for LPDDR5, DRAM self-refresh, panel self refresh support, support for sub-1W panels, and accelerators to help come back out of sleep states, some of which we’ve mentioned.On the firmware and software side, AMD is aiming to make Rembrandt a better transitional experience from being connected to power to being a mobile platform. Normally Windows relies on internal power plans for ‘Balanced’, ‘High Performance’, or ‘Battery Saver’ – sometimes OEMs even have their own unique power plans on top of their own software. From AMD’s perspective, they want users to have the benefits of both High Performance and Battery Saver without having to manually adjust these power plans. Which brings us to AMD’s new Power Management Framework, or PMF.PMF is an extension of a lot of previous notebook inputs, outputs, and controls – taking data from sensors such as skin temperature, but also SoC power, OS workloads, display information, noise profiles, then converting that into a ramping power profile that can offer anything from battery saver to high performance on a sliding scale.The key here is that graph – normal Windows offerings have those individual three points, whereas AMD Rembrandt, on select optimized systems, will enable by default a scalable profile that will move up and down the graph depending on external factors. When speaking to AMD, they said that this would be baked into the firmware and automatically enabled when running in the Windows-standard Balanced Profile. User can manually select other profiles to force into those modes, but Balanced Profile will be the PMF sliding scale.Users will not be able to disable PMF, but more than that, AMD states that it is up to the system vendor to announce if they are using PMF or not. Given that it's likely that few (if any) of them will bother to make that disclosure, I think this is somewhat of a frustrating decision – we can’t test this without a lever to disable it, whereas end-users won’t know if their system even has it or not.Finally, AMD lists its updates for Rembrandt in the display power section of the chip. As we move to more efficient processors coupled with high-resolution, high refresh rate panels, the power consumption of the panel is becoming a major factor. But part of it is down to the SoC inside.We’ve already mentioned Panel Self Refresh, the ability for a panel to update only the section that has actually changed from frame to frame, but AMD is saying that they can also do this with Freesync enabled. On top of this, Freesync allows the refresh rate during video fullscreen playback to be reduced to the native framerate of the video (e.g. 23.976Hz), thus saving power. The sub 1-watt panel support means that AMD has a list of validated panel vendors that can provide lower power panels (typically 1080p at 300 nits) for long battery life designs. Physically the new chip also implements new SVI3 regulators, which AMD claims provides a faster and more discrete control over the voltage required from the chip.On top of this is the graphics engine itself, Rembrant moves from a Vega 8 solution to an RDNA2 solution, offering more performance and better efficiency. This extends to AMD A+A Advantage support as well, offering advanced power control when paired with an AMD discrete graphics solution.In short, everything about the new chip is about control: going to sleep, and waking from sleep as quickly as possible.15W vs 28WOverall, we’re getting to a point in the laptop space where the vendors are now competing against each other on actual power consumption. Historically we would talk about U-series mobile processors at 15-28 watts, and then H-series at 45-65+ watts. In 2022, Intel has introduced P-series at 28 watts instead, and both companies are stating that due to improvements in design, the chassis that used to fit a 15-watt processor can now enable a 28-watt version.As a result, we’re seeing some really awkward comparisons if you go by official numbers. Both AMD and Intel are comparing last-generation 15-watt solutions to current-generation 28-watt solutions, or comparing 28 watt systems today against equivalent designs that housed other processors before. Be careful when reading those first-party data points. That being said, both companies also want to exhibit their notebook processors at their best, so we end up with the higher-powered H series anyway in some nice chonky designs. It won’t be until reviewers can get their hands on the regular, run-of-the-mill U/P series hardware that they'll be able test like-for-like in the same way.Our Review UnitFor the initial review, AMD shipped us the ASUS Zephyrus G14, one of the recent generation flagship models that are updated year-on-year with the latest hardware. We still havethe G14 that was shipped with the Ryzen 4000 Mobile (Zen 2) series, although for the Ryzen 5000, AMD went with the ASUS Flow X13, which is a more ultraportable design. The G14 is still in that bracket with a slightly larger screen, slightly beefier discrete graphics, and a bit more battery. There’s even an AniMe Matrix display on the back.AMD has paired each of these designs with the HS-branded processors. The HS models are tuned and optimized parts that AMD co-calibrates inside flagship notebook designs with its partners, so it becomes the obvious choice for AMD to sample laptops based on these chips for each launch for review. Not only that, depending on the thermal design of the laptop, the actual power setting provided by the vendor can change based on the design. We’ve had the following in for test:Ryzen 9 6900HS in an ASUS Zephyrus G14 (45W Default, 65W Turbo) + RX 6800S GPURyzen 9 5980HS in an ASUS Flow X13 (15W Default, 35W Turbo) + GTX 1650Ryzen 9 4900HS in an ASUS Zephyrus G14 (35W Turbo) + GTX 2060 GPUAs with most laptop processor launches, despite the rated TDP on the official processor listing, it’s up to the OEMs to configure and tune the exact performance to the cooling on each unit. This makes comparisons, aside from simply ‘chip vs chip’, quite difficult, as simply adjusting the processor frequency (rather than any other frequencies) has a direct impact on any IPC or performance-per-watt comparison. As a result, we rely on end-performance numbers based on the CPUs as shipped – but we’ve also tested the 6900HS in 35W mode just to see the difference.The other big factor in our ASUS Zephryus G15 is going to be the DDR5 memory. As we’ve seen on other platforms, moving to DDR5 can cause a variety of changes in performance for both CPU and gaming – it depends how memory bandwidth dependent the tests were. This is even more true for AMD, given that the memory frequency is also tied into the infinity fabric frequencies inside the processor. Over time AMD has disaggregated the two, but there’s still a level of synchronicity involved with additional dividers, meaning that memory frequency is still an important factor.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17276/amd-ryzen-9-6900hs-rembrandt-benchmark-zen3-plus-scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Ampere Goes Quantum: Get Your Qubits in the Cloud\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2022-02-16T14:01:00Z\n",
      "URL: https://www.anandtech.com/show/17255/ampere-goes-quantum-cloudbased-solutions-with-rigetti\n",
      "Content: When we talk about quantum computing, there is always the focus on what the ‘quantum’ part of the solution is. Alongside those qubits is often a set of control circuitry, and classical computing power to help make sense of what the quantum bit does – in this instance, classical computing is our typical day-to-day x86 or Arm or others with ones and zeros, rather than the wave functions of quantum computing. Of course, the drive for working quantum computers has been a tough slog, and to be honest, I’m not 100% convinced it’s going to happen, but that doesn’t mean that companies in the industry aren’t working together for a solution. In this instance, we recently spoke with a quantum computing company called Rigetti, who are working with Ampere Computing who make Arm-based cloud processors called Altra, who are planning to introduce a hybrid quantum/classical solution for the cloud in 2023.It’s All About the QubitsThe striking thing about quantum computing has always been the extravagant hardware required – a ‘golden steampunk chandelier’ of tubes and cables all required to bring the temperature of the hardware down hundredths of a degree above absolute zero. This minimizes thermal effects on the elements of a quantum computer, known as the qubit. Depending on the type of qubit involved, those cables can carry microwave signals, and how the chandelier is constructed often determines how many qubits are involved.Qubits are the quantum computational power, and the more you have (in theory) the more exponentially more computing power there is on tap. However, because quantum computing doesn’t deal in absolutes, sometimes those qubits are used for resiliency, which is needed at such extreme environments. You’ll find that quantum computers list an ‘effective’ number of qubits equivalent to the computational power, rather than the actual physical number present. Beyond that, there are different types of Qubits.Transmon qubits rely on superconducting electron pairs being controlled inside a three-dimensional cavity. A spin qubit controls individual electron spins with magnetic fields. Most companies use Transmon qubits (Google, IBM, Rigetti), whereas Intel dropped its Transmon development in favour of spin qubits. Exactly how many qubits a system needs to do ‘useful’ work is a hot topic in the literature, although Google claims it has performed computation impossible on classical computing with only 53 physical Transmon qubits – again, another hot topic for debate.The ultimate goal of quantum computing is to enable computing resources that can solve classical problems whose compute requirements are impossible within reasonable time frames. The typical example is Shor’s Algorithm, to find prime factors of number (essentially solving the underlying basis for cryptography that should take millions of years) in seconds. Another example is solving a typically quantum-like system, such as chemistry and biochemical interactions. Also optimization, going beyond typical ‘traveling salesman’ into machine learning – the idea is that quantum computing can assist training or inference to check all possible answers, simultaneously.Quantum computing has always been seen as a future horizon of where high-performance should go. However, it is one of those elements that always seems 10-20 years away. In the early 2000s it was seen as 10-20 years away, and the same is true today. However there are now more startups and funded ventures willing to put in more research to get these systems up and running. One of those is Rigetti, and today is an announcement of a collaboration with Ampere Computing.Put The Quantum In The Cloud, Ampere Plus RigettiFor the last few years, there has been a focus in putting high-performance computational resources within reach of everyone. The offering of cloud computing, web services, and 1000s of processors at your fingertips has never been more real, or been more easy. With enough money in your bucket, the cloud providers make it easy to spin up resources for storage, networking, services, or compute. Cloud computing like this is designed to scale as and when you need it. Rigetti wants to do the same with quantum computing.Rigetti Computing, founded in 2013, is a series C funded quantum computing startup with a public $200m investment to date. Late last year, it announced the start of its new scalable quantum computing infrastructure – with a chip containing 40 transmon-style qubits, multiple chips can be embedded onto a single package for a single quantum computing chandelier. The goal of these designs is to accelerate machine learning, both for quantum compute and classical compute, and as a result, they’re partnering with Ampere Computing which makes the Altra Max Arm-based CPUs.The goal of the partnership is to provide a cloud-native solution combining both classical and quantum computing. Spinning up an instance would include some qubits and some cores, allowing customers to use standard machine learning APIs that would be naturally split across the two types of hardware. In this heterogeneous combination, the goal is to take advantage of the quantum system to do what it does best, and then leverage the traditional compute resources with the Altra Max CPUs for machine learning scale out.Rigetti says that its solution will scale to hundreds of qubits, while Ampere resources can scale as naturally as most compute can. Rigetti chose Ampere as a partner in this instance because of what the company can provide – Ampere always states that its processors are cloud-native, or built for the cloud, and that its 128-core chip can provide 1024 cores in a traditional 2U server with Arm Neoverse N1 performance.At this point of the partnership, Rigetti and Ampere are at work developing a combination system up and running. Right now, the Ampere CPUs are to be part of the coupled performance resource, although Rigetti says that there could be a time where Ampere’s hardware might replace the FPGAs in the control units of the quantum system itself. The partnership aims to start working on a proof of concept, creating a local-to-Rigetti example of a cloud-native hybrid quantum/classical infrastructure, and creating a software stack optimized for machine learning. Rigetti says that it is already working with customers interested in the co-design to give itself targets for software optimizations.The timeline for the rollout is still early, with a proof-of-concept planned over the next few months, then deployment with tier 1 cloud partners through 2023. The idea is to initially work with key customers to help optimize their workflows to combine with the hardware. Then it’s simply a case of scale out – more qubits for quantum, more CPUs for classical. Ampere is set to launch Siryn this year, its own custom Arm core built on next generation process node technology, and we were told that the scope is to bring in future Ampere generations as they are developed.Rigetti says that it has made strides in enabling transmon qubits viable at scale. Intel dropped its transmon qubit program because it didn’t think it could scale, but also because they could create spin qubits fairly easily (however, control is a different part of that story). Rigetti plans to scale to the hundreds of qubits, allowing cloud customers to take a chunk of however many qubits they need at the time. One issue I brought up with them is synchronicity, and it sounds like they have a system that, in a traditional sense, can be asynchronous to scale. Rigetti believes there are elements to machine learning, both training and inference, that will scale with qubit count in this way.Is Quantum Computing still a distant hope? The promise here is a hybrid product, with quantum and classical resources, for cloud customers in 2023. I fully expect that to be a viable use case. However, as is always the question with quantum computing – what problem is it solving, and is it better than classical?\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17255/ampere-goes-quantum-cloudbased-solutions-with-rigetti\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AnandTech Interview with Miguel Nunes: VP for Windows and Chrome PCs, Qualcomm\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2022-02-14T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/17253/anandtech-interview-with-miguel-nunes-senior-director-for-pcs-qualcomm\n",
      "Content: During this time of supply crunch, there is more focus on ever on the PC and laptop markets – every little detail gets scrutinized depending on what models have what features and how these companies are still updating their portfolios every year despite all the high demand. One of the secondary players in the laptop space is Qualcomm, with their Windows on Snapdragon partnerships to bring Windows to Snapdragon powered laptops with x86 virtualization and a big bump in battery life as well as connectivity. The big crest on Qualcomm’s horizon in this space is the 2023 product lines, using CPU cores built by their acquisition of Nuvia. At Tech Summit in December 2021, we spoke to Qualcomm’s Miguel Nunes, VP of Product Management for Mobile Computing, who leads up the charge of the laptop division to see what’s coming down the pipe, and what measures Qualcomm are taking to bring a competitive product to market.Miguel has spent 12 years at Qualcomm, working through the product management of the smartphone, tablet, and now compute portfolio. This involves managing multiple design teams globally, directing strategy, and enabling partnerships with OEMs. This comes on the back of seven years at Intel as an application engineer and technical marketing manager at Intel. Miguel’s background is heavy in the interface between engineering and actual end product, and it’s clear in talking to him he likes to get his hands deep into how the systems he manages performs.Miguel NunesQualcommIan CutressAnandTechIan Cutress: My first question I think is one that's quite personal to me, because I've been waiting for it for so long! We finally got the announcement that 64-bit x86 emulation is coming to everything Snapdragon 7c and above running Windows 11. Why has it taken so long?Miguel Nunes:It took a bit of time to get right! If there is one thing we've seen going through a lot of these application journeys, is that there's a lot of badly written software out there! I mean, there is. It has nothing to do with the architecture really, but the software can be horribly… just the amount of issues you run into. It is not stuff you expect. Just strange stuff pops up that you need to work through.IC:Are you talking like instruction flow not really mapping properly to the core?MN:There was some of that, but you’ll probably understand what I mean when I say that half the stuff we saw was installer bugs doing ‘if machine’ type checks. So the install fails, or the driver doesn't unload because it's trying to put it in the x86 directory. I mean just little things like that, that has caused problems. So yeah, while it should have worked on paper but it doesn't because it's in the wrong place, or the DLL isn’t loading, stuff like that. We took a while to get there. In terms of instruction optimizations, I think there are some that aren't super optimized, but that wasn't the majority of issues. The majority of issues are mostly just loading issues, CPU detections, and there's a lot of software out there that use libraries they get from somewhere else. Those libraries are doing the right thing, so it's just a linking problem, and it took a bunch of time to get that stuff ported. So you'll see a bunch of open-source stuff ported as well. It just takes time to get that ready.IC:So it's not necessarily an instruction translation issue, it was dealing above that at the OS layer?MN:Yeah, it was stuff we didn’t expect. But it [Windows] is a legacy ecosystem that hasn't changed - so nobody's had that problem before!IC:And compared to some other x86 translation models by your competitors, who own their own ecosystem, sign the software, they don't necessarily have those issues.MN:[nods]That’s right.IC: There were some previews with Windows 10 with it enabled, but you’re doing a clean cut with Win11. Is there a technical reason behind locking it into Windows 11?MN:Not our decision. There’s nothing technical.IC:So it’s purely a next generation of hardware, it's all Windows 11, so let's have that as the starting point?MN:Yeah, it wasn't a Qualcomm decision. You’re right, because if you think about the previous OS is Windows 10, and Windows 11 is the new platform. It was a Microsoft decision of where it lands.IC: At this event, I know this year Qualcomm isn’t saying much about the Nuvia acquisition, but at the launch Qualcomm said that in 2022 there would be sampling silicon, for 2023 products. That was reiterated that at the investor day a month ago. So everything's still on course?MN:Yes.IC: Just to clarify for me, is Nuvia working on a specific core? Or is it a full SoC, paired with an Adreno GPU and Qualcomm’s 5G and others?MN:It's a good question, because I know everyone thinks ‘Nuvia chip’. It requires a lot more of a chip than what Nuvia had [when we acquired them]. They were mainly the CPU core at the time. There is a fabric around that as usual that glues it together, with our optimized memory IP and stuff around that. So think of it more on the CPU performance subsystem. On the GPU, we've got the GPU assets in Adreno. We're [going to be] making a lot of changes there too. On the GPU side we’ll be making it more PC friendly, and then we are scaling the other stuff such as AI, and other things like that too. Nuvia is part of the solution, but there's other stuff there of course.IC: Traditionally Qualcomm has used a big/little, or a big/medium/little hybrid design. It sounds to me that Nuvia is solely working on that high-performance core. Can you expand if they're doing an equivalent efficiency-core to go with it? Should we expect that to be something to be called Kryo?MN:We will have two core structures that will be similar to be what we already have. We really believe that the performance and the efficiency optimized combinations work. We see that in our long battery life data, because a lot of the workloads don't need that performance. So you will see that structure still in place - what exactly we do on each core, is TBD. But you will see a similar structure - you just won't see a bunch of big cores!IC:The goal with the Nuvia design is to scale from 7c+, all the way up?MN:Oh, eventually we want to use it across the board. This technology scales really well. As Cristiano and others have mentioned, our focus primarily is going to be after the higher performance computing first, but our roadmap will scale. The other thing we will see us try to do, which we've been trying for a while in this industry, is to accelerate. The notebook industry is a little bit slower than the mobile industry, so how do we accelerate the pace of innovation? So you'll see some activity there as well so that we can really drive innovation faster.IC: At the investor day, it was quite poignant that on the slide it said ‘Nuvia Team’. So should we think of the Nuvia team as a separate design house internally, compared to say, Kryo and Adreno? I'm wondering to what level they're integrated into the company? Last I heard, they were still in the same offices they were when they were a startup!MN:I think a lot of that is just COVID, regarding offices and things like that. But no, they're part of the company, and they're part of the team here. I see them as our CPU team. We do have different teams, so it's hard to say because some teams in one location will work on a core, others work on another core, but at the end of it, they're part of a team. They’re just part Qualcomm, they're not separate, just part of our CPU team.IC: In terms of internal roadmaps, the big focus with Nuvia is this first laptop-focused chip in 2023. Usually when we speak to companies, especially the size that Qualcomm is, we discuss 3-year, 5-year roadmaps, and then 7-year exploratory roadmaps. Is that all on the cards with the Nuvia team?MN:Oh absolutely. We're looking how we scale the capabilities way into the future. Do you want to see our long-term roadmap?IC:Yes, please!MN:[laughs]But for sure, you know it's something we want to leverage as much as we can. The technology is great, we've got the performance, we've got the efficiency, and we've got so much IP that we feel pretty good. You've probably gathered that, as we hinted at analyst day, we feel pretty good that we can build out. It’s really awesome.IC: One question that has come up which I think is relevant given what Apple is doing in the space - A lot of the senior staff at Nuvia are the people who actually were behind that chip. But because Qualcomm is focused on this Windows on Snapdragon ecosystem, do you see the competitor for that Nuvia laptop to be Apple because both the chips are ARM-based, or Intel, because with Intel, you're both using Windows as the main operating system?MN:That is a really good question. You know, I would honestly say both, and both in the sense that, there's going to be ‘within ecosystem’ competition, because one ecosystem is a Windows ecosystem and one is a Mac ecosystem. It also depends on the segment. I would say it's probably a segment question. Because if you look at certain segments, Windows is king, and that is saying they are the majority of that segment. So for there, Intel is our competition. Then for other segments, we can provide the same capabilities, if not better, than what Apple is doing. So it depends on the lens you're looking at it though, but it's really both. And we're fairly agnostic, and honestly, if it's Windows or if it's Chrome in that space, our value is independent of the operating system. There's a lot of work that goes into the operating system tuning and, getting the product working well, which Microsoft spends a bunch of time doing, but you know, our platform is independent of that.IC: So at this event, you're announcing Snapdragon 8cx Gen 3. Is it a holdover until the Nuvia-equipped chips are ready?MN:No, no, not at all. The way I would see it, and when you see the stuff we talked about when you get your hands on - the data we showed puts our 8cx Gen 3 as a mainstream product. So you'll see our comparisons are against mainstream Core i5, which is really still the mainstream CPU for consumers. So that's where 8cx Gen 3 competes really really well. You'll see the performance, the power, and you’ll see the AI story perform really well. That's what we meant to do.It was targeted there, targeted at the main segment. Our roadmap in the future takes us in a different level of performance. So it's just literally different tiers, and we believe we have to keep the tiers going, because this market is a killer market. Even our Snapdragon 7c+, the stuff we're doing there is really keeping the good/better/best model going and bringing value to the entry tiers. It’s really interesting actually - the entry tier has evolved so much over the past 24 months, more than I think anybody expected, and so there's a lot of value there, too. So we will continue to go after the good/ better/best or the low/mid/ high type of products.IC: So one of the things that has interested me is that Qualcomm plays a lot in this ultra-thin, ultra-portable market – it’s called an ACPC, but it's ultra-portable with additional connectivity in a traditional sense. What's the scope for going beyond that? Currently we see the ultra-portable market around 15-20 watts, maybe up to 28 watts, but then the laptop space also expands to 45 and 65 watts. Qualcomm has Adreno graphics IP that the company keeps saying it scales – but does it scale to something bigger?MN:It's an interesting question. You know, this wattage thing, either way, it drives me crazy, because what are you comparing it to? TDPs are a little bit mysterious as it depends on how you measure TDP. But we will deliver performance TDP, and this ‘higher level TDP performance’ at a lower tier. So even you'll see today, when we'll show you will show you i5 level performance, which is at a 22 watts TDP, we deliver it at like 9 watts. So we'll deliver perf at a TDP.Honestly, you know, to tell you the truth - we can scale pretty high. It's what the market wants, and what products we aim to compete with. But we want to compete in the ultra-mobility space because it’s important to us. We are not focused on desktops today and things like that, but if the market sees value, and our products are good in that market, it's something we definitely look at. We focus on the mobile market, but with a much higher level of performance at a lower TDP.IC:So just for my personal edification, when you say ‘22 watts performance at 9 watts’, that's great. But I just want that same efficiency at 22 Watts!MN:You want higher performance at the same TDP! Which again, it's a good question - very few people understand that. It's going to be an OEM design point choice. The OEM decide if it wants to build a thinner form factor, or it wants higher perf at the same TDP. I think there's probably a combination of both, but we will scale fairly well, but still at a significantly lower TDP than what you would get with others - significantly. So then the question is whether it is time [for Qualcomm to move into the new market]? I mean, I'm a big fanless believer, and so I like fanless designs. But we could scale on the technology. So it's really about what are the products the right design points/IC: I'm coming to your event with the Intel flagship EVO design laptop – a Core i7-1195G7 in the MSI Prestige 14. On the plane it gives me 16 hours battery life - which I know is still technically six hours less than what Qualcomm provided three years ago, but it runs all my software. At what point does Qualcomm demand a minimum specification? You already have reference designs - at what point do you design with your partners an equivalent to the Intel EVO ecosystem ensuring that all designs meet very specific certification, as well as driver support over 12/24 months ensuring that performance is still high? Is there scope for that sort of platform in your ecosystem?MN:You know we don't have a platform like that. It's interesting - we believe that every product should be optimized. That’s where we work with our partners - we really push that aggressively. Every product should be optimized to the best capabilities. You'll see this when we talk about security, as we’re not simply throwing a bunch of security features in there. We don't believe, we don't want an EVO programme, we don't want a different thing - everybody should get a good minimum. Why shouldn't you, as a consumer. get the best security? You don't want to go create a bunch of sub-brands and confusion in the market - we want every product to be highly optimized.Ultimately, just look for the Snpadragon logo, and you get security, and you get higher efficiency. Maybe it's a scale thing, as we can do it now, because we are working with a smaller number of designs. In reality, we could do it, and but we just want to get to a point where we the software is so highly optimized, we won't see a lot of product variants that are outside of components selection.IC:I’ve got a few choice words for some of your partners on that!MN:[laughs]It's not like getting in the x86 world, as when you design those things, cooling is a big deal because it’s going to throttle if you don’t optimize. That's not an issue for us. We have a good system design, and it's easier for people to follow - then it comes down to component choice: are they picking the right displays that are not power-hungry, stuff like that.IC: So I notice you have a Snapdragon 8cx Gen 3 laptop beside you - is that the Qualcomm reference design, or is that a partner design?MN:It’s our design.IC:What’s so special about it?MN:It's actually what we use to develop o. It's a real thing, it’s a real PC, and it's a real form factor. It's usable, and I use it as my daily device now, so it's a real thing.IC:So it's a development platform, not a reference design?MN:For us it’s a development platform, both hardware and software. We have a bunch of connectors in here that connect and stuff, but it's as close to a real product as possible.IC:It’s a very vibrant red!MN:Snapdragon red. That's why we had it made.IC:Do you mind me asking who your partner with that is?MN:We used one of the more mainstream ODMs. We designed it and they just manufactured it.IC:And you’ve got two cameras at the top?MN:Yes, and we have other demos with other cameras in the demo room.IC: Speaking of things that use AI functionality, I use NVIDIA’s RTX voice on my main PC for live recordings to help with background noise. But if I already have audio recorded, there's no RTX Voice-like post-processing software that I can just drag a file into. If you have the hardware, can Qualcomm do it?MN:When you asked me this last night (before the event), I said I’d look into it for you. The result is I think we can! We’re preparing a demo on it. I asked the internal team if we can post-process, and we actually can.IC: Also on the AI front, this has been one of Qualcomm’s strengths in smartphones. But when it comes to laptop design, your competitors have a problem with the fact that there aren't that many applications that they can focus on when they talk about AI, aside from camera, and they start clutching at straws to promote the AI. Ultimately they’re optimizing some software that has limited scope. So how are you approaching the messaging and the application as it comes to ACPC AI?MN:We'll have some demos on stage today with us with Microsoft, and you will see them talk about AI and some of the things they're doing. We see significant value for AI in productivity use cases. If you just think about all the stuff that's happening in the cloud, Office 365, there's a bunch of stuff. There's a lot of AI processing in cloud today, but you want to bring that closer to the edge. So a lot of productivity things for Office, and a lot of things in Microsoft Teams. A lot of it is centered on the camera, but it's not just for pictures, it's for taking the camera inputs and using it for things. So you'll see a lot of innovation around that.As you know, the processing power you need to run that stuff is very high. In our Snapdragon 8cx Gen 3, we have 29 TOPs of AI. It's just massive amounts of AI performance, and we will use it all.IC:Is that 29 TOPs for CPU, GPU plus Hexagon, all three combined together? (MN: Yes) Do you actually have software that can use all three simultaneously, or are you relying on three separate applications using it?MN:It depends on the software. There is software that could go across them, and then for some software it makes sense to use one accelerator versus the other. But you could go across them if you wanted to.IC:So could I argue that you're focusing a lot on corporate use case for AI acceleration first, because that's been your major market today?MN:Correct, yes. I think that's where we all relate to, because we've been using these things so much for Teams, and Zoom, and things like that. There is just so much stuff you can do to make those experiences better. Resolution cleanups, I mean, there are so many things you can do.IC:Video upscaling? So you can just transmit 240p and it scales to 1080p?MN:Lots of things like that. Cleaning the background lights, as cameras on these things aren't always great, to make it look better. So there are lots of different things around that, and we've got a big focus on camera too - cameras have become super important in PCs all of a sudden. So a lot of work on camera, audio, and it's a lot of use cases including productivity, so it's not just one thing.IC:Please mandate a minimum 1080p 60 fps camera in every Snapdragon laptop!MN:Oh, I wish I could! I would like to go higher than that! But yes, we know what you mean.IC: With everything Qualcomm is announcing at Tech Summit about ACPC, what do you think will fly under the radar that people should actually focus on?MN:It's a good question. There are certain things we're doing to camera, and with audio, where people have it good today, but it's not great. It could be so much better. The enhancements we're making are really going to make it so much better. Some of this stuff is not new per se, but it's significantly better than what most have, and it's possible to improve those experiences and I think a lot of the laptop experiences are becoming closer and closer to your phone.IC:Do those experiences still rely on the OEM buying into those features?MN:Not that much. In fact, one of the things we want to do, and back to your Evo questions, is just to make sure that some of this stuff is everywhere. Just the basic capabilities that everybody should get - it's not optional and it's just in your product. Everybody should have great audio, and we will work to make sure that those features are there. Even with cameras, as an example, we’re working with the sensor ecosystem to make sure that the PC OEMs have access to sensors that are actually better than 720p. There's a whole enablement piece to go make those things standard. Our goal is to ensure every device is going to have a great experience, not just the special ones that OEMs that choose to make.Many thanks to Miguel and his team for their time.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17253/anandtech-interview-with-miguel-nunes-senior-director-for-pcs-qualcomm\n",
      "Title: Hands-On With The Huawei P50 Pro: The 2022 Flagship with a Snapdragon 888 Option\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2022-02-09T16:00:00Z\n",
      "URL: https://www.anandtech.com/show/17246/handson-with-the-huawei-p50-pro-the-2022-flagship-with-a-snapdragon-888-option\n",
      "Content: For those of us outside the US, Huawei has maintained its presence in a number of markets in which it has grown its sales over the last decade. Even without access to Google Services or TSMC, the company has been producing hardware and smartphones as it pivots to a new strategy. To lead off in 2022, that strategy starts with the Huawei P50 Pro, the next generation of flagship photography camera. The P series from Huawei has often been the lead device for new cameras and new features to attract creators, and the model we have today is a new twist in the Huawei story: our model comes with a Qualcomm flagship Snapdragon chip inside.Perspective: Huawei and HardwareFor anyone outside the US, it’s not hard to notice that the noise around Huawei’s smartphone efforts has been dulled since it was put on the US entity list in May 2019. At that point the company was readying devices with the Kirin 990 chip on 7nm, and was in development of the next generation hardware on 5nm. Over the course of the next 12 months, knowing that it would be losing access to TSMC and Google services, various pivots had to be made to ensure that the smartphone business stayed solvent. The company had to take its final orders of smartphone silicon it had at TSMC through to mid-2020, and then try and implement its own path without access to leading edge manufacturing.For flagship smartphones, the Huawei P40, launched in April 2020, used the Kirin 990 5G hardware. In late 2020, after the final orders of chips were delivered, Huawei launched the Mate 40 Pro with the Kirin 9000, built on TSMC’s 5nm process. Huawei was a lead partner alongside Apple at TSMC on 5nm, and as a result the company built a stockpile of silicon not knowing when (or if) it would get access back. Questions were raised if Huawei had enough stock of Kirin 9000 chips to last the lifecycle of the Mate 40 Pro.Through 2021 however, the company did not launch a flagship smartphone outside of China. Normally we see Huawei alternate with every six months we would get a P-series focusing on camera, or a Mate series focusing on internals and everything else. But in 2021, we didn’t get either. Without building its in-house silicon, one of Huawei’s key value-adds it had for its ecosystem was lost, and that included optimizations with in-house AI designs of which part of its software was based.Cue 2022, and at the start of this year, Huawei has launched the P50 Pro and P50 Pocket outside of China. The P50 Pro is a different beast, as it comes in two versions, much like the Samsung Galaxy Ultra: Huawei offers the P50 Pro with either the same Kirin 9000 as the previous Mate 40 in China and select markets, or with Qualcomm’s 2021 flagship SoC, the Snapdragon S888, everywhere else. Due to the economics (and perhaps stock levels of Kirin), Huawei has come to an agreement with Qualcomm to buy top silicon for its top smartphone.Users will be quick to point out that the S888 in the P50 Pro is last year’s Qualcomm flagship SoC. Qualcomm recently announced the 2022 Snapdragon 8 Gen 1, which is set to be the lead chipset in a lot of smartphone brands, with devices launching very soon (Mobile World Congress is in a couple of weeks). The question as to why Huawei went with S888 rather than S8g1 is more of logistics – Huawei is finding a new cycle and needs to get to grips with where it will put flagships in future. The Kirin 9000/S888 is likely a better similar performance comparison than K9000 vs S8g1, so in order to keep all models equivalent, that’s how it had to go. Also, Huawei is likely organizing contracts for future smartphones which are purely Qualcomm (or purely MediaTek), and there’s a cycle that goes to that. The P50 Pro was simply too early in that cycle, but also helps Huawei sort the other business issues out.Ideally with a device that offers two different chips inside, we’d love the opportunity to compare both, however we only have the S888 version to test today. As perhaps one of the last S888 flagships, it will be interesting to see how Huawei has combined its design aesthetic (which is usually really good) with an opportunity to learn from a year’s worth of S888 flagships.Hands-on with the Huawei P50 ProOne thing that has impressed me over the years with Huawei devices is simply the refinement of the design ID. Since the P8 came to market, I’ve noted that Huawei’s efforts in design have been towards just making the device look good and fit well. Sometimes that has come with a splash of color, such as the Pantone models, but also similarly the understated models still stood out as premium.The only thing that really bothered me is that despite the looks, the inability for a device to stay still on a table without having a case due to the glass back had meant that using that clear plastic case that comes with the phone is a must. As a result, it sometimes dulls that multi-colored visual, as it did with the P30 Pro, or in this case with the Golden Black P50 Pro I have, makes it look cheap. It also means that despite the 6.6-inch screen being crammed into a svelte chassis with near zero bezel, a plastic case adds volume.The screen is a 6.6-inch OLED, with the wrap-around visual that we’re used to on devices this size, but it’s not actually wrap-around. This means you can actually hold the edges, and mistouches are far less frequent. Huawei does this with a very slight curve that is noticeable on the screen, but not enough to detract from a proper full-screen in-hand feel. The OLED display is a 2700x1288 resolution, which is actually a 21:10 aspect ratio and seemingly unique to the P50 Pro.Huawei lists it as having a 120 Hz refresh rate (as long as Dynamic refresh rate is selected in the options), with 1440 Hz PWM dimming for power response and 300 Hz touch sampling. While no official peak screen brightness is listed, we were able to get 600 nits at peak brightness on white. Huawei promotes the use of true 10-bit color support and DCI-P3 accuracy, although doesn’t put an official number on how much of the color gamut it covers (nor to what accuracy).Some of the display is lost with the front facing camera, as Huawei uses a punch-hole at the top to fit a 13 MP, 100-degree wide-angle f/2.4 design, capable of recording video at 4K30 or 1080p240 in slow motion with a 14 cm auto-focus (labelled AIS Pro) designed for selfies. Interestingly enough the front facing camera software will only photograph in 4:3 (3120x4160), 1:1 (3120x3120 which ends up being a 4:3 crop), or in full-screen 21:10 (4160x1888 ?!?), and not in standard 16:9. Actually in playing around with the camera, the rear camera won’t do 16:9 either.That being said, it’s hard not to notice the rear camera arrangement. Obsessed with the multiple circle design over several products, the P50 Pro implements what the company calls a ‘dual matrix’ design, with one circle of cameras having three units, and a second circle contains the telephoto, flash, and detectors. Combined, this makes a four camera solution.The main camera is a 50 MP 27mm f/1.8 with OIS and laser assisted auto-focus. The sensor by default uses 4-in-1 pixel fusion, providing a 12.5MP images, along with AI enhancements on top. By selecting the ‘high-res’ option in the ‘More’ mode, users can get the full 50MP output, also with AI. The secondary camera is a Leica-optimized monochrome camera (40 MP, f1/6), making a return from being absent in the last couple of generations. This allows for the black and white shots that Leica typically optimizes for, or provides extra contrast context for both focus and main image optimization.Having tested the camera lightly over the last few days, not in any rigorous fashion, there are a few images I'm quite proud of. For example:click through for full resolutionHuawei's night mode has always been one of the best on the market, with the use of multi-image input as well as AI enhancement. I compared the P50 Pro to my current daily device, the mid-range LG Velvet.LGVelvetHuaweiP50 ProAIOffAIOnNightModeFor these images, the LG Velvet in AI Off mode is what I was seeing in person. The fact that even with AI Off, the P50 Pro was doing a good job (aside from the non-16:9 aspect ratio), is a big plus.There is a 13 MP 120-degree ultra-wide angle 13 mm f/2.2 camera as the third camera, which is also being promoted for close-up macro shots to 2.5 cm, although we were able to get closer. In my short time with the device, I can close up to 1cm for macro shots when there’s enough light, although the field of unblurred vision is relatively small until the AI decides to kick-in Super Macro mode, at which point the small array of shots I’ve taken are really good for some of our close-up product photography.Super Macro ModeThe fourth camera is a 64 MP telephoto (f/3.5, OIS). Huawei will be promoting the P50 Pro as supporting 200x zoom, although this tears down as a 3.5x optical zoom at 16 megapixels (4-in-1 fusion again), 2x lossless zoom by cropping, and then beyond from 7x to 100x is a cropped-but-upscaled image.Taken at 35xScreenshot showing full sensor imageIf I take a screenshot while in this zoomed mode, one good addition at least is that it shows the full wide shot in the top corner, which was always an issue on previous devices of being able to actually find what you wanted to point at. That being said, despite promoting 200x, Huawei is fudging the numbers a bit here – the selectable range is 0.5x (ultrawide) to 100x (upscaled digital zoom).On top of the cameras, Huawei is promoting its XD Optics and XD Fusion Pro image engine. This is the latest update to the underlying software that combines images from multiple cameras based on a variety of internal algorithms to give you the best image, regardless of which sensor is used, regardless of the lighting and setup, and regardless of the scene. On top of the AI engine, Huawei aims to combine both the best raw data and the best raw post-processing for final images to which the P-series is famous for – some of the best in the industry. While I can’t today put a definitive stake in the ground to say where the P50 Pro sits, the images I have taken have been of a high quality worth posting on our reviews and sharing on social media.Other camera features on the P50 Pro include dual-view video mode, supporting both front and rear simultaneous recording, now in HD. Huawei now also has its own video editing tool called Petal Clip, aimed to be a simple-to-use editor optimized for the hardware.As with the previous generations, Huawei has enabled an in-screen fingerprint sensor on the P50 Pro, and it still remains one of the fastest and most accurate I have ever used. This can be used in conjunction with the face unlock feature, which was eerily simple to setup with a single photo.For the hardware inside our P50, as mentioned we’re running with the Qualcomm Snapdragon S888 edition coming to the UK. This has a total of eight cores: one Arm Cortex-X1 prime core at 2.84 GHz, three Arm Cortex-A78 at 2.42 GHz, and four Arm Cortex-A55 efficiency cores at 1.8 GHz. This is paired with Adreno 660 graphics. What this S888 does not have which some might find interesting is that this is a 4G model. Before Huawei’s link with TSMC was terminated, the leading edge 5G modems from Huawei were either integrated in the Kirin 9000, or were bigger for devices like home access points, and not smartphones. Even then, the S888 has a 5G model, but Huawei has decided not to integrate it, perhaps related to the entity list limitations on Qualcomm selling chips to Huawei, or simply because Huawei couldn’t enable the internal solution for it with what they had access to. But Huawei's Kirin 9000 model of the P50 Pro is also limited to 4G, so that must be an overall design decision.We have some performance graphs of tests we’ve been able to run. Nothing big and differentiating in this small hands-on, but it’s going to be a bit behind the curve of other flagships when s8g1 and Dimensity 9000 devices come to market. If anything, it means the pricing has to match other one-year flagships.Performance in the few tests we've run was decidedly lacklustre compared to the premium S888 devices we've already tested, even in performance mode. Geekbench 5 was still in the right field for single thread, but down 10% in mutlithread, and PCMark showed a sizeable difference. In idle power, we saw the Normal mode and Performance mode differ by 30%, showcasing that the Performance mode is more akin to a benchmarking mode. It is possible that the metrics here are more indicative of aiming for better efficiency with the SoC, leading to better battery life.When we look at graphics performance, the P50 Pro scores quite regular against other S888 devices in normal mode, but in performance mode, the GPU is basically cranked to the maximum. This was enough for the device to start showcasing overheating warnings and stop the test - we were only able to complete the test when we strapped a sizeable fan to the phone with a rubber band. This further confirms that the performance mode is moreso for benchmarks and it's really the normal mode numbers that is what are going to be experienced from day to day. It's worth noting that the P50 Pro in Norm mode reduced itself to the sustained values after only two cycles of our test, a couple of minutes each.Other features on the P50 Pro include models up to 12 GB of LPDDR5 and 512 GB of storage, whereas ours was an 8 GB / 256 GB edition, which might be the highest configuration for the Snapdragon version. Wi-Fi 6 and BT 5.2 are integrated, as well as a USB Type-C 5 Gbps connector for charging. Huawei lists the P50 Pro as IP68 rated, defined as resisting water ingress up to 1.5 meters for up to 30 minutes, which is sometimes missing from flagships as it does cost extra to enable it (and sometimes increases the device size and weight). Despite this, the P50 Pro is 8.5mm thick, and weighs 196g, which is in the ballpark of most other flagships.Normal mode battery life, 200 nitsThe battery inside is a 4260 mAh lithium-polymer unit, and although Huawei doesn’t explicitly list battery life, we achieved over nine hours at 200 nits using the maximum refresh setting, which puts it in the middle of our other S888 phones. It does support a wired 66 W SuperCharge as well (charger in box) and wireless 50 W SuperCharge, the latter requiring Huawei’s own charger stand sold separately - normally a $130 equivalent price, but for launch is down to $50.Aside from the Golden Black unit we have in for our brief test, Huawei is also launching Cocoa Gold, Pearl White, and Charm Pink. The Snapdragon S888 4G model of the P50 Pro, with 8 GB DRAM and 256 GB of storage, will retail for 1200 EUR. This puts it around the same price as the Samsung Galaxy S21 Ultra upon launch this time last year or the Xiaomi Mi 11 Ultra, both of which have 5G but it is trading some features such as memory and battery life. They both have Google Services, whereas Huawei does not, which has always been a limiting factor in this stage of Huawei’s story.AnandTechHuawei P50 ProSoCQualcomm Snapdragon S8881 x Cortex-X1 at 2.84 GHz3 x Cortex-A78 at 2.42 GHz4 x Cortex-A55 at 1.80 GHzAdreno 660 Graphics6th Gen AI EngineHisilicon Kirin 90001 x Cortex-A77 at 3.13 GHz3 x Cortex-A77 at 2.56 GHz4 x Cortex-A55 at 2.05 GHzMali G78MP24Dual NPU + microNPUDRAM / Storage8 GB + 128 GB8 GB + 256 GB8 GB + 512 GB12 GB + 512 GB+ proprietary NM CardDisplay6.6-inch OLED, 600 nits2700x1288 (21:10)120 Hz Refresh Rate300 Hz Touch1440 Hz PWM Dimming? DCI-P3Size158.8 x 72.8 x 8.5 mmWeight195 gBattery4360 mAh (rated), 4260 mAh (typical)Charging66 W SuperCharge Wired (in-box), 11V/6A50 W SuperCharge WirelessRearCamerasMain50 MP, f/1.8, 23mm, PDAF, Laser AF, OIS, 4-in-1 FusionB+W40 MP, f/1.6, 23mm, Autofocus, 4-in-1Telephoto64 MP, f/3.5, 90mm, PDAF, OIS, 3.5x Optical, 4-in-1Wide13 MP, f/2.2, 13mm, 120-degree, AutofocusOtherXD Fusion, 1080p960, Leica OpticsFront Camera13 MP, f/2.4, 13mm, 100-degree, AutofocusIOUSB Type-C 3.1Wireless (local)802.11ax (Wi-Fi 6) and Bluetooth 5.24096 QAM, 3.6 Gbps1024 QAM, 2.4 GbpsCellular4G FDD 1/2/3/4/5/7/8/12/17/18/19/20/26/28/32/664G TDD 34/38/39/40/41GeoGPS (L1 + L5 dual)AGPSGLONASSBeidou (B1l + B1C + B2a)GALILEO (E1 + E5a dual)QZSS (L1 + L5 dual)NavICGPS (L1 + L5 dual)AGPSGLONASSBeidou (B1l+C + B2a+b)GALILEO (E1 + E5a + E5b)QZSS (L1 + L5 dual)NavICIngress RatingIP68Dual SimSingle + NM Card, or Dual (also does Single + NM)Launch OSEMUI 12.0.1Harmony OS V2Launch Price8 GB + 256 GB:1200€It’s unclear how a 1200 EUR device, using last year’s flagship SoC, without 5G, is going to perform, even with all the added extras. If the new upcoming flagships are 1400 EUR and above, the Huawei becomes an interesting purchase comparison point for those that don’t need 5G – if upcoming flagships are still around 1200 EUR, then the Huawei P50 Pro needs to drop in price to be in consideration.Update: Since this review was written, Samsung launched the S22 series. The 6.8-inch S22 Ultra (at 12 GB / 256 GB) is 1299 EUR, featuring an integrated S-Pen, this year's flagship SoC, as well as new camera features, Google Services, and an IP68 rating. This is going to make the 1200 EUR P50 Pro a hard sell.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17246/handson-with-the-huawei-p50-pro-the-2022-flagship-with-a-snapdragon-888-option\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Interview with Alex Katouzian, Qualcomm SVP: Talking Snapdragon, Microsoft, Nuvia, and Discrete Graphics\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2022-01-31T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/17233/interview-with-alex-katouzian-qualcomm-svp-talking-snapdragon-microsoft-nuvia-and-discrete-graphics\n",
      "Content: Two driving forces are driving the current technology market: insatiable demand for hardware, and the supply chain shortages making it difficult to produce enough in quantity to fulfil every order. Even with these two forces in action, companies have to push and develop next generation technologies, as no competitor wants to sit on their laurels. That includes Qualcomm, and as part of the Tech Summit in late 2021, I sat down with Alex Katouzian, Qualcomm’s GM of Mobile, Compute, and Infrastructure to talk about the issues faced in 2021, the outlook for 2022, where the relationships lie, and where innovation is headed when it comes to smartphone and PC.We’ve interviewed Alex a couple of times here at AnandTech, and while he has been well trained in dealing with the likes of our pointed questions, we can tell he’s being open and honest, telling us as much as he can (and sometimes a little bit extra). Alex has been at Qualcomm for 20 years, working his way up through wireless mobile technologies, product management, and taking over the MCI business unit in 2017. Prior to that Alex was engineering multimedia and wireless chipsets, showcasing that his whole career has been about putting next-generation connectivity into as many gadgets as possible.Alex KatouzianSVP and GM MobileIan CutressAnandTechIan Cutress: Here at Qualcomm’s Tech Summit, you are announcing the new Snapdragon 8 Gen 1, with new marketing, and it’s a new chip on Arm v9. I kind of want to step back and look at the year as a whole – Qualcomm launched the Snapdragon 888, and that's been in a lot of flagships, accompanied by a lot of them with Qualcomm 5G RF solutions. I know you're an employee, and I know you're probably biased, but objectively what grade would you give Qualcomm for the year?Alex Katouzian: Well our Snapdragon 888 is top in premium tier solutions. When you look at the capabilities that were available to our customers and ecosystem through the technologies we provided, we topped the chart in terms of graphics capability, overall performance, AI capability, and camera performance - there are multiple phones on the 888 that are topping out on DXOmark. I would say our Elite Gaming capability was very good, we’ve done that in cooperation with the developers and game engine developers. All of them were very cottoned on. So overall, I would say a pretty high grade on the overall performance of a premium tier phone.IC: Would you say you've had any pinch points this year, that get solved next year?AK: Obviously the pandemic brought on some supply chain issues, and the severity of them fluctuated throughout from the end of 2020 through into 2021. But by being a fabless company, it allowed us the freedom to be able to move from fab to fab, not only on the latest digital parts of technology, but also on the old analog technology we've had the ability to move from fab to fab as well. We alleviated a lot of the supply issues based on the ability to have engineering resources available to move parts and make room for shipments to happen. Even though we still went through some supply crunches, and we probably will go through some supply crunches in 2022, we alleviated a lot of that, and we are able to get out our products.IC: Does moving from fab to fab like that disrupt the unique relationships with each foundry?AK: The relationship between a fabless semiconductor company like us and the foundry has to be very tight. But these were unprecedented times, and the ripple effects were great because the supply chain usually takes about 18 months to 2 years to come to fruition. When we had that shutdown period of say, 9-10 months, every facility in the supply chain was shutting down. They had to restart that, to recalibrate equipment, to hire staff, and any expansion plans were on hold. Then the surge in demand for consumer electronic equipment was so high that it takes time for it to catch up. Given these ‘unprecedented times’, I don't think that a foundry is going to blame a company like us to try to diversify in such a way that it allows us to ship products. If you look further down the supply chain, we are suppliers to our customers, and they're gonna have to diversify in their supply as well, and someone had to make that choice.IC: Aha, so goes both ways, both up your chain and down it.AK: Exactly!IC: So when I came to my first Qualcomm Tech Summit, the venue had specialist equipment installed for 5G because we were still very early, very nascent in 5G. The rollout at the time was more focused on industrial applications, but a good amount of the talk was on mmWave versus Sub-6. Where the US is focusing on mmWave, the rest of the world was focused on Sub-6. If feels kind of odd, but we're kind of still having those discussions! What has changed from Qualcomm’s point of view in those discussions?AK: I think more and more, the carriers are realizing that ‘a layered cake effect’ of their network, spreading out, becomes a more necessary thing to do in crowded environments or congested areas. mmWave roll-out, even though sometimes it a little bit more difficult because the amount of base station towers, you have to have a lot more of them, even though they are a little bit more difficult, carriers are realizing that it actually is much better to have mmWave in congested areas. Then, as you go further out to more suburban areas, that Sub-6 can cover and even more, maybe possibly even falling back to LTE.But that realization, and the deployment of the equipment - remember, we're at the end of the second year of the commercial release of 5G, and for every G, the transition takes between 10 to 12 years. So the rollout of mmWave is going to start to happen more. I think more countries are going to start to deploy: we have deployment in Japan; Korea is slowly starting to come up; China's starting to think about the fact there's lots of bandwidth available there for them. Then the sectors that mmWave can get applied to are so much wider than what we had before, in different sectors of say economy, manufacturing, retail - and then you have other means of deploying mmWave to private networks via small cell. Those are all helping. I think when you look at industrial, when you look at manufacturing, when you look at automotive, there’s a lot of room to grow for mmWave, and that expansion is going to be from the ecosystem.IC: For this event, we're learning a lot about Snapdragon 8 Gen 1. As you've said, and as the numbers show, you guys are a premium smartphone leader, continuing from the previous generation. In my mind, I'm juggling up the fact that as a premium leader, you want to maintain the market share, but you also need everything to work to maintain unit sales. So on some level, you have play it safe to make sure you have that product working correctly - but on other levels, you have to take risks with new features, and next generation technologies that may or may not work long term. Compare that to all your competitors that are trying to catch up, and can take those risks. How exactly do you balance that? Is there a right way to talk about how that is balanced?AK: I think the way to look at it is that the strategy at Qualcomm has been to take the risk on the technology on the largest channel that you have, and then re-use that technology in products in growth sectors that are adjacent to our largest channel. So mobile is obviously the largest segment we have at Qualcomm - so you will see the risk taking in advancing technology in mobiles first, even though when you're looking at a mobile first environment, power dissipation should be top of mind. However, let's say I want to apply AI capability to mobile, power is the number one criteria. Then I can figure out my performance to power ratio, and I can scale from there. But if I want to look at automotive, and I want to do autonomous driving, performance is the first criteria, power is the second criteria, maybe even the third. However, we have the ability to scale with our technology roadmap, so when we take the chance and the risk of putting certain technologies in mobile, we start to think about what the applications are so we take that risk, we put it in, and see if we can grow that market. Then we can figure out how to scale the base design into a market that needs performance first, and power second. So that's how we look at it across the board.It's the same exact application on CPU - if you remember, a few years ago, we had our architectural license with Arm. We brought in on our own CPU, and we took advantage of that, and we kind of tried to grow the market. At that time, we didn't have adjacent businesses that we were really growing - we were thinking about them, but we weren’t growing them, but the application is exactly the same. Now with Nuvia coming on board, we're going to do the same thing you know – a ‘one technology roadmap’ that can scale. We will first have that first instance of that in a PC chip sampling next year, getting to products in 2023. Then we're going to think about how to bring it into mobile, automotive, and then infrastructure application as well. So we’ve already thought about how to do the power performance scaling for different businesses we’re going after. Same with graphics, same with camera capabilities, same with video capabilities. Each of these technologies take the risk of it being introduced in mobile first, scale it, think about other markets that we go to and then we adjust our performance and power points.IC: It's great that you said video capabilities, and speaking about the camera, because one of the points that came out of the presentations and some of our briefings is that Qualcomm is pushing 8K HDR, and more advanced 4K HDR recording. But the silicon doesn't have an AV1 decode unit, and some people have said that it is because you guys aren't part of the AV1 Alliance. Can you shed some more light on the decisions there?AK: So when we plan our technologies out, sometimes on devices such as the premium tier, we have to think about things two or three years in advance. The timeframe that we need to introduce these devices is very critical based on the OEM launches, and the needs of the carriers. Given those circumstances, we haven't been able to accommodate AV1. We'll probably accommodate in the future, but up until now, we just haven't been able to do it. It’s not that we don't want to do it - we just haven't had the ability to plan it in in time to try to make that happen.IC: But you’ve got people working on it?AK: Oh, yes - we have multiple video experts. I mean, we contribute to standards on video constantly. We're involved with where it's heading. We just think that in time, we'll have those capabilities - there's no doubt. We just didn't have the opportunity to get it in time schedule-wise, planning it in such a way that it will hit our timeline.IC: When it comes to relationships - Qualcomm and Microsoft. The Windows on Snapdragon ecosystem is a long partnership, and we're now seeing 64-bit emulation coming with Windows 11, enabled with last-gen devices and future devices. How's that relationship progressing? You’ve had Microsoft on stage at this event!AK: Extremely well. Our partnership with Microsoft has always been solid, in terms of a cloud-and-software company cooperating with a technology-and-hardware company, to provide a solution that makes sense for the consumers. They're using all those buttons, across phones, across PC, and we're collaborating across IoT, even looking at AI capabilities, not just in phones but also in PCs, and possibly in data-center collaboration as well. However, our relationship on the PC side has been becoming more and more sophisticated as they learn the capabilities and technologies that we have. They are also taking the risk of exploring how multimedia capability can become more and more prevalent on the PC. As you know, I think you agree that mobile traits are coming to the PC and the realization that having the ability to be mobile and connected and having great multimedia, including camera, including audio, improving AI, all of that is becoming more and more important, along with performance capability of the CPU and GPU - a combination of which can become very well suited for a mobile workforce.But let me give you a little bit of a history. Every business Qualcomm has gotten into that has had incumbents. We've had three ingredients that we've put in to meet KPIs (Key Performance Indicators). One is having a disruptive technology that can make a difference, two is a long-term investment, and three is a go to market partner. I can go as far back as UMTS - we invested in UMTS for seven years before it became more mainstream and to grow. Now, obviously, we had a disruptive approach which was a highly integrated solution with a protocol stack that was worldwide tested, and we could make more OEMs horizontal versus vertical. So we had that disruption, we put in those seven years of investment, and our go-to-market partners were the horizontal OEMs that exist today. We did the same thing for Wi-Fi, and we did the same thing for RF front end.When we are getting into the PC market, we're doing the same thing. We have disruption, we have long-term investment, and we've been doing this for about five years now. We just invested in Nuvia, not just for the purpose of the PC, but a piece of it is for the PC, and we have our go-to-market partner in Microsoft for Windows, and we're working with Google Chrome based devices as well. So those three ingredients are met, and we’ll continue improving our devices and capabilities. This also means working with Microsoft end-to-end, to exploit all of the underlying technologies to their OS to be optimized to run on Snapdragon devices, and then drivers to expose all the technologies to users.IC: If I could get some clarity on the relationship between Qualcomm and Microsoft. There have been public discussions regarding what you call Windows on Snapdragon (named because of the relationship, although a lot of people like to call it Windows on Arm). There have been reports saying that, between Microsoft and Qualcomm, there's been an exclusive agreement for these Windows on Arm/Windows on Snapdragon collaboration exclusivity is slowly coming to an end. Is there anything you can comment on that?AK: I really can't comment on that, but as I said, the cooperation between us has never been this strong, and I think for the foreseeable future we're going to work with them more to increase the capabilities of Arm and/or Snapdragon based solutions.IC: So you mentioned Nuvia a couple of times, a topic very dear to my heart. I've met with Gerard, Manu, and John pre-acquisition, and I was really looking forward to their server chip they were going to produce. So I must admit a little bit of me died when you guys acquired them, but I must applaud that it was a clever move in acquiring them. Given what Qualcomm CEO Cristiano Amon was saying at the time, saying about ACPC since its inception, it really is the direction it needs to go in. I know Nuvia is more the topic for next year's summit, as you've said sampling in 2022, product in 2023. How is that acquisition going, and how is the team integrating?AK: Extremely well, extremely well. The founders are very active, and I can personally say I probably meet with them on a weekly basis, up to two or three times a week. My colleagues that are running the PC business within my business unit meet with them on a daily basis. We review roadmaps all the time, we review strategies, we are very cooperative on how to go to market, not just in the near future, but in the longer term as well. The system architecture, the chipset architecture, are all being discussed continuously - partitioning is being discussed continuously, so it's a very tight integration of this team, and that's exactly how it also happened in the past. For example, we bought two Wi-Fi companies, one is Atheros. We integrate those teams in within the Qualcomm group and kind of make them one Qualcomm, and then we proceed that way. That's exactly what's happening with the Nuvia team as well. And I would say, on the server side, that those are opportunistic businesses that we’ll go after. So we’ll see what happens.IC: Will the Nuvia core be called a Kryo core, or will you have any fancy marketing name for it?AK: I haven't thought about it yet – it definitely won't, or probably won’t, be Kryo. But we haven't thought about it yet.IC: Give me a call when you need a name!AK: Hah, sure!IC: If you can perhaps clear something up for me: is the Nuvia team making a single core, or both a big core or a little core? Or is it that they’re dealing fully with the SoC structure into which you add in the connectivity and the graphics?AK: It’s both, all the above. By that I mean that it's impossible for us to put out a chipset solution as sophisticated as this without having the entire system being taken into consideration. Think of it this way: the CPU by itself is part of the ‘one technology roadmap’, but so is graphics, and other things. Then we're really thinking about bringing a complete system solution to the PC and changing it in such a way that you don't go after the traditional designs. You know, we talked about this before, we're looking for bill of material savings, we're looking at design savings, we're looking at internal routing, we're looking at shell designs, and we’re looking at thermals - the whole thing. So it has to be a complete solution. So they're definitely involved in the whole SoC design, and they're involved in looking at multiple different cores, where it makes sense. Whether it's big cores, little cores, or a combination of how many.IC: Nuvia’s design will be Arm-based, and the Nuvia team comes from people who built Apple’s M1 and talent from Google. So will that chip compete primarily against Apple because it's ARM-based, or against Intel because it's Windows-based?AK: The devices that we're going to come out with based on the new CPU design and the new architecture, will compete head on with Apple.IC: So when you do your comparison charts, you'll be comparing against Apple?AK: Yes.IC: Is that because Intel is no longer in the picture? Or is that just because that’s where the market is?AK: The way we look at it is that we're not going after discrete designs: we're going after an SoC and the architecture that makes the best sense for the PC. Like I said, mobile traits are coming into the PC, and I agree with you that performance wise compared to mobile it’s much higher, it has more power dissipation capability, but we're going to try to make it a lot sleeker and have a lot more mobile based. So our comparison is to a company that can do both types of SoC and bring that capability to the PC, so we're really preparing ourselves for that.IC: Qualcomm has a very efficient graphics architecture in Adreno. It usually performs really well in the mobile SoCs, but we're currently at a time where the discrete GPU market is going bananas, and everything is selling. I would love to see a scaled Adreno GPU, and given that Intel is also coming out with its own, Qualcomm could be an amazing fourth competitor in that space. We would love to get your comments on that, but the key thing is: if you were to do that, what color would it be? AMD is red, Intel is blue, NVIDIA is green, and so is an Adreno discrete GPU going to be gold?AK: It's hard to say because we don't even have that product! But if you think along the lines of premium, yeah, you know, it would be some shade of gold. But think of it this way: we have to enter the PC market with a very sophisticated design that we've already worked on SoC-wise till now, but we're going to get a boost when it comes to CPU capability. We can scale our GPU capability just like we discussed before, and we have the ability to scale across these devices. So we definitely will have much more performing GPU and CPU cores. Obviously also concentrating on power dissipation, and those ratios have got to be right, so if we pick a design point within the PC, we'll definitely need those requirements. Then we can break off at discrete GPU if the business makes sense for it, but we definitely have the capability to scale to that level and the design capability to produce something in that market. Plus, the hard play is about having much more of an ecosystem. So once we get into the market more heavily, and get more games developed on there for example, and other applications, then, the possibility always existed.IC: To me, it’s a no brainer! Excellent, thank you so much Alex for your time.AK: Thanks Ian!We have an upcoming interview with Qualcomm's Miguel Nunes, Senior Director of ACPC, about the future of that line of products. Say tuned!\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17233/interview-with-alex-katouzian-qualcomm-svp-talking-snapdragon-microsoft-nuvia-and-discrete-graphics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: An Exclusive Interview with Mobileye CEO Prof. Amnon Shashua: The Future of Autonomous Driving\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2022-01-04T16:00:00Z\n",
      "URL: https://www.anandtech.com/show/17151/an-exclusive-interview-with-mobileye-ceo-prof-amnon-shashua-the-future-of-autonomous-driving\n",
      "Content: It’s hard to avoid that autonomous vehicles are a key part of how we are going to be discussing the technology and machine learning of the future. For the best part of a decade, we’ve been discussing the different levels of autonomy, from Level 1 (basic assistance) to Level 4 (full automation with some failover) and Level 5 (full automation with full failover), and what combined software and hardware solution we need to create it. One of the major companies in this space is Mobileye, acquired by Intel in 2017, and the company has recently celebrated 100 million chips sold in this space. Today we’re talking with CEO and Co-Founder Professor Amnon Shashua about the latest announcements from Mobileye at this year’s CES, including the company’s next-generation all-in-one single-chip solution for Level 4.Professor Shashua co-founded Mobileye in 1999, covering driving assistance and autonomous driving, focusing on both system-on-chip and computer vision algorithms. The company launched its IPO in 2014 on the New York Stock Exchange, and was acquired by Intel in 2017 for $15.3 billion in the largest Israeli acquisition to date, to which he retains his CEO role and is a Senior VP of Intel. The company is set to IPO again in 2022, with Intel remaining the primary stakeholder. Prof. Shashua is still an academic, achieving his PhD from MIT in 1993 on ‘Geometry and Photometry in 3D Visual Recognition’ and being a member of faculty at the Hebrew University of Jerusalem since 1996. He currently holds the Sachs Chair in Computer Science, and has since 2007. He has published over 120 peer-reviewed papers and holds over 94 patents, some of which have evolved to form startups for using AI for the visual and hearing impaired (OrCam) or natural language creation. He is also involved in a new digital bank for Israel, the first new bank in the region for 40 years.Prof. Amnon ShashuaCEO, MobileyeDr. Ian CutressAnandTechToday’s announcement at CES 2022 focuses on the EyeQ Ultra, a new all-in-one silicon solution designed for full Level 4 autonomous driving. The goal with EyeQ Ultra is to provide a package to be introduced into robo-taxis and regular consumer vehicles in the 2025 timeframe, at a total cost of $5000-$10000 for a system-level power of around 100 W. This comes along with new advancements in algorithm development and new sensors coming directly from Mobileye/Intel partnerships, which we discuss in the interview.Ian Cutress: You're here today because CES is a big deal for Mobileye - you're the co-founder of Mobileye, currently under the heading of Intel. For people who haven't necessarily heard of Mobileye before, can you kind of put the company into the context of where it sits today?Amnon Shashua:We are one of the biggest providers of driving-assisted technologies using computer vision. We have a family of silicon system on chips, called the EyeQ processors, that use front-facing cameras and other sensors to power a wide spectrum of driving assist customer functions. We have shipped 100 million chips to date, and we have been on this road for 22 years. We work with almost all the major and non-major car makers, and we are also working towards full autonomous driving. This covers the entire spectrum from driving assistance, premium driving assistance, level three/level four, computer vision, driving policy, and mapping all the components that you need to build an autonomous car. In the meantime, we're also supplying to the car industry, the technology for powering driving assist today.IC: One of the key elements to Mobileye is the high-performance low-power silicon design, paired with computer vision and autonomous driving algorithms. This week at CES, you're announcing the EyeQ Ultra – what exactly is EyeQ Ultra? I’ve read the press release, and there are some mind-boggling numbers involved.AS:So I know, when you build a business, it's all about cost/performance metrics. So it's not just enough to have very, very good performance, you have to have very, very good cost if you want to build a viable and sustainable business. This is what Mobileye has done over the years - today on the road is our fifth-generation SoC called EyeQ5. These SOCs are based on a number of proprietary accelerators - so it's not only that you have CPUs, but you also have accelerators with a wide spectrum of workloads. Not only is it deep learning compute, but it is also multi-threaded compute, cores that are similar to when to FPGA, cores that are SIMD, VLIW, very long instruction words - a variety or diversity of accelerator cores that Mobileye over the past 16 years has been developing. Today, we have EyeQ5.We are announcing the EyeQ6 at CES, which comes in two varieties, and the EyeQ Ultra. The EyeQ Ultra is an L4 autonomous vehicle on-chip. It has, in terms of some details, 64 accelerator cores which are divided into four families of cores. We have one family which is purely deep learning compute, we have another family which is CGRA, a coarse-grained reconfigurable array which is similar to FPGA. We have another family of cores that multi-threaded which is SIMD, VLIW. So, altogether there are 64 accelerator cores, 12 RISC-V CPU cores (each is 24 threads), and we have a GPU and a DSP by Arm. It's on a 5 nm process, and the first silicon is going to come out at the end of the fourth quarter of 2023, so two years from now. Normally in the cycle, from engineering sample to start-up project production, in all of our families of SoC, we start with a ‘cut one’ which is the first engineering sample. Then half a year later comes ‘cut two’ and then there is a PPAP (automotive supply chain) process which is very critical for making the SoC automotive grade. This takes somewhere between a half a year to a year, and then it's ready for production. So 2024 we can have vehicles with EyeQ Ultra on the road, but in terms of mass volume, the start of production automotive-grade, it's going to be in 2025. It's one package, one monolithic piece of silicon.IC: A lot of times when we talk about silicon in autonomous driving and vehicles, there has to be backups. Is the EyeQ Ultra solution two chips for redundancy, or are you trying to plan redundancy on just a single chip? Have the rules about what customers need regarding redundancy in that respect changed over the years?AS:So we're working at a system-level redundancy in many aspects. One level of system redundancy is about the sensing modalities that we have. We are working end-to-end using only cameras, and then end-to-end using only radars and LIDARs. So in the interest of the sensing state, the perception of perceiving the world is done through two parallel streams which do not talk to each other. The cameras do not talk with the radars and LIDARs, and the radars and LIDARs do not talk with the cameras. This gives us a system-level redundancy, we call that ‘true redundancy’. At the chip level, the 64 accelerator cores are divided into two pieces of 32 cores, creating an internal redundancy. There is an ASIL-D MCU outside of the EyeQ Ultra such that with the dual ‘ASIL-B plus ASILD’ MCU, we get an ASIL-D system. That’s at a product level – in addition, there is a fail operation, where we use an EyeQ6 Low. An EyeQ6 Low is a very, very small chip - it's about a five TOPS of silicon. It's very, very cost-efficient and that processes a number of cameras for a fail operation stream, a complete fail operation stream, such that if something goes wrong, the car can go into a safe state and stop on the side.IC: When we see companies talking about L4 autonomous driving, they have multiple chips in play - multiple silicon pieces, whether it's multiple CPUs or multiple GPUs. It sounds like the EyeQ Ultra is designed to be an all-in-one solution, with no other silicon compute resources in play (unless it's for redundancy or failover)?AS:We have gone through an evolution - today we have a system on the road which is just camera-based. It's not L4, but it is L2+. The first launch is in China, with the Geely Zeekr. It has 11 cameras around the car for 360 visuals and two EyeQ5 chips. The EyeQ5 is about 15 TOPS, 15 DL Tera- operations per second. So it has two such chips, and it provides an end-to-end capability. We have unedited videos on the internet showing how this car drives at multiple sites in Tel Aviv, Jerusalem, Munich, Tokyo, Paris, China, Detroit. I'm going to show this at the CES. So it's really end-to-end for L2+, but it doesn't have the robustness good enough to remove the driver out of the loop. For that, you need more compute and more sensor modalities in order to create redundancy.So now we have another system going to go into production in early 2024. It's going to have L4 capability. It's also with the Zeekr. It is it's going to be based on six EyeQ5 chips. So it will have L4 capability with a certain limitation of ODD, the operational design domain.We have on our robo-taxi an ECU called AVKIT 58. These are eight EyeQ5 chips, and this is powering our robo-taxi which has been debuted at IAA in Munich back in September. It's going to be on the road in the middle of 2022. and homologated by the end of 2022 in terms of the hardware to get all the permits to remove the driver from the road. The EyeQ Ultra is learning from all this evolution. So with EyeQ Ultra, after building the AV capability end-to-end, it's not only the perceptions or the driving policy or the control - you can see that from all those clips on the internet. We came to the conclusion of what is exactly the compute need that we must have in order to support a full L4, with an MTBF (mean-time between failures) high enough to remove the driver from the loop. We came to the conclusion that it is 10 times the EyeQ5, so the EyeQ Ultra is roughly a 10 x EyeQ5.Now, why a single chip? At the end of the day, the cost matters a lot for consumers. Cost matters a lot if you want to be involved in what will evolve to be consumer autonomous vehicles. In around 2025, there will be robo-taxis, and some consumer AV. Consumer AV means you buy a car, you pay somewhere around $10,000, and you get an option by a press of the button, the car becomes level four. So you can have ‘mind off, eyes off’ where you don't need even to sit on the driver's seat. So the cost here matters a lot.Also, the ECU with an EyeQ Ultra - the cost of that will be significantly less than $1,000. The way we designed it, the full level four system, with the cost of the ECU, the sensors, the compute, everything, in terms of cost will be significantly less than $5,000. That will enable an MSRP to the customer for around $10,000, and that's around 2025. So this is why it's very, very important to get this monolithic AV on-chip, given all the learnings that we have done, and all the evolution of multiple chips in order to understand exactly what we need in order to support a level four product.IC: It's almost crazy to think that the major cost in design in autonomous vehicle systems in the future is going to be the sensors and not the silicon. That's how it sounds!AS:That’s true! So with sensors, there are also some breakthroughs. So there are two types of sensors that the public is aware of - cameras and LIDARs. With LIDARs, there's a certain cost that is inherent. A LIDAR does not go down to the cost of a camera, and it will not go down to the cost of a radar. We're developing the next generation LIDAR - it's a frequency modulation coherent waves called FMCW LIDAR. I talked about it at the last CES.But the real breakthrough in my mind will come from the next generation of radars. And I'll share this at CES, I talked about at last CES, but I'll show a project that we are building. It's called a software-defined imaging radar, it has more than 2000 virtual channels, but there are many many other important elements, not only the number of virtual channels. That enables us to develop software running on the radar output that is so high resolution, whatever we do with cameras today, we can do with this radar. So we can treat this radar as a standalone sensor in terms of building a sensing state, perceiving the world around you. So you can look at very congested traffic with lots of lots of pedestrians and vehicles - today radars have no chance in separating the different objects, stationary and moving, and pedestrians, and you can have a pedestrian near a vehicle, a vehicle under a bridge, and all sorts of distractions that could happen that radars today cannot handle. This type of radar can handle it.Now, why is this a game-changer? Because the cost of such a radar is between a fifth to a tenth of the cost of the LIDAR. So now you can imagine a configuration where okay, we talked about the compute, you have a 360-degree of cameras, this is way below $1,000, you have a 360-degree of these imaging radars, which is also way below $1,000. You'll have a single front facing LIDAR, so front-facing which is the most critical part of the field of view. You'll have three-way redundancy, you have cameras, this imaging radar, and the LIDAR somewhere around $1,000. So you can see that all together, you're getting something which is way below $4,000 in terms of cost. And this is the key to bringing the cost level to a point in which we can have a consumer AV, because today, cost levels of self-driving systems are way above that, an order of magnitude above that, which is good enough for powering the robo-taxi, but not for a consumer level car.IC: With the EyeQ Ultra chip that you're announcing, I think the big number that astounds me is the 176 TOPs. When we hear about other autonomous vehicle L4 driving level systems, they're an order of magnitude beyond this, so it seems amazing that you guys can claim that you're going to have an L4 equivalent system with only 176 TOPs. I know you're not necessarily going to talk about competing solutions, but I think my question to you is if TOPs the right metric in order to understand the competitiveness between different solutions?AS:I think TOPs is not the right metric. There are actors that are trying to push TOPs as kind of the new horsepower metric, but it is very, very misleading. You know what, when people talk about TOPs, they really talk about deep learning TOPs. Even with deep learning TOPs, there are all sorts of details that are being emitted that doesn't include sparsity, it doesn't include the sparsity.On the convolutional neural networks, there are now many new types of architectures of neural networks. Then there are many, many types of different workloads, which are not related to deep learning calculations. This is one of the strengths of Mobileye, in which, given all those years of experience of building these systems, of computer vision-based systems, and also the driving policy, we have the right combination of accelerators and CPUs to bring us to a point which is very, very efficient.Now, it's not a new kind of a forward-looking thing. Take, for example, the Zeekr car I mentioned earlier. Already a few 1000s of vehicles have been shipped, and the functions are going to be updated over the air in the coming months, from basic driving aids to the full L2+ which provides an end-to-end capability of hands-free driving. It's not L3 or L4 yet, and you need the driver behind the steering wheel, but in terms of the capability, it's a full end-to-end hands-free driving. All of what we are showing in our unedited videos, about what this car can do, is with only 11 cameras and two EyeQ5 chips. Now it has only two EyeQ5 chips that are responsible for the entire perception of 11 cameras - this is lots and lots of compute camera processing. The cameras are 8 megapixel, so we're talking about lots and lots of data coming in to those two small EyeQ5 chips. ,But then you also have the driving policy, the planning, you know, in a self-driving system. You have sense, you have a plan, and you have to act, right? You have to do that the planning in compute. Planning is also a humongous amount of compute. If you look at competing solutions, we're able to do it on two chips, where each of them is 15 TOPs. So it tells you that TOPs is really not the right measure. You need to build a system on a chip that is sufficiently diverse, and you need to build the right algorithms that are purpose-built for the task. You have silicon vendors that are producing general-purpose chips and then they're only weak cores. To compete is try to define a new metric, which is this TOPs kind of horsepower metric. But life is much more nuanced than that.IC: So what about TOPs per Watt? What exactly is the right power for one of these systems that we should be thinking about? Some people are still talking about 1000W sitting in the trunk!AS:As you know, even Watts is misleading. For example, are you talking about only the deep learning engines? Are you talking about the entire SoC? Are you talking about the static power at 125 degrees? A temperature that affects so many details? When people talk about a wattage, are you talking about the system, the entire system power consumption? This is a better measure, not the chip.Mobileye has system on chips, which are behind the windscreen. So we are talking about an automotive-grade 125 degree junction temperature behind the windscreen, and performing very, very powerful compute in systems that are way below 3 or 4 watts. That’s the system, not just the chip! So giving you a number of watts on the EyeQ Ultra doesn’t say much - I mean I can give you a number, and it's way below 100 watts. But without going into all the details about how power consumption is being measured, these numbers are also meaningless. At the end of the day, you need to measure system-level power consumption, and we're building this such that the system-level power consumption is system level, not the chip level, system level is below 100 watts.IC:A hundred watts means we can put on a PCIe card then! That would be fun. Just to clarify, when you say 125 degrees, are you talking Fahrenheit or Celsius?AS:Celsius.IC:Okay, 125 Celsius because it’s automotive grade?AS:That’s right.IC:So that’s like Dubai, but also in the middle of the engine?AS:Exactly!IC: You mentioned the four types of accelerators that you have in the EyeQ Ultra - deep learning, VLIW, CGRA, and multi-threaded: can you be a bit more explicit on what these do and what partnerships you're leveraging for the IP in these?AS:The CPUs in EyeQ Ultra are RISC-V CPUs. In the previous generations of EyeQ5, those are MIPS CPUs, and EyeQ Ultra will have 12 RISC-V CPUs. Those are IPs that we license. Our accelerators are cores that we design, and this is part of our proprietary design. Those cores have been, you know, perfected over the past 15 years. So each type of core has been silicon in previous generations, but upgraded. It's been perfected better and better and better as the technology improves, as architectures of neural networks improve, as our understanding of what kind of algorithms we need. When we talk about algorithms, it's important to make a point where you need to create internal redundancies.So say, for example, you have an algorithm that is doing pattern recognition, detecting a car in front, and you try to perfect it as much as you can using data driven techniques. But then you want to also create another route, within the same chip, but completely different in algorithm. It could be an algorithm that takes a top view by piecing all the cameras together, and from the top view, detecting where the vehicles are. It could be an algorithm that is doing pixel-level labelling, labelling every pixel, whether it's a road or not a road. And when something is not a road, then no, it gives an indication that maybe you have either a static or a moving object. You can do pixel level segmentation for every pixel label what it is, is it a road? Is it a car? Is it a pedestrian? And so forth. So very different types of algorithms in the same chip.We have another algorithm called VIDAR, which is taking all those 11 cameras, and creating a 3D cloud of points just like a LIDAR. So a complete Depth Map. When you have a complete depth map, you can use a completely different type of interpretation, of visual interpretation, because now you have a 3D cloud of points, and now you want to detect cars.So every detection, every perceptual understanding that we want to make from the scene, we do it in multiple ways. This creates the need to diversify the type of algorithms and therefore the type of accelerator cores that supports those algorithms. And this is the need for those four families of accelerators. So it's not just one architecture of acceleration, it is multiple architecture, and then we have from each family, we have multiple cores, and I said the EyeQ Ultra has 64 accelerators.IC: This chip is meant to be four or five years away. In the chip space that I usually report on, that is quite early - I know automotive have long cycles on this however. But I'll pose the same question to you that I pose to the more standard sort companies I talk to - how confident are you in predicting where the algorithms are going to be that far out, such that you're confident that what you're designing today has the right balance of compute, interconnect, memory? Also, it terms of where the algorithms are going to be in four or five years?AS:Well this would have been a pertinent question, say five years ago! Today, we're at the point where we know exactly what we need to do in order to build a level four system, because we have been building it in smaller pieces. We’ve been building all the components, building the computer vision, and we have cars on the road with our computer vision doing end-to-end autonomous driving. We are building a separate stream of RADARs and LIDARs, and we have vehicles on the road using end-to-end autonomous driving with a safety driver now, but also autonomous driving without cameras, just relying on RADARs and LIDARs. We have been doing this for the past five years, building crowdsourcing technologies for building high-definition maps, and those maps are part of the perception right? We have done all of that, so we know exactly what the algorithms are - this is why we are designing the EyeQ Ultra today and not three years ago. We could have done that three years ago, and built a humongous chip to support L4, but then your question three years ago or five years ago would be very, very relevant. How would you know what kind of algorithms you need in order to support a L4?Today, we know these algorithms, and this is why we know exactly what the architecture of our AV on chip, autonomous vehicle on chip, should be. We took that as building blocks of the EyeQ5, since the EyeQ5 is running now all our autonomous vehicle development. As I said, it is two EyeQ5 today, we have six EyeQ5s for L4 capability in early 2024 as a consumer vehicle in the Zeekr. It's going to be announced at CES that we have on our robo-taxi eight EyeQ5s. So we know exactly what are the algorithms, and therefore the EyeQ Ultra is respectful for all those learnings. We know exactly what are the algorithms and the architectures that we need for a level four vehicle.IC: You said before the announcement for the Ultra chip, first silicon is in late 2023, production silicon in 2025. Can you go through where you are right now with the design? Do you have extensive RTL? It's clear that you're doing simulations? Where exactly do we stand, because you're 18 months away - which for most chips is a good chunk of the design cycle before first silicon.AS:We are way above the 50% in RTL. So in a few months, we should have the 100% RTL and start the back-end phase. We’re working with ST Micro - our partnership with ST Microelectronics goes from 2005 From EyeQ1, to EyeQ Ultra.IC: Other autonomous vehicles systems talk about how much they process - we've gone over the TOPs argument, but what sort of frames per second or response time is the EyeQ Ultra system targeting? Some vendors will focus on 60 frames per second, some will say 24 frames per second. What's your opinion there?AS:Even those numbers are sometimes meaningless, because there are many many details that are being emitted. At the control level, we support between 50 to 100 hertz. In terms of perception, we're working at 18 frames per second, which is sufficient for perception. There are also all sorts of processes that are slower, some are at 9 FPS, which are not critical. They're only for redundancy, and there are processes that are on 24 FPS also. So there are many details when we talk about frames per second, it's not one monolithic number.IC: On the discussion around chip packaging, is there anything special going on the packaging here with Ultra?AS:So the packaging technology is ST’s IP, and they have been working on packaging for us on the EyeQ5, the EyeQ4, the EyeQ3, and EyeQ6. It's a legacy, a continuing legacy of building automotive grade packaging for very high-performance computing. All of our SoCs are, at the time of production, cutting-edge high-performance compute, and ST is matching that with the right package.IC: In 2017, Mobileye was acquired by Intel, and you guys have been part of their financials since. We've seen the numbers for Mobileye go up and up and up over the last few quarters, so congratulations. But what exactly did the acquisition by Intel bring to Mobileye that perhaps wasn't there previously?AS:I would say there were three elements.One is manpower. So when we were acquired, we were at about 800 employees. Today, we are at 2500, but 800 of those 2500 are coming from Intel. So it's significant because we're not talking about individual people - we’re talking about entire teams. Building teams is difficult, so building that 800 people or team organically would have been very, very difficult.The second is technologies. The imaging radar that I talked about is from Intel teams that have moved to Mobileye. The FMCW LIDAR development is using Intel Silicon Photonics - these are special labs of Intel, but it's also Intel teams that have been moved to Mobileye. So we're talking about technologies that are still outside of Mobileye’s organic skills. In no dream of mine could I have imagined that Mobileye would develop a LIDAR or something, we do not have that skill at all. So this is coming from Intel.In terms of evangelizing our safety models, so back in 2017, we developed a safety model called Responsibility Sensitive Safety, RSS, which today is really the backbone of worldwide standardization of how you define the delicate line between safety and utility. As you know, the safest car is a car that doesn't move, but if you want to merge into congested traffic, you need to challenge other road users, so how you do it in a way that is from a societal point of view considered safe? So we developed that theory, made it transparent, published it, and then through Intel's policy and government groups, we have been evangelizing it. For example, there's an IEEE program called 2846. It's chaired by Intel, which is standardizing these kinds of questions. And RSS is really the starting point for asking those questions.I think it has been a very, very successful partnership between Intel and Mobileye, and still will be. I see a fruitful partnership going forward as well.IC: It's interesting you talk about the safety aspects, because over the past 10 years, it's been suggested that at some point, the technology will be there and we'll have L4 and L5 systems ready to go. Rather than the technology being limiting, it has been suggested that the main barrier to adoption will be that legal systems, where governments aren't ready to properly produce the laws around these types of technologies. How much thought explicitly do you put into that, into the point where you might be ahead of the legal system, ahead of the governments there?AS:So this is exactly the question that we asked ourselves back in 2017. If we don't do something on the legal front, we will get stuck, and at that time, the common wisdom was the mileage driven [would be convincing enough]. So the more mileage you drive without intervention, which meant the safer you are, was to our mind not a sufficient metric.When you need to put a machine on the road, it should be orders of magnitude safer than a human driver in order to be accepted from a societal point of view. How do you go and validate something like this? Validating the perceptual system is something that you can wrap your mind around and define it, but how do you go and validate the judgments that the machine makes? It needs to make judgments to merge into traffic, or if you want to change lane and the traffic is congested, so somebody else needs to slow down in order for you to merge. How do you do that in a way that is not reckless? How do you define what is reckless? How do you define what is a dangerous manoeuver? How do you define what is a safe manoeuver?All of those are lacking in rigid definitions. A machine needs to have formal definitions - it cannot make any move just based on heuristics, because you will not be able to defend it in court later when an accident happens. So this is when we started to develop this formal theory of what it means to drive safely, what are the assumptions that people make when they drive, and how can you code it into a formal theory. Also what kind of guarantees can you give - can you give a guarantee, for example, that you'll never cause an accident? This is exactly what we set out to do. In that RSS paper, and we wrote our two papers in that project, and as I said before, it is today, really the basis of all worldwide standardization.Today you see, for example, in Germany that there are laws that enable to remove the driver. For example, you need to homologate the design, and there are certain steps that you need to do. The UK Commission and EU Commission have legal language that gives you legal certainty for deploying an autonomous car. I think the US would also at some point have that proper language to create certainty for these kinds of technology. Israel has enacted the law as well, so it's getting there. I think 2022 and 2023, for robo-taxis, that is the correct sweet spot in terms of starting to see a deployment of autonomous ride-hailing. The learnings from it would then propel, with the right cost, in 2024 or 2025 for consumer autonomous vehicles.IC: So on the point on robo-taxis, maybe this is particularly region-specific, but I want to posit a possible scenario to you. A tobo-taxi goes to pick somebody up, but it’s a fare that a taxi with a human driver that wouldn't necessarily pick up, either because they either look ill or they're prepared to do damage to the car. If it was a human, they would just drive off and not accept the fare, but with a robo-taxi that's not part of that situation. So at your level, are you considering these sorts of situations that could be the endpoint of where your technology is going? Or are you kind of just happy to leave that into the hands of those that are deploying it at scale?AS:Well talking about the vandalism, there could be all sorts of things going on. So robo-taxi’s would also have a teller/operator. So the operator does not drive the car but can give instructions to the car, in case it gets stuck, in case it doesn't know what to do, and they can give it instructions. It can also solve this issue that you mentioned before, before we pick up a passenger, the operator has a view from all the cameras of the car and can make a decision of whether to pick up that passenger or not. But there are more critical issues that could come up, such as violence inside the car cabin. So the cabin is viewed from the operator's view of the cabin and can see what's going on, and can deactivate the car if necessary, or call for help. I think those issues are issues that you need to think about once you have robo-taxis at scale. I'm thinking 2022 or 2023, as robo-taxis are not yet at scale. So those issues can wait a bit, but once you really have this at scale, when you need to think about vandalism, you need to find the solutions, but I believe that those solutions will be found.IC: On a more personal note, today I'm talking to you as CEO of Mobileye. But you are also a Professor, with the Sachs Chair of Computer Science at the Hebrew University of Jerusalem. You've co-founded other companies in the AI space, and in doing my research on you, I saw that you're co-founding Israel's first bank in 40 years, a digital bank. Do you have enough hours in the day for what you do?AS:Well, as you know, what I do is all around artificial intelligence - everywhere where I think our AI can be transformative. Whether it is building autonomous cars and driving assist to save lives, or language models, building intelligence, general intelligence, you know, or disrupt banking as we know it. AI can help people with disabilities, whether they're blind or visually impaired, or have a hearing impairment, or are dyslexic. AI can help in all those areas, and in each such areas, I have a company for it. My sole executive position is at Mobileye, but for all those other companies I founded or co-founded, I take a chairman position, and then I guide the company. But in terms of an executive position of managing people, it's only at Mobileye.IC: So on the machine learning front, the industry focuses a lot on computer vision, natural language, recommendation engines, and those are some of the big ones that have extended into revenue-heavy industries. But are there any areas of machine learning research you'd like to investigate deeper on, or that the industry or academia hasn't really focused on so much?AS:Well, I think the new frontier of AI is language. Language gives you a window to intelligence, because when you have a machine that has only very, very strong pattern recognition capabilities, even say human level, or even exceeding human-level pattern recognition, you will still not say that this machine is intelligent. When you have a machine that can drive autonomously or play chess or play Go or play Minecraft, you know, exceeding human capability, you would not say this machine is intelligent. But with a machine that can master language, this is a window towards intelligence.If you can look at what search engines have done to society - what search engines have done is that you have an encyclopedia in your pocket, right? You don't need to remember things, just simply take out your smartphone and search and you get an answer to any factual question that you have. Language AI, in language, would bring intelligence into your pocket. So you'll have access to a wise person. It's not just asking a question, but have a conversation with Albert Einstein, or have a conversation you know, with a philosopher, or have a conversation with any kind of expert that you have in mind. It will all be in your pocket. This unlocks values that are even hard to imagine today. The language frontier opens up the door towards general intelligence. Five years ago, if an AI expert talked about general intelligence, this expert would have been treated with skepticism. Today, I think it's around the corner.Many thanks to Prof Shashua and his team for their time.Many thanks also to Gavin Bonshor for transcription.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17151/an-exclusive-interview-with-mobileye-ceo-prof-amnon-shashua-the-future-of-autonomous-driving\n",
      "Title: Mobileye Announces EyeQ Ultra: A Level 4 Self-Driving System In A Single SoC\n",
      "Author: Ryan Smith\n",
      "Date Published: 2022-01-04T16:00:00Z\n",
      "URL: https://www.anandtech.com/show/17165/mobileye-announces-eyeq-ultra-l4-auto-soc\n",
      "Content: While CES may have started out as the Consumer Electronics Show, the global event has over the years expanded to include everything from enterprise technologies to automobiles. As a result, the show has not only been a regular keystone event for things like CPU and GPU announcements, but it’s also become home to major automotive technology announcements as well. And for Intel’s autonomous driving subsidiary, Mobileye, those two paths are coming directly together this year, as this morning the group is announcing their next-generation Level 4 autonomous driving SoC, the EyeQ Ultra.Aimed for a 2025 release, the EyeQ Ultra is Mobileye’s most ambitious SoC yet, and not just for performance. In fact, as a site that admittedly rarely covers automotive-related announcements, it’s the relativelackof performance that makes today’s announcement so interesting to us. But we’re getting ahead of ourselves here; so let’s start with the basics.The EyeQ Ultra is Mobileye’s seventh generation automotive SoC, and is designed to enable Level 4 autonomous driving – otherwise known as “high automation” level driving. Though not quite the holy grail that is Level 5 (full automation), L4 is the more immediate goal for automotive companies working on self-driving cars, as it a degree of automation that allows for cars to start, and if necessary, safely stop themselves. In practice, level 4 systems are likely to be the basis of robo-taxis and other fixed-location vehicles, where such self-driving cars will only need to operate across a known and well-defined area under limited weather conditions.Mobileye already has the hardware to do L4 automation today, however that hardware is comprised of six to eight EyeQ chips working together. For the research and development phase that’s more than sufficient – just making it all work is quite a big deal, after all – but as L4 is now within Mobileye’s grasp, the company is working on the next step of productization of the technology: making it cheap enough and compact enough for mass market vehicles. And that’s where the EyeQ Ultra comes in.At a high level, the EyeQ Ultra is intended to be Mobileye’s first mass market autonomous driving SoC, To accomplish this, Mobileye is designing a single-chip L4 driving system – that is, all of the necessary processing hardware is contained within a single high-end SoC. So when attached to the appropriate cameras and sensors, the EyeQ Ultra – and the Ultra alone – would be able to driving a car as per L4 standards.But perhaps the most interesting thing about the EyeQ Ultra is that Mobileye intends to do L4 driving with a chip that is, on paper, not particularly powerful. The official performance figure for the chip is 176 TOPS, which to be sure, is a lot of performance today (in 2022). But it’s is a fraction of the performance that’s planned for high-end SoCs in the 2025 timeframe that Mobileye is targeting. In short, Mobileye not only believes that they can do L4, but that they can do it at a fraction of the performance and power consumption of their competitors.Ultimately, the argument that Mobileye will be coming to market with is that autonomous driving has matured enough as a field that not everything needs to be done in software or in highly flexible accelerators. Instead, it’s time to start building true ASICs, with highly specialized fixed-function (or otherwise limited flexibility) components that do one thing, and do it well. Overall it’s the natural path of progression for most task-specific processors, and Mobileye believes that self-driving automotive systems are finally ready to head that direction as well.Diving into speeds and feeds then, the EyeQ Ultra is going to have several different types of cores, each for a different task involved in operating an L4 self-driving vehicle. These are:12 RISC-V CPU coresArm GPUArm DSPSIMD coresVLIW coresCoarse grained reconfigurable array (CGRA) coresDeep learning coresThe latter 4 groups of cores comprise Mobileye’s “proprietary accelerators”, and are where the bulk of the work is done. According to Mobileye there are 64 such accelerator cores in all, though the company isn’t breaking down just how many cores are in each specific group. The entire chip, in turn, will be built on a 5nm process (presumably TSMC’s).Ultimately, Mobileye’s investment in so much limited-flexibility hardware means that the chipmaker has relatively little hardware available for high throughput general computing – and, as their reasoning goes, relatively little need for it either. With sufficient accelerator throughput, even 176 TOPS should be enough for an L4 autonomous vehicle.In Mobileye’s eyes, focusing so much on task specific hardware will bring a couple of benefits First and foremost is that it keeps down the total amount of silicon required. This not only allows them to get everything on to a single chip, but it keeps the associated system cost down. The second benefit is power: the fewer transistors you have to light up, the less power is spent. And even though power is relatively easy to come by in a car – particularly EVs – it can add up with multiple chips. So Mobileye is looking to make that a major feature differentiator with the EyeQ Ultra: if the lower cost doesn’t woo automakers, hopefully the lower power and cooling requirements will.Meanwhile, although Mobileye doesn’t name any specific competitor, overall their press release/sales pitch seems like a thinly veiled shot at NVIDIA, whose own Atlan SoC is due in the same 2025 time period. Indeed, even the timing of today’s announcement (8am PT) is set against when NVIDIA’s CES 2022 keynote starts, rather than timing it to go with Intel’s 10am keynote. So Mobileye would seem to have a very clear idea of who they believe is their chief rival.It’s a notable rivalry not only because of the firms involved (essentially Intel versus NVIDIA), but because of NVIDIA’s TOPS-centric promotion of their automotive SOCs. For reference, NVIDIA is promoting Atlan as offing over 1000 TOPS of throughput for the SoC alone, and that would be even greater throughput if used in a high-end multi-chip Drive PX setup, as NVIDIA likes to do.Consequently, Mobileye is very mindful of ending up in a specs war with NVIDIA, as the latter’s high deep learning throughput certainly looks attractive compared to Mobileye’s lower throughput accelerator-based approach. Fundamentally, the difference in TOPS throughput just reflects the difference in design philosophy between the two groups – NVIDIA’s software defined approach versus Mobileye’s more task-specific accelerators – but the company is rightfully concerned about how much better a big TOPS number will look.But perhaps the more important – albeit lingering – question is which approach is going to produce better results. Mobileye’s approach essentially locks into place their current technology attack and associated self-driving algorithms, while NVIDIA leaves the door open to more flexible approaches. But then what good is being flexible if it’s going to cost an arm and a leg, especially at a time when the industry as a whole is trying to bring costs down in order to get self-driving tech into more vehicles?Which to bring things full circle here, it’s this contrast that makes Mobileye’s approach with the EyeQ Ultra so interesting. While NVIDIA is taking the equivalent of the brute force approach, there’s little reason to doubt it will work. Mobileye’s greater investment in task specific hardware on the other hand carries more risk, but if they can deliver on what they promise, then they’d be doing so with a fraction of the silicon and a fraction of the power consumed, all of which would make for a big advantage over NVIDIA.For better or worse, it’s still the early days in the autonomous vehicle industry. Everything up until now has been one big string of experiments, and even when 2025 rolls around and the EyeQ Ultra heads out the door, it’s still going to be just the beginning of a much larger shift. So there’s still plenty of time for companies to jostle for position in the automotive market – but true mass market commercialization isn’t too far away. So for Mobileye and other automotive SoC makers, it’s time to start playing for keeps.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17165/mobileye-announces-eyeq-ultra-l4-auto-soc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD CPUs in 2022: Zen 4 in Second Half, Ryzen 7 5800X3D with V-Cache by Spring\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2022-01-04T15:42:00Z\n",
      "URL: https://www.anandtech.com/show/17152/amd-cpus-in-2022-ces\n",
      "Content: One of the things I look forward to every year is whether the major companies I write about are prepared to showcase their upcoming products in advance – because the year starts with the annual CES trade show, this is the perfect place. A company that’s able to present its 12-month portfolio comes across as confident in its ability to deliver, and it also gets the rest of us salivating at the prospect of next-generation hardware. This time around AMD steps up to the plate to talk about its new V-Cache CPU coming soon, and its new Zen 4 platform coming in the second half of the year.Now with V-Cache! One Sole CPU: Ryzen 7 5800X3DEvery CPU has levels of internal memory, known as cache, which starts as a bank of ‘Level 1’ fast but small memory, rising up to a ‘Level 2’ medium-sized medium speed memory, and then a ‘Level 3’ larger sized slower memory. Beyond this there’s the main DDR memory, which is super big, but super slow in comparison – main memory is 100x slower to access, but can hold a lot more data.Last year AMD announced that it had been working on stacked onboard memory in the form of cache. This V-Cache concept took one of the standard 8 core chiplets from the Ryzen 5000 series, which already had 32 MB of L3 cache, and stacked on top of it another 64 MB of L3 cache, giving a total of 96 MB. Stacking chips is difficult, and AMD has been working with TSMC to productize this advanced packaging technique.The main 8-core chiplet, built on TSMC 7nm, measures 82 mm2. This extra stacked chiplet is only 36 mm2, and sits directly above the cache already on the chip, so it does not cover the cores. The extra 64 MB of L3 cache chiplet is manufactured on a version of TSMC 7nm that is optimized for cache density, and so AMD has placed 64 MB on top of 32 MB directly. The cores are not covered for thermal reasons – the cores are where the power is used, and so thermal spaces are placed on top to make the top of the combined chip fully flat.AMD stated last year that it would be manufacturing consumer and enterprise versions of this increased cache in 2021 for launch in 2022. At the Data Center event last year in November, AMD announced the version that would go into servers, and called it Milan X. For consumers, AMD is announcing today what this increased cache version of Ryzen looks like. Introducing, the Ryzen 7 5800X3D.AMD is only going to launch a single Ryzen version of its V-Cache technology, using the Ryzen 7 5800X as a base. This means the chip is 8 cores, 16 threads, and 105 W TDP just like the regular R7 5800X, but with 96 MB of L3 cache now rather than 32 MB. It will run at 3.4 GHz Base, 4.5 GHz boost, be overclockable, and work in AMD 400-series and 500-series motherboards.AMD Ryzen 5000 Series Processors for DesktopZen 3 Microarchitecture (Non-Pro, 65W+)AnandTechCore/ThreadBaseFreq1TFreqL3C$IGPPCIeTDPSEPRyzen 9 5950X16323400490064 MB-4.0105 W$799Ryzen 9 5900X12243700480064 MB-4.0105 W$549Ryzen 9 590012243000470064 MB-4.065 WOEMRyzen 7 5800X3D8163400450096 MB-4.0105 W?Ryzen 7 5800X8163800470032 MB-4.0105 W$449Ryzen 7 58008163400460032 MB-4.065 WOEMRyzen 7 5700G8163800460016 MBVega83.065 W$359Ryzen 5 5600X6123700460032 MB-4.065 W$299Ryzen 5 5600G6123900440016 MBVega73.065 W$259Ryzen 3 5300G48400042008 MBVega63.065 WOEMThe processor will launch in the Spring (March/April we think), and exact pricing is yet to be announced.For those that have been following AMD’s V-Cache news over this past year, I bet you have exactly the same questions I did when AMD first briefed us on this announcement. Here is a summary of the questions I asked, and my interpretations of the responses.Why only Ryzen 7, not Ryzen 5/Ryzen 9?Because this is a new project for AMD, they want to find out how a processor like this will be welcomed into the market. A lot of users (myself included) expected AMD to go all-in with a big 16-core version, however anything Ryzen 9 requires two chiplets, and adding the extra V-Cache does require an extra cost in silicon and packaging. During a semiconductor shortage, I was told that this is the best way to get it into the hands of many people while also not in the super high-cost bracket. It also means one single unified 96 MB of L3 cache, without having to deal with two chiplets worth which might not be optimized immediately. Future versions of V-Cache on next-generation products may be expanded to other Ryzen members of the family.The frequencies are lower than the regular 5800X?The cache does add a few watts to the power both in terms of idle and load. Rather than bin a stricter chiplet, the decision was made to reduce the frequency a little, but still allow overclocking. The chip, while listed at 105 W, still has the 142 W package power tracking for motherboards that support it.Who is this chip for?The focus is on users playing video games over anything else. The extra cache is meant to help with communications with discrete graphics cards, offering additional performance above the regular R7 5800X. Productivity workloads are less likely to be affected, and for those users the regular Ryzen CPUs are expected to be better. The Ryzen 7 5800X3D is designed to be the ‘World’s Fastest Gaming Processor’ (when compared to the 5900X and 12900K).In terms of those performance metrics, AMD is quoting:From 1.0x to 1.4x at 1080p High vs Ryzen 9 5900X + RTX3080 (15% average)From 0.98x to 1.2x at 1080p High vs Core i9-12900K (DDR5) + RTX3080Both systems were running Windows 11.The critical element here I think is going to be the price. As I’m writing this piece, I can find the following prices:Ryzen 5 5600G (6C/12T*): $240Ryzen 5 5600X (6C/12T): $290Core i5-12600K (6P+4E*): $300Ryzen 7 5700G (8C/16T*): $340Ryzen 7 5800X (8C/16T): $369Core i7-12700KF (8P+4E): $390Ryzen 9 5900X (12C/24T): $540Core i9-12900KF (8P+8E): $590Ryzen 9 5950X (16C/32T): $730Even though AMD is promoting the Ryzen 7 5800X3D to be higher performance than the 5900X in gaming, if it goes anywhere north of $500, it might be badly received. At $500, it would be a +$130 add-on from the regular Ryzen 7 5800X. Are users willing to pay almost 30% more for triple the L3 cache for up to 15% more performance in gaming? Or is this just simply a play for the world’s fastest gaming processor, regardless of cost?Don’t get me wrong here, I think the technology is great. But in order for AMD to keep the same margins, it might be more expensive than people think. I’m looking forward to getting it in hand for review – let us know what sort of tests you want to see.Coming 2H 2022: Zen 4 on 5nm, with AM5, DDR5, and PCIe 5.0Perhaps not that surprising given all the information from last year, AMD has confirmed that Zen 4 based Ryzen CPUs coming in 2022 will be built on TSMC’s 5nm process (we assume N5), will be built on the AM5 socket, and feature DDR5 as well as PCIe 5.0. What we get new out of this disclosure are images of the new socket, and a render of the CPU form factor.With AM5, AMD is going to move to a Land Grid Array (LGA) style of processor, similar to Intel, eliminating the Pin Grid Array (PGA) that has been used on the current Ryzen desktop processors. AMD is no stranger to LGA, given that its EPYC enterprise processors and Threadripper processors both use it. The new AM5 socket is a 1718 pin design, with the pins in two orientations:If this sort of socket looks familiar to any of you, it’s because it appears to be a denser version of AMD’s old socket F back in 2006-2010. While that old socket at 1207 pins for Opteron enterprise processors, this one has 1718, so you’ll see us refer to it as LGA1718. Compared to Intel’s 12thGen Core processors that use an LGA1700 socket, both the major platforms are around the same number of pins.It is worth noting that this sort of socket, like the old Socket F, means that the bottom of the Zen 4 processors will be nothing but contact pads. The use of an LGA socket means the pin density is defined at the socket level, rather than on the processor, and it’s easier to design a socket with a higher pin density. But the pin-only rear means that some of the power circuitry for the chip will be both in-package and on the top, which is handy given that AMD is also showcasing what the CPU will look like.We’ve got another square-like CPU package, however to accommodate some of that power delivery the heatspreader has this sort of octopus arm design to it. The heatspreader is not rotationally symmetric, with the top/bottom (as shown) central arms being smaller than the left/right central arms. The processor also has two notches, one at the top and one at the bottom, just left of center to make sure that the processors are entered in the right way. There’s also that yellow arrow on the top left corner to help guide the user.On top of showcasing the CPU and the LGA1718 socket, AMD is confirming that the AM5 platform will support AM4 coolers. This means we should expect the mounting holes for AM5 to be the same, or at least the platform to accept both old and new.Next-Gen Ryzen, featuring Zen 4 cores, 5nm manufacturing, and the new AM5 socket, is coming to market in the second half (2H) of 2022. Core counts and everything else will come later – I suspect we’ll get a deep dive into the architecture sometime around August, at the Hot Chips industry event, or at a special AMD event around that time.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17152/amd-cpus-in-2022-ces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qualcomm’s 8cx Gen 3 for Notebooks, Nuvia Core in 2022/2023\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-12-22T21:30:00Z\n",
      "URL: https://www.anandtech.com/show/17127/qualcomms-8cx-gen-3-for-notebooks-nuvia-core-in-20222023\n",
      "Content: There are many column inches detailing Qualcomm’s design wins and marketing strategy, however to paint it all with a broad brush, it has often boiled down to ‘where can we stick our advanced wireless technology?’. The company has had great success with smartphones, cornering a large chunk of US market and sizeable numbers worldwide, and in the last few years has pivoted to new markets, such as automotive and virtual reality, but also tried to reinvigorate existing markets, such as notebooks and laptops. Since 2017, Qualcomm has wedged a new category into the market, dubbed the ‘Always Connected PC’, offering Windows control with extreme battery life and mobile connectivity. At this year’s Tech Summit, Qualcomm introduced its latest processor, however the real magic might come next year.Once Connected, Always Connected?Taking the modern laptop and notebook market as a whole, we’ve seen considerable design improvements over the last 5 years. Gone are the bulky behemoths with poor screens and an hour battery life, replaced by ultraportable devices that can chew through a modern workflow for hours without charge. The sleek designs of this end of the market suit those on the go, most notably academic and business users. On the other end, those bulky designs are now gaming powerhouses supporting virtual reality and immersive gaming – still an hour or two on the battery, but a proper portable workstation.It’s the ultraportable market where Qualcomm saw an opportunity. Even in 2016/2017, we were seeing designs from Intel and Apple with form factors that made the hardware ideal for the business user on the go. Qualcomm, already established in smartphones as one of the leading players (if not the leading player), identified an opportunity to offer a similar platform in the market, but one bundled with native mobile connectivity, like a smartphone.Working with Microsoft, we saw the introduction of Windows on Snapdragon based thin-and-light notebooks enter the market in mid-2017, using the previous generation flagship mobile processor. Initial responses to the platform were extremely positive when it came to battery life and as a web-focused work machine that could leverage an independent 4G connection from other devices. Unfortunately the performance, software compatibility, and cost of partner devices from HP, ASUS, and Lenovo were the downsides. For $1000+, users were expectingat a minimuma performance and software parity to their current x86 devices, which this first generation did not provide.Qualcomm acknowledged the nascent state of the platform – building a version of Windows for Arm (or as Qualcomm puts it, ‘for Snapdragon’, given the close collaboration) is difficult, especially one that also offers emulation of modern x86 software. Identifying the major applications used by customers and working with those software vendors to create ARM native versions, to speed them up, became a Sisyphean task in-of-itself. But on top of this, x86 emulation had been limited to 32-bit for now, eliminating power users from the potential market space. At this time, Qualcomm’s ‘Always Connected PC’ (ACPC) project was focused primarily on business users with typical Office/Web workflows.The second generation of ACPC used an updated processor, still based on a smartphone design, albeit with more leeway in performance at the higher thermal limits. This Snapdragon 850 generation saw adoption from Microsoft Surface, a big design win for the project. The 850 was the last ‘smartphone designed’ silicon, as from this point on Qualcomm created ACPC specific hardware optimized for notebook use cases – more die area, more graphics, a wider range of thermals. This also accompanied more support for Windows on Snapdragon, however even with the 2020 processor launch of the Snapdragon 8cx Gen 2, there were still software holes to patch and performance for power users still required. But 30+ day standby and 24hrs+ battery made the hardware very attractive.It was around the Snapdragon 850/Snapdragon 8cx timeframe that Qualcomm realised the ‘value addition’ of mobile connectivity like a smartphone was not the real selling point of the hardware. It was instead the battery – optimizing a smartphone platform for notebooks had significant benefits, allowing users to attend multi-day conferences without the need of a charger (or with the advent of Type-C, one charger for smartphone and laptop). Qualcomm further pushed the commercial credentials of its platform hard.Q4 2017: Snapdragon 835Q3 2018: Snapdragon 850Q4 2018: Snapdragon 8cx (first dedicated for notebooks)Q3 2019: Snapdragon SQ1 (faster 8cx for Microsoft)Q4 2019: Snapdragon 7c and 7 (cost down versions for education)Q3 2020: Snapdragon 8cx Gen 2 and SQ2Q2 2021: Snapdragon 7c Gen 2Q4 2021: Snapdragon 8cx Gen 3While the battery life was a big draw, one of the aims of Qualcomm’s ACPC project has been to showcase that having a laptop or notebook with a smartphone like connection enables use cases like no other – the ability to answer email without the need to deal with a small smartphone keyboard, for example, or manage presentations while on the road. For that commercial market, where Qualcomm is selling machines to businesses for their workforce, it makes a lot of sense.However, there have been other criticisms of that use case, such as the fact that people with a device like an ACPC probably also have a smartphone, and tethering is a thing. The USA was unique for a while in that a number of smartphone data plans disallowed tethering, which would have put a plus on a device like the ACPC with its own connection, however the rest of the world didn’t do this and the USA has seen sense since. A counter to this is the additional battery requirements on the smartphone, which could be a genuine concern.The other big criticism has come from the wireless carriers themselves – they haven’t adopted the ACPC mindset when it comes to offering applicable plans. In an ideal world, a user would have a single plan covering both smartphone and laptop SIMs and a single data bucket for both, however the carriers that are still charging through the nose for smartphone data haven’t considered the additional use case of a notebook. Some carriers consider ACPCs as tablets (eg a smartphone plan without calls), and end up very restrictive in a similar way. My local carrier wants to charge an additional +50% per month to add an additional ‘data-only line’, but then doesn’t allow me to pool data, requiring micromanagement in 500 MB chunks. In speaking with the carriers in the past couple of months, this way of doing things isn’t going to change soon. The attitude I got was that they’re comfortable extracting significant coin from those willing to pay. On top of that, when focusing to the commercial market, these are company costs and not personal costs, which are less likely to be under the same penny-for-penny scrutiny.So even though Qualcomm’s marketing focus for the ACPC has shifted slightly, from the connected PC to the all-day workhouse, the goal has always been clear: if we can show that a connected PC becomes a vital component of a company workforce, it’s a customer for life. Once connected, always connected.My last serious effort to use an ACPC was the Lenovo Yoga C630 WoS, a Snapdragon 850 design in mid-2019. I’ll be honest, the battery life was the biggest plus. It was light, easy to use, and great for events if I needed something just to work, and was guaranteed to not require charging day-to-day. I could be at a 3-day conference on a single charge – a charge I’d done at home before getting on the plane to get there. But there were performance issues, particularly when moving between applications, which I do often and frequently – the 8 GB of memory and 256 GB storage was gazumped on day one. Some of my software had to be emulated, whereas other critical applications in my workflow failed outright, as they were niche 64-bit only software that had no equivalent. Beyond that, Dropbox was limited to Windows S functionality, and the enterprise version of Google Drive wouldn’t even work. That ultimately put me back into the land of x86. Fix those issues, and Qualcomm would rope me back in. This is where the new Qualcomm Snapdragon 8cx Gen 3 fits in.Qualcomm’s New Snapdragon 8cx Gen 3, with 64-bit Emulation on Win11At this year’s Snapdragon Tech Summit in early December, Qualcomm announced its next generation ACPC processor, the 8cx Gen 3. With this new processor we should expect more performance, wide availability, but it also comes alongside another significant update. All 8cx Gen 3 machines will ship with Windows 11 as standard, and this version of Windows will support 64-bit emulation of x86 software out of the box. The goal here is that any software a power user needs, it should work on this new generation.The emulation support on Windows 11 is also regressive for a couple of generations – any system with a Snapdragon 8cx or newer processor will support it. The reason why it’s taken so long, according to Qualcomm’s Miguel Nunes, isn’t so much that the instruction emulation was hard – the hardest part was dealing with the swathes of bad software available in the market. In our conversations with both Miguel Nunes and Alex Katouzian, the culprit restricting the roll-out was bad software – software that calls unregistered DLLs, or are hardcoded to certain 32-bit directories despite being 64-bit, or software that loads beta libraries and such. Qualcomm says that this is now in a predominantly solved state, and we look forward to testing.(Some might wonder if Windows 11 had anything to do with it, but Qualcomm told us it didn’t. There was an insider version of Windows 10 that also has full emulation, but the idea have a holistic cut between Windows 10 and Windows 11 makes supporting the feature a lot easier we are told. All 8cx and newer hardware is Windows 11 compatible, with the only question being whether users will actually bother to upgrade. Personally I think this delineation, if it does help managing updates to the feature, is likely a good move to avoid long term technical debt across two versions of Windows.)As for the new processor, the Qualcomm Snapdragon 8cx Gen 3 is heralded by the company as the first 5nm processor to support Windows. I think they’re correct in promoting that statement, which incidentally is printed alongside two significant datapoints: Up to 85% faster CPU and +60% GPU over the previous generation. Those are big jumps.At the heart of the 8cx Gen 3 is a 4+4 core design, featuring four Arm Cortex X1 cores at 3.0 GHz and four A78 cores at 2.4 GHz. Compared to the previous generation, this is +2 generations of Arm core for performance and a different tier entirely for the efficiency core.Snapdragon ACPC SiliconAnandTechSD835SD8508cxGen 18cxGen 28cxGen 3Node10LPE10LPPN7N75nmPrime Cores4 x A732.60 GHz4 x A752.95 GHz4 x A762.84 GHz4 x A763.15 GHz4 x X13.00 GHzEfficiency Cores4 x A531.80 GHz4 x A551.80 GHz4 x A551.80 GHz4 x A551.80 GHz4 x A782.40 GHzGPUAdreno 540710 MHzAdreno 630710 MHzAdreno 680585 MHzAdreno 690660 MHzAdreno8cx Gen 3AIHexagon 682Hexagon 685Hexagon 690Hexagon 690Hexagon 8cx Gen 3Total TOPs-39929LPDDR4X2 x 32-bit3733 MT/s29.9 GB/s4 x 16-bit3733 MT/s29.9 GB/s8 x 16-bit4267 MT/s86.3 GB.s8 x 16-bit4267 MT/s86.3 GB/s8 x 16-bit4267 MT/s86.3 GB/sWhichever way you cut it, moving from A76 to X1 on the performance cores and A55 to A78 on the efficiency cores is a jump, and that ‘up to 85%’ CPU performance sounds like a good metric. Alongside these numbers, the chip also comes with a 14 MB total cache structure, which is 8 MB of L3 cache and 6 MB of system cache, the latter of which is used more as a DRAM buffer to accelerate quick accesses.Unfortunately Qualcomm was near silent on its new Adreno implementation, to the point of also refusing to give it a designation. On top of this, it is worth noting that Qualcomm doesn’t actually list what the architecture cores in any of its public materials – it wasn’t until a Q&A session at the Tech Summit when the cores were actually given, and only when the explicit question was directly asked. It seems this was accidental, as we didn’t get the details even in a 1-on-1 briefing. The frequencies were part of the presentation, but briefly. With this in mind, combined with the lack of generational numbers on the Adreno and Hexagon moving forward, it seems that Qualcomm is entering a phase of obfuscation when it comes down to disseminating its hardware specifications. If you’ve heard this story before, it’s very reminiscent of Qualcomm pre-2016. Part of this is down to, or at least as we see it, a new marketing strategy where the company is focusing more on ‘the experience’ rather than the technical details. It does feel stretched that this information wasn’t part of the presentation, not even as an auxiliary side note, as part of the Qualcomm ‘Tech’ Summit. For every company that states they’re focusing more on the experience than the specifications, my reaction is very poor – a large portion of our audience, either engineers, power users, enthusiasts, or financial analysts and those looking to disseminate the direction of the industry wants to know these things. Not everything revolves around what a potential end-user, such my grandma, a potential end-user, might experience, even if the more public-facing marketing is directed that way.Qualcomm is claiming that the SoC offers +25% performance at 25% lower power than the x86 competition, with a 60% better performance-per-watt on CPU and 40% performance-per-watt on GPU. Exact metrics weren’t disclosed, but Qualcomm did point to software such as Adobe Photoshop and Lightroom which have now been optimized for Adreno on Windows. On top of these numbers, Qualcomm is quoting 30% better battery than competing platforms, which for a 16hr flagship Intel device would put it at 22 hours. In my mind that feels like a slight regression, which might be indicative that the A78 cores in idle consume more power than the A55 cores did, even with the process node improvement. At that level, other hardware comes into play as well, such as the display and wireless connectivity.For wireless connectivity, the 8cx Gen 3 has built in Wi-Fi 6E through the FastConnect 6900, but also a range of 5G solutions: either the X65 at 10 Gbps DL, the X55 at 7.5 Gbps DL, or the X62 at 4.4 Gbps DL. On first reading, this sounds like there are three silicon designs, one for each modem. However I suspect we’re dealing with a single silicon design, and the modem delineations are merely a function of what license the OEM is willing to pay combined with whatever RF front end circuitry the OEM is also willing to pay for. For a cost down device for example, the X62 at half speed might be better suited, or for markets where X65 offers no additional value it might make sense to enable an X55 variant instead.Other functions include AI acceleration, which Qualcomm is quoting a 3x boost from 9 TOPs to 29 TOPs in the new chip. Note that this is CPU+GPU+Hexagon combined, which is unlikely to exist within the same software – speaking to Qualcomm, they say that different software can use different segments to get good asynchronous performance. The AI is set to be used for camera and video live processing, as well as echo cancellation and noise suppression. It’s clear that Qualcomm want to rival something like RTX Voice but in a much lower power envelope. One of my criticisms of RTX Voice is that I can’t process pre-recorded audio through it without a full play-back and a feedback loop, so when asked, Qualcomm confirmed that content creators will be able to post-process video and audio for noise cancellation.On security, the 8cx Gen 3 will be the first Windows-focused processor with built-inMicrosoft Pluton. Pluton is a superset of traditional TPM features, enabling root-of-trust and firmware authentication, potentially augmented with Microsoft Cloud based environments. I’d be remiss if I didn’t state that since the announcement, there have been somedeep criticisms of Plutonas a fundamental concept, tying it to an operating system vendor, and being a potential single point of failure for personal privacy depending on how it is implemented, especially outside the US. All the major Windows silicon companies are committed to hardware-based Pluton solutions long term, so it depends how much you consider these criticisms are valid, inevitable for the wider populace, or something that can collectively be addressed. On top of Pluton, we also have Layered Secure Boot, real time memory encryption, and Qualcomm’s Trusted Execution Environment.We should expect to see 8cx Gen 3 hardware come to market early next year. However, the big story is yet to come.Qualcomm x Nuvia, Sampling 2022, Hardware 2023In March 2021, Qualcomm acquired Nuvia, a server CPU startup working on its first core design. Nuvia’s lead engineers herald from the silicon design teams at Apple and Google, with the lead architect having been the lead architect at Apple during its M1 development phase. Qualcomm acquired Nuvia with the goal of creating hardware to not only compete, but demolish, the Windows market of its incumbents.At the time of the acquisition, the messaging was that the server core Nuvia was designing would be repurposed for Qualcomm’s ACPC efforts, with silicon due in 2022 for products in 2023. The company reiterated that same timeline at its Analyst event in late November, and at the Tech Summit. When speaking to whether the Nuvia team were creating a single performance core, a series of cores for performance/efficiency, or the key parts of the SoC, I didn’t really get a straight answer. That was perhaps to be expected, given that we’re still almost a year away – I suspect one of the Tech Summit December 2022 highlights next year will be this new Nuvia chip. However it does look like that Nuvia is initially making a single core for the first iteration, relying on regular Cortex designs if the SoC is going to be a heterogeneous design (we expect it will be).Image from@anshelsagon Twitter, Used with permissionWith a lot of hype about Nuvia and the credentials of the team behind it (I apologize, I’m probably responsible for an amount of that hype), we are in this sort of limbo state waiting for the fruits of that acquisition to come through. The 8cx Gen 3 will be a good platform showing an uptick in performance and leveraging 64-bit emulation on Windows 11, but the expectation is that Nuvia’s core design will amplify that performance up another step. Exactly where it will fit compared to Intel or Apple is really hard to say at this point, and we’re going to have to wait to next year. Which makes recommending an 8cx Gen 3 platform in 2022 difficult given that on every review, we’ll have to write a caveat of ‘watch out for the Nuvia designs coming 2023’.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17127/qualcomms-8cx-gen-3-for-notebooks-nuvia-core-in-20222023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Snapdragon 8 Gen 1 Performance Preview: Sizing Up Cortex-X2\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-12-14T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/17102/snapdragon-8-gen-1-performance-preview-sizing-up-cortex-x2\n",
      "Content: At the recent Qualcomm Snapdragon Tech Summit, the company announced its new flagship smartphone processor, the Snapdragon 8 Gen 1. Replacing the Snapdragon 888, this new chip is set to be in a number of high performance flagship smartphones in 2022. The new chip is Qualcomm’s first to use Arm v9 CPU cores as well as Samsung’s 4nm process node technology. In advance of devices coming in Q1, we attended a benchmarking session using Qualcomm’s reference design, and had a couple of hours to run tests focused on the new performance core, based on Arm’s Cortex-X2 core IP.The Snapdragon 8 Gen 1Rather than continue with the 800 naming scheme, Qualcomm is renaming its smartphone processor portfolio to make it easier to understand / market to consumers. TheSnapdragon 8 Gen 1(hereafter referred to as S8g1 or 8g1) will be the headliner for the portfolio, and we expect Qualcomm to announce other processors in the family as we move into 2022. The S8g1 uses the latest range of Arm core IP, along with updated Adreno, Hexagon, and connectivity IP including an integrated X65 modem capable of both mmWave and Sub 6 GHz for a worldwide solution in a single chip.While Qualcomm hasn’t given any additional insight into the Adreno / graphics part of the hardware, not even giving us a 3-digit identifier, we have been told that it is a new ground up design. Qualcomm has also told us that the new GPU family is designed to look very similar to previous Adreno GPU sfrom a feature/API standpoint, which means that for existing games and other apps, it should allow a smooth transition with better performance. We had time to run a few traditional gaming tests in this piece.On the DSP side, Qualcomm’s headlines are that the chip can process 3.2 Gigapixels/sec for the cameras with an 18-bit pipeline, suitable for a single 200MP camera, 64MP burst capture, or 8K HDR video. The encode/decode engines allow for 8K30 or 4K120 10-bit H.265 encode, as well as 720p960 infinite recording. There is no AV1 decode engine in this chip, with Qualcomm’s VPs stating that the timing for their IP block did not synchronize with this chip.Qualcomm's Alex KatouzianAI inference performance has also quadrupled - 2x from architecture updates and 2x from software. We have a couple of AI tests in this piece.As usual with these benchmarking sessions, we’re very interested in what the CPU part of the chip can do. The new S8g1 from Qualcomm features a 1+3+4 configuration, similar to the Snapdragon S888, but using Arm’s newest v9 architecture cores.The single big core is a Cortex-X2, running at 3.0 GHz with 1 MiB of private L2 cache.The middle cores are Cortex-A710, running at 2.5 GHz with 512 KiB of private L2 cache.The four efficiency cores are Cortex-A510, running at 1.8 GHz and an unknown amount of L2 cache. These four cores are arranged in pairs, with L2 cache being private to a pair.On the top of these cores is an additional 6 MiB of shared L3 cache and 4 MiB of system level cache at the memory controller, which is a 64-bit LPDDR5-3200 interface for 51.2 GB/s theoretical peak bandwidth.Compared to the Snapdragon S888, the X2 is clocked higher than the X1 by around 5% and has additional architectural improvements on top of that. Qualcomm is claiming +20% performance or +30% power efficiency for the new X2 core over X1, and on that last point it is beyond the +16% power efficiency quoted by Samsung moving from 5nm to 4nm, so there are additional efficiencies Qualcomm is implementing in silicon to get that number. Unfortunately Qualcomm would not go into detail what those are, nor provide details about how the voltage rails are separated, if this is the same as S888 or different – Arm has stated that the X2 core could offer reduced power than the X1, and if the X2 is on its own voltage rail that could provide support for Qualcomm’s claims.The middle A710 cores are also Arm v9, with an 80 MHz bump over the previous generation likely provided by process node improvements. The smaller A510 efficiency cores are built as two complexes each of two cores, with a shared L2 cache in each complex. This layout is meant to provide better area efficiency, although Qualcomm did not explain how much L2 cache is in each complex – normally they do, but for whatever reason in this generation it wasn’t detailed. We didn’t probe the number in our testing here due to limited time, but no doubt when devices come to market we’ll find out.On top of the cores is a 6 MiB L3 cache as part of the DSU, and a 4 MiB system cache with the memory controllers. Like last year, the cores do not have direct access to this 4 MiB cache. We’ve seen Qualcomm’s main high-end competitor for next year, MediaTek, showcase that L3+system cache will be 14 MiB, with cores having access to all, so it will be interesting to see how the two compare when we have the MTK chip to test.Benchmarking Session: How It WorksFor our benchmarking session, we were given a ‘Qualcomm Reference Device’ (QRD) – this is what Qualcomm builds to show a representation of how a flagship featuring the processor might look. It looks very similar to modern smartphones, with the goal to mirror something that might come to market in both software and hardware. The software part is important, as the partner devices are likely a couple of months from launch, and so we recognize that not everything is final here. These devices also tend to be thermally similar to a future retail example, and it’s pretty obvious if there was something odd in the thermals as we test.These benchmark sessions usually involve 20-40 press, each with a device, for 2-4 hours as needed. Qualcomm preloads the device with a number of common benchmarking applications, as well as a data sheet of the results they should expect. Any member of the press that wants to sideload any new applications has to at least ask one of the reps or engineers in the room. In our traditional workflow, we sideload power monitoring tools and SPEC2017, along with our other microarchitecture tests. Qualcomm never has any issue with us using these.As with previous QRD testing, there are two performance presets on the device – a baseline preset expected to showcase normal operation, and a high performance preset that opportunistically puts threads onto the X2 core even when power and thermals is quite high, giving the best score regardless. The debate in smartphone benchmarking of initial runs vs. sustained performance is a long one that we won’t go into here (most noticeably because 4 hours is too short to do any extensive sustained testing) however the performance mode is meant to enable a ‘first run’ score every time.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17102/snapdragon-8-gen-1-performance-preview-sizing-up-cortex-x2\n",
      "Title: Best Laptops: Holiday 2021\n",
      "Author: Brett Howse\n",
      "Date Published: 2021-12-01T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/9798/best-laptops\n",
      "Content: In our series of laptop buyers guides, here’s the latest update to our list of recommended laptops. All numbers in the text are updated to reflect pricing at the time of writing.With 2021 coming to a close, it is time again to look back at the laptop market. Despite bumps from the COVID pandemic, the laptop ecosystem continues to flourish, delivering a bevvy of new products based on new platforms.For 2021, AMD launched their very successful Ryzen 5000 series products (codenamed Cezanne) featuring the latest Zen 3 CPU cores paired with AMD Vega graphics. Intel, still basking in the glow of the Tiger Lake launch in late 2020 continues to feature the 11th generation Core platform that is now over a year old. Based on the Willow Cove cores and featuring a much-improved Intel Xe graphics solution, Intel's solution still provides excellent performance and efficiency for the PC laptop space.The big news of 2021 was Apple ditching x86 processors and beginning the process of moving their products to their in-house designs. While Macs won't be covered in this guide – either you want an Arm-based Mac or you want an x86-based PC – it was still an important milestone in the laptop space and not one to be ignored.Laptop Recommendations Holiday 2021SegmentModelStarting Price (As of writing)Entry Level / MidrangeMicrosoft Surface Laptop Go$499 USDAcer Swift 3$740 USDPremium UltrabooksDell XPS 13 9310$999.99 USDLenovo ThinkPad X1 Carbon$1168 USDConvertiblesMicrosoft Surface Pro 8$1099.99 USDDell XPS 13 2-in-1$999.99 USDDiscrete GPU LaptopsMicrosoft Surface Laptop Studio$1599.99 USDDell XPS 17$1499.99 USDAs always, we’ll break the guide down into several segments to serve various markets, from low-cost, to mid-range, to high end. This guide will focus on Windows PCs only - if you are in the market for a Mac, you of course have to buy a Mac.Entry LevelEveryone has a budget, and if yours is $500 to $700 for a laptop computer, the good news is that there are quality machines at this price now. A few years ago, it was a challenge to find solid-state drives and IPS display panels in machines under $1000 but no longer.Microsoft Surface Laptop GoMicrosoft's first entry on this list is their most affordable laptop outside of the education market, and the Surface brand continues to offer some key features to help it stand out from the competition. Displays are important, and Microsoft offers the taller 3:2 aspect ratio across their lineup which helps a lot with productivity tasks. Launching in October 2020 means that this model is still shipping with Intel's 10th generation Ice Lake platform, but the Core i5-1035G1 is a proper quad-core processor with HyperThreading support, built on Intel's 10 nm process.The catch with budget Surface devices though is that Microsoft tends to sneak in a very low-priced model to hit a target price point, but is one that you want to avoid. That is very much the case here, with the 4GB/64GB model featuring eMMC storage rather than SSD. Thankfully, the Laptop Go comes in an 8GB/128GB model with SSD storage, as well as a 256GB SSD option, and the SSD is replaceable.The Laptop Go comes in under 2.5 lbs for its 12.4-inch size making it very portable, hence the name. And, since it is a year old, prices have come down with the proper 8GB/128GB model on sale now for $549, with the eMMC model just $50 less and easily ignored.Acer Swift 3Acer's Swift 3 offers the premium feel and looks of a more expensive Ultrabook for less, while still offering great performance. Acer sells both an Intel and AMD version of the Swift 3 with the AMD Ryzen 7 5700U powering the latter, and featuring eight CPU cores. It pairs that with 8 GB of RAM and a 512 GB SSD as well. Acer's 1920x1080 display is an IPS panel offering the proper viewing angles but without the color accuracy of something like the Surface lineup would deliver. Still, it is a strong package and if you can get it for around $700, it is a good value.Premium UltrabooksIf you can spend a bit more, the step up to the premium segment of Ultrabooks brings with it improved design and displays.Dell XPS 13 9305Dell's latest version of the XPS 13 is the 9305 model, and while their naming scheme is confusing, the end result is not. The XPS 13 is simply one of the best designed notebooks around. Featuring an Intel's 11th gen Core lineup, and starting at 8GB of RAM and 256GB SSD storage and wrapped in an exquisitely-designed chassis, the XPS 13 is a stunner. Dell was a pioneer in the area of the slim-bezel design, and the XPS 13 9305 continues to provide an impressive screen to body ratio. Also impressive is the price which as of today is around $850 for this notebook with a 1920x1080 display and no touch support.Lenovo ThinkPad X1 Carbon 9th GenIs there anything more of a pure statement of a laptop as Lenovo's ThinkPad X1 Carbon? Possibly not. Lenovo has switched back to 16:10 displays for the X1 Carbon, and it is powered by Intel's Tiger Lake platform. You can get up to 32 GB of memory, 1 TB of SSD storage, and if you want, Lenovo will even sell you one with Linux pre-installed. This powerful and compact notebook is also very portable, coming in at just a hair under 2.5 lbs.ConvertiblesMicrosoft Surface Pro 8The Surface Pro series got a major overhaul this year. In fact, it is the first major redesign since the Surface Pro 3 which moved the lineup to the now iconic 3:2 aspect ratio display. Although a detachable tablet, the Surface Pro 8 packs in a lot of performance with up to 32 GB of RAM and up to 1 TB of storage. Built on the 11th generation Intel Core platform, the Surface Pro 8 also gets a big GPU bump thanks to the integrated Xe-LP graphics. The 13-inch 2880x1920 display offers 267 pixels per inch, and now features 120 Hz refresh. The display got bigger, but the overall design much less so, thanks to the smaller display bezels. The Surface Pro has become the gold standard in terms of detachable devices, thanks to its great design, good performance, and excellent display. For 2021, all of that got even better.Dell XPS 13 2-in-1 9310Outfitted with Intel's 11th gen Tiger Lake platform, the Dell XPS 13 2-in-1 is a masterful take on the 360° hinged notebook. Thin, light, and with excellent battery life, Dell has delivered everything that makes the XPS 13 amazing, and then added the additional functionality of a convertible hinge design. Right now, it is even the same price as the traditional clamshell XPS 13 making it even more appealing.High Performance Laptops​Thin and light is great, but sometimes you need a bit more grunt. These laptops add higher-power processors, coupled with discrete GPUs.Microsoft Surface Laptop StudioPacking in a 35-Watt Tiger Lake CPU and a discrete NVIDIA RTX 3050 Laptop GPU, this 14-inch 4 lb notebook offers a lot of performance for its size, and it does it in a wonderful new chassis. With an excellent 3:2 aspect display which is mounted to a center-mounted hinge, the Surface Laptop Studio also can transform into several modes to further increase its capabilities. The Surface Laptop Studio offers not only an excellent design, but a great keyboard, and Microsoft's first haptic touchpad. For those that need pen support, Microsoft's new pen design can be magnetically attached to the front lip of the notebook for easy storage and charging as well. The Surface Laptop Studio is not inexpensive, but it is a wonderful notebook to use.Dell XPS 17 9710Refreshed again for 2021, the XPS 17 now offers Intel's 11th Gen H-Series processors in the 45-Watt range, offering eight CPU cores, and Dell has upgraded the GPU offering as well, with up to a NVIDIA RTX 3060 laptop GPU. The 17-inch display features a 16:10 aspect ratio in either 1920x1200 or 3840x2400 resolutions. Although it packs in a 17-inch display, the XPS 17 is not much larger than a typical 15-inch notebook thanks to Dell's InfinityEdge display.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/9798/best-laptops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qualcomm Announces Snapdragon 8 Gen 1: Flagship SoC for 2022 Devices\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-11-30T23:00:00Z\n",
      "URL: https://www.anandtech.com/show/17091/qualcomm-announces-snapdragon-8-gen-1-flagship-soc-for-2022-devices\n",
      "Content: At this year’s Tech Summit from Hawaii, it’s time again for Qualcomm to unveil and detail the company’s most important launch of the year, showcasing the newest Snapdragon flagship SoC that will be powering our upcoming 2022 devices. Today, as the first of a few announcements at the event, Qualcomm is announcing the new Snapdragon 8 Gen 1, the direct follow-up to last year’s Snapdragon 888.The Snapdragon 8 Gen 1 follows up its predecessors with a very obvious change in marketing and product naming, as the company is attempting to simplify its product naming and line-up. Still part of the “8 series”, meaning the highest end segment for devices, the 8 Gen 1 resets the previous three-digit naming scheme in favor of just a segment and generation number. For Qualcomm's flagship part this is pretty straightforward, but it remains to be seen what this means for the 7 and 6 series, both of which have upwards of several parts for each generation.As for the Snapdragon 8 Gen 1, the new chip comes with a lot of new IP: We’re seeing the new trio of Armv9 Cortex CPU cores from Arm, a whole new next-generation Adreno GPU, a massively improved imaging pipeline with lots of new features, an upgraded Hexagon NPU/DSP, integrated X65 5G modem, and all manufactured on a newer Samsung 4nm process node.The new chip promises large increases in performance and efficiency in a lot of the processing elements, as well as new features enabling new user experiences. Let’s start over the basic specifications and drill down the details that we have on the chip:Qualcomm Snapdragon Flagship SoCs 2020-2021SoCSnapdragon 8 Gen 1Snapdragon 888CPU1xCortex-X2@3.0GHz 1x1024KB pL23xCortex-A710@2.5GHz 3x512KB pL24xCortex-A510@ 1.80GHz2x??KBsL26MB sL31xCortex-X1@ 2.84GHz 1x1024KB pL23xCortex-A78@ 2.42GHz 3x512KB pL24x Cortex-A55@ 1.80GHz 4x128KB pL24MB sL3GPUAdreno next-genAdreno 660 @ 840MHzDSP / NPUHexagonHexagon 78026 TOPS AI(Total CPU+GPU+HVX+Tensor)MemoryController4x 16-bit CH@ 3200MHz LPDDR5 / 51.2GB/s4MB system level cacheISP/CameraTriple 18-bit Spectra ISP1x 200MP or 108MP with ZSLor64+36MP with ZSLor3x 36MP with ZSL8K HDR video & 64MP burst captureTriple 14-bit Spectra 580 ISP1x 200MP or 84MP with ZSLor64+25MP with ZSLor3x 28MP with ZSL4K video & 64MP burst captureEncode/Decode8K30 / 4K120 10-bit H.265Dolby Vision, HDR10+, HDR10, HLG720p960 infinite recordingIntegrated ModemX65integrated(5G NR Sub-6 + mmWave)DL = 10000 MbpsUL = 3000 MbpsX60 integrated(5G NR Sub-6 + mmWave)DL = 7500 MbpsUL = 3000 MbpsMfc. ProcessSamsung4nm(unspecified)Samsung5nm (5LPE)CPUs: Cortex-X2 and Armv9 siblingsStarting off with the CPUs of the new Snapdragon 8 Gen 1 (I’ll shorthand it as S8g1 here and there): This is Qualcomm’s first chip featuringthe new Armv9 generation of CPU IPs from Arm, which includes the Cortex-X2, Cortex-A710, and Cortex-A510 in a big, middle, and little setup. Qualcomm continues to use a 1+3+4 core count, a setup that’s been relatively successful for the designers over the past few years and iterations ever since the Snapdragon 855.The Cortex-X2 core of the new chip clocks in at 3.0GHz, which is a tad higher than the 2.84GHz clock of the X1 core on the Snapdragon 888. This was actually a bit surprising to me, as I hadn’t expected much in the way of clock increases this generation, but it’s nice to see Arm vendors now routinely achieving this. For context, MediaTek’s recently announcedDimensity 9000achieves 3.05GHz on its X2 core, however that’s on a TSMC N4 node. In contrast, Qualcomm manufactures the Snapdragon 8 Gen 1 on a Samsung 4nm node. The company wouldn’t confirm if it’s a 4LPE variant or something more custom, hence why we’re leaving it as a “4nm” node description in the specification table.What is most surprising about the X2 core is that Qualcomm is claiming 20% faster performance or 30% power savings, the latter figure being especially intriguing.Samsung Foundry only describe a 16% reduction in powerin going from a 5nm to 4nm node, and obviously 30% is significantly better than what the process node promises. We asked Qualcomm what kind of improvements lead to such a large power decrease; however, the company wouldn’t specify any details. I particularly asked if the new X2 cores have their own voltage domain (Previous Snapdragon 1+3 big+middle implementations shared the same voltage rail), but the company wouldn’t even confirm if this was the case or not. Arm had noted that the X2 can havequite lower power at the same peak performance point of the X1, if Qualcomm’s marketing materials refer to such a comparison, then the numbers might make sense.The X2 core is configured with 1MB of L2 cache, while the three Cortex-X710 cores have 512KB each. The middle cores here are clocked slightly higher at 2.5GHz this year, a little 80MHz jump over the previous generation. Usually, the middle cores pay more attention to the power budget, so maybe this slightly increase does represent more accurately the process node improvements.Lastly, the new chip also makes use of four Cortex-A510 cores at 1.8GHz.Unlike the Dimensity 9000 from a couple of weeks back, Qualcomm does make use of Arm’s new“merged-core”approach of the new microarchitecture, meaning that the chip actually has two Cortex-A510 complexes with two cores each, sharing a common NEON/SIMD pipeline and L2 cache. The merged core approach is meant to achieve better area efficiency. Qualcomm rationalized the approach by saying that in everyday use cases with fewer threads active and overall low activity, having a single core able to access a larger L2 cache shared by two cores can result in better performance and efficiency. Unfortunately even while making this comment, the company wouldn’t actually detail what the L2 size was, whether it’s 512KB or 256KB – if it’s the latter, then the configuration definitely isn’t as aggressive as the Dimensity 9000.The new Armv9 CPU IPs from Arm also came with a new generation DSU (DynamiQ Shared Unit, the cluster IP) which the new Snapdragon makes use of. Qualcomm here opted for a 6MB L3 cache size, noting that this was a decision in balancing out system performance across target workloads.As for system caches, Qualcomm mentioned that the chip remains unchanged with a 4MB cache, and the memory controllers are still 3200MHz LPDDR5 (4x 16bit channels). It’s to be noted that, as with last year’s Snapdragon 888, the CPUs no longer have access to the system cache, in order to improve DRAM latency. We can’t help but make comparisons to MediaTek’s Dimensity 9000, which likely will have worse DRAM latency, but also offer up to 14MB of shared caches to the CPUs versus just 6MB on the Snapdragon 8 Gen 1. How the two chips will compare to each other remains to be seen in actual commercial devices.GPU: New Adreno architecture with no nameBack in the day, Qualcomm’s Adreno GPU architectures were easy to identify in terms of their family as well as performance levels. Particularly on the architecture side, the Adreno 600 series started off with the Adreno 630 in the Snapdragon 845 a few years ago, but unlike in previous iterations from the 400- and 500 series, we remained with that high-level description up until the Snapdragon 888 series.The Snapdragon 8 Gen 1 here changes things, and frankly, Qualcomm did a quite horrible job at marketing what they have this time around. The new GPU name completely drops any model number, and as such doesn’t immediately divulge that it’s part of a larger microarchitecture shift that in the past would have been marketed as a new Adreno series.Qualcomm notes that from an extremely high-level perspective, the new GPU might look similar to the previous generations, however there are large architectural changes included that are meant to improve performance and efficiency. Qualcomm gave examples such as concurrent processing optimizations that are meant to give large boosts in performance to real-world workloads that might not directly show up in benchmarks. Another example was that the GPU’s “GMEM” saw large changes this generation, such as an increase of 33% of the cache (to 4MB), and now being both a read & write cache rather than just a writeback cache for DRAM traffic optimizations.The high-level performance claims are 30% faster peak performance, or 25% power reduction at the same performance as the Snapdragon 888. Qualcomm also uncharacteristically commented on the situation of peak power figures and the current situation in the market. Last year, Qualcomm rationalized the Snapdragon 888’s high peak GPU power figures by noting that this is what vendors had demanded in response to what we saw from other players, notably Apple, and that vendors would be able to achieve better thermal envelopes in their devices. Arguably, this strategy ended up as being quite disastrous and negative in terms of perception for Qualcomm, and I feel that in this year’s briefing we saw Quaclomm attempt to distance themselves more from the situation, largely by outright saying that the only point of such peak performance and power figures is for vendors to achieve higher first-run benchmarking numbers.Unfortunately, unlike Apple, who actually use their GPU’s peak performance figures in transient compute workloads such as camera processing, currently the Android ecosystem just doesn’t make any advanced use of GPU compute. This admission was actually a breath of fresh air and insight into the situation, as it’s been something I’ve especially noted in our Kirin 9000,Snapdragon 888 and Exynos 2100andTensordeep-dives in criticizing all the new chips. It’s an incredibly stupid situation that, as long as the media continues to put weight on peak performance figures, won’t be resolved any time soon, as the chip vendors will have a hard time saying no to their customer’s requests to operate the silicon in this way.Qualcomm states that one way to try to alleviate this new focus on peak performance is to change the way the GPU performance and power curve behaves. The team stated that they’ve gone in to change the architecture to try to flatten the curve, to not only achieve those arguably senseless peak figures, but actually focus on making larger improvements in the 3-5W power range, a range where the Snapdragon 888 last year didn’t significantly improve upon the Snapdragon 865.That being said, even with a 25% decrease in power at similar Snapdragon 888 performance, the new Snapdragon 8 Gen 1likely still won’t be able to compete against Apple’s A14 or A15 chips. MediaTek’s Dimensity 9000 also should also be notably more efficient than the new Snapdragon at equal performance levelsgiven the claimed efficiency figures, so it still looks like Qualcomm’s choice of going with a Samsung process node, even this new 4nm one, won’t close the gap to the TSMC competitors.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17091/qualcomm-announces-snapdragon-8-gen-1-flagship-soc-for-2022-devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: MediaTek Announces Dimensity 9000: Supercharged Flagship SoC on 4nm\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-11-18T23:00:00Z\n",
      "URL: https://www.anandtech.com/show/17070/mediatek-announces-dimensity-9000\n",
      "Content: MediaTek over the last few years has generally always been regarded as the “other” SoC vendor in the mobile industry, with most media and consumer attention being paid to the flagship SoC products by the likes of Apple, Qualcomm, Samsung and HiSilicon. Indeed, the last time MediaTek had attempted a true flagship SoC was several years ago withthe Helio X20 and X30, before seeing very little success in the market and instead refocusing on the mid-range and “premium” segments.Today, MediaTek is looking to change this positioning. After seeing new-found success in the market, particularly seeing a fantastic 2020 and 2021, where the Taiwanese vendor is now able to claim the #1 spot with 40% market share, as well as a growing 28% of 5G SoC market share, the company is now also aiming for recognition and leadership position in the flagship SoC market – this is where the new Dimensity 9000 comes in.The Dimensity 9000 is MediaTek’s latest effort in creating a no-compromise flagship SoC, with the designers throwing in everything but the kitchen sink at it in terms of specifications, representing a lot of industry firsts, such as the first Armv9 SoC with Cortex-X2, A710’s and A510’s CPUs, a new Mali-G710 GPU, first LPDDR5X compatible SoC, astounding camera ISP claims, and the first outright publicly announced TSMC N4 silicon design in the industry. The list of features and capabilities is extensive, and the announcement today definitely represents MediaTek’s largest effort in generations and years.Starting off with the process node, MediaTek is able to claim a first in the industry, with the Dimensity 9000 being the world’s first TSMC N4 chip. Over the past few years, we’ve always been accustomed that either Apple or HiSilicon be the very first customers on TSMC’s latest leading-edge nodes. With HiSilicon being cut-off from TSMC, that left Apple as the obvious lead partner for TSMC’s new generation process nodes – however, the timing here just didn’t work out for the A15 as the N4 node just wasn’t ready yet. With Qualcomm currently being tied to Samsung Foundry for their flagships (arguably with not great success), this left a vacuum for where HiSilicon used to be, which MediaTek is now looking to fill. In fact, I think this would be the company’s first time where they’re truly on a leading edge node since the 20nm days.TSMC’s N4 nodeis supposed to be a smaller optical shrink over the N5 node, resulting in 6% more density, with similar single-digit improvements in performance and efficiency. TSMC had announced risk production for N4 to start in 3Q21, and with the Dimensity 9000 planned to hit commercial devices in 1Q22, the chip is likely the lead product for the process node.New MediaTek Flagship SoC 2022SoCDimensity 9000CPU1xCortex-X2@ 3.05GHz1x1024KB pL23xCortex-A710@ 2.85GHz3x512KB pL24xCortex-A510@ 1.80GHz 4x256KB pL28MB sL3GPUMali-G710MP10@ ~850MHzMemoryController4x 16-bit CH@ 3200MHz LPDDR5 / 51.2GB/s@ 3750MHz LPDDR5X / 60.0GB/s6MB System CacheISPImagiq790New-gen Triple 18-bit ISP9GPix/s processing throughputSingle Sensor up to 320MPTriple Sensor 32+32+32MPNPU5th Gen 4+2 core APUMedia8K30 & 4K120 encode &8K60 decodeH.265/HEVC, H.264, VP98K30 AV1 DecodeModem(LTE Category 24/18)(5G NR Sub-6)Mfc. ProcessTSMC N4There’s a lot to talk about the Dimensity 9000, so obviously enough as MediaTek advertises it as the first Armv9 SoC, let’s start off with the CPU configuration and the various IPs employed here.No-Compromise CPU SetupBeing an Armv9 SoC, this means that the company is refreshing all the CPU IPs, employing the new Cortex-X2, Cortex-A710, and Cortex-A510 IPs from Arm. We hadcovered the new generation CPUs extensive earlier this year, so be sure to read up on those articles.The Dimensity 9000 goes with a 1+3+4 CPU setup that has seen popularity in the market ever since Qualcomm had adopted the setup for the first timein the Snapdragon 855. For the performance cores, MediaTek uses the new Cortex-X2 cores, equipping them with the full 1MB of L2 cache, and clocking them at up to 3.05GHz. The clock frequency is higher than what we’re seeing from designs today on X1 cores such as the Snapdragon 888 or the Exynos 2100 at respectively 2.86 and 2.9GHz, but those competing SoCs were also on an inferior Samsung 5LPE process node. We don’t yet know exactly where the next-gen Snapdragon and Exynos chips will end up in terms of clocks, but I think it’s unlikely they will exceed the 3GHz mark, leaving the new Dimensity 9000 with a likely frequency advantage, and thus also a likely single-threaded performance leadership position amongst the Android SoC vendors.MediaTek does quote a +35% performance leap over current generation Android Flagship chips, which we assume is going to be a Snapdragon 888, however also states that efficiency is +37% better. This would mean that peak absolute power levels for the MediaTek 9000’s X2 cores would be similar to what we’re seeing from the X1 cores in a Snapdragon 888 today, which generally is a good position to be in, and the figures generally line up with what we expect from the IPC and process node differences between the designs.MediaTek did note that the performance leap in more memory-bound workloads to be much higher than more core-local workloads, for example SPECint2006 seeing a +35% increase, while GeekBench 5 only will see a +10.5% increase over the competition. This generally also lines up with our understanding of the Cortex-X2, pointing out to low IPC improvements in anything that’s not taking advantage of the increase caches of the CPU cluster.The middle cores of the Dimensity 9000 are 3x Cortex-A710 cores, equipped with 512KB L2’s, and clocked up to 2.85GHz. In this regard, MediaTek’s approach here is more similar to the Exynos 2100 in that it’s using quite high frequency mid-cores, in contrast to the lower 2.4GHz design point Qualcomm employs.Alongside the middle cores, we also see the new Cortex-A510 little cores, and here MediaTek is doing things quite differently compared to what we expected from the first iterations of the IP. Instead of using Arm’s new “merged-core” approach, where a Cortex-A510 complex can consist of two cores sharing a SIMD/FP pipeline as well as a shared L2, MediaTek completely ignores this design aspect of the IP and instead goes the traditional route of only using one core per complex, with each core thus having its own SIMD/FP pipeline and private L2 cache. The cache here lies in at 256KB, which is also quite large, and short of the 512KB maximum. In effect, what MediaTek has done here is to configure the A510 cores with a near-maximum performance setup. Whilewe still have our reservations about the cores, it’s good to see MediaTek not skimping out on the new designs.Due to the strongly configured middle cores, as well as well equipped little cores, the multi-core performance of the Dimensity 9000 is advertised as well exceeding the current Android competition, and falling in line with Apple achieves on the A15.At the cluster level, MediaTek also equips the DSU with 8MB of L3 – this is likely thenew generation DSU-110 as well.On the CPU side, the Dimensity 9000 is essentially configured in the most optimal way – MediaTek went all-out in terms of frequencies and caches, and it’s generally hard to imagine a more performant configuration than what the chip is currently set up with, at least in the context of Arm Cortex CPU IP.First LPDDR5X, Large System CacheAnother world first for the Dimensity 9000 is the fact that it’s the first chip announced to be compatible with LPDDR5X. The standard had only beenpublished by JEDEC in July of this year, so the fact that the chip already supports it means that MediaTek was working off a draft and should be fully compatible with the new standard. While the full standard is advertised to go up to 8533Mbps support, the chip here does limit itself to 7500Mbps, so that means +17% bandwidth compared to current generation LPDDR5-6400 solutions. Still, I hadn’t expected LP5X SoCs until next late next year, so this was definitely a surprise. Naturally, the memory controller still fully supports LPDDR5 at up to 6400Mbps in case a vendor chooses to employ different memory modules.The Dimensity 9000 is MediaTek’s first SoC also employing a system cache at 6MB. During the briefing, MediaTek noted that larger caches and SoC designs with system caches are definitely the way forward and is where everybody will be aiming for in the future. System level caches, or how we like to call them abbreviated, SLCs, are able to amplify performance of SoC blocks other than just the CPU, as well as reduce the memory traffic to DRAM, also having a positive benefit to power efficiency.GPU: Mali G710MP10On the GPU side of things, the MediaTek Dimensity 9000 is also the first SoC to see the deployment of the new Mali-G710 GPU. Earlier this year when we talked about the IP we had mentioned thatMediaTek was the only remaining vendorthat was expected to release an SoC with a larger Mali GPU implementation, given HiSilicon’s troubles andSamsung’s adoption of AMD RDNA GPUs.The configuration on the Dimensity 9000 is a 10-core. We have to remember here that in terms of per-core performance one new G710 core is roughly equivalent to two G78 cores, so in terms of size and performance the new chip’s GPUis roughly comparable to the Google Tensor G78MP20 GPU, plus maybe an expected 20% performance boost due to generational IP improvements. MediaTek noted the peak frequencies to be at around 850MHz (exact clock to be confirmed).In terms of performance figures, the company’s materials advertised +35% vs the current Android flagships, while efficiency being +60% better. All of this year’s flagships had been rather disappointing in gaming efficiency, and we saw absolute power figures reaching +7.5-9W on the leading Exynos, Tensor and Snapdragon chips. MediaTek noting that their efficiency advantage is significantly larger than their performance leap also suggests they’re using lower peak power levels that what we see today, which is definitely a welcome change.The company makes note of Ray Tracing capability, but this is simply a software API implementation rather than hardware, as the G710 doesn’t yet support this.MediaTek had a slide showcasing longer term performance versus an iPhone 13 with the A15, with the Dimensity 9000 being able to slightly exceed the performance of the iPhone. We saw thatthe new iPhones throttle to around 3-3.5W, and that under cellular conditions the phones are reported to perform even worse due to the bad thermals. MediaTek notes the comparison is made under a similar thermal budget, so hopefully the comparison is valid here. It's to be noted, as we wrote in our A15 review, comparing real-world games such as Genshin Impact for GPU analysis isn’t great as the game always runs at different internal resolutions or detail levels, especially between Android and iOS.That being said, MediaTek’s efficiency claims for the GPU do position it extremely well, and would likely allow it to effectively compete against the upcoming Snapdragon and Exynos chips which are still projected to arrive on less efficient process nodes.Low-Power Leadership ClaimsAn interesting claim from MediaTek is that they are achieving low power leadership, thanks to the new TSMC N4 node as well as the smart power management the SoC as well as the platform is designed with.The above figures are comparisons of platforms total power, excluding power supply towards the display panel. This means we’re seeing a power comparison of the SoC, DRAM, PMICs, cellular RF and Wi-Fi systems – essentially the “platform” components which the SoC vendors are generally responsive for and which they bundle their offerings with.Notable figures here are the media playback and recording numbers, where the Dimensity 9000 is said to have much lower power consumption than the competition. Gaming power is also said to be lower, but this is to be expected given the GPU efficiency and lower power claims.The one data-point I find most interesting is the home idle power. One of the hardest things to achieve in a silicon design is doing nothing in an efficient manner, this actually represents a large percentage of energy consumption and affects the baseline power of a device, and thus your every-day battery life. Getting -20% over the competition here is quite respectable.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17070/mediatek-announces-dimensity-9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Mountain Everest Max Mechanical Keyboard Review: Reaching New Heights in Build Quality\n",
      "Author: E. Fylladitakis\n",
      "Date Published: 2021-11-17T15:45:00Z\n",
      "URL: https://www.anandtech.com/show/17000/the-mountain-everest-max-mechanical-keyboard-review-exceptional-build-quality\n",
      "Content: Mountain is a newcomer in the PC peripherals industry. The German-based company started out a couple of years ago, kicking off their business with just a single mouse model. More recently, last year the company made a bold move by putting up their first (and rather unusual keyboard) for crowdfunding. Of course, the bold move was not the crowdfunding itself – this is a tactic that many companies result in nowadays, especially when they want to weigh an uncertain market demand – rather, it was the modular nature of the keyboard.That keyboard became the Everest. Designed with maximum customizability in mind thanks to its modular parts, the Everest has been built to compete as a top-tier mechanical keyboard, with all of the many advanced features come with the territory. And, as we'll see in our review, Mountain didn't just go for feature parity – to produce a keyboardas good asthe best keyboards on the market. Instead they've seemingly gone one better, constructing one of the best built keyboards we have ever seen at AnandTech.Ultimately, Mountain's engineering efforts have led to two different Everest products. With the modularity of their design serving as the marquee feature for the Everest, the flagship product for the keyboard lineup is the Everest Max, which is what we're reviewing today. The Everest Max is the whole kit and caboodle, as it were, offering the base tenkeyless keyboard as well as all of Mountain's additional modular components. Meanwhile, for those who only need a tenkeyless board without the modular frills – and with a lower price tag – Mountain also offers the Everest Core, which true to its name, only includes the base keyboard.Packaging and BundleWe received the Mountain Everest Max in a very cleverly designed cardboard box. The artwork on the box itself is minimal but it reveals tons of information once opened, and designed in such a way that will catch the eye of the user that opens it. It also provides outstanding shipping protection to the keyboard and its components. Every component comes into its own cardboard box, neatly packed inside a drawer underneath the compartment where the core of the keyboard sits.As we received the Max version of the Everest, we also received all of its modular parts available to this date. These are but two – a Numpad and a media dock. The company also includes tilt adjustment feet, a normal Esc keycap, five testing Cherry MX switches, a quick start guide, a few stickers, and a USB cable. The USB cable is removable and it has a Type-C connector on the keyboard’s side and a USB Type-A connector on the PC side. Mountain also includes a Type-C adapter, for those wishing to attach the Everest Max directly to a Type-C host port.The Max version also benefits from the inclusion of a high-quality, magnetic palm rest. It is cushioned and extremely soft to the touch. The magnetic grip on the keyboard is very light, perhaps too light, which can be a good or bad thing depending on how forceful users are with their wrists. The light magnetic force allows for it to be very easily removed and cleaned, without even having to move the keyboard a tiny bit – but this also means it can be unwittingly moved during a heated gaming session.The Mountain Everest Max Mechanical KeyboardThe Core of the Everest keyboard is not unlike most tenkeyless mechanical keyboards out in the market. If not for the subtle company logo at the top of the chassis and the silver Esc keycap, it would be difficult for most people to differentiate the Everest from other tenkeyless keyboards in the market.We received the US/International version of the keyboard. It has a typical ANSI layout, with seven 1.25x keys at the bottom row plus a full-size 6.25x Space Bar key. Mountain is using a clean, rounded fond on the keycaps. The standard keycaps are made from ABS but Mountain offers an upgrade to double-shot PBT keycaps for roughly $30 more. There are no extra buttons on the Core of the keyboard.The top of the keyboard is made out of aluminum, with a straight brushed surround and a circularly brushed top. This creates an eye-catching visual effect but some users may find it unappealing, as it does look like someone went crazy with a wire brush on it.The included palm rest matches the Core keyboard perfectly and is very comfortable to use. However, once the Numpad is attached, the palm rest does look aesthetically out of place. It is not necessarily uncomfortable as most people do not rest their wrists over the Numpad but it is making it seem as if the Numpad is a foreign body to the rest of the keyboard.Once all of the modular components are installed, the Everest Max ceases to be a simple tenkeyless keyboard and its true potential can be clearly seen. The numpad has four mini-screen keys on top of it, and the media dock has an LCD with a rotating dial alongside the small media control buttons and tiny informative LEDs.Users can configure the four display keys on the Numpad and the rotating dial LCD on the media dock via the keyboard’s software. It would not be an exaggeration to say that most (if not all) of the Everest’s advanced features reside on the modular parts.The bottom part of the keyboard is plastic, with three wide anti-slip strips and two circular tilt feet. There are many pathways to cleanly route the cable out of the keyboard’s connector at the center, including towards the front side of the keyboard.At the rear of the keyboard, we find two connectors, one Type-C connector, and one Type-A connector. The Type-A connector works just like a normal hub port and users can connect any USB device to it. The Type-C connector is meant as an attachment point for the media dock. Two more Type-C connectors can be found on either side of the keyboard, which are meant for the numeric pad.Underneath the keycaps of our sample, we found original Cherry MX Red switches. Mountain offers the Everest with either Cherry MX Red, Blue, or Brown switches. This is one of the very few keyboards in existence with original Cherry MX switches that are hot-swappable, allowing the user to change the switches of the keyboard at will. The switches are the RGB variants of course, with clear bodies. Mountain is using typical Cherry MX cross-type supports for the larger keys.The RGB lighting on the Everest Max is crisp and clear, yet not too strong. It is strong enough, especially for those who are going to use the keyboard in relatively dark rooms, but we have seen stronger backlighting with the same Cherry MX switches in the past. It is possible that this may be but an illusion, as the very dark surface of the keyboard absorbs nearly all light, negating any backlight bleed.Opening the keyboard up reveals another surprise, as we found an exceptionally well-designed and assembled device, going beyond even what we usually see even on top-tier products. This might be the first keyboard that we have seen the manufacturer using gap-filling foam material to soundproof and enhance the assembly mechanically. There may be a downside here though, as the material may trap moisture if someone spills a lot of liquid on the keyboard.The heart of the Everest is a Holtek HT32F52352 ARM processor. This is a 32-bit microcontroller with a 48 MHz operating frequency and 16 kB embedded SRAM. It is quite a bit of an overkill for a keyboard, yet it is always good to see some overkill on such top-tier performance devices.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17000/the-mountain-everest-max-mechanical-keyboard-review-exceptional-build-quality\n",
      "Title: Intel: Sapphire Rapids With 64 GB of HBM2e, Ponte Vecchio with 408 MB L2 Cache\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-11-15T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/17067/intel-sapphire-rapids-with-64-gb-of-hbm2e-ponte-vecchio-with-408-mb-l2-cache\n",
      "Content: This week we have the annualSupercomputingevent where all the major High Performance Computing players are putting their cards on the table when it comes to hardware, installations, and design wins. As part of the event Intel is having a presentation on its hardware offerings, which discloses additional details about the next generation hardware going into the Aurora Exascale supercomputer.Aurora is a contract that Intel has had for some time – the scope was originally to have a10nm Xeon Phibased system, for which the idea was mothballedwhen Xeon Phi was scrapped, and has been an ever changing landscape due to Intel’s hardware offerings. It was finalizeda couple of years agothat the system would now be using Intel’sSapphire Rapids processors (the ones that come with High Bandwidth Memory)combined with newPonte Vecchio Xe-HPC based GPU acceleratorsand boosted from several hundred PetaFLOPs to an ExaFLOP of compute. Most recently, Intel CEO Pat Gelsinger has disclosed that the Ponte Vecchio accelerator is achieving double the performance, above the expectations of the original disclosures, and thatAurora will be a 2+EF Supercomputerwhen built. Intel is expecting to deliver the first batch of hardware to the Argonne National Laboratory by the end of the year, butthis will come with $300m write-offon Intel’s Q4 financials. Intel is expecting to deliver the rest of the machine through 2022 as well as ramp up the production of the hardware for mainstream use through Q1 for wider spread launch in the first half of the year.Today we have additional details about the hardware.On the processor side, we know that each unit of Aurora will feature two of Intel’s newest Sapphire Rapids CPUs (SPR), featuring four compute tiles, DDR5, PCIe 5.0, CXL 1.1 (not CXL.mem), and will be liberally using EMIB connectivity between the tiles. Aurora will also be using SPR with built-in High Bandwidth Memory (SPR+HBM), and the main disclosure is that SPR+HBM will offer up to 64 GB of HBM2e using 8-Hi stacks.Based on the representations, Intel intends to use four stacks of 16 GB HBM2e for a total of 64 GB. Intel has a relationship with Micron, and the Micron HBM2e physical dimensions are in line with the representations given in Intel’s materials (compared to say, Samsung or SKHynix). Micron currently offerstwo versions of 16 GB HBM2E with ECChardware: one at 2.8 Gbps per pin (358 GB/s per stack) and one at 3.2 Gbps per pin (410 GB/s per stack). Overall we’re looking at a peak bandwidth then between 1.432 TB/s to 1.640 TB/s depending on which version Intel is using. Versions with HBM will use an additional four tiles, to connect each HBM stack to one of SPR’s chiplets.Based on this diagram from Intel, despite Intel stating that SPR+HBM will share a socket with traditional SPR, it’s clear that there will be versions that are not compatible. This may be an instance where the Aurora versions of SPR+HBM are tuned specifically for that machine.On the Ponte Vecchio (PVC) side of the equation, Intel has already disclosed that a single server inside Aurora will have six PVC accelerators per two SPR processors. Each of the accelerators will be connected in an all-to-all topology to each other using the new Xe-Link protocol built into each PVC – Xe-Link supports 8 in fully connected mode, so Aurora only needing six of those saves more power for the hardware. It’s not been disclosed how they are connected to the SPR processors – Intel has stated that there will be a unified memory architecture between CPU and GPU.The insight added today by Intel is that each Ponte Vecchio dual-stack implementation (the diagram Intel has shown repeatedly is two stacks side by side) will feature a total of 64 MB of L1 cache and 408 MB of L2 cache, backed by HBM2e.408 MB of L2 cache across two stacks means 204 MB per stack. If we compare that to other hardware:NVIDIA A100 has 40 MB of L2 cacheAMD’s Navi 21 has 128 MB of Infinity Cache (an effective L3)AMD’s CNDA2 MI250X in Frontier has 8 MB of L2 per ‘stack’, or 16 MB totalWhichever way you slice it, Intel is betting hard on having the right hierarchy of cache for PVC. Diagrams of PVC also show 4 HBM2e chips per half, which suggests that each PVC dual-stack design might have 128 GB of HBM2e. It is likely that none of them are ‘spare’ for yield purposes, as a chiplet based design allows Intel to build PVC using known good die from the beginning.On top of this, we also get an official number as to the scale of how many Ponte Vecchio GPUs and Sapphire Rapids (+HBM) processors we need for Aurora. Back in November 2019, when Aurora was only listed as a 1EF supercomputer, I crunched some rough numbers based on Intel saying Aurora was 200 racks and making educated guesses on the layout – I got to 5000 CPUs and 15000 GPUs, with each PVC needing around 66.6TF of performance. At the time, Intel was already showing off 40 TF of performance per card on early silicon. Intel’s official numbers for the Aurora 2EF machine are:18000+ CPUs and 54000+ GPUs is a lot of hardware. But dividing 2 Exaflops by 54000 PVC accelerators comes to only 37 TeraFlops per PVC as an upper bound, and that number is assuming zero performance is coming from the CPUs.To add into the mix: Intel CEO Pat Gelsinger only a couple of weeks ago said thatPVC was coming in at double the performanceoriginally expected, allowing Aurora to be a 2EF machine. Does that mean the original performance target for PVC was ~20 TF of FP64? Apropos of nothing, AMD’s recentMI250X announcement last weekshowcased a dual-GPU chip with 47.9 TF of FP64 vector performance, moving to 95.7 TF in FP64 matrix performance. The end result here might be that AMD’s MI250X is actually higher raw performance than PVC, however AMD requires 560 W for that card, whereas Intel’s power numbers have not been disclosed. We could do some napkin math here as well.Frontier uses 560 W MI250X cards, and is rated for 1.5 ExaFlops of FP64 Vector at 30 MW of power. This means Frontier needs 31300 cards (1.5 EF / 49.7 TF) to meet performance targets, and for each 560 W MI250X card, Frontier has allocated 958 Watts of power (30 MW / 31300 cards). This is a 71% overhead for each card (which means cooling, storage systems, other compute/management etc).Aurora uses PVC at an unknown power, is rated for 2 ExaFlops of FP64 Vector at 60 MW of power. We know that PVC has 54000+ cards to meet performance targets, which means that the system has allocated 1053 W (that’s 60 MW / 54000) per card to include the PVC accelerator and other overheads required. If we were to assume (a big assumption I know) that Frontier and Aurora have similar overheads, then we’re looking at 615 W per PVC.This would end up with PVC at 615 W for 37 TF, against MI250X at 560 W for 47.9 TF.This raw discussion fails to discuss specific features each card has for its use case.Compute GPU Accelerator ComparisonConfirmed NumbersAnandTechIntelAMDNVIDIAProductPonte VecchioMI250XA100 80GBArchitectureXe-HPCCDNA2AmpereTransistors100 B58.2 B54.2 BTiles (inc HBM)47106 + 1 spareCompute Units1282 x 110108Matrix Cores1282 x 440432INT8 Tensor?383 TOPs624 TOPsFP16 Matrix?383 TOPs312 TOPsFP64 Vector?47.9 TFLOPS9.5 TFLOPSFP64 Matrix?95.7 TFLOPs19.5 TFLOPSL2 / L32 x 204 MB2 x 8 MB40 MBVRAM Capacity128 GB (?)128 GB80 GBVRAM Type8 x HBM2e8 x HBM2e5 x HBM2eVRAM Width?8192-bit5120-bitVRAM Bandwidth?3.2 TB/s2.0 TB/sChip-to-Chip Total BW88 x 100 GB/s12 x 50 GB/sCPU CoherencyYesWith IFWith NVLink 3ManufacturingIntel 7TSMC N7TSMC N5TSMC N6TSMC N7Form FactorsOAMOAM (560 W)SXM4 (400W*)PCIe (300W)Release Date202211/202111/2020*Some Custom deployments go up to 600WIntel also disclosed that it will be partnering with SiPearl to deploy PVC hardware in the European HPC efforts. SiPearl is currently building anArm-based CPU called Rheabuilt on TSMC N7.Moving forward, Intel also released a mini-roadmap. Nothing too surprising here - Intel has plans for designs beyond Ponte Vecchio, and that future Xeon Scalable processors will also have options enabled with HBM.Related ReadingIntel's Aurora Supercomputer Now Expected to Exceed 2 ExaFLOPS PerformanceIntel Teases Ponte Vecchio Xe-HPC Power On, Posts Photo of Server ChipAnalyzing Intel’s Discrete Xe-HPC Graphics Disclosure: Ponte Vecchio, Rambo Cache, and GelatoIntel’s 2021 Exascale Vision in Aurora: Two Sapphire Rapids CPUs with Six Ponte Vecchio GPUsIntel’s Xe for HPC: Ponte Vecchio with Chiplets, EMIB, and Foveros on 7nm, Coming 2021Bringing Geek Back: Q&A with Intel CEO Pat GelsingerIntel Architecture Day 2021: A Sneak Peek At The Xe-HPG GPU ArchitectureIntel to Launch Next-Gen Sapphire Rapids Xeon with High Bandwidth MemoryIntel’s Xeon & Xe Compute Accelerators to Power Aurora Exascale SupercomputerSiPearl Lets Rhea Design Leak: 72x Zeus Cores, 4x HBM2E, 4-6 DDR5AMD Announces Instinct MI200 Accelerator Family: Taking Servers to Exascale and Beyond\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17067/intel-sapphire-rapids-with-64-gb-of-hbm2e-ponte-vecchio-with-408-mb-l2-cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Cerebras Completes Series F Funding, Another $250M for $4B Valuation\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-11-10T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/17061/cerebras-completes-series-f-funding-another-250m-for-4b-valuation\n",
      "Content: Every once in a while, a startup comes along with something out of left field. In the AI hardware generation, Cerebras holds that title, with their Wafer Scale Engine. The second generation product, built on TSMC 7nm, is a full wafer packed to the brim with cores, memory, and performance. By using patented manufacturing and packaging techniques, aCerebras CS-2 features a single chip, bigger than your head, with 2.6 trillion transistors. The cost for a CS-2, with appropriate cooling, power, and connectivity, is ‘a few million’ we are told, and Cerebras has customers that include research, oil and gas, pharmaceuticals, and defense – all after the unique proposition that a wafer scale AI engine provides. Today’s news is that Cerebras is still in full startup mode, finishing a Series F funding round.The new Series F funding round nets the company another $250m in capital, bringing the total raised through venture capital up to $720 million. In speaking to Cerebras ahead of this announcement, we were told that this $250 million was for effectively 6% of the company, bringing the valuation of Cerebras to $4 billion. Compared to Cerebras’ last Series E funding round in 2019, where the company was valued at $2.4 billion, we’re looking at about $800m extra value year on year. This round of funding was led by Alpha Wave Ventures, a partnership between Falcon Edge and Chimera, who are joining Cerebras’ other investors such as Altimeter, Benchmark, Coatue, Eclipse, Moore, and VY.Cerebras explained to me that it’s best to get a funding round out of the way before you actually need it: we were told that they already had the next 2-3 years funded and planned, and this additional funding round provides some more on top of that, allowing the company to also grow as required. This encompasses not only the next generations of wafer scale (apparently a 5nm tape-out is around $20m), but also the new memory scale-out systemsCerebras announced earlier this year. Currently Cerebras has around 400 employees across four sites (Sunnyvale, Toronto, Tokyo, San Diego), and is looking to expand to 600 by the end of 2022, focusing a lot on engineers and full stack development.Cerebras Wafer ScaleAnandTechWafer ScaleEngine Gen1Wafer ScaleEngine Gen2IncreaseAI Cores400,000850,0002.13xManufacturingTSMC 16nmTSMC 7nm-Launch DateAugust 2019Q3 2021-Die Size46225 mm246225 mm2-Transistors1200 billion2600 billion2.17x(Density)25.96 mTr/mm256.246 mTr/mm22.17xOn-board SRAM18 GB40 GB2.22xMemory Bandwidth9 PB/s20 PB/s2.22xFabric Bandwidth100 Pb/s220 Pb/s2.22xCost$2 million+arm+leg‽To date Cerebras’ customers have been, in the company’s own words, from markets that have traditionally understood HPC and are looking into the boundary between HPC and AI. This means traditional supercomputer sites, such as Argonne, Lawrence Livermore, andPSC, but also commercial enterprises that have traditionally relied on heavy compute such as pharmaceuticals (AstraZeneca, GSK), medical, and oil and gas. Part of Cerebras roadmap is to expand beyond those ‘traditional’ HPC customers and introduce the technology in other areas, such as the cloud –Cirrascale recently announced a cloud offeringbased on the latest CS-2.Coming up soon is the annual Supercomputing conference, where more customers and deployments are likely to be announced.Related ReadingCerebras In The Cloud: Get Your Wafer Scale in an InstanceCerebras Unveils Wafer Scale Engine Two (WSE2): 2.6 Trillion Transistors, 100% YieldCerebras Wafer Scale Engine News: DoE Supercomputer Gets 400,000 AI Cores342 Transistors for Every Person In the World: Cerebras 2nd Gen Wafer Scale Engine TeasedCerebras’ Wafer Scale Engine Scores a Sale: $5m Buys Two for the Pittsburgh Supercomputing CenterHot Chips 2019 Live Blog: Cerebras' 1.2 Trillion Transistor Deep Learning ProcessorHot Chips 2020 Live Blog: Cerebras WSE ProgrammingHot Chips 2021 Live Blog: Machine Learning (Graphcore, Cerebras, SambaNova, Anton)\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17061/cerebras-completes-series-f-funding-another-250m-for-4b-valuation\n",
      "Title: The Intel 12th Gen Core i9-12900K Review: Hybrid Performance Brings Hybrid Complexity\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-11-04T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/17047/the-intel-12th-gen-core-i912900k-review-hybrid-performance-brings-hybrid-complexity\n",
      "Content: Today marks the official retail availability of Intel’s 12thGeneration Core processors, starting with the overclockable versions this side of the New Year, and the rest in 2022. These new processors are the first widescale launch of a hybrid processor design for mainstream Windows-based desktops using the underlying x86 architecture: Intel has created two types of core, a performance core and an efficiency core, to work together and provide the best of performance and low power in a singular package. This hybrid design and new platform however has a number of rocks in the river to navigate: adapting Windows 10, Windows 11, and all sorts of software to work properly, but also introduction of DDR5 at a time when DDR5 is still not widely available. There are so many potential pitfalls for this product, and we’re testing the flagship Core i9-12900K in a few key areas to see how it tackles them.Let’s Talk ProcessorsSince August, Intel has been talking about the design of its 12thGeneration Core processor family, also known as Alder Lake. We’ve already detailed over 16000 words on the topic, covering the fundamentals of each new core, how Intel has worked with Microsoft to improve Windows performance with the new design, as features like DDR5, chipsets, and overclocking. We’ll briefly cover the highlights here, but these two articles are worth the read for those that want to know.Intel Architecture Day 2021: Alder Lake, Golden Cove, and Gracemont DetailedIntel 12th Gen Core Alder Lake for Desktops: CPUs, Chipsets, Power, DDR5, OChAt the heart of Intel’s processors is a hybrid, or heterogeneous, core design. The desktop processor silicon will have eight performance cores (P-cores) and eight efficiency cores (E-cores), the latter in two groups of four. Each of the cores is designed differently to optimize for their targets, but supports the same software. The goal is that software that is not urgent runs on efficiency cores, but time-sensitive software runs on performance cores, and that has required a new management control between the processor and Windows has been developed to enable Alder Lake to work at its best. That control is fully enabled in Windows 11, and Windows 10 can get most of the way there but doesn’t have all the bells and whistles for finer details – Linux support is in development.The use of this hybrid design makes some traditional performance measurements difficult to compare. Intel states that individually the performance cores are +19% over 11thGeneration, and the efficiency cores are around 10thGeneration performance levels at much lower power. At peak performance Intel has showcased in slides that four E-cores will outperform two 6thGeneration cores in both performance and power, with the E-core being optimized also for performance per physical unit of silicon. Alternatively, Intel can use all P-cores and all E-cores on a singular task, up to 241W for the Core i9 processor.On top of all this, Intel is bringing new technology into the mix with 12thGen Core. These processors will have PCIe 5.0 support, but also DDR5-4800 and DDR4-3200 support on the memory. This means that Alder Lake motherboards, using the new LGA1700 socket and Z690 chipsets, will be either DDR4 or DDR5 compatible. No motherboard will have slots for both (they’re not interchangeable), but as we are quite early in the DDR5 lifecycle, getting a DDR4 motherboard might be the only way for users to get hold of an Alder Lake system using their current memory. We test both DDR4 and DDR5 later on in the review to see if there is a performance difference.A small word on power (seethis articlefor more info) – rather than giving a simple ‘TDP’ value as in previous generations, which only specified the power at a base frequency, Intel is expanding to providing both a Base power and a Turbo power this time around. On top of that, Intel is also making these processors have ‘infinite Turbo time’, meaning that with the right cooling, users should expect these processors to run up to the Turbo power indefinitely during heavy workloads. Intel giving both numbers is a welcome change, although some users have criticized the decreasing turbo power for Core i7 and Core i5.As we reported last week, here are the processors shipping today:Intel 12th Gen Core, Alder LakeAnandTechCoresP+E/TE-CoreBaseE-CoreTurboP-CoreBaseP-CoreTurboIGPBaseWTurboWPrice$1kui9-12900K8+8/242400390032005200770125241$589i9-12900KF8+8/242400390032005200-125241$564i7-12700K8+4/202700380036005000770125190$409i7-12700KF8+4/202700380036005000-125190$384i5-12600K6+4/162800360037004900770125150$289i5-12600KF6+4/162800360037004900-125150$264Processors that have a K are overclockable, and those with an F do not have integrated graphics. The graphics on each of the non-F chips are Xe-LP graphics, the same as the previous generation.At the top of the stack is the Core i9-12900K, with eight P-cores and eight E-cores, running at a maximum 241 W. Moving down to i7 gives eight P-cores and four E-cores at 190 W, and the Core i5 gives six P-cores and four E-cores at 150 W. We understand that future processors may have six P-core and zero E-core designs.Compare at $550+AnandTechCoresP+E/TP-CoreBaseP-CoreTurboIGPBaseWTurboWPriceR9 5950X16/3234004900-105142$799i9-12900K8+8/2432005200770125241$589*R9 5900X12/2437004800-105142$549* AMD Quotes RRP, Intel quotes 'tray' as 1000-unit sales. Retail is ~$650The Core i9-12900K, the focus of this review today, is listed at a tray price of $589. Intel always lists tray pricing, which means ‘price if you buy 1000 units as an OEM’. The retail packaging is often another +5-10% or so, which means actual retail pricing will be nearer $650, plus tax. At that pricing it really sits between two competitive processors: the 16-core Ryzen 9 5950X ($749) and the 12-core Ryzen 9 5900X ($549).Let’s Talk Operating SystemsSuffice to say, from the perspective of a hardware reviewer, this launch is a difficult one to cover. Normally with a new processor we would run A vs B, and that’s most of the data we need aside from some specific edge cases. For this launch, there are other factors to consider:P-core vs E-coreDDR5 vs DDR4Windows 11 vs Windows 10Every new degree of freedom to test is arguably a doubling of testing, so in this case 23means 8x more testing than a normal review. Fun times. But the point to drill down to here is the last one.Windows 11 is really new. So new in fact that performance issues on various platforms are still being fixed: recently a patch was put out to correct an issue with AMD L3 cache sizes, for example. Even when Intel presented data against AMD last week, it had to admit that they didn’t have the patch yet. Other reviewershave showcaseda number of performance consistency issues with the OS when simply changing CPUs in the same system. The interplay of a new operating system that may improve performance, combined with a new heterogeneous core design, combined with new memory, and limited testing time (guess who’s CPUs were held in customs for a week), means that for the next few weeks, or months, we’re going to be seeing new performance numbers and comparisons crop up.From Intel’s perspective, Windows 11 brings the full use of its Thread Director technology online. Normally the easiest way to run software on a CPU is to assume all the cores are the same - the advent of hyperthreading, favoured core, and other similar features meant that add-ins were applied to the operating system to help it work as intended at the hardware level. Hybrid designs add much more complexity, and so Intel built a new technology called Thread Director to handle it. At the base level, TD understands the CPU in terms of performance per core but also efficiency per core, and it can tell P-core from E-core from favoured P-core from a hyperthread. It gathers all this information, and tells the operating system what it knows – which threads need performance, what threads it thinks needs efficiency, and what are the best candidates to move up or down that stack. The operating system is still king, and can choose to ignore what TD suggests, but Windows 11 can take all that data and make decisions depending on what the user is currently focused on, the priority level of those tasks, and additional software hooks from developers regarding priority and latency.The idea is that with Windows 11, it all works. With Windows 10, it almost all works. The main difference Intel told us is although Windows 10 can separate cores apart, and hyperthreads, it doesn’t really understand efficiency that well. So its decisions are made more in regards to performance requirements, rather than performance vs efficiency. At the end of the day, all this should mean to the user is that Windows 10 tries to minimizes the run-to-run variation, but Windows 11 does it better. Ultimate best-case performance shouldn’t change in any serious way: a single thread on a P-core, or across several P-cores for example, should perform the same.Let’s Talk TestingThis review is going to focus on these specific comparisons:Core i9-12900K on DDR5 vs the CompetitionCore i9-12900K on DDR5 vs Core i9-12900K on DDR4Power and Performance of the P-Core vs E-CoreCore i9-12900K Windows 11 vs Windows 10Normally when a new version of Windows is launched, I stay as far away from it as possible. On a personal level, I enjoy consistency and stability in my workflow, but also when it comes to reviewing hardware – being able to be confident in having a consistent platform is the only true way to draw meaningful conclusions over a sustained period. Nonetheless, when a new operating system is launched, there is always the call to bulk wholesale move testing to a new platform. Windows 11 is Windows 10 with a new dress and some details moved around and improved, so it should be easier than most, however I’m still going to wait until the bulk of those initial early adopter issues, especially those that might affect performance are solved, before performing a flat refresh of our testing ecosystem. Expect that to come in Q2 next year, where we will also be updating to NVMe testing, and soliciting updates for benchmarks and new tests to explore.For our testing, we’re leveraging the following platforms:Alder Lake Test SystemsAnandTechDDR5DDR4CPUCore i9-12900K8+8 Cores, 24 Threads125W Base, 241W TurboMotherboardMSI Z690 UnifyMSI Z690 Carbon Wi-FiMemorySK Hynix2x32 GBDDR5-4800 CL40ADATA2x32 GBDDR4-3200 CL22CoolingMSI Coreliquid360mm AIOCorsair H150i Elite360mm AIOStorageCrucial MX500 2TBPower SupplyCorsair AX860iGPUsSapphire RX460 2GB (Non-Gaming Tests)NVIDIA RTX 2080 Ti (Gaming Tests), Driver 496.49Operating SystemsWindows 10 21H1Windows 11 Up to DateUbuntu 21.10 (for SPEC Power)All other chips for comparison were ran as tests listed in our benchmark database,Bench, on Windows 10.Highlights of this reviewThe new P-core is faster than a Zen 3 core, and uses 55-65 W in STThe new E-core is faster than Skylake, and uses 11-15 W in STMaximum all-core power recorded was 272 W, but usually below 241 W (even in AVX-512)Despite Intel saying otherwise, Alder Lake does have AVX-512 support (if you want it)!Overall Performance of i9-12900K is well above i9-11900KPerformance against AMD overall is a mixed bag: win on ST, MT variesPerformance per Watt of the P-cores still lags Zen3There are some fundamental Windows 10 issues (that can be solved)Don’t trust thermal software just yet, it says 100C but it’s notLinux idle power is lower than Windows idle powerDDR5 gains shine through in specific MT tests, otherwise neutral to DDR4\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17047/the-intel-12th-gen-core-i912900k-review-hybrid-performance-brings-hybrid-complexity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Google's Tensor inside of Pixel 6, Pixel 6 Pro: A Look into Performance & Efficiency\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-11-02T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/17032/tensor-soc-performance-efficiency\n",
      "Content: It’s been about two weeks since Google officially announced their newest flagship devicesin the form of the Pixel 6, and Pixel 6 Pro. The two new Pixel phones are inarguably Google’s largest shift ever since the Pixel series was introduced, showcasing major changes in essentially every aspect of the devices, sharing very little in common with their predecessors besides the Pixel name. Featuring brand new displays, camera systems, body designs, and internal hardware at seemingly extremely competitive pricing, the phones seem to be off to an extremely good start and competitive positioning Google hasn’t had in a long time.One of the biggest changes, and most interesting to our readers, is the fact that the Pixel 6 and Pixel 6 Pro come powered on by Google’s own “Tensor” SoC. And it’s here where there’s quite a bit of confusion as to what exactly the Tensor is. Google explains that the Tensor is Google’s start in a journey towards the quest of enabling new kinds of workloads, which in the company’s words, were simply not possible or achievable with “standard” merchant silicon solutions. Taking advantage of Google research’s years of machine learning experience, it’s a chip that’s heavily focused towards ML as its primary differentiating feature, and what is said to allow the Pixel 6 phones to have many of the new unique feature exclusive to them.Today, we’re giving the Tensor SoC a closer look. This includes trying to document what exactly it’s composed of, showcasing the differences or similarities between other SoCs in the market, and better understanding what kind of IPs Google has integrated into the chip to make it unique and warrant calling it a Google SoC.The Chip ProvenanceOfficially, per Google’s own materials, the Tensor is a Google SoC fully designed by the company. And while the overall truth of this will vary based on your definition of “design”, the chip follows a seemingly close cooperation between Google and Samsung LSI, in the process blurring the lines between a traditional custom design and semi-custom design-for-hire chips such AMD’s console APUs.Starting off at the very highest level, we have the actual name of the SoC. “Google Tensor” is quite abstract in that, for the time being, the chip doesn’t have any particular model number attached to it in terms of official marketing. So whether the next-gen will be marketed “Tensor 2” or something else will remain to be seen. Internally, Google calls the chip the “GS101”, and while I’m not entirely sure here what GS stands for, it’s likely Google SoC or Google Silicon. For quite some time now we’ve also heard the “Whitechapel” being reported, although I’ve seen no evidence that this was a reference to the actual chip but in the very early stages.On the silicon side, the chip has another model number, with the SoC’s fused chip identification following Samsung’s Exynos naming scheme. Here we find the chiphas an ID of “0x09845000”, which corresponds to what would be S5E9845 (Edit:It's actually S5P9845). The latest Samsung LSI SoC, for reference, is the Exynos 2100, which is identified as the S5E9840.Of course, why would the Google SoC follow an Exynos internal naming scheme? That’s where we can begin to see some of the provenance of the design. It’s been widely reported for some time that a few years back, Samsung opened up itself to semi-custom silicon design offers. A piece fromAugust 2020 from ETNewsseems to correctly describe Samsung’s business plan and how it pertains to the Google chip (as well as describing a Cisco design win):“Samsung Electronics is set to manufacture semiconductor chips for Cisco Systems, which is the top network equipment maker in the world, and Google and it is responsible for the entire semiconductor manufacturing process from design to production.[…]Samsung Electronics is currently working on a development phase that involves chip design.[…]Samsung Electronics also obtained an order from Google regarding manufacturing of more than one chip. It is heard that Google requested a semiconductor that will go into a sensor that can measure body movements rather than for processors that go into current IT devices and an unprecedented application processor (AP).[…]Samsung Electronics is carrying out a different approach as it looks to actively utilize its technologies in chip design. Its strategy is to provide “customized” technologies and features that its customer needs even from a design stage and secure consignment production as well.What’s important here is the latter description of the process – where rather than simply acting as a pure-play contract manufacturer, Samsung is acting as a fully engaged party in the design of the silicon. This could very much be compared to an ASIC design service, with the exception being that Samsung is also a merchant SoC vendor as well as a manufacturer for the silicon, something that’s quite unique in the industry, and thus something of a special situation.Having the chip in our hands now, as well as having the open-source insight into the characteristics of it, we can start breaking down what exactly the Google Tensor is:Google Tensor and Samsung Exynos 2100: Similar But DifferentSoCGoogle TensorExynos 2100CPU2xCortex-X1@ 2.80GHz 2x1024KB pL22xCortex-A76@ 2.25GHz 2x256KB pL24x Cortex-A55@ 1.80GHz 4x128KB pL24MB sL31xCortex-X1@ 2.91GHz 1x512KB pL23xCortex-A78@ 2.81GHz 3x512KB pL24x Cortex-A55@ 2.20GHz 4x64KB pL24MB sL3GPUMali G78MP20@848 MHz (shaders)996 MHz (tiler / L2)Mali G78MP14 @854 MHzMemoryController4x 16-bit CH@ 3200MHz LPDDR5 / 51.2GB/s8MB System CacheISPHybrid Exynos + Google ISPFull Exynos ISP Blocks+ DSPNPUGoogle edgeTPU@1066MHzExynosNPU@ 1352MHzMediaSamsung Multi-Function Codec8K30 & 4K120 encode &8K60 decodeH.265/HEVC, H.264, VP9AV1 DecodeGoogle \"BigOcean\"4K60 AV1 DecodeModemExynos Modem 5123External(LTE Category 24/18)(5G NR Sub-6)(5G NR mmWave)Exynos Modem 5123Integrated(LTE Category 24/18)(5G NR Sub-6)(5G NR mmWave)Mfc. ProcessSamsung5nm (5LPE)Same Blood TypeIn the very fundamentals of what an SoC is, the Google Tensor closely follows Samsung’s Exynos SoC series. Beyond the usual high-level blocks that people tend to talk about in an SoC, such as CPUs, GPUs, NPUs, and other main characteristics, there’s the foundational blocks of a chip: these are the fabric blocks and IP, the clock management architecture, power management architecture, and the design methodology of the implementing those pieces into actual silicon. While on paper, a Samsung Exynos, a MediaTek Dimensity or a HiSilicon Kirin, or even a Qualcomm Snapdragon (on the CPU side) might have similar designs in terms of specifications – with the same high-level IP such as Cortex CPU or Mali GPUs from Arm – the chips will still end up behaving and performing differently because of the underlying SoC architecture is very different.In the case of the Tensor, this “chassis” builds upon the IP Samsung uses on their Exynos SoCs, utilizing the same clock management and power management architecture. Going further up in the IP hierarchy we find additional similarities among high-level IP blocks, such as memory controllers, fabric IP, PHY IP for all kinds of externally facing interfaces, and even the larger IP functional blocks such as ISP or media decoders/encoders. The fun thing is that these things arenow publicly scrutinizeable, and can be compared 1:1 to other Exynos SoCs in terms of their structures.This leads us to Google’s claim of the Tensor being their own design – which is true to an extent, but how true that is can vary based on your definition of “design” and how in-depth you want to go with that. Although the Tensor/GS101 builds upon Exynos foundational blocks and IPs – and likely was even integrated and taped-out by Samsung – thedefinitionof the SoC is in Google’s control, as it is their end-product. While things are very similar to an Exynos 2100 when it comes to Tensor’s foundation and lowest level blocks, when it comes to the fabric and internal interconnects Google’s design is built differently. This means that the spiderweb of how the various IP blocks interact with each other is different from Samsung’s own SoC.A practical example of this is how the CPU cores are integrated into the SoC. While on the Exynos 2100 the CPU cluster seemingly lies very clearly in a smaller, more defined Samsung Coherent Interconnect, the Tensor SoC integrates the CPU clusters in a larger CCI that appears to either be a very different configuration of the interconnect setup, or is a different IP altogether. Meanwhile there are still some similarities, such as having one predominant memory traffic bus connected to the memory controllers and one other lower-traffic “internal” bus for other IPs, which is how Exynos SoCs tend to separate things. It should be possible to reverse-engineer and map out the SoC in more detail, however that’s a time-consuming matter out of the scope of this piece.The CPU Setup - 2x X1 + 2x A76 + 4x A55While we could go on and on talking about SoC architecture, let’s curtail that for now and jump into the more visible and practical differences of the Tenor SoC, starting off with the CPU cluster.Google’s CPU setup is quite unusual from other SoCs in that it features a 2+2+4 configuration. While this isn’ttrulyexceptional – Samsung had this very same setup for the Exynos 9820 and Exynos 990 – the X1+A76+A55 configuration on the Tensor is currently unique in the market. Most other vendors and implementations out there have shifted over to a 1+3+4 big+mid+little CPU configurations.On the Cortex-X1 side, Google’s use of a pair of cores means that, in theory, the performance of the chip with two heavy threads should be higher than any other Android SoC which only have a single big large performance core. The frequencies of the X1 pair come in at 2.8GHz, slightly lower than the 2.86GHz of the Snapdragon 888 and 2.91GHz of the Exynos 2100 X1 cores. Google equipped the cores with the full 1MB of L2 cache, similar to the S888 and double that of the E2100 configuration.As for the middle cores, Google has employed Cortex-A76 cores, which has been a hot topic for discussion. At first glance, it’s seemingly a bit irrational considering both the Cortex-A77 and A78 offer higher performance and higher energy efficiency. The cores are clocked at 2.25GHz and come with 256KB of L2. We haven’t received a clear explanation from Google as to why they used the A76, but I do think it’s likely that at the time of design of the chip, Samsung didn’t have newer IP ready for integration. The chip has been brewing for some time and while it does feature X1 cores, maybe it was too late in the process to also shift over to newer middle cores. I do not think there was a purposeful choice of using A76 cores instead of A78, since as we’ll see in our performance benchmarks that the older design underperforms.On the little cores, there are 4x A55 cores at 1.8GHz. In contrast to Samsung’s own Exynos chips, Google has decided to equip the cores with 128KB of L2 caches rather than just 64KB, so they’re more in line with the Snapdragon 888 configuration. One odder choice from Google is that the L3 cache of the cluster is on the same clock plane as the A55 cores, which has latency and power implications. It’s also at odds with the dedicated L3 clock plane we see on the Exynos 2100.Another Fat Mali GPU: G78MP20 At High ClocksEarlier rumors about the SoC indicated that it would come with a Mali-G78 generation GPU, however we didn’t know the exact core count or clocks of the design. Google has since confirmed the MP20 configuration, which is the second-largest Mali GPU configuration, behind only the Kirin 9000 and its massive 24-core unit. I had initially theorized that Google was likely running the GPU at low frequencies to be able to optimize for energy efficiency, only to end up rather shocked to see that they’re still running the GPU at a peak clockspeed of 848MHz for the shader cores, and 996MHz for the tiler and L2. The Google Tensor, if I’m not mistaken, seems to be the first confirmed G78 implementation actually taking advantage of Arm’s split clock plane design of the G78, which allows the shared GPU fabric to run at a higher frequency than the actual shader cores – and hence why it has two frequencies.The actual frequencies are extremely high. The Exynos 2100’s G78MP14 already ran at 854MHz, and it was a chip which we deemed to have very high peak power figures; but here Google is adding 42% more cores and is not backing down on frequency. So that’s very eye-brow raising and concerning in terms of peak GPU power, concerns which we’ll see materialize in the latter GPU evaluation section.LPDDR5, 8MB SLC CacheThe memory controllers on the Google Tensor appear to be the same as on the Exynos 2100, supporting LPDDR5 in a 4x 16-bit channel configuration for a total peak theoretical bandwidth of 51.2GB/s.Google also integrated 8MB of system cache, and for me it isn’t exactly clear if this is the same IP Samsung uses on the Exynos 2100. Seemingly they’re both 8MB, but I’m leaning towards saying that it’s a different IP, or at the very least a different version of the IP, as there are some real differences in the way it’s architected and how it behaves.Google here makes very extensive usage of the SLC for improving the performance of the SoC blocks, including their own custom blocks. The SLC allows itself to be partitioned and to dedicate SRAM regions to particular IP blocks on the SoC, giving them exclusive access to all or parts of the cache in varying use-case situations.A Custom Hybrid ISP PipelineUsually when people or companies talk about SoC ISPs, these are always depicted as being a single monolithic IP block. In reality what we call an “ISP” is a combination of different specialized IP blocks, each handling different tasks in what we call the imaging pipeline. The Google Tensor here is interesting in that it takes bits and pieces of what Samsung uses on their Exynos chips, and also integrates custom Google-developed blocks into the pipeline – something Google actually talked about in their presentation of the SoC.The imaging system uses IP blocks that correspond to an Exynos imaging pipeline, such as pixel phase detection processing units, contrast autofocus processing units, image scalers, distortion correction processing blocks and view-dependent occlusion texture function processing blocks. What’s lacking here is that some other processing blocks are missing, which I imagine are related to more post-processing computation blocks that Samsung uses.The Google developed IP blocks in the ISP chain seem to be their own 3AA IP (Auto-Exposure,Auto-White Balance,Auto-Focus), as well as a custom pair of temporal noise-reduction IP blocks that are able to align and merge images. These are likely the custom blocks that Google was talking about when saying that they’ve developed blocks which help accelerate the kind of image processing that they employ as part of the Pixel lineup’s computational photography, and inarguably represent very important parts of the image processing pipeline.Google's edgeTPU - What Makes the Tensor a TensorBy now, it’s been quite clear that the big central talking point of the Google Tensor has been its TPU – or its Tensor Processing Unit. The TPU is, as its name implies, a custom Google developed-IP block that the company has been working on for a few years now. Until now, Google just called it the TPU inside the Tensor SoC, but at the driver level the company calls the block their “edgeTPU”. This is quite interesting as signals that the block is related to theASIC “Edge TPU”that Google had announced back in 2018. The discrete chip had been advertised at 4 TOPs of processing power in 2 Watts of power, and while Google doesn’t advertise any performance metrics on the TPU inside the Tensor, there are entries showcasing the block goes up to 5W of power. So if the two are indeed related, then given the significant process node advantages and overall much newer IP, the performance figures of the Tensor TPU (sic) should be extremely significant.The block is very much the pride of Google’s silicon team, telling us that it’s using the latest architecture for ML processing that’s been optimized for the way Google’s R&D teams run machine learning within the company, and promises to allow for opening up the kind of new and unique use-cases that were the main goal for making a custom SoC in the first place. We’ll go into the product-side use-cases in a more Pixel focused review later on, but the performance metrics of the TPU do appear to be impressive.The TPU block also seems to come with some sort of block that Google calls “GSA”. This is just speculation on my part here based on the drivers, but this seems to be some sort of control block that is in charge of operating the TPU firmware, and I think contains a quad-core Cortex-A32 CPU setup.Media Encoders, Other StuffOn the media encoder side, the Tensor SoC uses both Samsung’s own Multi-Function Codec IP block (which is identical to what’s used on the Exynos series) as well as what appears to be a Google IP block that is dedicated to AV1 decoding. Now this is a bit weird, as Samsung does advertise the Exynos 2100 as having AV1 decode abilities, and that functionality does seem to be there in the kernel drivers. However on the Galaxy S21 series this functionality was never implemented on the Android framework level. I have no good explanation here as to why – maybe the IP isn’t working correctly with AV1.The Google IP block, which the company calls “BigOcean”, is a dedicated AV1 decoder, and this does actually expose AV1 decoding ability to the Android framework. The very weird thing here is that all it does is AV1 – every other encoding and decoding of other formats is left over to the Samsung MFC. It’s an interesting situation and I’m left to wonder where things evolve in the next-gen SoC.Other differences for the Tensor SoC are for example the audio subsystem. Samsung’s SoC low-power audio decoding subsystem is thrown out in favor of Google’s own block design, I didn’t dwell too much into it but generally both blocks have the same task of allowing low-power audio playback without needing to wake up large parts of the SoC. I think this block (or the GSA) is also responsible as the always-on context-hub for sensor data aggregation, with the Tensor here using Google’s IP and way of doing things versus the Exynos variant of the same block.Google also employs a fixed function hardware memory compressor in the form of a block called Emerald Hill, which provides LZ77 compression acceleration for memory pages, and can in turn be used to accelerate ZRAM offloading in swap. I’m not sure if the Pixels are currently running this out of the box, but should be able to be confirmed by seeing “lz77eh” in/sys/block/zram0/comp_algorithm, if somebody is able to read that out. As an anecdote, as far back as 5 years ago Samsung integrated similar hardware compression IP blocks into their SoCs for the very same task, but for some reason those were never enabled for shipping devices. Maybe the energy efficiency didn’t pan out as they thought it would.External Exynos Modem - First non-Qualcomm mmWave Phones?Since it’s a phone SoC, naturally the Tensor needs some sort of cellular connectivity. This is another area where Google is relying on Samsung, using the company’s Exynos Modem 5123. But, unlike the Exynos 2100 and its integrated modem, the Tensor uses a discrete external variant. As to why it’s discrete, it’s likely that with the massive GPU, larger CPU setup (two X1’s with full 1MB L2’s), and unknown size of the TPU, that the Tensor chip is quite large even in relation to the Exynos 2100.Source:PBKreviewsAnother theory on my side is that Google would somehow still be tied to Qualcomm for US networks – either for CDMA or mmWave 5G connectivity. Surprisingly, it seems this isn’t the case, as the Pixel 6 series ships with the Exynos modem across the globe. That makes the Pixel 6 family particularly interesting, as it seems that this is the first non-Qualcomm mmWave implementation out there. For reference, Samsunghad talked abouttheir mmWave RFICs and antenna modules back in 2019, saying there were plans for 2020 devices. Whether that meant designs starting in 2020 (which the Pixel 6 series would be) or commercial availability wasn’t clear at the time, but it seems that these are the first commercial phones with the solution. I don’t expect to have mmWave coverage here for myself for another few years, but third-party reportsshowcase the phone reaching up to 3200Mbpswhileother field-tests showing around halfof the practical speeds of Qualcomm devices. I hope more people in the next weeks and months will have the opportunity to dive deeper into the modem’s performance characteristics.Semi-Custom Seems AptOverall, the Google Tensor ends up being almost exactly what we expected the chip to be, from the earliest reports of a collaboration between Google and Samsung. Is it a Google chip? Yes, they designed it in the sense that they defined it, while also creating quite a few Google-unique blocks that are integral to the chip's differentiation. Is it a Samsung Exynos chip? Also yes, from a more foundational SoC architecture level, the Tensor has a great deal in common with Samsung’s Exynos designs. In several areas of the Tensor there are architectural and behavioral elements that are unique to Samsung designs, and aren’t found anywhere else. To that end, calling the Google Tensor a semi-custom design seems perfectly apt for what it is. That being, said, let’s see how the Tensor behaves – and where it lands in terms of performance and efficiency.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17032/tensor-soc-performance-efficiency\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Bringing Geek Back: Q&A with Intel CEO Pat Gelsinger\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-10-29T15:30:00Z\n",
      "URL: https://www.anandtech.com/show/17042/bringing-geek-back-qa-with-intel-ceo-pat-gelsinger\n",
      "Content: One of the overriding key themes of Pat Gelsinger’s ten-month tenure at Intel has been the eponymous will to ‘bring geek back’ to the company, implying a return to Intel’s competitive past which relied on the expertise of its engineers to develop market-leading products. During this time, Pat has showcased Intel’s IDM 2.0 strategy, leveraging internal production, external production, and an update to Intel’s foundry offering, making it a cornerstone of Intel’s next decade of growth. The first major launch of this decade happened this week, at Intel’s Innovation event, with the announcement of 12thGen Core, as well as updates to Intel’s software strategy up and down the company.After the event, Intel invited several media and an analyst or two onto a group session with CEO Pat, along with CTO Greg Lavender, a recent new CTO hire coming from Pat’s old stomping ground at VMWare. In light of the announcements made at Intel Innovation, as well as the financial quarterly results released just the week prior, and the state of the semiconductor supply globally, everyone had Intel at the forefront of their minds, ready to ask for details on Intel’s plan. Each person got the chance to ask a single question in the 30-minute session, although with a couple of minutes at the end, I managed to ask a second.Pat GelsingerCEOGreg LavenderCTOThe following is a tidied-up transcript.Dan Hutchinson, VLSI Research: You've been talking a lot about IDM 2.0 – I like the concept. But what is it that makes it different from ‘IDM 1.0’ that elevates it above buzzword status? [I preface this] In the sense of the belief I've come to that ‘IDM fabless’/fab-light concepts are really just distinctions with very little difference.Pat Gelsinger:For us, when we talk about IDM 2.0, we talk about three legs. One is a recommitment to IDM 1.0, and Intel’s design and manufacturing at scale. Second is the broad leveraging of the foundry ecosystem. Third, and most importantly, is becoming a foundry, and swinging the doors of Intel Fabs, our packaging, and our technology portfolio - being wide-open to engage with the industry to design on our platforms with our IP. It also involves opening up the x86 architecture, as well as all of the other IP blocks, graphics, IO, memory, and others. By adding second element, but most importantly the third element of that, to me is what makes IDM 2.0.The other thing I'd add to that, Dan, is that I use this language inside and outside the company: IDM makes IFS better, and IFS makes IDM better. IFS meaning our Intel Foundry Services. In a quick example of both, for IDM, my foundry customers get to leverage all of the R&D and IP that I'm creating through my internal design. Normally foundries have to go create all of that, but Intel can leverage the 10s of billions of capital, many billions of R&D, and the most advanced components research on the planet. It’s essentially for free to my foundry customers. It’s an extraordinary asset. That includes making x86 and other design blocks available for foundry customers. At the same time, Foundry is driving us to do a better job in IDM. We're engaging with a third-party IP ecosystem more aggressively, and with the EDA tool vendors more aggressively. For instance, our engagement with Qualcomm - they're driving us to do a more aggressive optimization for power and performance, more than our more performance-centric product lines would be. So they're making IDM better by the engagement with IFS customers. Standardized PDKs, and other things. So, IDM and IFS, if this really gets rolling as I'm envisioning it to be, they're powerfully going to be reinforcing each other.Timothy Prickett Morgan, The Next Platform: I'm trying to understand the new Aurora system: the original machine was supposed to be north of [an exaflop] and $500 million. Now it's two Exaflops, or in excess of two Exaflops, and you've got a $300 million write-off for federal systems coming in the fourth quarter. Is that a write-off of the original investment, or is Argonne getting the deal of the century on a two [exaflop] machine?PG:[Since] the original concept of Aurora, we've had some redefinitions of the timelines and the specifications associated with the project efforts. Obviously some of those earlier dates when we first started talking about the Aurora project we've moved out and changed the timelines for a variety of reasons to get there. Some of those changes lead to the write-off that we're announcing right now. The way the contract is structured, part of it is that the moment that we deliver a certain thing, we will incur some of these write-offs simply from the accounting rules associated with it. As we start delivering it, some of those will likely get reversed next year as we start ramping up the yields of the products. So some of it just ends up being how we account for and how the contracts were structured.On the two versus one Exaflop: largely it’s PVC, you know, Ponte Vecchio. The core of the machine is outperforming the original contractual milestones. So when we set it up to have a certain number of processors, and you can go do the math of what two [Exaflop] is, we essentially overbuilt the number of sockets required to comfortably exceed one Exaflop. Now that PVC is coming in well ahead of those performance objectives for some of the workloads that are in the contract, we're now comfortably over two Exaflop. That's pretty exciting at that point - that we will go from one to two pretty fast.But to me, the other thing that's really exciting in this space is our Zetta Initiative. What we've said is that we're going to be the first to Zettascale by a wide margin. We're laying out as part of the Zetta initiative what we have to do in the processor, in the fabric, in the interconnect and the memory architecture, what we have to do for accelerators, and the software architecture to do it. So Zettascale in 2027 – it’s a huge internal initiative that's going to bring many of our technologies together for a 1000x gain in five years. That's pretty phenomenal.Ian Cutress, AnandTech: Earlier in the year, as part of this IDM 2.0 initiative, one of the big announcements was a collaboration with IBM. IBM is obviously big in the field, with lots of manufacturing and research expertise. We were promised more color on that partnership by the end of the year - is there any chance you can provide that color today?PG:I don't have a whole lot more to say, but let me just characterize it a little bit. It's one of the things I think you will be hearing more from us before the year is out. But basically, we're partnering with IBM - think of them as another components research partner with us, in advanced semiconductor research, as well as in advanced packaging research. We're partnering with IBM in those areas. Also, IBM is looking for a partner as well on their manufacturing initiatives, and what they require for their products and product lines. We're aligning together on many of the proposals that you've seen, such as the Ramp-C proposal that we were just granted. Phase one of that involves IBM partnering with us in those areas, so it’s a fairly broad relationship. You'll be hearing more from us before the year-end I expect.Mark Hachman, PCWorld: I have a very simple, high-level manufacturing question. Your new statement is about keeping up, or superseding Moore's law, over the next decade. Is this something that you think is going to be unique to Intel, or do you think your competitors will also keep pace?PG:I described four things today that I think enable us to go ‘at Moore's Law’ or ‘Super Moore's Law’.I said EUV, and EUV’s obviously available to the industry, but we're going to be advantaged at High-NA (Intel has stated it will be first to High-NA EUV with ASML). It’s the second generation of EUV. We also talked about RibbonFETs, the fundamental new transistor architecture. As we look at where we are versus others, we think we're comfortably ahead of anybody else with our Gate-All-Around structure that we'll be putting into mass manufacturing in 2024. So we feel comfortably ahead, and I would also say that since the planar transistor design, through strained metal gate, though FinFET – Intel has always led in transistor architecture. So I think that we're advantaged meaningfully in RibbonFET, and with PowerVIA, nobody else has anything like it. So I think we're fundamentally highly differentiated with our backside power delivery architecture. Also the packaging technology - I think with Foveros, the EMIB, with Omni Foveros, these technologies are comfortably ahead of where others are in the industry.So if we bring that together, now with a multi-tile or multi-chip approach, with a 2.5D/3D packaging approach, we're going to be well above Moore's Law by the end of the decade. I think we're going to be comfortably ahead of anybody else in the industry.It's not that nobody else [in the industry] is going to be participating, but I expect as we look at those [technologies] coming together, we're just going to be adding advantage over these four domains as we look out over the rest of the decade. That's what we've said, and as we think about our IDM 2.0 strategy, getting back to parity, getting back to leadership, and then we will have sustained leadership. We are laying down the tracks for sustained leadership across the process technology. Moore's Law is alive and well, and as I said in the keynote, until the periodic table is exhausted, we ain't done.Linley Gwennap, The Linley Group: When you're talking about Moore's law, I'm wondering how you are defining Moore's Law these days - is it transistor density, is it transistors per dollar, is it power/performance, is it a particular cadence? Or is it just that Intel is still making progress at some rate?PG:I'm defining it as a doubling of transistors [per product]. Obviously we’re going from a single tile to multi-tile, but if you look at the transistor density of a nominal package size, we'll be able to double the number of transistors in that in a sustained way. Trust me - before my TD team let me make those statements today, they analyzed this from quite a few dimensions.Now, if you click inside of it, we do expect that for transistor density per unit silicon area, we are in a pretty good curve right now. [This includes] the move to RibbonFET, EUV solving lithography issues, and PowerVIAs solving some of the power delivery issues. I think the transistor density per unit area by itself doesn't give you a doubling of the number of transistors per two years, but it's going to be in a pretty good curve for the next 10 years - that is our expectation. When you then combine that with 2.5D/3D packaging, that's when you go ‘Super Moore's Law’ and you can start to break the tiles into smaller, more manufacturable, common components. With technologies like EMIB, we can tie them together with almost effective on-die interconnect characteristics. [It also enables] a better job at doing power management with PowerVIA, so you're not hitting your head on any of the power limits, even though power will be a big issue there. So it really is the combination of lithography breakthroughs, power breakthroughs, new transistor structures, but most importantly, the 2.5D/ 3D packaging technologies.Charlie Demerjian, SemiAccurate: One of the key elements in your messaging has been a commitment to being more open and more transparent with the industry, with hardware, software, and all the extra details. This is good, as Intel has pretty much stopped talking about the technology these past few years, sliding in the wrong direction in my view - things like transistor densities or die sizes on products that are launching the next day haven’t been shared. I’ve been requesting Intel to be more open, for years, only to not get any real satisfactory explanation. If you are committed to being more transparent, could you look into providing these numbers [and numbers like these] in the future?Edited for clarityPG:Thanks Charlie - we're anxious to see you in person as well, sometime soon! So as we're swinging the doors open wide, we are getting much more engaged with the technology, and with technologists [like yourself]. I think we're going to be doing a lot more in this regard.These particular comments, I'll say I haven't heard them before this. Feel free to send me an email with your top five that you'd like to see from us, and we'll have robust conversations. I won't promise anything today, because I don't know why people have been hesitant in those regards. But we do want to be engaged with technologists, with developers, and with our tech analyst community. Send me some thoughts directly, and I'll happily follow up on them.Paul Alcorn, Tom’s Hardware: Investing in fab capacity is one of the most capital-intensive exercises imaginable. Those are big multi-billion dollar bets, and they have to be placed years in advance. Can you tell us what type of measures Intel is taking to ensure that it doesn't overinvest or have excess capacity in the event of an industry downturn?PG:Great question. One thing we do is extraordinary amounts of market modeling and industry modeling for PC growth, server growth, network growth, graphics growth, etc. So a lot of that goes into our LRP, our five-year Long-Range Plan. We want to have better [insight] on where the market is going than anyone else - but then against that we're applying what we call Smart Capital.For instance, right now, I lust for more fab capacity. Intel has under-invested [in fab capacity] for a number of years. Intel always used to have a spare shell (basically a fab building without any manufacturing equipment). This is [going to be] one of the principles of our Smart Capital – we always have to have spare shells. If you take an Intel 20A Fab, it’s going to cost around $10 billion - but you can invest about $2b in the first two years [on the shell]. It's actually a fairly capital-efficient time, where you get a lot of time benefit - two years for only $2 billion, and then you can populate it with equipment in years 2-4 when the fab comes online. So part of our initiative is to build shells - get that greenfield capacity in place such that we can build shells and have more flexibility. [It also gives us] choice in the timing of the actual capital build.Secondly, also with our IDM 2.0 strategy, we said we're going to consistently use external foundries. With that, maybe a quarter of my capacity is from my external foundry partners. But I'm also going to have tiles that run on both internal manufacturing and on external foundries, so that I'm going to have flexibility as I move up or down that curve - to be able to flex things in or flex things out, as appropriate.Third, we've said we're going to capitalize some of these expanded capabilities based on government investments as well as customer investments. If a customer of our foundry business, wants 10k wafer starts per week in their capacity from us, that's a whole fab. So we're going to have prepays and other contractual relationships with them that offset our capital risk in a fair way. This is exactly what TSMC, Samsung, and Global Foundries are doing now. So we'll leverage that capacity. You've also heard about the CHIPS Act, and the equivalent in Europe. There is government willingness to invest in this area. That's going to help us moderate some of that capital risk.So those are the three tenets of Smart Capital.Underneath that, let's say that I [end up with] too much capacity. That’s something I can't even fathom for the next couple of years! But let's say that we're actually in a situation that I actually have a little bit too much capacity. What am I going to do? Well, I'm going to go win more market share. We've had declines in market share in some areas, so we would go apply that capital and gain more share back in PC, in servers, and in other places. We see that capacity-constrained market and we go and gain share, and we believe we're in a position as our products get better to do that in a very profitable way. Secondly, we'll go gain more foundry customers. It is an exciting new business, and right now all of the foundry industry is highly constrained on capacity - so we go win more foundry. Or third, I'll go flex more capacity from our use of external foundries to internal manufacturing as well.The strategy that we're setting up gives me extraordinary capitalizable efficiency, as well as capitalizable flexibility. Ultimately all three of these ones I just told you are highly margin generative for the company, that’s whether I'm gaining market share, whether I'm gaining more foundry customers, or moving from external to internal which has a much better margin structure. So overall, this is how we're laying out our smart capital strategy, and as that comes to scale over the next couple of years I think it just positions us in a phenomenal way.Marco Chiapetta, HotHardware: Based on what your competitors are doing in the client space, it seems that a tighter coupling with software and operating systems is necessary to extract the maximum performance and efficiency from future platforms. [Does that mean] Intel's engagement with Microsoft and other OS providers changing at all, or will the OS providers affect how processors are designed moving forward?PG:The case in point right now is what we just did with Intel Bridge technology (enabling Android on Windows). Panos Panay from Microsoft was here, and they took a major piece of technology and are making it now part of standard Windows that they're shipping. It's not the first time that Microsoft has done that with Intel, but it's been a long time since we've had that major functionality [partnership], and Microsoft and Intel are redefining our relationship, between Satya and I.We've termed it as the ‘Virtual Vertical’ - how do we bring the same benefits of a much more intimate dedicated partnership with each other to revitalize and energize the PC ecosystem. I think Bridge Technology is a great example, as it brings the entire Android application ecosystem into the PC environment. This is a great example and more such things are well underway.Greg Lavender:We have hundreds of resources in software engineering working very closely with Microsoft, every day, all day, and every night, with the core internals of Windows optimized for our platforms. We co-developed this HGS+ capability that we have talked about, which provides machine learning heuristics to the Windows scheduler with our E-core/P-core capabilities. We are looking, in real-time during the execution of Windows, at applications on Windows, and to assign those threads to the most optimal cores at the right time. Then even our oneAPI compiler - if you use it to compile capabilities that you run on Windows, you get a 10% performance boost over other compiler capabilities. In my team, we do a lot of work to help with the benchmarking to make sure that all the graphics performance meets all the performance requirements. We run all the Cinebench benchmarks as well, to tune things up to get the optimal power performance efficiencies out of the platforms!My team does all the BIOS, all the firmware. We're very close with all the vendors, the Linux community, and Microsoft, with Android and Chrome OS, to make sure that we have secure firmware as we digitally sign all of our firmware. Microsoft's a big adopter in Azure cloud with SGX, and they have many applications running securely in using the secure enclaves - our Trusted Execution environments in Azure Cloud. So we have a very tight partnership with them, it’s not just with Windows on the PC!As Pat mentioned the Bridge Technology, we call it Houdini, because we did a bunch of magic to basically allow those Android apps [to work]. As I mentioned in my talk, the Windows Subsystem for Linux version 2 (WSL2) lets you run a complete Linux distro. [With Bridge] we run an actual Android distro into that hidden virtual machine on Hyper-V. Then when you load those applications into Windows, they sit on the Windows File System, so you just launched them like you normally would, but they actually execute in that Android OS that's in WSL. We just make it completely transparent, there's no big performance hit, and we do lots of acceleration with the graphics to make a good user experience, including for Android games. This is the kind of very close partnership we have with Microsoft. We've had it for years.Any More Questions?Ian Cutress, AnandTech: You've spoken about offering x86 IP in foundry services – could you go into detail here? Will Intel will offer x86 IP in the form of ISA licenses, core licenses, and will that IP explicitly only to be manufactured at Intel, or other foundries? Also, will there be an appreciable lag between what x86 IP you offer customers, and what x86 IP you use internally?PG:We are going to make x86 cores available, as standard IP blocks, on Intel Foundry Services. So if you use an Intel process, we will have versions of the x86 cores available. I do say cores, because it's going to be based on the cores that we're building – the E-cores and P-cores for standard product lines, we will be making them available. So you could imagine an array of E-cores, or P-cores, as a Xeon-like product that's combined with unique IP from a cloud vendor. That'd be a great example of where we'd have such hybrid designs, it's not their design, it's not our design, but it's bringing those pieces together.We're working on the roadmap of cores right now. That’s to say which ones become available, [whether that’s] big cores, little cores, which ones come first, and the timing for those - there's also a fair amount of work to enable the x86 cores with the ecosystem. As you all know very well, the Arm ecosystem has developed interface standards, and other ways to composite the entire design fabric. So right now we're working on exactly that. A fair amount of the IP world is using Arm constructs versus x86 constructs right now - these are interrupt models, things like memory ordering models, and there's a variety of those differences that we're haggling through right now.We’re also working with customers - we have quite a lot of interest from customers for these cores. So we're working with them, picking some of the first customers, and we have a range of interest from embedded use-cases up to HPC use-cases, with data center and cloud customers somewhat in the middle of those. So it’s a whole range of different applications, but we're working through the details of that now. We clearly realize the criticality of making this work inside of our design partners and our customers, in the common EDA environments, and enabling the rest of their IP that they're bringing to the discussion.Then the lag side - there's got to be minimal lag. My IP teams internally, they act like a Synopsis IP team, because they're delivering for instance, an E-core or P-core to the Xeon design team, or to the Meteor Lake design team, [or others]. So as those cores become available, we're going to be making those available to our internal teams and our external teams somewhat coincidently going forward so there's not a lag. We're making the IP available, and as customers start getting firmed up, as their design ideas firm up, the cores will be more mature, to answer your question. But we do not expect that there's any meaningful, if any, lag.Many thanks to Pat and Greg for their time.Thanks also to Gavin Bonshor for transcription.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17042/bringing-geek-back-qa-with-intel-ceo-pat-gelsinger\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AnandTech Interviews Mike Clark, AMD’s Chief Architect of Zen\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-10-26T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/17031/anandtech-interviews-mike-clark-amds-chief-architect-of-zen\n",
      "Content: AMD is calling this time of the year as its ‘5 years of Zen’ time, indicating that back in 2016, it was starting to give the press the first taste of its new microarchitecture which, in hindsight, ultimately saved the company. How exactly Zen came to fruition has been slyly hidden from view all these years, with some of the key people popping up from time to time: Jim Keller, Mike Clark, and Suzanne Plummer hitting the headlines more often than most. But at the time AMD started to disclose details about the design, it was Mike Clark front and center in front of those slides. At the time I remember asking him for all the details, but as part of the 5 Year messaging, offered Mike for a formal interview on the topic.Michael T Clark is a Corporate Fellow at AMD, starting at the company in 1993, fresh out of his degree at Illinois Urbana-Champaign. His role has evolved from a base engineer in processor design, all the way up to Lead Architect on several of AMD’s key processor designs, all the way to Chief Architect of Zen. Exactly what Mike has done in the meantime is somewhat of a mystery, so I get to probe him on that as well! Currently Mike is in charge of Zen and its roadmap, both for the products in the market today to several generations away from now. Unfortunately Mike won’t disclose what’s in Zen 7 just yet, but it was worth asking.Mike ClarkZen Lead Architect, AMDDr. Ian CutressAnandTechIan Cutress: You’ve been at AMD since 1993, from leaving university, which is almost 30 years. It was funny to try and find some sort of documented work history for you through your time at AMD - aside from your Zen appearances there is little to go on! Could you give us an overview of some of those projects you’ve worked on and what you did on them on your way up to Zen?Mike Clark:So I started right out of school, right out of the University of Illinois, and started work on K5. That was our first ground-up [design] on x86, which was awesome. [When I finished university], I had several offers, but I chose AMD because it was the only one that actually let me own a block of the CPU design, which back then was crazy! Not only did you own the RTL, you owned the verification of your own block (and we learned that's a really bad idea). You owned the physical design, you had a physical designer you worked with, but you ran the synthesis tools yourself. So you took your block, and went fromsoup to nuts(beginning to end).So that's what I kind of cut my teeth on, to learn the discipline. I was also on the TLB, and nobody knew how an x86 TLB worked back then. Since we were just second sourcing Intel, I had to go lower and figure out and reverse engineer how the x86 TLB works - it was a ton of fun! I learned a lot.From there we ended upbuying NexGen, getting K6, and I helped integrate it in. Then we did K7, and I was like the lead microcode guy on K7. That was what I call the Dream Team - each block lead on K7 was just awesome. I learned so much from those guys and that's where I really learned how to build a great microarchitecture.From there, I did the Greyhound (K9) core, I was the lead architect there, which was a derivative of K8. Then we were doing the whole Bulldozer thing - I worked on that. I was a Lead Architect on the Steamroller version, but I worked on all of them in different roles. Then I became the Lead Architect of Zen.I'm in charge of the whole Zen roadmap now, but here at AMD a Lead Architect goes from high-level design, all the way to silicon, then to post silicon by engaging with customers. You really learn which of your decisions were good and which were bad. You feel the pain when you hear work is put on the software community [that didn’t need to be], and so you do better the next time. You really are with the design for a long time, and I really believe in the fact that you don't just work though pre-silicon, or even the execution phase, and just move on - you have to feel the pain of everything in your design, so you can be a better architect.So now I run the roadmap, and then we have a team of great people who are now lead architects on all the Zen architectures.IC: What would your official title be then?MC:Leader of the Core Architecture, or Leader of the Core Roadmap I would say. I don't really think about titles that much.Zen: The BeginningIC: This quarter for AMD is all about the 5 Years of Zen and Ryzen, ever since those press events and first microarchitecture disclosures at Hot Chips in August 2016. Realistically, when did the Zen journey start for you - who were the big names, and were you the lead architect off the bat?MC:Well, it started in 2012 for me. We realized we needed to do something different from the Bulldozer line. Jim came in and helped re-organize the team, and I was the Lead Architect. So it's been almost 10 years for me.For personnel, since we started in 2012, there are so many people, and the team is awesome. I am so thankful that I get to represent the work of so many awesome engineers. Suzanne Plummer was the lead of the Zen team, managing the team, and was just keeping the team together, she was just awesome. Then there’s alsoMike Tuuk,Tim Wilkens,Jay Fleischman,Leslie Barnes- all kinds of people that were contributing from all parts of the company to make Zen a success.So it's kind of funny to say I’ve been working on it since 2012 - if I go back, I still have our HLD (High-Level Design) deck that we did for Zen. You wouldn't believe how different, after taking five years to get something to production, it looks. I mean the bones are still there, you see it, but so many things changed along the way. That's one of the keys of this business - being able to be dynamic and have things change because it's such a long time. But also still be able to deliver a competitive design, it’s pretty amazing. Once in a while, when we were starting up, when the teams were worried or feeling weird about their HLD, I'm the one who turned around and said that ‘this is what Zen was, everything's not going to be perfect coming out HLD, stuff is going to change, and it's going to get better’. So that's the art of this job.IC: Is it ever realistic to be able to pivot a design based on what competitors just released? Or is do you still have that two year lead on changes?MC:It matters - we can. You'd be surprised at how quickly we can respond. It still feels like a long time, but we're constantly evaluating the competition and comparing ourselves to them, trying to make sure we're staying on track. One part of it is that we have to set our own goals as well. We can't wait for them, and that's when we've seen historically what has happened in the industry - we set those aggressive goals for ourselves and just try to hit them independent of what the competition is doing as well. Now we keep our eyes on them of course.IC: One of the cool stories out of the Ryzen saga is that CPU development funding was frozen and ring-fenced away from the rest of the business, at a time when AMD was struggling financially. How did that benefit you, or did any limitation manifest from your perspective, either practically or emotionally?MC:It definitely takes a big investment because of the long timeframe. With that long lead time, it's tough for the business - the market wants a product every year, and you keep trying to refresh, waiting for the new big thing to come. So it was definitely necessary so that we could do what we needed to do to get the job done.It was a tough time. I mean, one of the hardest problems we had was holding the team together. A lot of people did leave, and it was a very aggressive programme. From where we were, we spent a lot of time both trying to convince people that we would succeed. Even with succeeding, we still knew that if the competition keeps going on at their track, they may still be ahead of us when the first one comes out. That's what we needed to do to get a solid base out there, then bring out Zen 2 and Zen 3, and really we get ourselves on a trajectory where we can be a leader in the industry.IC: In a recent interview I had with Jim Keller, Jim mentions a large 8am meeting on chip design - lots of disagreements, but he mentioned you were one of the people who had a staunch belief that you could succeed. What was it like to be summoned to that meeting, discussing those ideas, and looking back to the success Zen has had?MC:For me, it's awesome! That engineering exchange is what we would call ‘concept’, which we do for every big project. At that time, for such a big transition, I would say there were probably more arguments than usual.I mean, we hadn't done SMT before, and we hadn't done an micro-op cache before, and there were a lot of people that thought that doing both of those in the same core was going to be a disaster. I had to convince people. With the Bulldozer threading model, we learned a lot, and there's a lot of SMT-like stuff in Bulldozer, so we've learned the ways to do SMT even though we hadn't done it in the execution unit or the data cache. So it wasn't really that big of a step for us as it would be for someone who done none of it before.With the micro-op cache, we did a similar thing on a project that got cancelled, and really we should have been doing in Bulldozer 2. For Zen, we needed to do it to hit our hit aggressive goal of a 40% IPC uplift. I think from discussions like that, the people who saw it was possible stayed on, and some of those who thought it was not possible decided to go their own way.But that's engineering, right? I mean, it's tough. We know that engineers are good at smelling out bullshit, so you have to be very careful that you don’t give them completely impossible goals - they'll see that it's impossible, and they won't set themselves set up to fail. You can’t have easy goals either, so you have to find that nice balance of not impossible goals but really hard goals, and then tell them if we don't get there, it's still going to be alright as well. But we have to set these aggressive goals, and if get the engineers on-board, you'd be amazed at how hard they work to get it done.IC: I asked Jim if he was the father of Zen, and he said he was one of the ‘crazy uncles’. Are you the father of Zen?MC:I definitely agree with Jim that it took a lot of people to make Zen. But yeah, I think I am the father of Zen - I mean, in the sense that I gave its name. I was there in 2012 on the first day, and I was with it all the time. I know everything good and bad about it, just like parents know their own kids - you know what they're good at, and what they're not so good at, and I've felt the pain of all our bad parts, and I've seen the joy of all our goodness. But like a child you know, you have it, like you have the chip, and you finally have to let it out in the world. You don't have control over it anymore! You don’t have control, and other people judge it, and you take it personally. I’ve been with this for so long, five years since those first disclosures, and I get emotionally invested. Because of that, you know, other people can come and go and move on, but I’ve been with Zen from the beginning to the end. So I do consider myself a Father in that respect. But then like I said, it took an amazing team to make it happen - I didn't make it happen by myself, just like raising the child doesn't have one person either. It takes so many people.IC: Recently AMD CMO John Taylor mentioned that there’s an interesting story behind the Zen and Ryzen naming. What’s that story?MC:So with the Bulldozer architecture, I guess I've learned over all those years that building x86 cores is all about finding the right balance in the architecture between frequency, IPC, power, and area. We weren't there with Bulldozer, and so I felt that our new project needs a name that talks about what our true goal is, which is to have a balanced architecture. ‘Zen’ as a name made sense to me for what we're doing.I think that from the Ryzen point of view, when they showed it to me the first time, they were a little nervous that I might not like it. But when I saw it, when I saw the Enzo, when I saw it was an open Enzo, with the beauty of imperfection (and we know Zen is not perfect, all cores have their problems), it just perfectly represented what I thought of when I named it in 2012. It was as though it was just this perfect synergy. No one really had talked to me about it until they showed it to me, I think they were nervous. When they showed it to me, it's like, that is awesome. It was exactly what I was thinking without even telling them.IC: Then when the lawyers came back and said, yes, we can trademark it, you're two thumbs up!MC:[laughs] One thing I love is that it sounds a lot like rising, as in Ryzen and rising. It has that secondary feel to that - I thought it was brilliant that AMD was rising back up. It's just genius marketing and naming to me.IC: Alongside Zen we learned about Project Skybridge, the ability to put an x86 SoC and an Arm SoC on the same socket. Do you know how far along the Arm version of Skybridge, we know as K12, was in development before AMD went full bore for Ryzen?MC:Originally Zen and K12 were, I think, we call them sister projects. They had kind of the same goals, just a different ISA actually hooked up. The core proper was that way, and the L2/L3 hierarchy could be either one. Then of course, in Skybridge, the Data Fabric could be either one. There was a whole team doing the K12, and we did share a lot of things you know, to be efficient, and had a lot of good debates about architecture. Although I've worked on x86 obviously for 28 years, it's just an ISA, and you can build a low-power design or a high-performance out any ISA. I mean, ISA does matter, but it's not the main component - you can change the ISA if you need some special instructions to do stuff, but really the microarchitecture is in a lot of ways independent of the ISA. There are some interesting quirks in the different ISAs, but at the end of the day, it's really about microarchitecture. But really I focused on the Zen side of it all.Core DesignIC: When we talk about cores and products, it’s always about time to market and meeting deadlines. Realistically, how shortly after the design is signed off do you start thinking about how things could have been design better, and where the low hanging fruit is?MC:A year even before a first tape-out, you realize the funny thing about microarchitecture is that certain decisions drive certain decisions that drive certain decisions, so if the first decision was bad, there's a lot of rework to get back down the right track. We try to make those first decisions as best as we can, but when some of them need to be redone, it's too late. Hopefully any issues are with decisions that are further down the path. But that's kind of the reality of microarchitecture.But that that is kind of where we maybe see our strategy is. We know that when we do a grounds up redesign [of a core], that there are going to be a lot of those opportunities to improve what we did before [with an updated derivative]. So we want our derivative chip to be a big derivative, and make it worth the 12 to 18 months it's going to take to get it done. Then having done that, we know that doing a second derivative isn’t usually worth the effort - there's not much you can gain from that in a performance/power view. You could always add more to it, but you have more power, and you kind of bolt things more on the side rather than really get in and redo the guts of the machine.So our strategy is really to do grounds up design, do a derivative, and then come back with another totally ground-up design where we re-thought everything through the pipeline. We may still reuse things, like we still have an OP cache and we're not getting rid of OP cache, but we might end up doing it in a much different way, such as a way where it interfaces with instruction cache, or the way it feeds into machine changes. Perhaps we need to rethink how it works to get it wider, and we have to really rethink the concept, not simply just make the dispatch or execute wider. The whole machine has to understand that, so we have to basically tear up the whole thing and put it down in block diagrams on a clean sheet of paper. That means that as you go along, and as our guys go to code it, it's a case of finding which parts aren’t changing, reusing code, or building new. We still use parts of the old designs, and if it's a part that isn't really changing this time, we might decide that it is good enough for this design.So every three years, we're pretty much redesigning it all. We have to manage the power so we can put these widgets in a whole new pipeline and really control the power of them so they don't just burn all the extra power equivalent to the IPC.First A0 Zen Needed ChillingIC: Are you the sort of architect that will be in the room during the bring-up of the first silicon back from the fabs? And if so, what's that like? What's the atmosphere in the room?MC:I would love to, but a lot of times they won't even let me in there! I get in there later. When it first comes back, there's a lot of bring up activities with BIOS and firmware that really don't even involve the core. So there's not a lot of need for myself or the lead architect of that generation at that time. But very quickly, as the team brings it up and gets it going and booting, is when it matters.On the original Zen, one of the wonderfully funny stories I have is that I was the first A0 silicon, we definitely had some issues. We had to run it really cold to get it running. We were waiting for the fixed A1 to come back, or even it was A0+, to solve the issues so we didn't have to run it cold. One of the engineers was like ‘have you tried out the patch on it yet?’ – I say ‘no, I'm sitting here waiting for it to dry’. When we keep it so cold that condensation builds up, every so often we have to stop, let it dry off. Then as soon as it dries off, I'll let you know if your patch worked. It's crazy, but I love a working lab - I'm definitely an architect who likes to get his hands dirty. Unfortunately I don't get to get them as dirty as I used to, and I get involved usually much later when you have really hard problems.IC: So it's funny that you bring up that story, because I'm kind of interested in it. When you get that A0 silicon back, and it doesn't work right at normal temperature, what tells you that you have to put it on cold? Who thinks of that? How does one come to the idea that that is what it needs to do just to run properly? And then how do you go about finding the fix for A0? Plus, is that a design fix? Or is it a manufacturing fix? How do you find the difference between the two?MC:Now in that case, we have lots of ways to test the silicon, you know. We have DFT (design for test) teams to realize that low level circuits aren't working properly. We have strong circuit team that does debug issues like this, and they realize it’s a problem with temperature, but then suggest if it was cold, you can still work on it. They also say what is wrong with the circuit so that it can be fixed and build an A0+, to get things that can run at normal temps. Again, the amount of great engineers working on any given product here at AMD is amazing, and I like to think of myself as well rounded, but there are people that are just way better than me at a lot of things!Future Design ConsiderationsIC: One of the modern design choices of the modern x86 core is the decode width of the variable instruction set - Intel and AMD's highest performance cores, all the way back since Ryzen, have been 4-wide. However, we're seeing dual 3-wide designs or 6-wide designs, relying on the op-cache to save power. Obviously 4-wide was great for AMD in Zen 1, and we're still at 4-wide for Zen 3: where does the roadmap go from here, and from a holistic perspective how does the decode width size of x86 change the fundamental IPC modelling?MC:I think it comes back to that balance aspect, in the sense that I think going beyond four with the number of transistors and the smarts we have in our branch predictor, and the ability to feed it worked fine. But we are going to go wider, you're going to see us go wider, and to be efficient, we'll have the transistors around the front end of the machine to make it the right architectural decision. So it's really having the continuous increase in transistors that we get, allowing us to beef up the whole design to continue to get more and more IPC out of it.IC: How reliant are you on your competitive analysis and workload prediction teams when it comes to the basic floorplan designs? If you're back in 2012, trying to predict 20 16 workloads, that's a bit of a leap?MC:We have great teams there, and in some sense, the problem you're stating is kind of independent of the team. Either you're trying to build a processor on the software of today, or the software five years from today. That's where a lot of it comes down to experience as an architect, understanding seeing what you're seeing in the performance traces, but also going beyond and realizing the wider implications. With the four-wide decode for example, a lot of the compilers have optimizations they do because you have a four wide machine. But when we give them something wider, they will be updated to realize how to compile the code to make it even better. So we'll see we only managed to get 10 to 15% IPC on these older codes that when we launched, but as the compilers developed, they'll be able to extract more and more out of our future designs based on what they get out from our current design.IC: On the concept of cache – AMD’s 3D cache announcement leading to products coming next year is obviously quite big. I'm not going to ask you about specific products, but the question is more about how much cache is the right amount? It’s a stupidly open ended question, but that's the way it's intended!MC:It's a great question! It's not just even about how much is the right amount, but at what level, what latency, what is sharing the cache and so on. As you know, those are all trade-offs that we have to decide how to make, and understand what that will mean for software.We have chosen that our core complex is going to have to a split L3 (in VCache). If we had one gigantic L3 shared across all the threads, the more you share a giant L3 across the threads, the latency of a given thread gets longer. So you're making a trade-off there of sharing, or getting more capacity and a lower thread count versus the latency it takes to get it. So we balanced for trying to hit on that lower latency, providing great capacity at the L3 level. That's the optimization point we've chosen, and as we continue to go forward, getting more cores, and getting more cores in a sharing L3 environment, we’ll still try to manage that latency so that when there are lower thread counts in the system, you still getting good latency out of that L3. Then the L2 - if your L2 is bigger then you can cut back some on your L3 as well.So it's a fascinating field - cache trade off studies have been going on forever, and they will continue forever of how to balance out the cache hierarchy for the core.IC: It’s funny that you bring up the L2, because I'm not sure if you saw IBM's recent announcement on their z16 / Telum chip. They've got very large L2 caches, but they're using them as a virtual L3 as well. Have you looked into that at all, and does that seem appetizing?MC:Yeah, we've definitely looked into it.Will Walkeris the head of our cache team, and he is an awesome architect. Like I said, every HLD (High Level Design) we go through the same questions, the same designs, look at different design points, and then have to settle on one of them. Even sometimes post HLD, things change, and if we decided to switch to a different design point, we can do that. So yeah, it's a constant evolving architecture.IC: TSMC has showcased an ability to stack 12 die with TSVs, similar to the V-Cache concept. Realistically, how many layers could be supported before issues such as the thermals of the base die become an issue?MC:There’s a lot to architecting those levels beyond the base architecture, such as dealing with temperature, and there's a lot of cost too. That probably doesn’t answer your question, but different workloads obviously have different sensitivity to the amount of cache, and so being flexible with it, being able to have designs both with stacking and without stacking, is critical because some workloads. [Always having stacked cache] would be way too expensive for the performance uplift it would bring for some use cases. I can't really comment on how many levels of stacking we can do or we will do, but it's an exciting technology that kind of continues to grow.IC: To what extent has AMD incorporated machine learning into its EDA tools? Both at this point, or to what extent in the future?MC:I don't think I'm allowed to say that definitively, but I think, you can probably assume that everyone is using some form of machine learning through data to improve everything in all our business processes.IC:So that’s a very careful answer!IC: Initially Zen started with a 4-core CCX, and now the base in Zen 3 is an 8-core complex. Are there limits to how big the complex can be in its current form, such as the ring bus, and what things have to change as that complex grows?MC:We build a very modular core cache hierarchy for all our different markets, from high-end servers all the way down to the low-end notebooks. So those environments desire more or fewer cores, and trying to meet them efficiently with as few designs as possible is also another interesting architectural goal. You would like to think you can just focus on one design at a time, such that we have a core roadmap for X or Y and there can be multiple of them, but there's not. We have to figure out how to leverage those designs across all those markets. Some markets like the high-end server are going crazy for more cores, whereas others are not increasing their consumption of cores at the same rate. We do see core counts growing, and we will continue to increase the number of cores in our core complex that are shared under an L3. As you point out, communicating through that has both latency problems, and coherency problems, but though that's what architecture is, and that's what we signed up for. It’s what we live for - solving those problems. So I'll just say that the team is already looking at what it takes to grow to a complex far beyond where we are today, and how to deliver that in the future.IC: Do you see any part of the Xilinx acquisition becoming part of the Ryzen future?MC:Oh, definitely. I can't really comment on anything particular. We sell SoCs, but we obviously integrate a lot of IP into them. If you look their IP and our IP, you can probably see some natural synergy there that you will likely see in the future. I look forward to getting those guys on board and working with them going forward. It’s a great team.IC: IPC is always a golden goal of high-performance processor design, and one of the benefits of smaller process nodes is more transistors, bigger buffers, more execution ports, and larger caches. How do you approach how to make the core ‘smarter’, rather than simply ‘bigger’, and what key elements in modern x86 designs are the limiting factors?MC:I think IPC gets all the glory! What it really is – I call it the ‘Wheel of Performance’ because there's four main tenets – performance, frequency, area and power. They really are all equal in a sense and you have to balance them all out to get a good design. So if you go for a really high frequency but crush IPC, you can end up with a really bad design, and increased area. If you go really hard on IPC and that adds a lot of area and a lot of power, you can be going backwards. So that's really the critical part like we said, we're trying to get that IPC but we have to get it in a way that optimizes the transistor use for both area and power, and frequency too. We want to be able to put a bunch of cores in and just add IPC and grow area, we're not making real progress.I get it, that's my job, and it’s my Lead Architects’ job to try to find that right balance. I think that was one of the biggest part of Zen – power had been a strong part of what we cared about. We looked at power like everything else in the wheel, and from the high-level execution we would get weekly feedback on how we're doing, know what area we used, and know what our IPC is. Actually, we did not have right tools early in the design to let us know where we were in power, and by the time we did, there was very little we could do - we were in that deep in execution where those decisions were made that if we had to redo a bad power design, we would have to rip up the whole thing and we would never hit schedule now.For the original Zen, we had to go create those new tools, and it was really a stress point on the team. Since they were new, a lot of it is just that it was so early in the design, and they weren’t perfect, none of our performance or frequency tools were perfect, but people use them enough that they trust them. We had to really come in with the knowledge that those tools weren’t perfect, but if you get the vectors right, and we're making decisions, that they're good enough and we managed to get over that hurdle and really properly use them.Feedback is like any other part of the design. Being able to drive that 40% IPC in a more efficient design with Zen - that was one of the first sticking points we said about it, but if you are going to have 40% more IPC but add 40% power, it won’t go anywhere.IC: In a recent AMD video on the 5 Years of Zen celebration, it was discussed that having a more scalable core was the preferred approach for the future compared to a hybrid design. Where are the difficulties in building a core with that much scale, from milliwatts to dozens of watts per core - is it specifically in logic design, power design, or manufacturing?MC:I mean it's all of those! As an architect, we have to consider all the markets we're wanting to focus on. If I want to hit this IPC at this frequency at this power, we can't think of the core as one thing and one set of targets - it has to be many sets of targets, and have it planned like that from the beginning. How it's going to scale up and down into those markets has been another part of the Ryzen and Zen success, in that we haven't solely been trying to use technology to fit a different hole of the market. We thought about how to scale to all those markets, and designed it to be capable of doing that upfront. That way it's easy for the backend to change the product for those different markets and execute.IC: Long-term R&D roadmaps are usually quoted at the 3/5/7 year timescale. As AMD has grown, especially recently, how has that changed inside AMD for you?MC:Not really. I mean, even back in 2012 we were thinking well beyond Zen, especially since your customers demand it, right? They're not going to switch over to using you if you don't have a long term roadmap. Our customers demand that when they want to do business with us, and of course our own teams demand that they want - our teams want to see a roadmap! There were a lot of people, even internally, that were worried we weren't going to be able to sustain the rate of progress. It is a very risky strategy, - tearing the whole core up every three years is risky. But to me, I've managed to convince everyone, and it’s what the market requires. If we don't do it, someone else will.Zen 5, Zen 8, and Everything ElseIC: You mentioned in a publicity video in April 2018 for AMD that you were working on Zen 5. We’re three years on - does that mean you’re working on Zen 8 now?MC:You’ve done the math pretty well, I’ll say that! I got a little flak saying that (in 2018) by the way, but I think you know how hard this business is. Like I was saying earlier, you have to be dynamic and willing to have a process that you can change as the market changes around you. If you build exactly what you set out to do on day 1, you'll put out something that nobody wants.IC: Finally, what should AMD users look forward to?MC:It's going be great! I wish I could tell you of all what's coming. I have this annual architecture meeting where we go over everything that's going on, and at one of them (I won't say when) the team and I went through Zen 5. I learned a lot, because of nowadays as running the roadmap, I don't get as close to the design as I wish I could. Coming out of that meeting, I just wanted to close my eyes, go to sleep, and then wake up and buy this thing. I want to be in the future, this thing is awesome and it's going be so great - I can't wait for it. The hard part of this business is knowing how long it takes to get what you have conceived to a point where you can build it to production.Many thanks to Mike Clark and his team for their time.Many thanks to Gavin Bonshor for transcription.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17031/anandtech-interviews-mike-clark-amds-chief-architect-of-zen\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Apple's M1 Pro, M1 Max SoCs Investigated: New Performance and Efficiency Heights\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-10-25T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/17024/apple-m1-max-performance-review\n",
      "Content: Last week, Apple had unveiled their new generation MacBook Pro laptop series, a new range of flagship devices that bring with them significant updates to the company’s professional and power-user oriented user-base. The new devices particularly differentiate themselves in that they’re now powered by two new additional entries in Apple’s own silicon line-up, the M1 Pro and the M1 Max. We’ve covered the initial reveal inlast week’s overview article of the two new chips, and today we’re getting the first glimpses of the performance we’re expected to see off the new silicon.The M1 Pro: 10-core CPU, 16-core GPU, 33.7bn TransistorsStarting off with the M1 Pro, the smaller sibling of the two, the design appears to be a new implementation of the first generation M1 chip, but this time designed from the ground up to scale up larger and to more performance. The M1 Pro in our view is the more interesting of the two designs, as it offers mostly everything that power users will deem generationally important in terms of upgrades.At the heart of the SoC we find a new 10-core CPU setup, in a 8+2 configuration, with there being 8 performance Firestorm cores and 2 efficiency Icestorm cores. We had indicated in our initial coverage that it appears that Apple’s new M1 Pro and Max chips is using a similar, if not the same generation CPU IP as on the M1, rather than updating things to the newer generation cores that are being used in the A15. We seemingly can confirm this, as we’re seeing no apparent changes in the cores compared to what we’ve discovered on the M1 chips.The CPU cores clock up to 3228MHz peak, however vary in frequency depending on how many cores are active within a cluster, clocking down to 3132 at 2, and 3036 MHz at 3 and 4 cores active. I say “per cluster”, because the 8 performance cores in the M1 Pro and M1 Max are indeed consisting of two 4-core clusters, both with their own 12MB L2 caches, and each being able to clock their CPUs independently from each other, so it’s actually possible to have four active cores in one cluster at 3036MHz and one active core in the other cluster running at 3.23GHz.The two E-cores in the system clock at up to 2064MHz, and as opposed to the M1, there’s only two of them this time around, however, Apple still gives them their full 4MB of L2 cache, same as on the M1 and A-derivative chips.One large feature of both chips is their much-increased memory bandwidth and interfaces – the M1 Pro features 256-bit LPDDR5 memory at 6400MT/s speeds, corresponding to 204GB/s bandwidth. This is significantly higher than the M1 at 68GB/s, and also generally higher than competitor laptop platforms which still rely on 128-bit interfaces.We’ve been able to identify the “SLC”, or system level cache as we call it, to be falling in at 24MB for the M1 Pro, and 48MB on the M1 Max, a bit smaller than what we initially speculated, but makes sense given the SRAM die area – representing a 50% increase over the per-block SLC on the M1.The M1 Max: A 32-Core GPU Monstrosity at 57bn TransistorsAbove the M1 Pro we have Apple’s second new M1 chip, the M1 Max. The M1 Max is essentially identical to the M1 Pro in terms of architecture and in many of its functional blocks – but what sets the Max apart is that Apple has equipped it with much larger GPU and media encode/decode complexes. Overall, Apple has doubled the number of GPU cores and media blocks, giving the M1 Max virtually twice the GPU and media performance.The GPU and memory interfaces of the chip are by far the most differentiated aspects of the chip, instead of a 16-core GPU, Apple doubles things up to a 32-core unit. On the M1 Max which we tested for today, the GPU is running at up to 1296MHz - quite fast for what we consider mobile IP, but still significantly slower than what we’ve seen from the conventional PC and console space where GPUs now can run up to around 2.5GHz.Apple also doubles up on the memory interfaces, using a whopping 512-bit wide LPDDR5 memory subsystem – unheard of in an SoC and even rare amongst historical discrete GPU designs. This gives the chip a massive 408GB/s of bandwidth – how this bandwidth is accessible to the various IP blocks on the chip is one of the things we’ll be investigating today.The memory controller caches are at 48MB in this chip, allowing for theoretically amplified memory bandwidth for various SoC blocks as well as reducing off-chip DRAM traffic, thus also reducing power and energy usage of the chip.Apple’s die shot of the M1 Max was a bit weird initially in that we weren’t sure if it actually represents physical reality – especially on the bottom part of the chip we had noted that there appears to be a doubled up NPU – something Apple doesn’t officially disclose. A doubled up media engine makes sense as that’s part of the features of the chip, however until we can get a third-party die shot to confirm that this is indeed how the chip looks like, we’ll refrain from speculating further in this regard.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17024/apple-m1-max-performance-review\n",
      "Title: Google Announces Pixel 6, Pixel 6 Pro: The New Real Flagship Pixels\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-10-19T20:30:00Z\n",
      "URL: https://www.anandtech.com/show/16939/google-announces-pixel-6-pixel-6-pro-the-new-real-flagship-pixels\n",
      "Content: Today, after many weeks, even months of leaks and teasers, Google has finally announced the new Pixel 6 and Pixel 6 Pro – their new flagship line-up of phones for 2021 and carrying them over into next year. The two phones had been teased quite on numerous occasions and have probably one of the worst leak records of any phone ever, and today’s event revealed little unknowns, but yet still Google manages to put on the table a pair of very interesting phones, if not, the most interesting Pixel phones the company has ever managed to release.Attacking the market with a two-prone approach, a more affordable “premium” range Pixel 6 starting at $599, essentially a similar price-bracket as the Pixel 5, yet with a much more capable phone, and what Google themselves call “their first true flagship”, the new Pixel 6 Pro, sporting all bells and whistles you expect from a contemporary high-end phone, yet still coming in at a quite reasonable $899.Both phones are introducing new designs, new camera hardware, and new screens, but also quite interestingly, a new custom SoC which Google calls the “Tensor”.Google Pixel 6 SeriesPixel 6Pixel 6 ProSoCGoogle \"Tensor\"2xCortex-X1@ 2.80GHz2x Cortex-A76 @ 2.25GHz4x Cortex-A55 @ 1.80GHzMali G78MP20DRAM8GB LPDDR5-640012GB LPDDR5-6400Display6.4\" AMOLED2340 x 108090Hz Refresh6.71\" AMOLED3120 x 1440120Hz VRR Refresh (LFD)SizeHeight160.4mm163.9mmWidth75.1mm75.8mmDepth8.2mm8.9mmWeight207g210gBattery Capacity4614mAh (Typical)30W5003mAh (Typical)30WWireless Charging21W23WRear CamerasMain50MP GN1 1/1.31\" 1.2µm4:1 Binning to 12.5MP / 2.4µmf/1.8525mm eq. / 82° FoVOISTelephoto-48MP IMX586 1/2.0\" 0.8µm4:1 Binning to 12MP / 1.6µm4x Optical Periscopef/3.5104mm eq. / 23.4° FoVOISUltra-Wide12MPf/2.216.3mm eq. / 106.5° FoVExtra-Front Camera8MPf/2.011.1MPf/2.2Storage128/256GBI/OUSB-CWireless (local)802.11 (Wifi 6E),Bluetooth 5.2Cellular4G + 5G NR NSA+SA Sub-6GHz+mmWave (Select markets)Special FeaturesFull-range stereo speakersSplash, Water, Dust ResistanceIP68Dual-SIM1x nano-SIM + eSIMLaunch OSAndroid 12Launch Price8+128GB: $599 / 649€8+256GB: $ / €12+128GB: $899 / 899€12+256GB: $ / €12+512GB: $ / €Starting off with what is probably the most enigmatic part of the phones, the new Google Tensor: the company hadteased the chip back almost 2 months ago, where we speculated our first ideas on the topic. Today, while generally not revealing too much new info, we can now officially confirm several specifications of the chip.On the CPU side of things, Google went with a 2+2+4 CPU setup, with the top performance CPUs being Cortex-X1 cores at 2.8GHz. This is quite unusual as generally most other vendors had moved onto a single “peak” performance core, whereas Google here is employing two of them. Samsung in previous yeas always had 2 high performance cores for several of their Exynos SoCs in similar 2+2+4 setups, and while their custom CPUs were lacking, the setup itself with the core count seemed to be working well, so I find Google’s strategy here nothing too exotic.What however is very exotic, is the usage of Cortex-A76 cores at 2.25GHz as the two middles cores of the setup.In interviews with other publications, Google explains that they’re going for better sustained performance at lower power levels, however for me this explanation makes absolutely no sense whatsoever, as both the A77 and A78 generations have large efficiency improvements included in their updated microarchitectures, and Qualcomm, SLSI and MediaTek all have good implementations of middle cores, which can go quite low in power consumption. Maybe Google is doing something here we don’t understand, and we’ll keep an open mind till we measure performance and efficiency, but it’s a very eye-brow raising aspect of the SoC.Finally, the two X1 cores and two A76 cores are accompanied by four Cortex-A55 cores at 1.8GHz. Google had also confirmed the L3 cachecomes in at 4MB, which is in line with the competition, and that we have a 8MB system cache, similar to what we see on the Exynos 2100.The GPU is a Mali-G78MP20, which would make this amongst one of the largest implementations out there, just shy of the MP24 monster of the Kirin 9000. We don’t know yet the frequencies of the unit, but I suspect Google is running it slower to allow for better energy efficiency.What was weird about today’s event is that Google had made no mention of Samsung, and presented the chip as purely a Google product. We’ve had seen rumours about a collaboration with Samsung LSI in terms of Google developing a custom chip for some time now, and seen evidence that the Tensor SoC is using quite a considerable amount of Exynos IP blocks. I’m not sure what kind of a deal the two companies have made in terms of marketing, but it’ll be definitely interesting to see the die shots of the chip and willing to make a few bets on the die marking showcasing “S5E9845”, let’s see how that pans out. The partnership with Samsung also would explain how Google is able to make an SoC with cellular connectivity – that is, if they’re indeed using a modem from Samsung. Google showcases different Pixel 6 models depending on whether they have mmWave or not, if we find a Qualcomm modem in there then that’d be quite an interesting development.While the roots and foundations of the Tensor SoC remain hazy, what isn’t hazy is that the design definitely features some Google developed IP blocks, with as a fully new custom TPU (or NPU), designed by Google’s ML R&D groups to function best with Google’s own ML models, and custom ISPs, which also allow Google to accelerate image processing the way Google wants to do things, and which would differentiate the Pixel 6 phones from any other device in the market.Particularly, Google explains that one killer feature enabled by the new ML capabilities is 30/60fps video processing where each frame has processing done to it via Google’s custom models, something which was previously not possible or achievable on other SoCs.The phones come either in 8GB of LPDDR5 (Pixel 6), or 12GB (Pixel 6 Pro), with UFS 3.1 storage from 128GB, 256GB, or a Pro exclusive 512GB variant.The Pixel 6 Pro is a quite feature rich device, most notably characterised by the 6.71” screen with 3120 x 1440 resolution. The display here is a LTPO/HOP panel with hardware variable refresh (LFD) with 10-120Hz refresh rates, just like what we see on Samsung’s Galaxy Note20 Ultra or S21 Ultra, meaning you should be able to enjoy 120Hz without major battery life impact. It remains to be seen if there’s some low brightness power compromise on the Pixel 6 Pro, or if Google managed to avoid the issue such as on the iPhone 13 Pro series.The phone is large, but not overly so, with a footprint of 163.9 x 75.8mm, but is a bit thicker at 8.9mm. The weight here is 210g, so it’s a little lighter than an S21 Ultra, which is welcome. The battery is a “standard” 5000mAh for a phone of this size, so Google definitely doesn’t appear to have any weaknesses on the specs side of things for the 6 Pro.The regular Pixel 6 is a bit more conservative in its design, mostly due to the display not having curved edges, and showcasing notably larger bezels. It’s a 6.4” screen with 2340 x 1080 resolution, and the OLED panel is only capable of 90Hz refresh rate, so I don’t expect any more advanced panel tech being employed.With a 4614mAh battery, the phone’s footprint of 160.4 x 75.1mm isn’t actually very much smaller than that of the Pixel 6 Pro – so this isn’t a traditional case of the cheaper variant being smaller, it’s just simply cheaper. The weight of the phone is also only 3g lighter than the Pixel 6 Pro even though it lacks a whole camera module and has slightly smaller battery.Both variants of the phones have IP68 rating, Wi-Fi 6E, and stereo speakers.After many years of camera stagnation, the new Pixel 6 phones do a major revamp of the camera system this time around. First of all, in terms of design, Google is embracing the need for a camera bump, and have gone with this “bar” layout with the modules oriented across the width of the phone, which pretty much gives the new Pixel 6 devices an unmistakable look that I assume will define them in the general public. It looks good, and it’s something different to the many corner camera island designs we’ve seen from so many vendors.The main camera sensor is now a 1/1.31” format Samsung GN1 sensor, with a native 50MP resolution and binning down via a Quad-Bayer colour filter to 12.5MP for regular shots. This is a massive hardware departure for the Pixel line-up, and the quite antiquated camera sensors we’ve seen used for many Pixel generations now. The aperture is f/1.85, and Google advertises a 82° field of view, which should correspond to a 25mm equivalent focal length.The ultra-wide is a “normal” 12MP sensor, Google here doesn’t advertise is exact specifications, but the optics are quite average at f/2.2 and a 16.3mm equivalent focal length, or 106.5° FoV, which is a bit narrower than most other phones, but in line with what we’ve seen on the Pixel 5.The Pixel 6 Pro exclusively has also a 48MP periscope telephoto. The magnification is 4x relative to the main sensor, and we should be seeing an IMX586 sensor here. Essentially, the specifications here are identical to the periscope telephoto of the Galaxy S20 Ultra, and I actually think that this is a good thing, as the 10x module of the S21 Ultra has a too great quality gap at intermediary magnifications, and the 6 Pro is able to take advantage of the dual resolution of the sensor to still allow both high-quality magnifications at 4x as well as 8x magnification.In terms of photography, Google of course continues to use its proprietary image processing algorithms, and alongside the augmented video recording capabilities, we should also see great leaps in still image capture quality.Extremely Competitive Pricing at $599 and $899At a starting price of $599, the Pixel 6 is a quite cheap and affordable device, especially for US readers which typically have much less choices in this segment. The phone compromises quite a bit on the display, but should that be something you can live with, the package represents significantly higher value than what we’ve seen with the Pixel 5. European pricing is also interesting at 649€ - though here there are more options to consider.The Pixel 6 Pro looks to me like an extremely well-balanced flagship device – and I would have very little in terms of reproach to any of its features or hardware specifications. At a starting price of $899, or 899€, it’s also starting less than what we’ve see on the S21 Ultra – naturally of course, the Pixel 6 Pro comes 8 months later.Google’s pricing here seems to hit a sweet spot balance between what the devices deliver, and where the competition is situated, which is actually a pretty shocking development for the Pixel line-up, which in past years has seen confusion as o where Google wants to be heading with their phones. The one negative I’d mention is that Google still appears to have extremely spotty availability in some markets- the phone is unavailable in BeNeLux countries where I’m situated, which is always head-scratching. That being said, we’re looking forward to the Pixel 6 series, as it appears to be Google most solid ever Pixel releases, which an extremely positive development for the company’s smartphone business.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16939/google-announces-pixel-6-pixel-6-pro-the-new-real-flagship-pixels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Arm DevSummit 2021 Keynote Live Blog: 8am PT (15:00 UTC)\n",
      "Author: Ryan Smith\n",
      "Date Published: 2021-10-19T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/17020/the-arm-devsummit-2021-keynote-live-blog-8am-pt-1500-utc\n",
      "Content: This week seems to be Arm's week across the tech industry. Following yesterday's Arm SoC announcements from Apple, today sees Arm kick off their2021 developer's summit, aptly named DevSummit. As always, the show is opening up with a keynote being delivered by Arm CEO Simon Segars, who will be using the opportunity to lay out Arm's vision of the future.Arm chips are already in everything from toasters to PCs – and Arm isn't stopping there. So be sure to join us at 8am PT (15:00 UTC) for our live blog coverage of Arm's keynote.10:57AM EDT- We're here for this year's Arm developer summit keynote10:58AM EDT- Like pretty much everything else this year, this is once again a virtual show in light of the coronavirus pandemic10:58AM EDT- So Arm's schedule and content is tweaked a bit to account from that10:59AM EDT- Arm is not unique in this, but the switch to virtual shows means that there's a much greater focus on videos that are produced ahead of time11:00AM EDT- Which means keynotes have been going from just talk-and-applause to more flashier, Apple-like presentations11:00AM EDT- And here we go11:01AM EDT- Starting with a pre-roll video11:01AM EDT- 200 billion Arm-based chips have been deployed11:03AM EDT- Oh, no, apparently that was a fake-out. The stream hosts are talking for a bit, before Simon's keynote actually gets started11:03AM EDT- Okay, *now* here we go11:03AM EDT- And here's Simon11:04AM EDT- Simon is starting off with a focus on the convergence of both the hardware and software ecosystems11:05AM EDT- Simon is recapping his own history with computing and how he got started11:05AM EDT- Back in the days of the Sinclair ZX81 and its 1KB of memory11:06AM EDT- Simon is thanking Sinclair himself for inspiring a generation of techies11:07AM EDT- Simon eventually went on to become an EE, and joining Arm11:08AM EDT- Continuing the walk down memory lane, Simon is now recapping the first embedded in-circuit emulation capabilities that were added to Arm in the 90s11:08AM EDT- And ultimately, the steps needed to have a CPU become a building block of a bigger chip11:09AM EDT- \"At our core, we're an engineering-focused organization\"11:09AM EDT- Now on to the not-too-distant future11:10AM EDT- Armv9 will be the core of Arm's next decade of computing IP. It's already shipping, and will show up in more and more IP and chips as time goes on11:10AM EDT- Arm designed Armv9 to run the full spectrum of compute, from HPC down to micro sensors11:11AM EDT- While also helping developers innovate on top of Arm's own IP11:11AM EDT- And of course, all of this saves time for chip design11:12AM EDT- Meanwhile, security and cyberattacks remain an ongoing issue for the world and for the tech industry building this hardware11:13AM EDT- Addressing this problem requires fundamentally rethinking matters11:14AM EDT- Simon believes that Arm shares in the responsibility for improving security11:14AM EDT- One such step Arm is taking is with Confidential Compute support in Armv911:15AM EDT- Moving on to the subject of decarbonization11:15AM EDT- Arm is committed to achieving carbon neutrality by 202311:16AM EDT- And of course, Arm's focus on energy efficiency is a way to help reduce power consumption (and thus CO2 generation)11:17AM EDT- And now a short testimonial from CloudFlare and why they use Arm11:18AM EDT- Moving on again to the changing nature of workloads. Starting with IoT11:19AM EDT- Simon is discussing one such example of a power line sensor that can detect if a line is damaged11:20AM EDT- Arm this week has announced Arm Total Solutions for IoT11:20AM EDT- 5G is another focus area for Arm11:20AM EDT- Arm is expecting a lot more than just handsets to be attached to 5G networks11:21AM EDT- Including the pieces necessary to further push edge computing11:21AM EDT- Arm is announcing the Arm 5G Solutions Lab to help develop the future of Arm-powered 5G hardware11:22AM EDT- What does Arm think is the most important technology of the next 50 years? AI11:22AM EDT- \"AI is entering a new stage of development\"11:23AM EDT- Arm expects AI to \"enhance the full spectrum of computing\"11:23AM EDT- The first devices incorporating Arm Ethos AI accelerators are starting to ship11:24AM EDT- Now a word on the Arm-NVIDIA merger11:25AM EDT- Simon thinks the combination of the two companies will be in a good position to tackle the future of AI11:25AM EDT- Arm's focus for the future will be supplying the tools needed to help hardware and software developers develop AI systems in a quick and efficient manner11:26AM EDT- \"Our purpose is to unlock the power of technology\"11:27AM EDT- And that's a wrap from Simon. Arm's VP of IoT, Mohamed Awad, is up in a moment11:28AM EDT- Simon's keynote was just under 30 minutes, so there's a couple of minutes of talking heads here before Awad's presentation starts11:32AM EDT- And here's Mohamed Awad, with a presentation titled \"Designing with Systems in Mind\"11:33AM EDT- Starting things off with robots11:34AM EDT- Recapping Asimov's 3 laws of robotics11:34AM EDT- And what modern laws look like11:35AM EDT- Slowly, science fiction about robotics will become less fictional11:36AM EDT- How Arm brings up systems, from definition and design to the final hardware development11:37AM EDT- With Arm providing the tools11:37AM EDT- These days Arm technology is baked into a immense number of specialized mobile devices11:38AM EDT- One area Arm is looking to improve the capabilities of mobile hardware is with their Memory Tagging Extensions for developers, to help them secure their software11:38AM EDT- And what Arm has learned in mobile they are applying to infrastructure11:39AM EDT- For Neoverse, Arm has not only worked on scaling up for cores, but also how those cores will be connected to other hardware, and how developers will create software for them11:41AM EDT- \"Establishing Arm in the cloud hasn't been easy\"11:41AM EDT- Awad credits designing things with a focus on whole systems as helping them to break into the cloud market11:42AM EDT- \"It's intelligence that will enable IoT to boost productivity\"11:42AM EDT- \"We must be honest about how hard it is to build IoT systems today\"11:43AM EDT- \"These systems are complex and massively fragmented\"11:43AM EDT- So Arm aims to help simplify the process11:43AM EDT- Such as Project Cassini11:44AM EDT- Arm has kicked off a related initiative: Project Centauri, a similar initiative for M-class systems11:45AM EDT- Arm partners shipped over 25B chips last year11:45AM EDT- Arm is doubling down on Corstone11:45AM EDT- Which will further help to accelerate SoC development times11:46AM EDT- One of Arm's customers, Alif, was able to achieve over an 800x performance improvement in ML versus their past M-class design11:47AM EDT- Now on to Arm Total Solutions for IoT, which was formally announced yesterday11:48AM EDT- Arm Virtual Hardware: develop on virtual hardware to deploy to physical hardware11:48AM EDT- Basically, an even greater focus on developing inside of a simulator11:49AM EDT- Arm partners are already using virtual hardware for their development processes11:49AM EDT- Including Amazon Lab126 and Himax11:50AM EDT- And now a short testamonial from AWS about using simulations for development11:52AM EDT- And Google's TensorFlow Mobile group11:53AM EDT- By making the virtual hardware available when the corstone is available, it allows software and hardware developers to get started right away, without waiting for silicon11:54AM EDT- \"We're fully committing to this Total Solutions approach\"11:54AM EDT- And that extends into Arm's IoT roadmap11:54AM EDT- Virtual hardware will be available at no cost while it's in beta11:55AM EDT- Arm believes this will take years off of product design cycles11:55AM EDT- And that's a wrap. Thank you for joining us.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17020/the-arm-devsummit-2021-keynote-live-blog-8am-pt-1500-utc\n",
      "Title: Apple Announces M1 Pro & M1 Max: Giant New Arm SoCs with All-Out Performance\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-10-18T20:00:00Z\n",
      "URL: https://www.anandtech.com/show/17019/apple-announced-m1-pro-m1-max-giant-new-socs-with-allout-performance\n",
      "Content: Today’s Apple Mac keynote has been very eventful, with the company announcing a new line-up of MacBook Pro devices, powered by two different new SoCs in Apple’s Silicon line-up: the new M1 Pro and the M1 Max.The M1 Pro and Max both follow-up onlast year’s M1, Apple’s first generation Macsilicon that ushered in the beginning of Apple’s journey to replace x86 based chips with their own in-house designs. The M1 had been widely successful for Apple, showcasing fantastic performance at never-before-seen power efficiency in the laptop market. Although the M1 was fast, it was still a somewhat smaller SoC – still powering devices such as the iPad Pro line-up, and a corresponding lower TDP, naturally still losing out to larger more power-hungry chips from the competition.Today’s two new chips look to change that situation, with Apple going all-out for performance, with more CPU cores, more GPU cores, much more silicon investment, and Apple now also increasing their power budget far past anything they’ve ever done in the smartphone or tablet space.The M1 Pro: 10-core CPU, 16-core GPU, 33.7bn Transistors in 245mm²The first of the two chips which were announced was the so-called M1 Pro – laying the ground-work for what Apple calls no-compromise laptop SoCs.Apple started off the presentation with a showcase of the packaging, there the M1 Pro is shown to continue to feature very custom packaging, including the still unique characteristic that Apple is packaging the SoC die along with the memory dies on a single organic PCB, which comes in contrast to other traditional chips such as from AMD or Intel which feature the DRAM dies either in DIMM slots, or soldered onto the motherboard. Apple’s approach here likely improves power efficiency by a notable amount.The company divulges that they’ve doubled up on the memory bus for the M1 Pro compared to the M1, moving from a 128-bit LPDDR4X interface to a new much wider and faster 256-bit LPDDR5 interface, promising system bandwidth of up to 200GB/s. We don’t know if that figure is exact or rounded, but an LPDDR5-6400 interface of that width would achieve 204.8GB/s.In a much-appreciated presentation move, Apple actually showcased the die shots of both the M1 Pro and M1 Max, so we can have an immediate look at the chip’s block layout, and how things are partitioned. Let’s start off with the memory interfaces, which are now more consolidated onto two corners of the SoC, rather than spread out along two edges like on the M1. Because of the increased interface width, we’re seeing quite a larger portion of the SoC being taken up by the memory controllers. However, what’s even more interesting, is the fact that Apple now apparently employs two system level cache (SLC) blocks directly behind the memory controllers.Apple’s system level cache blocks have been notable as they serve the whole SoC, able to amplify bandwidth, reduce latency, or simply just save power by avoiding memory transactions going off-chip, greatly improving power efficiency. This new generation SLC block looks quite a bit different to what we’ve seen on the M1. The SRAM cell areas look to be larger than that of the M1, so while we can’t exactly confirm this right now, it could signify that each SLC block has 16MB of cache in it – for the M1 Pro that would mean 32MB of total SLC cache.On the CPU side of things, Apple has shrunk the number of efficiency cores from 4 to 2. We don’t know if these cores would be similar to that of the M1 generation efficiency cores, or if Apple adopted the newer generation IP from the A15 SoC – we had noted that the new iPhone SoC had some larger microarchitectural changes in that regard.On the performance core side, Apple has doubled things up to 8 cores now. Apple’s performance cores were extremely impressive on the M1, however were lagging behind other 8-core SoCs in terms of multi-threaded performance. This doubling up of the cores should showcase immense MT performance boosts.On the die shot, we’re seeing that Apple is seemingly mirroring two 4-core blocks, with the L2 caches also being mirrored. Although Apple quotes 24MB of L2 here, I think it’s rather a 2x12MB setup, with an AMD core-complex-like setup being used. This would mean that the coherency of the two performance clusters is going over the fabric and SLC instead. Naturally, this is speculation for now, but it’s what makes most sense given the presented layout.In terms of CPU performance metrics, Apple made some comparisons to the competition – in particular the SKUs being compared here were Intel’s Core i7-1185G7, and the Core i7-11800H, 4-core and 8-core variants of Intel’s latest Tiger Lake 10nm 'SuperFin' CPUs.Apple here claims, that in multi-threaded performance, the new chips both vastly outperform anything Intel has to offer, at vastly lower power consumption. The presented performance/power curves showcase that at equal power usage of 30W, the new M1 Pro and Max are 1.7x faster in CPU throughput than the 11800H, whose power curve is extremely steep. Whereas at an equal performance levels – in this case using the 11800H's peak performance – Apple says that the new M1 Pro/Max achieves the same performance with 70% lower power consumption. Both figures are just massive discrepancies and leap ahead of what Intel is currently achieving.Alongside the powerful CPU complexes, Apple is also supersizing their custom GPU architecture. The M1 Pro now features a 16-core GPU, with an advertised compute throughput performance of 5.2 TFLOPs. What’s interesting here, is that this new much larger GPU would be supported by the much wider memory bus, as well as the presumably 32MB of SLC – this latter essentially acting similarly to what AMD is now achieving with their GPU Infinity Cache.Apple’s GPU performance is claimed to vastly outclass any previous generation competitor integrated graphics performance, so the company opted to make direct comparisons to medium-end discrete laptop graphics. In this case, pitting the M1 Pro against a GeForce RTX 3050 Ti 4GB, with the Apple chip achieving similar performance at 70% less power. The power levels here are showcased as being at around 30W – it’s not clear if this is total SoC or system power or Apple just comparing the GPU block itself.Alongside the GPU and CPUs, Apple also noted their much-improved media engine, which can now handle hardware accelerated decoding and encoding of ProRes and ProRes RAW, something that’s going to be extremely interesting to content creators and professional videographers. Apple Macs have generally held a good reputation for video editing, but hardware accelerated engines for RAW formats would be a killer feature that would be an immediate selling point for this audience, and something I’m sure we’ll hear many people talk about.The M1 Max: A 32-Core GPU Monstrosity at 57bn Transistors & 432mm²Alongside the M1 Pro, Apple also announced a bigger brother – the M1 Max. While the M1 Pro catches up and outpaces the laptop competition in terms of performance, the M1 Max is aiming at delivering something never-before seen: supercharging the GPU to a total of 32 cores. Essentially it’s no longer an SoC with an integrated GPU, rather it’s a GPU with an SoC around it.The packaging for the M1 Max changes slightly in that it’s bigger – the most obvious change is the increase of DRAM chips from 2 to 4, which also corresponds to the increase in memory interface width from 256-bit to 512-bit. Apple is advertising a massive 400GB/s of bandwidth, which if it’s LPDDR5-6400, would possibly be more exact at 409.6GB/s. This kind of bandwidth is unheard of in an SoC, but quite the norm in very high-end GPUs.On the die shot of the M1 Max, things look quite peculiar – first of all, the whole top part of the chip above the GPU essentially looks identical to the M1 Pro, pointing out that Apple is reusing most of the design, and that the Max variant simply grows downwards in the block layout.The additional two 128-bit LPDDR5 blocks are evident, and again it’s interesting to see here that they’re also increasing the number of SLC blocks along with them. If indeed at 16MB per block, this would represent 64MB of on-chip generic cache for the whole SoC to make use of. Beyond the obvious GPU uses, I do wonder what the CPUs are able to achieve with such gigantic memory bandwidth resources.The M1 Max is truly immense – Apple disclosed the M1 Pro transistor count to be at 33.7 billion, while the M1 Max bloats that up to 57 billion transistors. AMD advertises 26.8bn transistors for the Navi 21 GPU design at 520mm² on TSMC's 7nm process; Apple here has over double the transistors at a lower die size thanks to their use of TSMC's leading-edge 5nm process. Even compared to NVIDIA's biggest 7nm chip, the 54 billion transistor server-focused GA100, the M1 Max still has the greater transistor count.In terms of die sizes, Apple presented a slide of the M1, M1 Pro and M1 Max alongside each other, and they do seem to be 1:1 in scale. In which case, the M1 we already know to be 120mm², which would make the M1 Pro 245mm², and the M1 Max about 432mm².Most of the die size is taken up by the 32-core GPU, which Apple advertises as reaching 10.4TFLOPs. Going back at the die shot, it looks like Apple here has basically mirrored their 16-core GPU layout. The first thing that came to mind here was the idea that these would be 2 GPUs working in unison, but there does appear to be some shared logic between the two halves of the GPU. We might get more clarity on this once we see software behavior of the system.In terms of performance, Apple is battling it out with the very best available in the market, comparing the performance of the M1 Max to that of a mobile GeForce RTX 3080, at 100W less power (60W vs 160W). Apple also includes a 100W TDP variant of the RTX 3080 for comparison, here, outperforming the NVIDIA discrete GPU, while still using 40% less power.Today's reveal of the new generation Apple Silicon has been something we’ve been expecting for over a year now, and I think Apple has managed to not only meet those expectations, but also vastly surpass them. Both the M1 Pro and M1 Max look like incredibly differentiated designs, much different than anything we’ve ever seen in the laptop space. If the M1 was any indication of Apple’s success in their silicon endeavors, then the two new chips should also have no issues in laying incredible foundations for Apple’s Mac products, going far beyond what we’ve seen from any competitor.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17019/apple-announced-m1-pro-m1-max-giant-new-socs-with-allout-performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Apple 2021 Fall Mac Event Live Blog 10am PT (17:00 UTC)\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-10-18T16:22:00Z\n",
      "URL: https://www.anandtech.com/show/17016/the-apple-2021-fall-mac-event-live-blog-10am-pt-1700-utc\n",
      "Content: 12:57PM EDT- Following last month’s announcement event of Apple’s newest iPhone and iPad line-ups, today we’re seeing Apple hold its second fall event, where we expect the company to talk about all new things Mac. Last year’s event was a historic one, with Apple introducing the M1 chip and new powered Mac devices, marking the company’s move away from x86 chips from Intel, taking instead their own future in their hands with their own custom Arm silicon. This year, we’re expecting more chips and more devices, with even more performance to be release. Stay tuned as we cover tonight’s show.12:58PM EDT- We're about to start so hold on to your seats.01:00PM EDT- And here we go01:01PM EDT- Intro video starting, retro reminder about the Mac bootup sound was created.01:01PM EDT- Apple is starting with the ususal artistic-styled opening video, this time playing with the Mac startup sound01:02PM EDT- Tim taking the stage.01:02PM EDT- And here's Tim Cook01:02PM EDT- Focusing on two areas: Music and Mac01:03PM EDT- Music production on Mac has always been quite prevalent due to many professional editing tools.01:03PM EDT- Talking about music consumption in all of Apple's devices as well as Apple Music01:04PM EDT- \"Taking the Apple Music experience forward\"01:05PM EDT- Apple Music integration into Siri for a music assistance experience01:05PM EDT- New Apple Music plan \"Voice plan\"01:06PM EDT- New Apple Music plan that is only accessible over Siri01:06PM EDT- Moving fast onto the Homepod Mini01:07PM EDT- New colours for HomePod Mini - infused into the mesh fabric01:09PM EDT- $99 starting in November01:09PM EDT- Tim talking about AirPods now - some exciting news coming up01:10PM EDT- Talking about spatial audio Dolby Atmos mastered audio content.01:11PM EDT- 3rd Generation AirPods01:11PM EDT- \"New AirPods with Spatial Audio\"01:12PM EDT- Force Sensor on the side for control01:12PM EDT- Newly redesigned dynamic driver for better audio quality.01:13PM EDT- Adaptive EQ integration - this is actually a huge deal.01:13PM EDT- Adaptive EQ is able to adapt the frequency response to your ears for a better accurate audio experience.01:14PM EDT- $179 - orders starting today and availability next week.01:14PM EDT- Moving onto Mac - the meat of the show01:15PM EDT- \"Tight integration between hardware and software that is unprecedented\"01:15PM EDT- One year since the introduction of the M101:15PM EDT- Mac had its best year ever since the M1 introduction01:16PM EDT- Hints at a new MacBook Pro01:16PM EDT- \"Reimagining the MacBook Pro\"01:16PM EDT- \"M1 Pro\" - next chip in M1 family01:16PM EDT- Johny Srouji taking the stage01:17PM EDT- \"The M1 has shocked the PC world\"01:17PM EDT- \"Some users need even more\"01:17PM EDT- M1 Pro is all bout more power01:17PM EDT- Contrasting CPU and discrete GPUs in traditional architectures01:19PM EDT- 10-core CPU, 16-core GPU01:19PM EDT- Two new chips01:20PM EDT- M1 MAX01:20PM EDT- Based on M1 Pro, but bigger01:20PM EDT- 57 billion transistors01:21PM EDT- 32-core GPU01:21PM EDT- \"Industry leading performance per watt\"01:21PM EDT- \"Crushes the competition\"01:22PM EDT- 1.7x the perf/w than the competition01:23PM EDT- 100W less power than a comparable discrete GPU setup at 160W01:24PM EDT- 2.5x to 3x faster GPU speed at comparable power levels01:24PM EDT- \"The world's most powerful chip for a notebook\"01:24PM EDT- \"These new chips are phenomenal\"01:25PM EDT- \"Improvements across the system in the most demanding apps\"01:25PM EDT- Intelligent allocation and scheduling between performance and efficiency cores01:26PM EDT- ML 3-20x faster than a Core i901:27PM EDT- All Pro apps optimised for M1 Pro and M1 Max01:27PM EDT- 10000 universal apps now available - meaning both x86 and Arm binary compatible01:28PM EDT- Catching up the fast pace here - the 57bn transistor M1 MAX is quite insane and vastly exceeds anything else on the market - we're not longer talking a SoC with a GPU but a GPU with a SoC alongside it01:30PM EDT- Apple's GPU wattage comparisons would point out peak performance around that of a Radeon 670001:30PM EDT- \"it's just awesome for our vision for Apple Silicon come to life\"01:30PM EDT- Moving onto the MacBook Pro itself01:32PM EDT- 16\" and new 14\" models01:32PM EDT- The 14\" model would offer amazing performance in in the package01:32PM EDT- New thermal design and fans that are super quiet01:33PM EDT- TouchBar has been retired for normal keys01:33PM EDT- Connectivity next01:34PM EDT- on one side, HDMI, USB-C, and SDcard reader01:34PM EDT- Two USB-C, 3.5mm and Magsafe on the other side01:35PM EDT- 2 ProDisplay XDR for M1 Pro, 3 of them for M1 MAX01:35PM EDT- \"New breathtaking display\"01:35PM EDT- Thinner bezels, but now has a camera notch.01:36PM EDT- Static pixel density between the 16\" and 14\" models, with different resolutions.01:36PM EDT- Static pixel density between the 16\" and 14\" models, with different resolutions.01:36PM EDT- 120Hz refresh rate01:37PM EDT- 10-bit panel for 1bn colours01:37PM EDT- Mini-LED backlight01:38PM EDT- \"Thousands of mini LEDs\"01:38PM EDT- \"Extreme dynamic range\"01:38PM EDT- New camera and audio system next01:39PM EDT- 2x better low-light performance and 1080p01:40PM EDT- 4-speaker with tweeters and woofers01:40PM EDT- 80% more bass, and deeper frequencies01:42PM EDT- Huge performance claims both on CPU and GPU - 4x a Radeon 5600M01:43PM EDT- 64GB DRAM on the M1 MAX01:44PM EDT- 7.4GB/s SSD points at PCIe 4.0 4x or 3.0 8x01:44PM EDT- 2x better Lightroom Classic, 4x code compile in Xcode battery life01:45PM EDT- 21 hours video battery life on the 16\" model01:45PM EDT- \"The new MacBook Pro is simply extraordinary\"01:48PM EDT- $1999 and $249901:48PM EDT- \"A huge step in the Mac's transition to Apple Silicon\"01:50PM EDT- That's a wrap for today's show - stay tuned for our articles.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/17016/the-apple-2021-fall-mac-event-live-blog-10am-pt-1700-utc\n",
      "Title: The Ampere Altra Max Review: Pushing it to 128 Cores per Socket\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-10-07T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/16979/the-ampere-altra-max-review-pushing-it-to-128-cores-per-socket\n",
      "Content: It’s been a little over a year since Ampere started to deliver their first generation Altra processors.The “Quicksilver” design with 80 Neoverse N1 coreswas the first merchant Arm silicon on the market who really went “all-out” in terms of performance targets, aiming for the best of what AMD and Intel had to offer, ending up in a very competitive standing against the newest EPYC CPUs and leapfrogging Intel’s offerings.Since that first review, the competition has released two new generation platforms, thenewer EPYC Milan chips, showcasing a good generational boost, and Intel dramatically narrowing the performance gapwith the new Ice Lake-SP Xeon parts.Related Reading:The Ampere Altra Review: 2x 80 Cores Arm Server Performance MonsterAMD EPYC Milan Review Part 2: Testing 8 to 64 Cores in a Production PlatformAMD 3rd Gen EPYC Milan Review: A Peak vs Per Core Performance BalanceIntel 3rd Gen Xeon Scalable (Ice Lake SP) Review: Generationally Big, Competitively SmallFor the Arm ecosystem and Ampere in particular, things naturally also aren’t standing still; following the first-gen 80-core Quicksilver design, we had long expected the 128-core “Mystique” Altra Max design. Arguably a bit latecompared to Ampere’s initial Q4 2020 projections, we’ve now finally had our hands on the new many-core monster for today’s initial review.Pushing it to 128 CoresThe new Altra Max is a quite exciting part, but it’s also relatively straightforward design compared to the original Altra parts. While the original chip had been pushing 80 Neoverse-N1 cores, the new Altra Max is pushing 128 cores. While there are also slightly improved technical differences between the two chip generations, that is mostly the main large differentiation between the two designs.Ampere is still continuing to offer both Altra and Altra Max chips in their product line-up, with the Max parts in particular filling the high-core count SKU segment:Ampere Altra SKU ListAnandTechCoresFrequencyTDPPCIeDDR4PriceAltra Max \"Mystique\"M128-30(Tested)1283.0 GHz250 W128x G48 x 3200$5800M128-281282.8 GHz230 W128x G48 x 3200$5500M128-261282.6 GHz190 W128x G48 x 3200$5400M112-301123.0 GHz240 W128x G48 x 3200$5100M96-30963.0 GHz220 W128x G48 x 3200$4550M96-28962.8 GHz190 W128x G48 x 3200$4250Altra \"Quicksilver\"Q80-33(Tested)803.3 GHz250 W128x G48 x 3200$4050Q80-30803.0 GHz210 W128x G48 x 3200$3950Q80-26802.6 GHz175 W128x G48 x 3200$3810Q72-30723.0 GHz195 W128x G48 x 3200$3590Q64-33643.3 GHz220 W128x G48 x 3200$3810Q64-30643.0 GHz180 W128x G48 x 3200$3480Q64-26642.6 GHz125 W128x G48 x 3200$3260Q64-24642.4 GHz95 W128x G48 x 3200$3090Q32-17321.7 GHz45 W128x G48 x 3200$800The unit we’re testing today, the flagship Altra Max M128-30, with 128 cores and a 3.0GHz clock (again, noteworthy congratulations of Ampere’s straightforward and descriptive part naming), with a maximum TDP of 250W.Much like the first-generation parts, platform side features are all identical throughout the product stack, always featuring the maximum 128 lanes of PCIe 4.0 and 8-channel DDR4-3200 capabilities.Comparing the M128-30 to the Q80-33, the new Altra Max part is able to fit in 60% more cores, albeit at 10% lower frequency, within the same advertised TDP. It’s to be noted that TDP here doesn’t mean power consumption, and in our initial review of the Q80-33 we noted that the chip in many workloads hovered at power levels much below the TDP, possibly explaining why and Ampere was able to grow the core count this much even though the chip isn’t on a fundamentally different process node (TSMC N7), though it’s on a better implementation.The SKU list for the new Altra Max parts is interesting in that there’s only parts from 96 cores onwards, with anything below that still being serviced by the original Altra SKUs. It’s very likely that due to the process node maturity of the N7 node that Ampere here likely has few chips yielding with fewer cores, and the higher clocks and larger cache of the Quicksilver chips would be better served for lower core count deployments anyhow.In terms of pricing, Ampere is quite aggressive, vastly undercutting both AMD and Intel’s flagship parts MSRPs, though as always, what large customers and hyperscalers pay are most of the time never in line with those prices anyhow – but it’s still a large win for Ampere in terms of visible pricing.The Altra Max is extremely straightforward in terms of deployment: following some initial required firmware updates, it’s essentially a drop-in solution on the existing Altra platforms, which is exactly what we did for our review, re-using the originalMount Jade reference server from Wiwynn. The only practical note to make here is that at time of writing, Ampere currently doesn’t have a dual capable firmware stack that would enable swapping around from Altra to Altra Max and vice-versa, our initial setup was a one-way upgrade, with interoperability firmware still being something in the works for the future.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16979/the-ampere-altra-max-review-pushing-it-to-128-cores-per-socket\n",
      "Title: The Apple A15 SoC Performance Review: Faster & More Efficient\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-10-04T13:30:00Z\n",
      "URL: https://www.anandtech.com/show/16983/the-apple-a15-soc-performance-review-faster-more-efficient\n",
      "Content: A few weeks ago, we’ve seen Apple announce their newest iPhone 13 series devices, a set of phones being powered by the newest Apple A15 SoC. Today, in advance of the full device review which we’ll cover in the near future, we’re taking a closer look at the new generation chipset, looking at what exactly Apple has changed in the new silicon, and whether it lives up to the hype.This year’s announcement of the A15 was a bit odder on Apple’s PR side of things, notably because the company generally avoided making any generational comparisons between the new design to Apple’s own A14. Particularly notable was the fact that Apple preferred to describe the SoC in context of the competition; while that’s not unusual on the Mac side of things, it was something that this year stood out more than usual for the iPhone announcement.The few concrete factoids about the A15 were that Apple is using new designs for their CPUs, a faster Neural engine, a new 4- or 5-core GPU depending on the iPhone variant, and a whole new display pipeline and media hardware block for video encoding and decoding, alongside new ISP improvements for camera quality advancements.On the CPU side of things, improvements were very vague in that Apple quoted to be 50% faster than the competition, and the GPU performance metrics were also made in such a manner, describing the 4-core GPU A15 being +30% faster than the competition, and the 5-core variant being +50% faster. We’ve put the SoC through its initial paces, and in today’s article we’ll be focusing on the exact performance and efficiency metrics of the new chip.Frequency Boosts; 3.24GHz Performance & 2.0GHz Efficiency CoresStarting off with the CPU side of things, the new A15 is said to feature two new CPU microarchitectures, both for the performance cores as well as the efficiency cores. The first few reports about the performance of the new cores were focused around the frequencies, which we can now confirm in our measurements:Maximum Frequency vs Loaded ThreadsPer-Core Maximum MHzApple A151234Performance 132403180Performance 23180Efficiency 12016201620162016Efficiency 2201620162016Efficiency 320162016Efficiency 42016Maximum Frequency vs Loaded ThreadsPer-Core Maximum MHzApple A141234Performance 129982890Performance 22890Efficiency 11823182318231823Efficiency 2182318231823Efficiency 318231823Efficiency 41823Compared to the A14, the new A15 increases the peak single-core frequency of the two-performance core cluster by 8%, now reaching up to 3240MHz compared to the 2998MHz of the previous generation. When both performance cores are active, their operating frequency actually goes up by 10%, both now running at an aggressive 3180MHz compared to the previous generation’s 2890MHz.In general, Apple’s frequency increases here are quite aggressive given the fact that it’s quite hard to push this performance aspect of a design, especially when we’re not expecting major performance gains on the part of the new process node. The A15 should bemade on an N5P node variant from TSMC, although neither company really discloses the exact details of the design. TSMC claims a +5% frequency increase over N5, so for Apple to have gone further beyond this would have indicated an increase in power consumption, something to keep in mind of when we dive deeper into the power characteristics of the CPUs.The E-cores of the A15 are now able to clock up to 2016MHz, a 10.5% increase over the A14’s cores. The frequency here is independent of the performance cores, as in the number of threads in the cluster doesn’t affect the other cluster, or vice-versa. Apple has done some more interesting changes to the little cores this generation, which we’ll come to in a bit.Giant Caches: Performance CPU L2 to 12MB, SLC to Massive 32MBOne more straightforward technical detail Apple revealed during its launch was that the A15 now features double the system cache compared to the A14.Two years ago we had detailed the A13’s new SLCwhich had grown from 8MB in the A12 to 16MB, a size that was alsokept constant in the A14 generation. Apple claiming they’ve doubled this would consequently mean it’s 32MB now in the A15.Looking at our latency tests on the new A15, we can indeed now confirm that the SLC has now doubled up to 32MB, further pushing the memory depth to reach DRAM. Apple’s SLC is likely to be a key factor in the power efficiency of the chip, being able to keep memory accesses on the same silicon rather than going out to slower, and more power inefficient DRAM. We’ve seen these types of last-level caches being employed by more SoC vendors, but at 32MB, the new A15 dwarfs the competition’s implementations, such as the 3MB SLC on the Snapdragon 888or the estimated 6-8MB SLC on the Exynos 2100.What Apple didn’t divulge, is also changes to the L2 cache of the performance cores, which has now grown by 50% from 8MB to 12MB. This was actually thesame L2 size as on the Apple M1, only this time around it’s serving only two performance cores rather than four. The access latency appears to have risen from 16 cycles on the A14 to 18 cycles on the A15.A 12MB L2 is again humongous, over double compared to the combined L3+L2 (4+1+3x0.5 = 6.5MB) of other designs such as the Snapdragon 888. It very much appears Apple has invested a lot of SRAM into this year’s SoC generation.The efficiency cores this year don’t seem to have changed their cache sizes, remaining at 64KB L1D’s and 4MB shared L2’s, however we see Apple has increased the L2 TLB to 2048 entries, now covering up to 32MB, likely to facilitate better SLC access latencies. Interestingly, Apple this year now allows the efficiency cores to have faster DRAM access, with latencies now at around 130ns versus the +215ns on the A14, again something to keep in mind of in the next performance section of the article.CPU Microarchitecture Changes: A Slow(er) Year?This year’s CPU microarchitectures were a bit of a wildcard. Earlier this year,Arm had announced the new Armv9 ISA, predominantly defined by the new SVE2 SIMD instruction set, as wellas the company’s new Cortex series CPU IPwhich employs the new architecture. Back in 2013,Apple was notorious for being the first on the market with an Armv8 CPU, the first 64-bit capable mobile design. Given that context, I had generally expected this year’s generation to introduce v9 as well, but however that doesn’t seem to be the case for the A15.Microarchitecturally, the new performance cores on the A15 doesn’t seem to differ much from last year’s designs. I haven’t invested the time yet to look at every nook and cranny of the design, but at least the back-end of the processor is identical in throughput and latencies compared to the A14 performance cores.The efficiency cores have had more changes, alongside some of the memory subsystem TLB changes, the new E-core now gains an extra integer ALU, bringing the total up to 4, up from the previous 3. The core for some time no longer could be called “little” by any means, and it seems to have grown even more this year, again, something we’ll showcase in the performance section.The possible reason for Apple’s more moderate micro-architectural changes this year might be a storm of a few factors – Apple had notablylost their lead architecton the big performance cores, as well as parts of the design teams, to Nuvia back in 2019 (lateracquired by Qualcomm earlier this year). The shift towards Armv9 might also imply some more work done on the design, and the pandemic situation might also have contributed to some non-ideal execution. We’ll have to examine next year’s A16 to really determine if Apple’s design cadence has slowed down, or whether this was merely just a slippage, or simply a lull before a much larger change in the next microarchitecture.Of course, the tone here paints rather conservative improvement of the A15’s CPUs, which when looking at performance and efficiency, are anything but that.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16983/the-apple-a15-soc-performance-review-faster-more-efficient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Surface Laptop Studio, Surface 8 Pro Lead Microsoft's New Surface 2021 Lineup\n",
      "Author: Brett Howse\n",
      "Date Published: 2021-09-22T16:00:00Z\n",
      "URL: https://www.anandtech.com/show/16958/surface-laptop-studio-surface-8-pro-lead-2021-surface-iineup\n",
      "Content: In anticipation of the upcoming Windows 11 launch, Microsoft is introducing an almost complete top to bottom refresh of their Surface device lineup. Some devices are getting some minor tweaks, while other devices are completely new. As tends to be the case, all of them feature quirks which are distinctively Surface.Surface Refresh 2021ComponentSurface Laptop StudioSurface Pro 8Surface Pro XCPUCore i5-11300HCore i7-11370HConsumer:Core i5-1135G7Core i7-1185G7CommercialCore i3-1154G4Core i5-1145G7Core i7-1185G7Microsoft SQ 1Microsoft SQ 2GPUCore i5 - Intel Iris XeCore i7 - NVIDIA RTX 3050 TiCommerical: RTX A2000 OptionCore i5/i7: Intel Iris XeCore i3: Intel UHDSQ1: Adreno 685SQ2: Adreno 690Display14.4-inch PixelSense Flow display2400 x 1600201 PPIUp to 120 Hz RefreshDolby Vision13-inch PixelSense Flow display2880 x 1920267 PPIUp to 120 Hz Refresh13-inch PixelSense Flow display2880 x 1920267 PPIRAM16 / 32 GB LPDDR4x8 / 16 / 32 GB LPDDR4x8 / 16 GB LPDDR4xStorage256 GB to 2 TB SSDWi-Fi: 256 / 512 GB / 1TBLTE: 128 / 256 GB128 / 256 / 512 GB SSDNetworkingWi-Fi 6Bluetooth 5.1Wi-fi 6Bluetooth 5.1>br />Optional LTEWi-Fi 5Bluetooth 5LTE OptionI/O2 x Thunderbolt 41 x Surface ConnectHeadset jack2 x Thunderbolt 41 x Surface ConnectHeadset jack2 x USB-C 3.2 Gen 21 x Surface Connect1 x nano SIMBatteryUp to 19 hours65 W Adapter (i5)102 W Adapter (i7)Up to 16 hours60 W AdapterUp to 15 hoursCamera1080p front cameraWindows Hellow IR5.0 MP 1080p FrontWindows Hello IR10.0 MP 4K Rear5.0 MP 1080p FrontWindows Hello IR10.0 MP 4K RearDimensions (inches)12.7 x 9.0 x 0.711.3 x 8.2 x 0.3711.3 x 8.2 x 0.25Weighti5: 3.83 lbs / 1.74 kgi7: 4.00 lbs / 1.81 kg1.96 lbs / 891 grams1.7 lbs / 774 gramsStarting Price (USD)$1,599.99$1,099.99$899.99Surface Laptop StudioThe one new design in the lineup is the Surface Laptop Studio which brings some exciting changes to Surface. The most obvious design element is the new Dynamic Woven Hinge, which lets the display ease forward. This is not a new concept but does add some versatility to the design which is one of the elements Surface is most known for. Compared to the Surface Book design, which featured a detachable display, the new Surface Laptop Studio will be much easier to transition from one mode to another.The new 14.4-inch PixelSense display also gets some new branding thanks to the inclusion of a 120 Hz refresh rate, which Microsoft is branding as Flow touch. The increased refresh rate is a welcome addition to the lineup, and is also included on some of the other Surface devices being announced today. The increased smoothness is always welcome for GUI tasks, but will also be a nice addition when using the inking experience with the new Surface Slim Pen 2, which can be stored under the keyboard on the Surface Studio Laptop. Interestingly, and perhaps to keep costs down, the 14.4-inch display offers a resolution of 2400x1600, in the now standard Surface 3:2 aspect ratio. This translates to just 201 pixels-per-inch, well short of the 267 PPI found on the Surface Pro and 260 PPI on the Surface Book. It is very much in-line with the Surface Laptop.Microsoft has been offering one of the best trackpad experiences on the PC for several years now, and the Surface Laptop Studio adds a new Precision Haptic touchpad to the mix. There is little doubt what lineup they are trying to compete against, and hopefully the experience works as well as the Mac’s haptic design.The new Surface Laptop Studio packs in a lot of performance too. Microsoft has opted for the new Tiger Lake H35 series processors, with Core i5-11300H and Core i7-11370H offerings. The Core i5 model utilizes the Intel Xe GPU, while the Core i7 models will feature NVIDIA Ampere GPUs. Consumer models will be outfitted with the GeForce RTX 3050 Ti, while commercial customers can choose the RTX A2000.Memory options are 16 or 32 GB of LPDDR4x, and for storage, Microsoft is offering 256 GB to 2 TB SSD options, and like most of the Surface devices now, the SSD is user replaceable and no longer the soldered in BGA drive.Although somewhat late to the party, the Surface Laptop Studio also features two Thunderbolt 4 ports, as well as the traditional magnetic Surface Connect expansion/charging port. Microsoft has been very slow to adopt changing expansion port choices, so it is nice to see the new model offering the latest right out of the gate.The Surface Laptop Studio looks like a great addition to the Surface lineup. Prices start at $1600 and go up from there, with it being available for pre-order today. What this probably means is that the Surface Book will be removed from the lineup as this new design offers a very similar alternative, but without the somewhat complicated and error-prone detachable display.Surface Pro 8The most iconic design from Microsoft is certainly the Surface Pro, and that design really came into its own with the launch of the Surface Pro 3. For 2021, Microsoft is calling the Surface Pro 8 “the most significant leap forward since Pro 3” and has improved the design to bring it into a more modern era, without removing the aspects that make it such an iconic look.The first big change is the display, which now comes in at an even 13-inches across, compared to 12.3-inches on the previous models, and mimicking the Surface Pro X. The Surface team did most of that by shrinking the display bezels further, as overall dimensions are very similar to the outgoing model. The new display features a 267 PPI 2880x1920 resolution, and offers up to 120 Hz refresh, much like the Surface Laptop Studio. The 11% larger display is also 12.5% brighter than the Surface Pro 7, and still features the individually calibrated display that all Surface devices offer. Microsoft is still shying away from wider-than sRGB color gamuts which is still likely the right decision until the software side comes around if it ever does.Based on the Intel Evo platform, Surface Pro 8 ships with Intel’s 11th gen processors with the Core i5-1135G7 and Core i7-1185G7 in the consumer models. Commercial customers can choose a Core i3-1115G4, Core i5-1145G7, or Core i7-1185G7, and the latter two optionally have LTE as well. The Surface Pro 8 starts with 8 GB of LPDDR4x, which is really the bare minimum with Microsoft Teams, and 16 and 32 GB options. Storage options are 128 GB to 1 TB, depending on the configuration.Like the Surface Laptop Studio, the Surface Pro 8 also finally adds Thunderbolt 4 ports, with not one but two of the Type-C connectors, in addition to the traditional magnetic Surface Connect offering.Love it or hate it, video conferencing is a big part of the modern workforce, and the Surface Pro 8 features a 5.0 MP front camera for full HD video, as well as offering Windows Hello biometric sign-in. On the back is a 10.0 MP camera which supports up to 4K video. There are dual far-field microphones, as well as 2-Watt stereo speakers with Dolby Atmos support.Finally, Microsoft has updated the detachable keyboard to include storage for the Surface Slim Pen. There are, of course, new type-covers for this model since the exterior dimensions have changed slightly.Surface Pro has always been a great device from Microsoft, although the company has erred on the side of caution when updating it. The Surface Pro 8 looks to be a very nice re-think of the classic design, improving it in all the area where it needed some tweaks, without losing the essence of Surface Pro.Surface Pro XThe Arm powered Surface Pro X does not get much of a refresh this year, although there will now be a new Wi-Fi only version starting at $900. It is still powered by the same Microsoft SQ1 or SQ2 processors. Surface Pro X will begin shipping with Windows 11 which offers both x86 and x64 application emulation.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16958/surface-laptop-studio-surface-8-pro-lead-2021-surface-iineup\n",
      "Title: The Xiaomi 11T & 11T Pro Review: Two Chips, With a Battery Focus\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-09-15T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16935/the-xiaomi-11t-pro-review-cheaper-with-a-battery-focus\n",
      "Content: Today Xiaomi is announcing three new devices – the 11T, the 11T Pro and the 11 Lite 5G NE. We’ve had the first two in for review for a bit now and are able to give some first-hand experiences with the phones today.The Xiaomi 11T series – which by the way isn’t called anymore “Mi” in the name, it’s literally just “Xiaomi 11T” now, are supposed to be additions to the company’s flagship line-up, but coming in at lower price points. We hadreviewed the Mi 11 back in Marchand the big brother,the Mi 11 Ultra in July, so make sure to read those pieces as the 11T series fits into the line-up, albeit being different.What defines the 11T series is the fact that they’re slightly lower priced and more budget than the original Mi 11, notably on the display side which has now been reduced to a 1080p panel. Build quality is also different, and finally, we’re seeing a different set of SoC options depending on whether you get the regular 11T or the higher-end 11T Pro.Xiaomi 11T Series11T11T ProSoCMediaTek Dimensity 12001x Cortex-A78 @ 3.00GHz3x Cortex-A78 @ 2.60GHz4x Cortex-A55 @ 2.00GHzMali-G77MP9Qualcomm Snapdragon 8881xCortex-X1@ 2.84GHz3xCortex-A78@ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 660 @ 840MHzDRAM8GB LPDDR5-64008/12GB LPDDR5-6400Display6.67\" AMOLED2400 x 1080120Hz Refresh480Hz TouchHDR10+ /10-bit panelSizeHeight164.1mmWidth76.9mmDepth8.8mmWeight203g204gBattery Capacity5000mAh (Typical)67W Charging5000mAh (Typical)120W ChargingWireless Charging--Rear CamerasMain108MP HM2 1/1.3\" 0.7µm9:1 Binning to 12MP / 1.4µmf/1.7524mm eq.Telephoto5MP (Macro only)f/2.448mm eq.ExtraTelephoto-Ultra-Wide8MPf/2.2120° FoVExtra-Front Camera16MPf/2.45Storage128/256GBI/OUSB-CWireless (local)802.11 (Wifi 6),Bluetooth 5.2Cellular4G + 5G NR NSA+SA Sub-6GHzSpecial FeaturesCapacitive side fingerprint sensor (power button)Full-range stereo speakersIR BlasterSplash, Water, Dust ResistanceNo ratingDual-SIM2x nano-SIMLaunch OSAndroid 11 w/ MIUIAndroid 11 w/ MIUILaunch Price8+128GB: 499€8+256GB: 549€8+128GB: 649€8+256GB: 699€12+256GB: 749€Starting off with the SoCs, the two 11T devices are absolutely interesting phones as besides the usual Samsung Galaxy devices each year who are powered by Snapdragon and Exynos SoCs, Xiaomi is employing a similar strategy here with the 11T series in dual-sourcing both from Qualcomm and from MediaTek.The regular 11T is powered by the MediaTek Dimensity 1200 “Ultra” – we’re not sure what the Ultra here stands for but it seems it’s related to a high clocked NPU – there are no differences on the CPU clocks. The SoC features a Cortex-A78 at up to 3.00GHz, three A78’s at 2.60GHz, and four A55’s at 2.00GHz. The GPU is a MaliG77MP9, although we can’t confirm the frequency. This is MediaTek’s highest end chip at this moment in time, so it’s quite interesting. What also makes it special is that it’s on TSMC’s 6nm process node, and it’s the first chip we’ve had in our hands on this node.The Snapdragon 888 needs no introduction, and that’s what’s powering the 11T Pro. There are a few oddities with the chip though, which we’ll cover in the system performance section, but the way Xiaomi is configuring the chip isn’t quite flagship level in terms of behaviour.What’s very exciting about the two devices with differing SoCs is that both phones are otherwise absolutely identical in specifications. It’s essentially the same phone, just with a different SoC, and different charging capabilities. For apples-to-apples chipset comparisons, it rarely gets better than this, though we’ll see that there are behavioural discrepancies.The back of the phone is relatively generic with its glass back. One thing to note here is that the phone is a bit more on the wider side at 76.9mm. Weight is reasonable at 203-204g due to the 5000mAh battery.In general, the build quality and design of the phone isn’t quite as attractive and sleek as what we saw on the Mi 11, and it does feel like a slightly cheaper / lower end model in the Mi 11 flagship series.On the camera side of things, Xiaomi is employing a 108MP sensor, but this is not the HMX that was found in the Mi 11 but rather the smaller HM2 sensor which uses 0.7µm pixels compared to 0.8µm on the sibling. The optics are 24mm equivalent focal length at f/1.75 aperture, but without OIS.There’s a 5MP telephoto module but this is only used for macro shots, it can’t be used for any actual zooming in.Finally, there’s a basic 8MP ultra-wide with f/2.2 and 120° field of view – it’s really basic. In general, the whole camera setup is simple and Xiaomi doesn’t give it too great specifications.Xiaomi is focusing around fast-charging in these devices, notably the 11T Pro comes with a 120W charger. It performs as advertised; however, we’ll be talking about the topic in more depth in the review.The 11T starts at 499€, which is quite reasonable, and the 11T Pro starts at 649€ - which is actually quite steep given that one can have the Mi 11 for around 715€ nowadays, the phone would have to somehow rationalise itself in its differentiations compared to its series sibling.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16935/the-xiaomi-11t-pro-review-cheaper-with-a-battery-focus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Apple Announces iPhone 13 Series: A15, New Cameras, New Screens\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-09-14T20:00:00Z\n",
      "URL: https://www.anandtech.com/show/16934/apple-announces-iphone-13-series\n",
      "Content: Today Apple held its fall 2021 iPhone launch event, and we’ve gotten 4 new iPhones from the new iPhone 13 series: the iPhone 13 mini, the iPhone 13, iPhone 13 Pro and iPhone 13 Pro Max. This year’s phones follow last year’s rather large generational upgrades – although this year Apple also has quite a few big features on the menu such as better cameras and new much improved 120Hz displays on the Pro models. Battery life also has seen a larger emphasis, with Apple claiming the new iPhones last longer than their predecessors, achieved through both component efficiency improvements as well as new larger batteries.It’s also where we see Apple’s newest A15 chip: Years been in the focus of the industry, the new SoC promises iterative improvements, with some of Apple’s claims being a little eye-brow raising, more on this in a bit.Let’s go over the various specifications of the new device set:Apple iPhone 13 Series SpecificationsiPhone 13 miniiPhone 13iPhone 13 ProiPhone 13 Pro MaxSoCApple A15 Bionic2 × Avalanche4 × Blizzard4-core GPUApple A15 Bionic2 × Avalanche4 × Blizzard5-core GPUDRAM??Display5.42\" OLED2340 x 1080800nits peak6.06\" OLED2532 x 1170800nits peak6.06\" OLED2532 x 11701000nits peak10-120Hz VRR6.68\" OLED2778 x 12841000nits peak10-120Hz VRRSizeHeight131.5 mm146.7 mm160.8 mmWidth64.2 mm71.5 mm78.1 mmDepth7.65 mmWeight141g174g204g240gBattery Life? mAh? mAh? mAh? mAhWireless ChargingMagSafe Wireless Charging up to 15WQi Compatible (7.5W)Rear CamerasMain12MP 1/1.87\" 1.7µm26mm eq.f/1.6Sensor-shift IS (IBIS)12MP 1/1.67\" 1.9µm26mm eq.f/1.5Sensor-shift IS (IBIS)Tele-Photo-12MP77mm eq.f/2.8OISUltra-Wide12MP13mm eq. (120°)f/2.412MP13mm eq. (120°)f/1.8Front Camera12MPf/2.2Storage128GB256GB512GB128GB256GB512GB1024GBI/OApple LightningWireless (local)802.11axWi-Fi with MIMO + Bluetooth 5.0 + NFCCellular5G (sub‑6 GHz and mmWave**)Gigabit LTE with 4x4 MIMO and LAA**US models onlySplash, Water, Dust ResistanceIP68up to 6m, up to 30 minutesDual-SIMnano-SIM + eSIMLaunch Price128 GB:$699£679€799256 GB:$799£779€919512 GB:$899£979€1149128 GB:$799£779€899256 GB:$899£879€1019512 GB:$1099£1079€1249128 GB:$999£949€1149256 GB:$1099£1049€1269512 GB:$1299£1249€14991024 GB:$1499£1449€1729128 GB:$1099£1049€1249256 GB:$1199£1149€1369512 GB:$1399£1349€15991024 GB:$1599£1549€1829Starting off with the new internals, the new iPhone 13 series are powered by Apple’s newest A15 Bionic SoC. Like last year, the A15 is manufactured on a 5nm process node. Apple naturally doesn’t specify exactly which variant of node it is, but given the timing and the evolution of TSMC’s offering, we suspect it will be on the new N5P node, which is an iteration on last year’s N5 node.The new design follows Apple’s 2+4 CPU configuration that we’ve seen being used for the last couple of generations. We’re looking at two new performance cores and four new efficiency cores. Apple this year didn’t disclose too much information about the new CPU, most notably, the company refrained from making generational comparisons to the predecessor A14 chip, instead opting to compare itself to the competition, again something that we don’t see Apple do much in their silicon talks.Here, they’re claiming that the new A15 will be +50% better than the next-best competitor. The next-best competitor is Qualcomm’s Snapdragon 888 – if we look up our benchmark result set, we can see that the A14 is +41% more performant than the Snapdragon 888 in SPECint2017 – for the A15 to grow that gap to 50% it really would only need to be roughly 6% faster than the A14, which is indeed not a very large upgrade. Apple also didn’t comment on any new ISA features such as Armv9/SVE2, so it seems that the CPU doesn’t feature it?Back in early 2019, Apple had lost their lead architect (Gerard Williams III) and a portion of their CPU design team when several of the team went on tofound and work at Nuvia, which wasacquired earlier this year by Qualcomm. While I’m not certain, the time gap here certainly could match and the new CPU time to market, and be the first signs of that talent loss and team reshuffle. As a note, Apple went on to hire Arm’s lead architect Mike Filippo, likely working on a new CPU family.Another theory is that Apple decided to focus more on reducing power and energy efficiency this generation, given their massive lead in CPU performance. This actually would be a much more welcome theory, but one that we won’t be able to confirm until we get our hands on devices.What was even more weird, was the fact that the A15 is the first SoC where Apple has decided to bin into two different performance models. The regular iPhone 13 mini and iPhone 13 are receiving an A15 with 4-core GPUs, while the Pro models are receiving a 5-core GPU configuration. In fact, if I’m not wrong, this would be the first time ever we see a functional block binned SoC in a mobile phone at all, as I don’t remember any company ever doing this before today (Normally mobile SoCs are power binned).For the lower performance 4-core GPU model, Apple again was weird with their performance predictions as they focused on the competition, and not the generational gains. The improvements here over the currently best performing competitor is said to be +30%. Taking GFXBench Aztec as a baseline, we see the A14 was around +18% faster than the Snapdragon 888. The slower A15 would need to be +10% faster than the A14 to get to that margin.The faster 5-core A15 is advertised as being +50% faster than the competition, this would actually be a more sizeable +28% performance improvement over the A14 and would be more in line with Apple’s generational gains over the last few years.Of course, all these figures are just speculation for now, as we don’t know exactly what workloads Apple references to, and there are quite larger variations that can be measured. We’ll have to verify things on actual devices.Apple noted a lot of other SoC-side improvements, such as making mention that they’ve doubled the system level cache (SLC) to what presumably would be 32MB. There’s also a new display engine, likely to deal with 120Hz, and new video decoders and encoders – I wonder what kind of format they support now; Apple did mention hardware ProRes support.Finally, the Neural Engine boosts its performance to 15.8TOPs over 11 TOPs, even though it still features the same 16 core count.One thing we missed being mention from Apple is whether the new SoC uses new memory or not. We’ve seen adoption of LPDDR5 in the market, and the A14 notably lacked this. If the A15 also didn’t have it, that would be quite weird.Similar Design, But RefinedThe new iPhones follow up on the industrial design introduced with the iPhone 12 series last year. This means the similar flat-edges and metallic finish.The only two distinguishing visual changes of the iPhone 13 mini and iPhone 13 is the fact that the dual-cameras are now oriented diagonally, rather than vertically, and on the front side of the phone the new notch is 20% smaller.The fact that Apple still continued with a notch rather than a more modern design is likely to be controversial and some will make fun of Apple, given the competition’s rapid design iterations and experience with hole-punch or under-display cameras, but Apple has never been very adventurous in the design department.The normal and mini models have had their display characteristics boosted to match those of the iPhone 13 Pro models last year, and what this likely means is that those models have moved to a higher tier panel with the newer OLED emitters.The iPhone 13 Pro and Pro Max are in turn moving to display panels to a tier higher than last year’s Pro models. The new features here are focused around the new variable refresh rate 120Hz panel, which is able to dynamically switch between 10 and 120Hz depending on content. It’s likely the same type of LTPO technology that we’ve seen earlier this year with the Galaxy S21 Ultra, and should bring significant battery life increases and smoothness to the Pro models. Brightness also has gone up to 1000nits.Apple stated that all new models feature a new internal component arrangement, allowing for better footprint usage within the phone, and allowing for more space for the batteries as well as the camera system.Apple quoted figures such as +1.5h for the iPhone 13 mini, +2.5h for the iPhone 13, +1.5h for the iPhone 13 Pro, and +2.5h for the iPhone 13 Pro Max, all relative to their predecessors.While Apple typically doesn’t state absolute battery capacities, we’re seeing weight of the phones go up; 135g -> 141g, 164g -> 174g, 189g -> 204g, and 228g -> 240g. Particularly the Pro models are getting extremely heavy, so I do question Apple’s insistence on steel frames here.New Cameras EverywhereThe camera setup on the new iPhone 13 line is all new, on all models. Although the types of cameras haven’t changed, still ultra-wide, wide and telephoto where applicable, we’re seeing new sensors and modules on all units.The iPhone 13 and mini feature a dual-camera setup. The main module here has received a new sensor, still 12MP (valid for all modules on all models), however increases in size and thus also in pixel pitch from 1.4µm to 1.7µm. The optics are 26mm equivalent, with an f/1.6 aperture. In terms of stabilisation, Apple has moved from OIS to an IBIS system, or what they call sensor-shift image stabilisation, where instead of the optics, the sensor itself is stabilised.The ultra-wide angle is similar, with a large 120° field of view at 13mm focal length equivalent and f/2.4 optics, but Apple does say it’s a new faster sensor, even though it’s not larger.The Pro models have seen larger camera updates, in the literal sense. This year’s Pro models feature a much larger camera island, and a visibly larger camera lenses and thicker camera bumps.The main sensor is again upgraded to a larger size, this time with 1.9µm pitch pixels, which at 12MP translates to a 1/1.67” sensor – still not quite as large as what we see from some Android vendors. The aperture is a large f/1.5, and is also stabilised via IBIS.The ultra-wide does not appear to have a different sensor on the Pro models, however Apple takes advantage of the thicker module size to enable a much wider f/1.8 aperture.Finally, the telephoto module comes in at a 77mm equivalent focal length, or 2.96x magnification over the wide module. Apple didn’t comment on the pixel size so we can’t infer sensor size, but aperture is a bit smaller at f/2.8.I just want to note that I’m happy Apple has retained the same camera setup on the regular Pro and the Pro Max models – something they didn’t do on last year’s models, and which also many other competitors fail to do between their differently sized flagships, it means you’re not missing out on features just because you chose the smaller variant.In terms of software photography prowess, the new phones features Smart HDR 4, promising further improvements to tone-mapping and retention of shadow and highlight details, as well as now finally improving night mode to work on all modules (The telephoto previously didn’t have the feature).There are also new features such as stylisation of pictures, a filter effect that works within the imaging pipeline rather than a post-processing effect. Video recording gets a new Cinematic Mode feature for cinematic looking focus pulls and shifting, enabled automatically in the app via ML and subject tracking, as well as ProRes video recording.Static Pricing, Better PhonesThe new iPhones are much improved this year, with a larger focus on displays, battery life, and camera quality. Performance metrics were a bit vague this time around, so we’ll have to see whether Apple here has invested into power efficiency, or indeed slowed down in progress.iPhone 13 miniiPhone 13iPhone 13 ProiPhone 13 Pro MaxLaunch Price128 GB:$699£679€799256 GB:$799£779€919512 GB:$899£979€1149128 GB:$799£779€899256 GB:$899£879€1019512 GB:$1099£1079€1249128 GB:$999£949€1149256 GB:$1099£1049€1269512 GB:$1299£1249€14991024 GB:$1499£1449€1729128 GB:$1099£1049€1249256 GB:$1199£1149€1369512 GB:$1399£1349€15991024 GB:$1599£1549€1829On the positive side, all new models now start at the same pricing as their predecessors, and the regular iPhone 13 and mini now start at a 128GB baseline storage model, and doubling up the options stack. The Pro models have the same pricing this year, however add in a super-premium 1TB storage option at a higher price category.The phones will be available for preorder this Friday, with availability September 24th.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16934/apple-announces-iphone-13-series\n",
      "Title: The Axon 30 with Under Display Camera: Hands-on Mini-Review\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-09-03T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16925/the-axon-30-with-under-display-camera-handson-minireview\n",
      "Content: A few weeks ago, ZTE had announced a new device in their flagship line-up, the new Axon 30. The phone is an interesting device for a few reasons: it’s one of the more rare devices this year that opted to use the Snapdragon 870 rather than the newer Snapdragon 888 chip, also pricing itself much more aggressively versus the higher-endAxon 30 Ultra which we reviewed also around a month ago.What also makes the Axon 30 interesting is the fact that this is now the second-generation under-display camera implementation from ZTE. Although we never had the chance to experience the first-gen Axon 20, we’re now starting to see more vendors attempt to implement the new technology in their new devices, and the Axon 30 is a good opportunity to have a literal closer look at how these under-display cameras work.ZTE Axon 30 SeriesAxon 30 UltraAxon 30SoCSnapdragon 8881xCortex-X1@ 2.84GHz3xCortex-A78@ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 660 @ 840MHzSnapdragon 8701x Cortex A77@3.2GHz3x Cortex A77@ 2.42GHz4x Cortex A55@ 1.80GHzAdreno 650 @ ?MHzDRAM8/12 GB LPDDR5Storage128/256GB UFS 3.1Display6.67\" AMOLED2400 x 1080 (20:9)144Hz300Hz Touch6.92\" AMOLED2460 x 1080 (20.5:9)120Hz360Hz TouchSizeHeight161.53 mm170.2 mmWidth72.96 mm77.8 mmDepth8.0 mm7.8 mmWeight188 grams189 gramsBattery Capacity4600mAh65W charging (PD3.0)4200mAh65W charging (PD3.0)Wireless Charging--Rear CamerasWide64MP IMX6860.8µm pixels (1.6µm 4:1 16MP)f/1.6 w/OIS26mm eq.64MP IMX6860.8µm pixels (1.6µm 4:1 16MP)f/1.8 w/OIS26mm eq.Main64MP GW30.7µm pixels (1.4µm 4:1 16MP)f/1.935mm eq.-Ultra-wide64MP GW30.7µm pixels (1.4µm 4:1 16MP)f/2.213mm eq.8MPf/2.213mm eq.Telephoto8MPf/3.4 w/OIS(Periscope design)120mm eq.-Extra5MP Macro2MP DoFFront Camera16MPf/2.516MP (4MP 2.24µm)f/2.45I/OUSB-C 3.1USB-C 3.0Wireless (local)802.11ax WiFi-6EBluetooth 5.2 LE + NFC802.11ax WiFi 6Bluetooth 5.1 LE + NFCOther FeaturesDual SpeakersUnder-screen optical fingerprint sensorUnder-screen optical fingerprint sensorDual-SIMDual nanoSIMDual nanoSIMLaunch Price8+128GB: $749 / £649 / €74912+256GB: $849 / £739 / €8498+128GB: $499 / €49912+256GB: $599 / €599The Axon 30 as aforementioned is one of the rare devices this year released with the Snapdragon 870 SoC. As a reminder, the 870 is a higher binned variant of the Snapdragon 865 and 865+, this time increasing the CPU frequency on the fastest Cortex-A77 core to up to 3.2GHz – quite considerably faster than the 2.84GHz of the X1 cores of the Snapdragon 888, and still faster clocked than the 3GHz Snapdragon 888+. Due to the power efficiency regressions or non-progress of the Snapdragon 888’s manufacturing node, the Snapdragon 870 should still be a perfectly viable and competent SoC in a flagship device in 2021.The device comes in either 8+128 or 12+256GB options, similar to the Axon 30 Ultra. Other device internals of the phone are a bit more conservative versus the Ultra variant, such as the lack of WiFi 6E, only USB 3.0 connectivity, and some notable features lacking such as only a single bottom-firing speaker.The display is a very large 6.92” AMOLED at 2460 x 1080 resolution. It still features high refresh rate up to 120Hz, but even now increases the touch sampling rate to 360Hz to reduce latency and increase responsiveness.The phone is extremely large; at 77.8mm width it’s considerably wider than even most “large” devices out there, and definitely dwarfs the Axon 30 Ultra. Because of the quite thin side frame design and curved back glass, as well as the quite thin 7.8mm body, the phone does still manage to have good handling and does feel smaller than what it really is.The rear of the phone is defined by the camera setup: It’s mostly just a dual-camera device for all practical purposes. The main sensor is the same as on the Axon 30 Ultra, the IMX686 at 64MP resolution which bins down to 16MP 1.6µm in regular shots. The difference here is that the Axon 30 has a smaller f/1.8 aperture optics compared to the Axon 30 Ultra.Alongside the main camera there’s also a small 8MP ultra-wide unit with f/2.2 optics. There’s also a 5MP macro and a 2MP DoF camera – but these are generally irrelevant for photography in most use-cases. In general, the Axon 30 evidently puts a much lower focus on the camera system than the Axon 30 Ultra.The phone has one further pratical drawback compared to the Axon 30 Ultra and that is that it’s only featuring a single bottom firing speaker. The Axon 30 Ultra was able to use the earpiece speaker as a stereo unit, and although quality there wasn’t great at least it’s better than just a mono-speaker setup like on the Axon 30.Naturally, the most interesting aspect of the phone is the fact that it’s one of the very rare devices today with an under-screen camera. While this isn’t ZTE’s first foray into the technology, it’s definitely a much more mature and interesting implementation we’ll be taking a closer look at.The Axon 30 comes at much cheaper pricing than the Axon 30 Ultra, starting at only $/€499 – which very much puts it well below the usual flagship device price point, so we should adjust our expectations accordingly.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16925/the-axon-30-with-under-display-camera-handson-minireview\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Did IBM Just Preview The Future of Caches?\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-09-02T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16924/did-ibm-just-preview-the-future-of-caches\n",
      "Content: At Hot Chips last week, IBM announced its new mainframe Z processor. It’s a big interesting piece of kit that I want to do a wider piece on at some point, but there was one feature of that core design that I want to pluck out and focus on specifically. IBM Z is known for having big L3 caches, backed with a separate global L4 cache chip that operates as a cache between multiple sockets of processors – with the new Telum chip, IBM has done away with that – there’s no L4, but interestingly enough, there’s no L3. What they’ve done instead might be an indication of the future of on-chip cache design.Caches: A Brief PrimerAny modern processor has multiple levels of cache associated with it. These are separated by capacity, latency, and power – the fastest cache closest to the execution ports tends to be small, and then further out we have larger caches that are slightly slower, and then perhaps another cache before we hit main memory. Caches exist because the CPU core wants data NOW, and if it was all held in DRAM it would take 300+ cycles each time to fetch data.A modern CPU core will predict what data it needs in advance, bring it from DRAM into its caches, and then the core can grab it a lot faster when it needs it. Once the cache line is used, it is often ‘evicted’ from the closest level cache (L1) to the next level up (L2), or if that L2 cache is full, the oldest cache line in the L2 will be evicted to an L3 cache to make room. It means that if that data line is ever needed again, it isn’t too far away.An example of L1, L2, and a shared L3 on AMD's First Gen Zen processorsThere is also the scope of private and shared caches. A modern processor design has multiple cores, and inside those cores will be at least one private cache (the L1) that only that core has access to. Above that, a cache may either be a private cache still local to the core, or a shared cache, which any core can use. An Intel Coffee Lake processor for example has eight cores, and each core has a 256 KB private L2 cache, but chip wide there is a 16 MB shared L3 between all eight cores. This means that if a single core wants to, it can keep evicting data from its smaller L2 into the large L3 and have a pool of resources if that data wants to be reused. Not only this, but if a second core needs some of that data as well, they can find it in the shared L3 cache without having to write it out to main memory and grab it there. To complicate the matter, a 'shared' cache isn't necessarily shared between all cores, it might only be shared between a specific few.The end result is that caches help reduce time to execution, and bring in more data from main memory in case it is needed or as it is needed.TradeoffsWith that in mind, you might ask why we don’t see 1 GB L1 or L2 caches on a processor. It’s a perfectly valid question. There are a number of elements at play here, involving die area, utility, and latency.The die area is an easy one to tackle first – ultimately there may only be a defined space for each cache structure. When you design a core in silicon, there may be a best way to lay the components of the core out to have the fastest critical path. But the cache, especially the L1 cache, has to be close to where the data is needed. Designing that layout with a 4 KB L1 cache in mind is going to be very different if you want a large 128 KB L1 cache instead. So there is a tradeoff there – beyond the L1, the L2 cache is sometimes a large consumer of die space, and while it (usually) isn’t as constrained by the rest of the core design, it still has to be balanced with what is needed on the chip. Any large shared cache, whether it ends up as a level 2 cache or a level 3 cache, can often be the biggest part of the chip, depending on the process node used. Sometimes we only focus on the density of the logic transistors in the core, but with super large caches, perhaps the cache density is more important in what process node ends up being used.Utility is also a key factor – we mostly speak about general purpose processors here on AnandTech, especially those built on x86 for PCs and servers, or Arm for smartphones and servers, but there are lots of dedicated designs out there whose role is for a specific workload or task. If all a processor core needs to do is process data, for example a camera AI engine, then that workload is a well-defined problem. That means the workload can be modelled, and the size of the caches can be optimized to give the best performance/power. If the purpose of the cache is to bring data close to the core, then any time the data isn’t ready in the cache, it’s called a cache miss – the goal of any CPU design is to minimize cache misses in exchange for performance or power, and so with a well-defined workload, the core can be built around the caches needed for an optimum performance/cache miss ratio.Latency is also a large factor in how big caches are designed. The more cache you have, the longer it takes to access – not only because of the physical size (and distance away from the core), but because there’s more of it to search through. For example, small modern L1 caches can be accessed in as little as three cycles, whereas large modern L1 caches may be five cycles of latency. A small L2 cache can be as low as eight cycles, whereas a large L2 cache might be 19 cycles. There’s a lot more that goes into cache design than simply bigger equals slower, and all of the big CPU design companies will painstakingly work to shave those cycles down as much as possible, because often a latency saving in an L1 cache or an L2 cache offers good performance gains. But ultimately if you go bigger, you have to cater for the fact that the latency will often be larger, but your cache miss rate will be lower. This comes back to the previous paragraph talking about defined workloads. We see companies like AMD, Intel, Arm and others doing extensive workload analysis with their big customers to see what works best and how their core design should develop.So What Has IBM Done That is So Revolutionary?In the first paragraph, I mentioned that IBM Z is their big mainframe product – this is the big iron of the industry. It’s built better than your government-authorized nuclear bunker. These systems underpin the critical elements of society, such as infrastructure and banking. Downtime of these systems is measured in milliseconds per year, and they have fail safes and fail overs galore – with a financial transaction, when it is made, it has to be committed to all the right databaseswithout fail, or even in the event of physical failure along the chain.This is where IBM Z comes in. It’s incredibly niche, but has incredibly amazing design.In the previous generation z15 product, there was no concept of a 1 CPU = 1 system product. The base unit of IBM Z was a five processor system, using two different types of processor. Four Compute Processors (CP) each housed 12 cores and 256 MB of shared L3 cache in 696mm2built on 14nm running at 5.2 GHz. These four processors split into two pairs, but both pairs were also connected to a System Controller (SC), also 696mm2and on 14nm, but this system controller held 960 MB of shared L4 cache, for data between all four processors.Note that this system did not have a ‘global’ DRAM, and each Compute Processor had its own DDR backed equivalent memory. IBM would then combine this five processor ‘drawer’, with four others for a single system. That means a single IBM z15 system was 25 x 696mm2of silicon, 20 x 256 MB of L3 cache between them, but also 5 x 960 MB of L4 cache, connected in an all-to-all topology.IBM z15 is a beast. But the next generation IBM Z, called IBM Telum rather than IBM z16, takes a different approach to all that cache.IBM, Tell’em What To Do With CacheThe new system does away with the separate System Controller with the L4 cache. Instead we have what looks like a normal processor with eight cores. Built on Samsung 7nm and at 530mm2, IBM packages two processors together into one, and then puts four packages (eight CPUs, 64 cores) into a single unit. Four units make a system, for a total of 32 CPUs / 256 cores.On a single chip, we have eight cores. Each core has 32 MB of private L2 cache, which has a 19-cycle access latency. This is a long latency for an L2 cache, but it’s also 64x bigger than Zen 3's L2 cache, which is a 12-cycle latency.Looking at the chip design, all that space in the middle is L2 cache. There is no L3 cache. No physical shared L3 for all cores to access. Without a centralized cache chip as with z15, this would mean that in order for code that has some amount of shared data to work, it would need a round trip out to main memory, which is slow. But IBM has thought of this.The concept is that the L2 cache isn’t just an L2 cache. On the face of it, each L2 cache is indeed a private cache for each core, and 32 MB is stonkingly huge. But when it comes time for a cache line to be evicted from L2, either purposefully by the processor or due to needing to make room, rather than simply disappearing it tries to find space somewhere else on the chip. If it finds a space in a different core’s L2, it sits there, and gets tagged as an L3 cache line.What IBM has implemented here is the concept of shared virtual caches that exist inside private physical caches. That means the L2 cache and the L3 cache become the same physical thing, and that the cache can contain a mix of L2 and L3 cache lines as needed from all the different cores depending on the workload. This becomes important for cloud services (yes, IBM offers IBM Z in its cloud) where tenants do not need a full CPU, or for workloads that don’t scale exactly across cores.This means that the whole chip, with eight private 32 MB L2 caches, could also be considered as having a 256 MB shared ‘virtual’ L3 cache. In this instance, consider the equivalent for the consumer space: AMD’s Zen 3 chiplet has eight cores and 32 MB of L3 cache, and only 512 KB of private L2 cache per core. If it implemented a bigger L2/virtual L3 scheme like IBM, we would end up with 4.5 MB of private L2 cache per core, or 36 MB of shared virtual L3 per chiplet.This IBM Z scheme has the lucky advantage that if a core just happens to need data that sits in virtual L3, and that virtual L3 line just happens to be in its private L2, then the latency of 19 cycles is much lower than what a shared physical L3 cache would be (~35-55 cycle). However what is more likely is that the virtual L3 cache line needed is in the L2 cache of a different core, which IBM says incurs an average 12 nanosecond latency across its dual direction ring interconnect, which has a 320 GB/s bandwidth. 12 nanoseconds at 5.2 GHz is ~62 cycles, which is going to be slower than a physical L3 cache, but the larger L2 should mean less pressure on L3 use. But also because the size of L2 and L3 is so flexible and large, depending on the workload, overall latency should be lower and workload scope increased.But it doesn’t stop there. We have to go deeper.For IBM Telum, we have two chips in a package, four packages in a unit, four units in a system, for a total of 32 chips and 256 cores. Rather than having that external L4 cache chip, IBM is going a stage further and enabling that each private L2 cache can also house the equivalent of a virtual L4.This means that if a cache line is evicted from the virtual L3 on one chip, it will go find another chip in the system to live on, and be marked as a virtual L4 cache line.This means that from a singular core perspective, in a 256 core system, it has access to:32 MB of private L2 cache (19-cycle latency)256 MB of on-chip shared virtual L3 cache (+12ns latency)8192 MB / 8 GB of off-chip shared virtual L4 cache (+? latency)Technically from a single core perspective those numbers should probably be 32 MB / 224 MB / 7936 MB because a single core isn’t going to evict an L2 line into its own L2 and label it as L3, and so on.IBM states that using this virtual cache system, there is the equivalent of 1.5x more cache per core than the IBM z15, but also improved average latencies for data access. Overall IBM claims a per-socket performance improvement of >40%. Other benchmarks are not available at this time.How Is This Possible?Magic. Honestly, the first time I saw this I was a bit astounded as to what was actually going on.In the Q&A following the session, Dr. Christian Jacobi (Chief Architect of Z) said that the system is designed to keep track of data on a cache miss, uses broadcasts, and memory state bits are tracked for broadcasts to external chips. These go across the whole system, and when data arrives it makes sure it can be used and confirms that all other copies are invalidated before working on the data. In the slack channel as part of the event, he also stated that lots of cycle counting goes on!I’m going to stick with magic.Truth be told, a lot of work goes into something like this, and there’s likely still a lot of considerations to put forward to IBM about its operation, such as active power, or if caches be powered down in idle or even be excluded from accepting evictions altogether to guarantee performance consistency of a single core. It makes me think what might be relevant and possible in x86 land, or even with consumer devices.I’d be remiss in talking caches if I didn’t mention AMD’s upcoming V-cache technology, which is set to enable 96 MB of L3 cache per chiplet rather than 32 MB by adding a vertically stacked 64 MB L3 chiplet on top. But what would it mean to performance if that chiplet wasn’t L3, but considered an extra 8 MB of L2 per core instead, with the ability to accept virtual L3 cache lines?Ultimately I spoke with some industry peers about IBM’s virtual caching idea, with comments ranging from ‘it shouldn’t work well’ to ‘it’s complex’ and ‘if they can do it as stated, that’s kinda cool’.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16924/did-ibm-just-preview-the-future-of-caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Hot Chips 2021 Live Blog: New Tech (Infineon, EdgeQ, Samsung)\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-08-24T00:20:00Z\n",
      "URL: https://www.anandtech.com/show/16905/hot-chips-2021-live-blog-new-tech-infineon-edgeq-samsung\n",
      "Content: 08:22PM EDT- Welcome to Hot Chips! This is the annual conference all about the latest, greatest, and upcoming big silicon that gets us all excited. Stay tuned during Monday and Tuesday for our regular AnandTech Live Blogs.08:22PM EDT- Going to start here in about 10 minutes08:30PM EDT- Should just about to start08:32PM EDT- First up is Infineon08:32PM EDT- Next gen automotive challenges08:33PM EDT- Let's go climb a mountain08:33PM EDT- Literally drive up a mountain!08:34PM EDT- Evolving technologies - Battery, Sensing, AI08:35PM EDT- Adaptable architectures with high availability without any legacy impact08:35PM EDT- Machine Learning - workload specific compute08:35PM EDT- fast security accelerators for authentification08:36PM EDT- E-architecture evolution08:36PM EDT- Connectivity - logical attacks, spoofing - any connection out is an attack vector08:36PM EDT- Need fail-safe system08:38PM EDT- Moving towards future architectures with an ethernet backbone and a central computer08:38PM EDT- ALso helps reducing cost08:38PM EDT- Infineon Aurix and Tricore architecture08:38PM EDT- Designed around two decades ago - Tricore08:38PM EDT- Aurix in production since 2015, Tricore since 199508:39PM EDT- Adding modern features as time goes on08:39PM EDT- 500 MHz in latest gen08:39PM EDT- new accelerators - parallel processing, enhanced DSPs08:40PM EDT- ASIL D safety, security standards08:40PM EDT- Hardware isolation at the core level, 8 VMs per core and Hypervisor08:40PM EDT- Fine granular access protection, DMA protection08:41PM EDT- 2 x 5 Gbit ethernet, accelerated MACsec support, hardware acceleration for encryption08:41PM EDT- two PCIe 3.0 x1 lanes08:42PM EDT- Full CPU architectgure layout08:42PM EDT- six cores at 500 MHz08:42PM EDT- Debug and Trace08:42PM EDT- SIMD Vector DSP and Scalar core08:42PM EDT- ARC EV71FS Parallel Processing Unit08:43PM EDT- Software stack08:44PM EDT- Security - Security cluster08:57PM EDT- Supports automotive encryption, intrusion detection, physical or digital08:57PM EDT- Sorry, Internet cut out for 10 minutes, ISP went borked08:58PM EDT- Just in the Q&A section now of this talk. Going to cut losses, and just wait for the next talk in 2 minutes09:02PM EDT- Second talk is EdgeQ - Open RISC-V 5G Radio Access Networks09:03PM EDT- One of the emerging companies09:04PM EDT- First software programmable SoC for AI and 5G09:04PM EDT- 5G basestation on a single chip09:04PM EDT- 50+ SoCs launched, 2 billion modems shipped, $100b revenue generated09:05PM EDT- Was in stealth until end of last year09:05PM EDT- Next Generation RAN09:06PM EDT- Banding for 5G is important09:06PM EDT- Progression of 5G RAN over time09:07PM EDT- OpenRAN using Off-the-shelf hardware09:07PM EDT- Migration to a cloud native model09:08PM EDT- Central Unit, Distributed Unit, Radio Unit09:08PM EDT- Signal processing09:09PM EDT- Requires scheduling of users09:09PM EDT- Multiple RUs to one central unit09:10PM EDT- DU is a hybrid architecture - mixed special hardware or general hardware09:10PM EDT- What's needed is the open interfaces between each section09:11PM EDT- 5G programmable baseband DSP09:12PM EDT- one EdgeQ is in the Radio Unit09:12PM EDT- Distributed Unit has multiple EdgeQ chips for signal processing09:13PM EDT- Developing a converged SoC09:13PM EDT- Need a programmable DSP engine09:14PM EDT- RISC-V with 50+ custom instructions09:14PM EDT- eight-core Arm Neoverse CPU subsystem09:14PM EDT- Accelerators, IO subsystem, PCIe, USB, Ethernet09:14PM EDT- GNU Tool Chain09:14PM EDT- Massively parallel09:17PM EDT- Supports multiple configurations and is software upgradeable09:17PM EDT- beamforming, other intense operations09:18PM EDT- gang up to 4 chips for up to 40 Gbps09:18PM EDT- Life of a packet within a chip09:20PM EDT- 'Profound disruption in 5G and ORAN'09:20PM EDT- Sampling Now09:21PM EDT- Q&A time09:22PM EDT- Q: Process Node - A: Not disclosing public, but TSMC FinFet09:22PM EDT- Q: Neoverse cores? A: E1, at 2 GHz09:23PM EDT- Q: TDP range? A: Not disclosing. Base station unprecedented. Power is low. Very competitive for this implementation. Maybe in the teens09:23PM EDT- Q: RISC-V and Arm, What's the RISC-V base? A: Licence IP from Andes, but functionality is custom09:30PM EDT- Samsung time09:30PM EDT- HBM2-PIM09:32PM EDT- Been working with vendors on PIM for a while09:33PM EDT- What is PIM - rather than move data to teh CPU or accelerator for basic operations, do it right in memory09:33PM EDT- PIM proof of concept is difficult, only Samsung so far09:34PM EDT- Designed to be inserted into current solutions09:34PM EDT- Expanding the pyramid of storage09:35PM EDT- Aquabolt-XL, system level 1st gen PIM memry based on HBM2 Aquabolt09:36PM EDT- Memory bound workloads, such as AI09:37PM EDT- or perhaps crypto?09:37PM EDT- 2x system performance at 70% energy09:38PM EDT- PIM unit has 3 units09:38PM EDT- FP16 SIMD, controller, and register files09:39PM EDT- No additional timing impact on memory09:39PM EDT- Still Samsung specific, working with JEDEC for proper spec09:40PM EDT- Works by using current signalling techniques with no overhead09:41PM EDT- Use PIM library replacements for AI and recompile09:41PM EDT- Python, BLAS, GEMM09:42PM EDT- PIM execution blocks09:42PM EDT- HBM2 8Hi stack has 4 PIM + 4 HBM dies09:42PM EDT- Compute bandwidth is 1.23 TB/s and 4.92 TB/s off-chip + on-chip09:43PM EDT- Synthetic benchmark testing09:44PM EDT- Best performance gain on batch 109:44PM EDT- +5.4% power compared to regular HBM09:45PM EDT- Is that iso-capacity?09:46PM EDT- Evaluation with reduced power overall09:46PM EDT- Reduced overall system power and execution time09:46PM EDT- Natural Language Processing09:47PM EDT- Xilinx model with HBM2-PIM, coming September ?09:47PM EDT- U280+PIM test results09:48PM EDT- Neural Networks09:48PM EDT- 3.4x perf/watt09:49PM EDT- Can also be applied to LPDDR5, such as LPDDR5X-640009:49PM EDT- based on simulation results09:50PM EDT- Camera use cases09:51PM EDT- DIMM level PIM09:51PM EDT- DDR4/DDR5 compatible09:51PM EDT- Requires a buffer09:52PM EDT- AXDIMM buffer09:52PM EDT- Evaluation system09:53PM EDT- Those add-in boards look fun09:53PM EDT- can I have one09:53PM EDT- PoC on a Broadwell server09:54PM EDT- GDDR6 and HBM3 in the future09:55PM EDT- HBM3 will have FP16 and FP32, currently only INT8 and INT1609:55PM EDT- Trying to introduce JEDEC standard with HBM3 by initial spec at end of year09:55PM EDT- Q&A time09:56PM EDT- Q: How does PIM manage coherence with host? A: memory vision will be offload will not be cached, but those applications have low data reusability09:58PM EDT- Q: Does software need to know HBM-PIM is there? A: Yes need to recompile10:01PM EDT- Q: +5.4% power is iso-capacity A: Not answered and evaded10:02PM EDT- That's all for today! Come back tomorrow!\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16905/hot-chips-2021-live-blog-new-tech-infineon-edgeq-samsung\n",
      "Title: Hot Chips 2021 Live Blog: DPU + IPUs (Arm, NVIDIA, Intel)\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-08-23T21:20:00Z\n",
      "URL: https://www.anandtech.com/show/16903/hot-chips-2021-day-1-dpu-ipus-live-blog-arm-nvidia-intel\n",
      "Content: 05:27PM EDT- Welcome to Hot Chips! This is the annual conference all about the latest, greatest, and upcoming big silicon that gets us all excited. Stay tuned during Monday and Tuesday for our regular AnandTech Live Blogs.05:30PM EDT- Just waiting for this session to start, should be a couple of minutes05:30PM EDT- Arm up first with its Neoverse N2 cores05:34PM EDT- Roadmap, objectives, core architecture, system architecture, performance, conclusions05:34PM EDT- Second generation infrastructure followiung N105:34PM EDT- 4-128 core designs05:35PM EDT- 5G infrastructure to cloud data centers05:35PM EDT- Arm sells IP and definitions05:35PM EDT- SBSA/SBBR support05:36PM EDT- Marvell is already using N2, up to 36 in an SoC05:36PM EDT- High speed packet processing05:36PM EDT- All about SpecINT score with DDR5 and PCIe 5.005:37PM EDT- N2 with Arm v905:37PM EDT- Two lots of Scalable Vector Extensions, SVE, SVE205:37PM EDT- BF16 support, INT8 mul05:38PM EDT- Side channel security, SHA, SM3/405:38PM EDT- *SHA3/SHA51205:38PM EDT- Persistent memory support05:38PM EDT- memory partition and monitoring05:39PM EDT- Gen on gen improvements with virtualization05:39PM EDT- +40% IPC uplift05:39PM EDT- Similar power/area as N1, maximizes perf/Watt05:39PM EDT- an intense PPA trajectory05:40PM EDT- 3.6 GHz max core frequency05:40PM EDT- N1 on 7nm, vs N2 on 5nm05:41PM EDT- uArch - Most structures are biggers05:41PM EDT- bigger05:42PM EDT- Fetch more per cycle on the front end - increase branch prediction accuracy05:42PM EDT- Enhanced security to prevent side-channel05:43PM EDT- More bigger structures on the back end05:44PM EDT- N2 has Correlated Miss Caching (CMC) prefetching05:45PM EDT- Latency improvement on L2 as a result of CMC05:45PM EDT- 32% IPC improvement at iso-frequency05:46PM EDT- SPEC2006 was 40% mentioned earlier05:47PM EDT- Coherent Mesh Network - CMN700 - chiplets and multi-socket05:47PM EDT- Also CXL support05:48PM EDT- improvements over 600 - double mesh links, 3x cross sectional BW05:48PM EDT- Programmable hot-spot re-routing05:49PM EDT- Composable Datacenter SoCs - chiplets and IO dies and super home dies05:51PM EDT- balancing memory requests05:51PM EDT- control for capacity or bandwidth05:52PM EDT- Cbusy - throttling outstanding transactions to the CPU - affects hardware prefetcher aggressiveness05:53PM EDT- Cbusy and MPAM meant to work together05:54PM EDT- Resulting in best performance05:56PM EDT- Compared to the market with N205:56PM EDT- integer performance only05:57PM EDT- 'Real world workload' numbers based on pre-silicon models05:58PM EDT- Up to 256 cores of N2 should be fun05:59PM EDT- hit the market in the next few months05:59PM EDT- Q&A05:59PM EDT- Q: Is N1/N2 at iso-freq - what freq on slide 10? A: a range of power modes, quoted 2-2.5 GHz which is what customers will use06:01PM EDT- Q: Cbusy for a heterogeneous multi-die system? A: All IPs will get the CBusy information and throttle requests,06:03PM EDT- Q: MPAM cache partitioning? weight? A: It can do. But also support fine grain threshholds for control - you can tune based on capacity without overpartitioning06:03PM EDT- Second talk of the session - NVIDIA DPU06:04PM EDT- Idan Burstein, co-authored NVMoF06:04PM EDT- Architecture and platform use-cases06:05PM EDT- Data center is going through a revolution06:05PM EDT- Fully disaggregate your server between compute, memory, acceleration, storage, and software. Requires accelerated networking and DPUs to control it all06:06PM EDT- 10-20x bandwidth deployed per server requires better networking06:06PM EDT- a Datacenter infrastructure workload06:08PM EDT- Moving infrastructure workloads to the CPU is a bad idea06:08PM EDT- Need appropriate offload06:08PM EDT- Data pass acceleration needed06:09PM EDT- Bluefield-206:09PM EDT- Roadmap06:09PM EDT- Currently shipping BF-2, announced BF-3 with double bandwidth06:09PM EDT- BF-4 is 4x BF-306:09PM EDT- BF-4 also uses NVIDIA AI06:10PM EDT- 22 billion transistors06:10PM EDT- PCIe 5.0 x3206:10PM EDT- 400 Gb/s Crypto06:10PM EDT- 300 equivalent x86 cores06:10PM EDT- 16 cores of Arm A7806:10PM EDT- DDR5-5600, 128-bit bus06:11PM EDT- supports 18m IOPs06:11PM EDT- Connect-X 706:11PM EDT- DOCA Framework06:11PM EDT- Program on DOCA on BF-2, scales immediately to BF-3 and BF-406:12PM EDT- 3 different programmable engines06:12PM EDT- 16x Arm A78 - server level processor06:12PM EDT- 16 cores, 256 threads (SMT16?) datapath accelerator06:12PM EDT- ASAP - As soon as possible programmable packet processor flow pipeline06:13PM EDT- Bluefield-4X06:13PM EDT- Bluefield-3X06:13PM EDT- Not ASAP, ASAP-squared06:15PM EDT- Isolated boot domain in RT OS06:16PM EDT- PCIe tuned for DPU06:17PM EDT- Differentiating between the datapaths - software defined networking stack06:18PM EDT- accelerating the full path from host to network06:18PM EDT- it says bare metal host - can it do virtual hosts?06:19PM EDT- Encryption, Tunneling, NAT, Routing, QoS, Emulation06:19PM EDT- 100G DPDK06:19PM EDT- Million packets per second06:20PM EDT- vs AMD EPYC 7742 64-core06:20PM EDT- This is Bluefield 206:20PM EDT- TCP flow with 100G IPSEC06:22PM EDT- Storage processing acceleration06:22PM EDT- Data in-flight encryption06:24PM EDT- NVMe over Fabric06:27PM EDT- Cloud-native supercomputing with non-blocking MPI performancew06:27PM EDT- Accelerated FFT performance across multi-node HPC06:28PM EDT- DPU isolates Geforce Now - 10 million concurrent users06:28PM EDT- +50% more users per server06:28PM EDT- Push more concurrent users06:29PM EDT- Bluefield 3X has onboard GPU06:30PM EDT- Support for CUDA06:30PM EDT- GPU + DPU + network connectivity, fully programmable on a single PCIe card06:31PM EDT- Q&A Time06:31PM EDT- Q: Cortex A rather than N1/N2 A: A78 was the most performing core at the time06:31PM EDT- Q: Add CXL to future Bluefield? A: Can't comment. See CXL as important06:33PM EDT- Q: RT-OS cores? A: Designed internally, arch is RISC-V compatible06:34PM EDT- Q: Can DPU accelerate RAID construction? A: Yes it can - trivial and complex06:36PM EDT- That's the end, next up is Intel06:36PM EDT- Bradley Burres06:37PM EDT- Driving network across the datacenter for Intel06:38PM EDT- Same five minute intro about IPUs as the previous talks06:40PM EDT- IPU over time has been gaining more control to free up CPU resources. Move these workloads to the IPU = more performance!06:41PM EDT- 'solving the infrastructure tax'06:41PM EDT- Mount Evans06:41PM EDT- Developed with a CSP06:41PM EDT- Baidiu or JD ?06:42PM EDT- 16 Neoverse N1 cores06:42PM EDT- 200G Ethernet MAC06:42PM EDT- PCIe 4.0 x1606:42PM EDT- NVMe storage with Optane recognition06:42PM EDT- Advanced crypto and compression acceleration06:42PM EDT- Software, Hardware, Accelerator co-design06:43PM EDT- solving the long-tail infrastructure issue06:44PM EDT- Dataplane on the left, compute on the right06:44PM EDT- Support 4 socket systems with one Mount Evans06:44PM EDT- RDMA and ROCE v206:44PM EDT- QoS and telemetry up to 200 million packets per second06:45PM EDT- Inline IPSec06:46PM EDT- N1s at 3 GHz06:46PM EDT- three channels of dual mode LPDDR4 - 102 GB/s bandwidth06:46PM EDT- Engines for crypto06:47PM EDT- Intel didn't just glue assets together06:49PM EDT- P4 programmable pipeline06:51PM EDT- Most applications for IPU is 'brownfield' - has to be dropped in to current infrastructure06:54PM EDT- Now talking system security with isolation and recovery independent of workloads and tenants06:55PM EDT- QoS, uptime06:56PM EDT- malicious driver detection06:56PM EDT- Futureproofing06:56PM EDT- Compliant to NSA standards and FIPS140. Said something about 2030?06:57PM EDT- More info at the intel On event06:57PM EDT- Q&A Time06:58PM EDT- Q: PPA with Arm vs IA A: IP available and schedule picked Arm06:59PM EDT- Q: SBSA compliant? A: Yes06:59PM EDT- Q: TDP? A: work within PCIe power07:00PM EDT- Q: Work with SPR given both crypto? A: Yes07:00PM EDT- Q: Does Mount Evans replace server PCH? A: No, orthogonal07:02PM EDT- Q: specific to Xeon A: Use with any CPU07:02PM EDT- THat's a wrap, time for a keynote!07:02PM EDT- .\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16903/hot-chips-2021-day-1-dpu-ipus-live-blog-arm-nvidia-intel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Hot Chips 2021 Keynote Live Blog: Designing Chips with AI, Synopsys\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-08-23T19:20:00Z\n",
      "URL: https://www.anandtech.com/show/16902/hot-chips-2021-day-1-keynote-1-designing-chips-with-ai-synopsys\n",
      "Content: 03:26PM EDT- a few minute until the Synopsys keynote starts03:28PM EDT- Today's Keynote Speaker is CEO Aart de Geus about Machine Learning in EDA tools03:28PM EDT- Here's a preview03:28PM EDT-https://www.anandtech.com/show/16784/using-ai-to-build-processors-google-was-just-the-start-says-synopsys03:32PM EDT- 'If you know chips, you know Synopsys'03:33PM EDT- At the center of every advance in EDA since03:33PM EDT- Undergrad in Switzerland03:34PM EDT- Being able to see the world in different perspectives03:34PM EDT- Hot Chips is where you find the best architects03:35PM EDT- New SysMoore Era03:35PM EDT- The mission for the next 20 years - architecting intellegence03:35PM EDT- Architecting of smarts03:35PM EDT- Smart drive impactg03:36PM EDT- Smart Everything03:36PM EDT- Ideas and engineering -duct tape and WD4003:37PM EDT- Impact of printing press - 20m books after printed press was 'redescovered'03:38PM EDT- Software and Silicon03:39PM EDT- Add in Thermal effects03:39PM EDT- Solving systemic issues, security, silicon life cycle, functionality03:39PM EDT- balancing all the requirements03:39PM EDT- Applying the vision with optionality in the middle03:39PM EDT- Techonomics03:40PM EDT- ML algorithms need to be computing in time03:40PM EDT- Growth in the amount of data03:40PM EDT- 'Can we do that AI thing'03:41PM EDT- Data generated by humans is minmal03:41PM EDT- compared what's generated by machines03:42PM EDT- Using AI to be more effective with system requests and requirements03:42PM EDT- Systemic Complexity vs Exponential Ambition = Sysmoore03:43PM EDT- Systemic complexity vs scale complexity - we've solved scale complexity03:44PM EDT- replacing no because with yes if03:44PM EDT- Moore's Law doesn't always mean smaller devices03:45PM EDT- Angrsrom has the word Angst in it!03:45PM EDT- A booster pack to Moore's Law03:45PM EDT- System of Chips03:45PM EDT- AI is driving a rethink in architectures03:45PM EDT- Function and form03:46PM EDT- Form must follow function03:48PM EDT- Sorry the animations here keep overwriting each other03:48PM EDT- Talking about architecture and development03:49PM EDT- blocks vs construction vs time - all about the next level of abstraction03:49PM EDT- Levels of abstraction03:51PM EDT- Now time for EDA03:51PM EDT- IP reuse03:51PM EDT- Integrating ML into EDA03:52PM EDT- Now for circuit simulation03:54PM EDT- Developing AI for EDA03:54PM EDT- Using AI to do more03:54PM EDT- AI enables better automation - EDA already has lots of automation03:54PM EDT- Arm enabled IP reuse03:55PM EDT- looking hardware at latency and capacity of communication03:55PM EDT- 4nm in production today03:56PM EDT- Fusion compiler - 500 cases03:56PM EDT- Everyone of Synopsys tools has some form of machine learning03:57PM EDT- Can you enable ML on design flow03:57PM EDT- Using you enable autonomous design? YES03:58PM EDT- But it gets harder with AI03:58PM EDT- Moving form win/lose to better/worse04:00PM EDT- Design Space Optimization, not just exploration04:00PM EDT- using everything they've learned in the past04:00PM EDT- results from real designs04:00PM EDT- This is in december 1904:01PM EDT- Best result is well above any human design04:01PM EDT- Tested 10-100x more than a human design team04:02PM EDT- This is untrained04:02PM EDT- Using a trained version04:03PM EDT- Here's another example of a customer result04:04PM EDT- Single engineer vs a team04:04PM EDT- A third example04:05PM EDT- Learning from final results04:07PM EDT- Free t-shirt on the QR code04:07PM EDT- with the diagram in04:07PM EDT- Using DSO.ai at the microarchitecture level04:08PM EDT- Can you get AI to make microarch adjustments?04:08PM EDT- Architecture searches04:08PM EDT- with cells04:09PM EDT- Using AI to optimize for a software workload04:10PM EDT- Software design might blow out a physical design e.g. dynamic power peaks04:11PM EDT- SW power emulators04:11PM EDT- Simulate millions of cycles of software on a design04:12PM EDT- Analyze software power hotspots04:12PM EDT- Using AI to optimize for thermal issues, switching, or errata04:13PM EDT- Everything in the chain matters04:14PM EDT- 25% better power by simulating software workload04:14PM EDT- optimization in design by analyzing software04:16PM EDT- Now going back to buildings04:17PM EDT- Beauty in chip design04:18PM EDT- Understanding what to right to do something04:18PM EDT- all about trust and teamwork04:19PM EDT- Using EDA to develop chips to run better EDA tools to design better chips04:19PM EDT- One of the greatest chips Aart has ever seen04:20PM EDT- Great engineers are great artists04:20PM EDT- 'Be catalysts of the impossible'04:21PM EDT- Q&A Time04:24PM EDT- Q: Can ML help with Spec compliance with IP blocks? A: Yes, depends on the model. Coverage is a function of how many errors you can detect. knowing your test structures, learn from patterns to how to stimulate your devices. Intersection of verification and optimization04:24PM EDT- Q: How long does DSO take to converge / resources are needed in the examples? A: It's a function of the size of the complexity. 20 cores ran from days to a week04:25PM EDT- Q: What does DSO need through input? A: RTL, areas of focus, every variable you need04:27PM EDT- Q: Will constaints become suggestions? A: Yes in that direction - you will learn where to trust where making something less fixed will end with a better result.04:28PM EDT- Q: What ML tools did Synopsys use. A: Secret sauce!04:30PM EDT- Q: Do we need new description languages aside from RTL? A: Yes, but you're the pro! Whenever you can optimize the lower steps, it moves you up the stack is architect - in you can enrich the language on the optionality, this is how you will never be happy but you will make it better04:32PM EDT- That's a wrap\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16902/hot-chips-2021-day-1-keynote-1-designing-chips-with-ai-synopsys\n",
      "Title: An AnandTech Interview with Jim Anderson, CEO of Lattice Semiconductor\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-08-20T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16879/an-anandtech-interview-with-jim-anderson-ceo-of-lattice-semiconductor\n",
      "Content: In our coverage of the semiconductor space, we typically think of two main vectors of hardware – the CPU and the GPU. Beyond that, we look at FPGAs, microcontrollers, and this decade is bringing the advent of the dedicated AI processor. What ties all of these products together is actually the FPGA - a field programmable gate array that allows a skilled technician to essentially build a custom circuit out of configurable gates. This means an FPGA can be used to design and simulate a full CPU or GPU, but also an FPGA offers a reconfigurable way to offer optimized compute power that adapts to the needs of its users without the cost of millions or tens of millions to design dedicated silicon. One of the first FPGA companies on the market was Lattice Semiconductor, which now focuses on small power efficient FPGA designs that end up in everything from consumer devices to servers.We’ve been loosely following Lattice for a number of years, however three years ago the company went through a bit of a change. It hired Jim Anderson, the then AMD SVP of Computing and Graphics who had overseen the launch of Ryzen, the brand of processors that has re-energized the company from near bankruptcy to a number of years of extended market share growth and profitability. Jim and I met frequently at AMD events and we spoke at depth at the state of the consumer product landscape as well as how the semiconductor space was evolving. When it was announced he took the role of CEO at Lattice, I was a little taken aback, but glad that he had found a new challenge the complemented his background in semiconductor design and expertise.Over those three years at Lattice, Jim has initiated a cultural shift that is playing out in the company roadmaps – new products, a more agile approach, and a need to focus on enabling machine learning at every part of its product stack. The recent financial disclosures at Lattice show an increasing demand for its hardware, as well as the company making strides to double its addressable market over the next five years. I thought this would be a good time to reconnect with Jim to find out exactly what he’s doing at Lattice to earmark the next generation of growth at this foundational FPGA company.Jim AndersonCEO, Lattice SemiconductorDr. Ian CutressAnandTechFrom a career standpoint, Jim has a lot of experience. Academically he holds a MS in EE and CS from MIT and an MBA from MIT, and in his career he spent 8 years at Intel as a CPU architect on Xeon and Itanium along with strategic planning, 8 years at LSI in strategic planning and marketing of network components, a year at Axxia/Intel as the GM of Networking, and then 3 years at AMD leading up Ryzen. Now he is three years into the role of CEO at Lattice.Ian Cutress: You’ve now been CEO of Lattice for three years, and coming from the position of SVP of AMD’s Computing and Graphics business group, what made you make the jump from consumer hardware to power-efficient FPGAs?Jim Anderson: Well I loved it at AMD. I was super happy there - I love the people at AMD. Great people, really innovative, very determined. I'm a product guy - I always have passion about the product, and in particular that product we were launching that day back in Italy, I was always super excited about. I love the products.But when Lattice reached out to me, it was kind of hard to pass up the opportunity, because Lattice is a company that's one of the original founding FPGA companies from the early 80s. It's a company that's been around for about 40 years, it's got a great history of innovation, and it’s kind of a unique place that innovates and innovates around small and really power efficient FPGAs. Those devices go into all sorts of applications across many different markets. It is a great company. I felt like if I joined Lattice that I could help it get on a stronger path moving forward, help it build better products, and build products more quickly.So that's kind of what attracted me to Lattice and the last three years have been a lot of fun. We've completely rebuilt the product line at Lattice, we're on a great trajectory, and actually, I'm even more excited about the years to come. We have some really great products on the roadmap.IC: Most users who know about Lattice understand that it's an FPGA company, but when most of us speak about FPGAs, we refer to the big names in the industry: Xilinx and Altera (which is now part of Intel). Can you talk about how Lattice positions itself in the market compared to the big names, and why that matters?JA: Yeah, absolutely. First of all let me point out that if you look at unit volume size, actually Lattice is the highest unit volume manufacturer of FPGAs. We're the biggest by unit volume. Where we specialize is in small size, power efficient, and very, very easy to use FPGAs. Because they're really power efficient, and they're small size, they can go into all sorts of different applications that really large FPGA just can't go in. So either from a physical space constraint or a power constraint, our devices can reach all sorts of applications that our competitors can't, and so you'll find us in industrial, all sorts of industrial IoT applications, industrial automation, or robotics. You'll find us in communications, computing, consumer devices, and automotive electronics as well. So we really specialise in the small power efficient part of the market.Those two competitors that you mentioned, which are traditional competitors, they really focus on the very large, very high power, complex FPGAs. We focus on the small size and power efficient. You would be amazed at the number of applications we go into. Whenever I meet with a new customer, I'm always amazed at some of the new innovative crazy applications they're using our chips in.To give you a sense of the size, this is one of our smallest FPGAs on the end of tweezers here.I'm showing it on the end of the tweezers, and this is about 1.4 millimeters square. Obviously we make some bigger devices as well, but devices this size can go into applications that other chips can’t, where there are either size or power constraints.Since you mentioned the last time we met three years ago, if we compare our FPGA to this Threadripper chip from AMD, [this is what it looks like].So I've now moved to the total other end of the spectrum! So myself and the Lattice team are really innovating at the other end of the spectrum: small, power efficient, easy to use, and like I said, important to lots of different applications.IC: Your PR team sent a strip of those tiny FPGAs to me. When I opened it, I wondered what they were! I couldn’t see them - I asked myself why have they sent me a bit of plastic? I had to get the zoom macro on my smartphone to kind of see them, and even then I still can't read what's printed on it.JA: When I first started looking more closely at Lattice before I joined, that was one of the things that I found really fascinating. I spent a lot of my career working at the other end of the spectrum on big high performance CPUs and things like that, so to work at the other end of the spectrum on the smaller size, power efficient FPGAs I thought was pretty fascinating. Well, I'm glad you got the samples!These@latticesemiFPGAs that just arrived in the mail are smaller than 1mm x 1mm. I can hardly see them without a magnifying glass of some sort.If I try and bite into these, they'll get consumed. Maybe I can use them as sprinkles.pic.twitter.com/BHbxrH0HbQ— 𝐷𝑟. 𝐼𝑎𝑛 𝐶𝑢𝑡𝑟𝑒𝑠𝑠 (@IanCutress)July 28, 2021IC: You've been CEO at Lattice for three years now, and I think most of our audience may recognize you from the three years you spent at AMD in charge of the client division as Ryzen was first launched. I've looked through your history and you've got experience as a CPU architect at Intel, strategic planning and marketing on networking silicon for LSI, and your academic background is a Masters in EE from MIT. What ends up drawing you from architecture, to planning, to networking, to processors and graphics, and now to Lattice?JA: The way that you said that, it makes it sound like I can't hold down a job, right?! [laughs]I would say the common thread through my career is always that I've worked in the semiconductor industry - but you're right, and I've worked in a number of different functions. I started as a CPU architect at Intel working on Xeon and Itanium chips. But I’ve worked on multicore DSP chips, really complicated network processors used in communications, lots of ASICs, CPUs for client devices, graphics, and now FPGAs. So the common thread is always within the semiconductor industry. I think that, for me, the semi industry is really exciting, because it's basically the foundational layer of the entire tech industry. It is the fundamental substrate of the rest of the industry and it's pervasive in our lives. Anytime you touch an electronic device, you're touching the semiconductor industry in some way. So I'm just fascinated by the whole spectrum of devices built in the semi industry, and so now the opportunity to work on FPGAs at Lattice. The semi industry is great, and I like all parts of it.IC: Every time I seem to mention FPGAs in my work, comments always arise as to 'why not just build an ASIC?'. Can you explain why Lattice's customers, or anyone that uses an FPGA, chooses to do so over dedicated hardware or a software solution?JA: There are both technical reasons and there are also economic reasons.On the technical side, a lot of times our customers are trying to innovate on their system designs. They're trying to figure out how they can add new features or new capabilities that differentiate them in the marketplace. A lot of times [the solution is] an FPGA, which is incredibly customizable and adaptable. It can be a key part of the system that allows the customer to really customize and adapt their system. Then one of the benefits of the FPGA is not just that you can customize it for exactly what you need, versus say a standard product, but you can reprogram it over the lifetime of the system. So let's say, as your market changes or as you want to incorporate new features, [it can be updated]. Actually, we have a lot of customers that run artificial intelligence algorithms on our FPGAs. Those algorithms are constantly evolving, and so the fact that they could just reprogram the FPGA as the evolution of the AI algorithm changes, that's a big benefit. It provides future proofing for the platform, and so that's a big reason why customers design in FPGAs.Another reason [to use an FPGA] that you mentioned is ASICs, or rather the difficulty in scaling them. If you're going to build a truly custom chip for your application, that takes 18 to 24 months minimum to go build that chip, right? From the initial architectural concept, to when you have something that's production ready. Your needs could have easily changed over those two years that it takes to develop that chip - whereas with an FPGA, you customise it right away for exactly what you need. If the needs change over those one to two years, no problem - you just reprogram the thing. Our chips are really power efficient and size optimized. A lot of times there's not a big advantage to a custom chip, in terms of power or cost.Then kind of on the economic side, look at the expense of doing a fully custom chip today. You know that expense has escalated incredibly over the last 20+ years I've been in the industry. It used to be inexpensive to create your own custom chip, you know – it is way more expensive now, both in terms of development effort, mask costs, etc. So it's really seldom that it actually makes economic sense, especially with the type of FPGAs we have, and the cost points that we can hit, that there's really no economic benefit as well.So for all those reasons, if you look at the history of the FPGA industry, it's grown as fast as the semiconductor industry in total, or actually faster in many years and in many cases. So that's why you see, over the past 40 years, FPGAs have continued to grow, and that the market is very healthy as customers continue to adopt that and in all sorts of applications.IC: Where can most end-users expect to find a Lattice-based product in their lives today?JA: I think if you're using any electronic device, [you’re probably interacting] with a Lattice device somehow during your daily life, either directly, or maybe indirectly. For instance, a data centre or some of the industrial stuff that we do, for instance, in servers. Now, if you look in servers, either enterprise class servers or servers that are in big hyperscale data centres, obviously end users are accessing data and data centres such as big hyperscale data centres all the time. In those you would find a Lattice chip. In almost every new server today, way over 80% of servers have at least one Lattice chip, if not more. Those chips are doing control management of the platform and security as well.You're now starting to find Lattice FPGAs in client computing devices, where we're providing a number of kinds of new functionalities. Also in communications infrastructure, so either wireline or wireless infrastructure like the 5G infrastructure, lots of devices use Lattice. Then all sorts of consumer electronics, high-end audio systems, home automation systems, automotive electronics - I'm trying to make sure I run through the whole list here! But I think that gives you a sense of it all.Ultimately we have over 9000 customers, and if you look over the last four years, we've shipped about a billion devices. If you think about that, about a billion lattice chips in all sorts of applications, you’ll find us all over the place.IC: When I spoke to Esam Elashmawi a few weeks ago, he explained to me that Lattice is having an ever present increase in the server market, having silicon in around 20% of servers a few years back to around 80% today. When I review the hardware, I’ve always noticed the Lattice logo there, but I didn't realize how expansive Lattice's growth in that market has been. Why exactly are we seeing Lattice silicon becoming a vital part of the enterprise motherboard market?JA: Yeah, great question! It's definitely been a big growth area for us. Our position in servers has grown considerably over the last few years, and we expect it to continue to grow. As Elam said, if you go back two or three years, about 20% of servers shipped with a Lattice piece of silicon. A few years ago they were doing more basic tasks, sort of power management or basic control functions on the server platform. Now, if you zoom forward to today, over 80% of servers today in the latest generation that's in volume, all ship with at least one piece of Lattice silicon doing more control and management of the server platform. Moving forward, we're doing more security functionality as well.It used to be that, years ago, people were just worried about security more at the software layer. But now, there's concern of security all the way down to the hardware platform. So we have specific Lattice devices that are designed to provide what's called platform security or platform resilience. So what our devices do is they go to and check to make sure for instance, as your servers booting up, that the hardware itself hasn't been corrupted, or also the firmware hasn't been corrupted. So it will look at the firmware version before the system starts up, verify that it's the correct firmware version, and that nobody's corrupted or loaded the wrong piece of firmware. Then if it detects a wrong piece of firmware, because we have a golden copy stored within the solid state memory within the Lattice chip, it will actually swap and repair that firmware. That's just one example of the kind of new capabilities that we're bringing to a server. But if you look at today's servers, over 80% ship with Lattice silicon.In the next generation that's just starting to ramp, our attach rate will actually start to exceed 1x, meaning that on average server platforms will have multiple Lattice chips used. So when that happens we'll actually be shipping more chips into the server market than the total number of servers that are shipped! Also, our ASPs (average selling price) continues to grow upwards, because we continue to bring more value to each server generation. So this has been a great growth area for us. Based on the multi-generational discussions we're having with our customers, we expect it to continue to be a good growth area.By the way, I should mention, because I figured you might ask based on my history, is that we are CPU agnostic. Lattice supports both Intel and AMD platforms, and in fact, we support ARM based servers today. That's actually an advantage for our customers, and that's important that we're able to service all architectures, so we're totally agnostic.IC: I was going to ask then - do you make very specific versions of your FPGAs for AMD, for Intel, or even for Ampere? Or do they just take the ones off the shelf and optimize it how they want?JA: That's exactly right, [the can take the ones off the shelf] and that's the beauty of it. That one FPGA design can then be customized by server OEM or even each server customer, because they want it to have their unique value addition. Our server customers are putting their own custom customization into the FPGA, and that helps them customize or differentiate their platform, but the beauty is that we can use a single device to service that.IC: One of the things I keep noticing on Lattice financial calls is a mention of a culture shift internally around the 2018 timeframe, which just happens to be about the time you took the role of CEO! Can you explain what was in place at that time, and how you've adjusted Lattice to be to what looks like path to sustained growth / what that sort of culture shift looks like? I think you alluded to aligning the product roadmap, but does it go beyond that?JA: I think it goes beyond that. I think there has been a pretty significant cultural change at Lattice over the last few years. When I joined in September of 2018, shortly after that, I brought on a new leadership team. We went and recruited for all the kind of key functions like engineering, sales, marketing, and supply chain. [We recruited] deep industry veterans, people that have been in the FPGA industry not just for a few years, but for decades and have multiple decades of experience. So we rebuilt the leadership team with industry experts, and then we did make a pretty big cultural shift at the company.I will say there were a couple of cultural attributes that Lattice has always had, that we definitely have encouraged and we've maintained. For example, Lattice has always been very customer centric, and in fact if you ask our customers they'll tell you one of the reasons they love working with Lattice is because we've always been attuned to our customers. So we're definitely continuing to encourage that, but also Lattice is a really collaborative place. We collaborative internally, and the groups work really well together, but we also collaborate really well with our customers.We're encouraging those cultural attributes, but we did encourage or bring some new attributes - one of them is around speed and agility, which I feel like in the tech industry is really important. It can become a really important competitive advantage. I think that speed and agility matter a lot more than the size of the company, or [compared to] the size of the resources. I think the ability to go fast, get products to market fast, and to adapt quickly is absolutely a competitive advantage.That's something we've really encouraged since day one, since I've joined. I think you can see that play out in our product roadmap - if you look at the number of new products we brought out over the last two years versus previous Lattice history, we've tripled the rate of new products that we're bringing to market. Our cadence is three times faster than it used to be. That's great for our customers - they love it because they've got new and fresh products coming out all the time from Lattice. So I think that's a marker of that cultural shift towards more speed and agility.Then the other one I would say that we really encouraged was innovation. We always say innovation, and everybody says that, but I would say it's about being bold about the innovative steps that you’re willing to take. We are being much more bold about the future innovation that we're driving. So I think that's something we've really been encouraging the team to do - be much more bold in terms of their thoughts about where we drive the technology and the product roadmap.The cultural shift has been a big part of it when I think about the progress that we’ve made. But the other piece that I would say, that we sort of mentioned early on like I said, is that you know I'm a product guy. At the end of the day our customers, the one thing they care most about is our products. So right away, in the first six months after I joined, we totally rebuilt the product roadmap and rebuilt it for not just the next year or two, but the next 5+ years. It’s completely rebuilt, really jacked up where we were headed in terms of the performance and the capabilities and the features. We're now really starting to see the benefits of that in this year, and in the years to come.IC: I did want to talk about product roadmaps because you've kind of been slowly announcing how you've realigned your products over the last few months. Going from one or two products a year, a very linear cadence, there are now multiple derivatives in a parallel design flow. You've also said this helps double the potential addressable markets from $3 billion to $6 billion. Aside from just raw revenue dollars, what's the goal here? How does an FPGA company innovate? Is it more about explicit customer demand, or is it more pathfinding?JA: It's always customer centric, our innovation. We always say this inside of Lattice - our innovation is always customer centric and market application centric. All the innovation that we're doing is to try to solve a customer problem or enable a new application - that's really what our innovation is directed to. We did make a big change over the last two to three years in how we do that innovation. From a product roadmap perspective, we used to be very serial in how we build products. We would build one product with one architecture, and then the next product would have a new architecture. We were very sequential.What we've done now is take a platform approach, both from the hardware and the software perspective, and design each FPGA platform from the very beginning with the mind that we would build multiple derivatives. We would also be building versions of that FPGA platform that were say optimized around particular applications or customer needs.So that platform approach has allowed us to dramatically speed up the number of new products that we can bring to market, and then it has allowed us to do new products that are more optimized for particular applications. One example of that would be security. We’re now developing FPGAs that are optimized for doing platform security. The other examples would be FPGAs that are more optimized for doing artificial intelligence processing at the edge of the network - inference processing in edge applications. So that platform approach has helped us to increase the number of products, but also helped us really tune our products for particular applications, especially the really big growth applications, or the applications that meet the needs of our big customers.IC: Does optimization in that sense always mean taking some of the logic gates away and building, say, a hardened crypto accelerator, or a hardened AI accelerator? Does it go beyond that?JA: It can be that, but we may also tune [the frequency and bandwidth of] the FPGA fabric architecture, or the implementation itself, to be better aligned to a particular application or needs. We use a variety of different techniques.IC: That's through profiling customer workflows? You get together with customers to understand what they're doing, and what they're trying to solve?JA: Exactly! We spend a lot of time with our customers making sure we understand their needs, not just for the next 12 months, but for the next 3+ years. We then map those customer needs, or application needs, back to the particular FPGA architectures [that they are interested in].IC: You mentioned earlier about client computing - you're very much in the sense of promoting FPGAs to help with AI accelerated camera analysis, people's webcams on devices, that sort of thing. Where exactly are we expecting to see Lattice in client computing in that regard?JA: Client computing has been a growth area for us over the last few years - our client computing falls within our computing and communications segment, and that segment has been growing really well. It grew double digits last year, and it grew another 15% year over year in the most recent quarter so. Client computing has been a big contributor to that. There were a couple of new significant customer platforms that began ramping into production last year, and then are ramping into full production this year. And then we're engaged with a number of OEMs, and new client computing platforms.When we say client computing, we're talking about basically PCs and tablets, those types of devices. But some of the interesting applications that we're getting used in are things like artificial intelligence. For instance, let's say I'm looking at my laptop, I'm doing work, but I turn away and I look away - maybe I'm having a conversation with somebody for a few minutes and I'm not looking at the laptop. Our device will be analyzing the video signal, and it will detect that you're still in front of the laptop, but that you've looked away. It will detect that using AI algorithms, and then the laptop screen will dim to save the battery power. Then as soon as you look back, it'll bring the screen back up. That may seem like not a big deal, but when you start to add up the power savings, the screen on the laptop burns a tremendous amount of power. If you save all of that battery life over the course of a full day, you can save quite a bit, and drive quite a bit of battery efficiency.The other thing that our devices or FPGAs can do is watch the video input, and let's say somebody comes up behind you and they are shoulder surfing by looking over your shoulder at what you're working on. It can detect that there is somebody else in the frame, and then put a little red dot on screen or whatever to notify you that somebody is behind you, watching what you're doing. There's a bunch of other examples as well, but we can do this because our devices are small and really power efficient - we can do this at incredibly low levels of power.Then another example would be security, some of that same security that we talked about on server devices is also applicable to client devices. So that's another potential application - but you know, one of the things we really like about this market is that if you look at the server space, which we already talked about, we've done really well and really proliferated Lattice across the server space. But the great thing about the client computing market is it's 20 times larger in terms of unit size. There are 20 clients for every one server, and so to us that's just a huge TAM opportunity to bring Lattice devices into that market and enable all new sorts of functionality and capability for end users.IC: So the demonstration about looking away and having people over your shoulder, there is one company who has demonstrated that today, and that would be Intel. Are you working with Intel on that? Can I make that connection?JA: [laughs] Well, what I would say is that we would view Intel in the PC segment as a strategic partner. We would certainly work with them on enabling those types of experiences. What I will point out is that our devices can do that functionality at ridiculously low power, so we have a big power efficiency advantage, and on a laptop device as you know power efficiency is a premium. So our devices can do that at a level that I don't think anyone else in the industry can. We would view Intel as a potential platform partner there along with everybody else in the ecosystem.IC: That's a very crafty way of saying no. I love it!IC: It makes me question with that sort of topic whether you would actually be working with the platform developer, or perhaps that would be more sort of an OEM enablement strategy - maybe it's something that the OEM wants to do, on top of a platform provided by a higher level partner?JA: We would work with both - we do two things. We'd work directly with the OEM that's building the platform - we'd certainly have engaged with them. But then we'd also be working with the ecosystem as well to make sure that our parts are interoperable and work with the rest of the ecosystem partners, so we'd been doing both in parallel.IC: One of the things with FPGAs - I get a lot of feedback for is that they're just hard to develop for. You need to know how to use them before you use them, which sounds like the wrong way to learn how to code! What exactly is Lattice doing to kind of ease that transition for people who may understand software, but are kind of new to the hardware?JA: This is really one of our main mantras at Lattice. [We want] to make the use of our devices as easy as possible. So this is something we've driven a tremendous amount of improvement, just over the last few years, to make the devices very easy to use. This is not just for people that are familiar with FPGAs, but for developers that may have never used an FPGA in their entire career.So one of the big things that we've been doing over the last two to three years is developing what we call ‘application specific software solution stacks’. So think about these as pre-built tools and libraries that a customer can take off the shelf and use that to abstract out the complexity of an FPGA. It allows the customer to use our devices at a level of abstraction that they're comfortable with.This is something we've been investing a tremendous amount of effort into, and we built out a pretty significant portfolio of these applications solutions and they make it very easy for the customer to adopt our devices into their systems so they can get the innovation. They can get access to the innovation that we're driving and get that easily, but also get to market much more quickly. Also, even if a customer is familiar with FPGAs, it can help them switch from a competitor's FPGA to our FPGA, so it can ease that switching or transition.We're building out a portfolio of these stacks, and we brought four of them to market to date. The first one that we developed was called SenseAI. And that's a software stack that's specifically for artificial intelligence, for doing inference processing especially in edge applications. Whether those are consumer applications or industrial applications, it really helps enable inference processing on our devices at the edge of the network. That was the first one.The second one was around embedded vision processing, and enabling embedded vision processing on our devices. The third one and we kind of touched on this earlier was actually around security and platform hardware security, making it really easy to use our devices for doing hardware security. Then the fourth one, which we just launched in May, was around factory automation. This is making it easy to design Lattice devices for doing all sorts of industrial automation tasks, as well as robotics.So each one of these are, like I said, a pre-built solution that the customer can take and use as is, orif they want to customize it, they can do that. But it has really lowered the barrier, or reduced the effort necessary, to design devices into systems. We're going to continue to build out, we've got additional software stacks on the roadmap, and we'll continue to build out a wider portfolio here.IC: To explain it my way, that would be like having precompiled libraries that manage the FPGA, and all you do is you call the library and the functions they're in and it does it automatically?JA: That would be a good analogy!IC: So we spoke about the communications and compute segment earlier - Lattice's biggest market now is communications and compute, and it's not often we get a chance to direct users to think about what telecommunication companies (telcos) need out of their silicon. What makes Lattice FPGAs the right fit for telcos, and how are they using them?JA: It's not just Lattice FPGAs, but the communications industry has a long history of using FPGAs in all sorts of applications. It’s not just wireless applications, which you're asking about, but wireline as well. You will find FPGAs used in all sorts of different places in communications.In wireless applications, like the new 5G infrastructure that's being built out, you find Lattice devices in the control plane. If you look at a base station, which is basically processing at the bottom of the tower, and then the antenna at the top of the tower, you find Lattice devices in the baseband unit at the bottom of the tower, but also in the radio heads that are at the top of the tower, and the towers which are transmitting the signals. What we're doing there is controlling the management of the system, the power management, and then some security functionality moving forward as well.A lot of times the reason is that in communications, especially wireless infrastructure, that FPGAs get used as we kind of think of 5G simplistically as one monolithic standard worldwide. That's not the case - there are umbrellas of standards that exist, and each region has different frequencies, different frequency bands, different local customizations, and then also the 5G or any wireless standard is evolving over time as well. A wireless infrastructure OEM may start to design a system before the 5G standard is fully completed, so they build FPGAs in because they need the ability to customize for, let's say, different geographies. They can reprogram the FPGA to adapt the system for different geographies with different unique requirements, rather than building a dedicated hardware system for each geography. Or if the standard evolves over time, there are new capabilities that they have to integrate into the system. They can reprogram the FPGA to adapt and include that new functionality in the system.So it's a combination of the flexibility, as well as the adaptability, and some future proofing. That's why you see FPGAs get used. The 5G infrastructure has been a great growth area for us - we were really early in the 5G build out worldwide. You know, if you try to get a 5G signal, a true 5G signal, you know that there's a lot of build out that needs to happen, especially in North America and Europe. We're still early in that, and it is a great growth area for Lattice.IC: You mentioned the control plane in 5G, and there's also the data plane - the telcos use FPGAs initially in the data plane to do all the data compute, and then replace it with their own ASIC over time. In the control plane, FPGAs tend not to be replaced - that kind of sustains your growth in that market? Is that fair to say?JA: Yeah, that's exactly right. We're usually designed into the control plane. In the data plane, where the data is flowing through, that's where the primary data stream is flowing through the system. When a system is initially launched, the large FPGAs designed by our two traditional competitors may get used initially in those systems, and they may get replaced by ASICs because those very large FPGAs are extremely high power, and they're truly expensive too. So they may get replaced by ASICs.Now our FPGAs, which are on the control plane, are smaller, power efficient, and generally don't get replaced by ASICs because, for all the reasons we talked about earlier, there's just not a big economic reason to change them, or from a technical standpoint, there's not a lot of power efficiency savings either.IC: With those large pieces of silicon from the main competitors, we see them be aggressive on packaging and using the latest process technology node. I assume these small iCE FPGAs aren't made on some 7 nm process are they? Otherwise, I wouldn't be able to see them, I think!JA: [laughs] Well, I'm not sure the size would be that terribly different! A lot of times on a device like the iCE, the size of the silicon may be IO limited. The IO has both an analog and a digital component. The analog doesn't scale very well with technology, but the digital part does. So when you get to a certain size, being on the latest bleeding technology node actually isn't that big of a benefit, and in a lot of cases, a bigger technology node is perfectly fine. In some cases actually, it has some advantages. What we do with our devices is that we're always picking the technology node that's really optimized for the size of device that we're developing but also the customer needs that we're trying to get - and the time to market as well. For a lot of cases, it just doesn't make any sense for us to be on the bleeding edge of the technology.Jim with a ThreadripperIC: I'd be remiss if I didn't get a chance to ask you about your time at AMD. I remember you and me talking over dinner in Maranello about the vision of the company at the time. You had the upper hand because you knew what was coming down the pipe! But are there any fond memories, or any special moments?JA: It's been three years now, the memory starts to get fuzzy!IC: But now you're not under NDA!JA: That's true, actually! [laughs] First of all, when I think back about my time at AMD, my really fond memories are about the people. I keep in contact with a lot of people that I worked with at AMD who I consider really good friends. It was just a great group of people - you know, innovative, creative. AMD was always a really scrappy team. I loved that about AMD, and the people that are really determined. So that's always a fond memory.If I had to think about what events or what things that AMD did that I have great memories about, launching the first Ryzen chips was absolutely a great highlight at the time that I was at AMD. It was the client business unit that launched the first Zen based devices, and of course, you were there for that launch! We launched it first into desktop, followed by mobile later. The amount of work that the team put into that, how hard we worked, and then the excitement and anticipation of just being able to finally bring Ryzen to market was really exciting.Then the event that you mentioned at the very beginning, which was kind of the last event that I was at AMD, which was the Threadripper 2 launch. I loved that just because, I kind of always had a special place in my heart for Threadripper, because on that product line we just would sort of throw out the rulebook and just do whatever we can. Whatever the most extreme thing we could do, we would do on Threadripper, and I always have fun with that product line.But yeah, it was a great time. The last three years at Lattice look fantastic; it feels like we've made a lot of progress over the last three years. We’ve rebuilt a product portfolio. But when I look forward, I'm much more excited about where we're headed over the next 3 to 5+ years. I think the next few years are much more exciting for the company than even the past three years have been. So I’m really excited about where we're headed.IC: Do you have any project inside Lattice that is also sort of throws the rulebook out?JA: Well, it's possible! It's possible. Stay tuned.Many thanks to Jim Anderson and his team for their time.Also thanks to Gavin Bonshor for transcription.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16879/an-anandtech-interview-with-jim-anderson-ceo-of-lattice-semiconductor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Intel Architecture Day 2021: Alder Lake, Golden Cove, and Gracemont Detailed\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-08-19T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16881/a-deep-dive-into-intels-alder-lake-microarchitectures\n",
      "Content: This week Intel held its annual Architecture Day event for select press and partners. As with previous iterations, the company disclosed details about its next generation architectures set to come to the market over the next twelve months. Intel has promised the release of its next-generation consumer and mobile processor family, Alder Lake, to come by the end of the year and today the company is sharing a good number of details about the holistic design of the chips as well as some good detail about the microarchitectures that form this hybrid design: Golden Cove and Gracemont. Here is our analysis of Intel’s disclosure.Alder Lake: Intel 12thGen CoreAs mentioned in previous announcements, Intel will launch its Alder Lake family of processors into both desktop and mobile platforms under the name of Intel’s 12thGen Core Processors with Hybrid Technology later this year. This is Intel’s second generation hybrid architecture built on Intel 7 process node technology. The hybrid design followsIntel Lakefield designsfor small notebooks launched last year. The nature of a hybrid design in Intel nomenclature involves having a series of high ‘Performance’ cores paired with a number of high ‘Efficiency’ cores. Intel has simplified this into P-core and E-core terminology.For Alder Lake, the processor designs feature Performance cores based on a new Golden Cove microarchitecture, and Efficiency cores based on a new Gracemont architecture. We will cover both over the course of this article, however the idea is that the P-core is preferential for single threaded tasks that require low latency, and the E-core is better in power limited or multi-threaded scenarios. Each Alder Lake SoC will physically contain both, however Intel has not yet disclosed the end-user product configurations.Each of the P-cores has the potential to offer multithreading, whereas the E-cores are one thread per core. This means there will be three physical designs based on Alder Lake:8 P-core + 8 E-core (8C8c/24T) for desktop on a new LGA1700 socket6 P-core + 8 E-core (6C8c/20T) for mobile UP3 designs2 P-core + 8 E-core (2C8c/12T) for mobile UP4 designsIntel typically highlights UP4 mobile designs for very low power installs, down to 9 W, whereas UP3 can cover anything from 12 W to 35 W (or perhaps higher), but when asked about the power budgets for these processors, Intel stated that more detail will follow when product announcements are made. Intel did confirm that the highest client power, presumably on the desktop processor, will be 125 W.Highlighted in our discussions is how modular Intel has made Alder Lake. From a range of base component options, the company mixed and matched what it felt were the best combination of parts for each market.Here it shows that four E-cores takes up the same physical space as one P-core, but also that the desktop hardware will at most have 32 EUs (Execution Units) for Xe-LP graphics (same as the previous generation), while both of the mobile processors will offer 96 physical EUs that may be disabled down based on the specific line item in the product stack.All three processors will feature Intel’s next generation Gaussian Neural Accelerator (GNA 3.0) for minor low power AI tasks, a display engine, and some level of PCIe, however the desktop processor will have more. Only the mobile processors will get an Image Processing Unit (IPU), and Thunderbolt 4 (TBT), and here the big UP3 mobile processor gets four ports of Thunderbolt whereas the smaller UP4 will only get two. The desktop processor will not have any native Thunderbolt connectivity.A bit more info on the Desktop Processor IO and InterconnectWe’ll cover a bit more detail about the core designs later in this article, but Intel did showcase some of the information on the desktop processor. It confirmed explicitly that there would be 16 total cores and 24 threads, with up to 30 MB of non-inclusive last level/L3 cache.In contrast to previous iterations of Intel’s processors, the desktop processor will support all modern standards: DDR5 at 4800 MT/s, DDR4-3200, LPDDR5-5200, and LPDDR4X-4266. Alongside this the processor will enable dynamic voltage-frequency scaling (aka turbo) and offer enhanced overclocking support. What exactly that last element means we’re unclear of at this point.Intel confirmed that there willnotbe separate core designs with different memory support – all desktop processors will have a memory controller that can do all four standards. What this means is that we may see motherboards with built-in LPDDR5 or LPDDR4X rather than memory slots if a vendor wants to use LP memory, mostly likely in integrated small form factor designs but I wouldn’t put it past someone like ASRock to offer a mini-ITX board with built in LPDDR5. It was not disclosed what memory architectures the mobile processors will support, although we do expect almost identical support.On the PCIe side of things, Alder Lake’s desktop processor will be supporting 20 lanes of PCIe, and this is split between PCIe 4.0 and PCIe 5.0.The desktop processor will have sixteen lanes of PCIe 5.0, which we expect to be split as x16 for graphics or as x8 for graphics and x4/x4 for storage. This will enable a full 64 GB/s bandwidth. Above and beyond this are another four PCIe 4.0 lanes for more storage. As PCIe 5.0 NVMe drives come to market, users may have to decide if they want the full PCIe 5.0 to the discrete graphics card or notIntel also let it be known that the top chipset for Alder Lake on desktop now supports 12 lanes of PCIe 4.0 and 16 lanes of PCIe 3.0. This will allow for additional PCIe 4.0 devices to use the chipset, reducing the number of lanes needed for items like 10 gigabit Ethernet controllers or anything a bit spicier. If you ever thought your RGB controller could use more bandwidth, Intel is only happy to provide.Intel did not disclose the bandwidth connectivity between the CPU and the chipset, though we believe this to be at least PCIe 4.0 x4 equivalent, if not higher.The Alder Lake processor retains the dual-bandwidth ring we saw implemented in Tiger Lake, enabling 1000 GB/s of bandwidth. We learned from asking Intel in our Q&A that this ring is fully enabled regardless of whether the P-cores or E-cores are being used – Intel can disable one of the two rings when less bandwidth is needed, which would save power, however based on previous testing this single ring could end up drawing substantial power compared to the E-cores in low power operation. (This may be true in the mobile processors as well, which would have knock on effects for mobile battery life.)The 64 GB/s of IO fabric is in line with the PCIe 5.0 x16 numbers we saw above, however the 204 GB/s of memory fabric bandwidth is a confusing number. Alder Lake features a 128-bit memory bus, which allows for 4x 32-bit DDR5 channels (DDR5 has two 32-bit channels per module, so 2 modules still), however in order to reach 204 GB/s in that configuration requires DDR5-12750; Intel has rated the processor only at DDR5-4800, less than half that, so it is unclear where this 204 GB/s number comes from. For perspective, Intel’s Ice Lake does 204.8 GB/s, and that’s a high-power server platform with 8 channels of DDR4-3200.This final slide mentions TB4 and Wi-Fi 6E, however as with previous desktop processors, these are derived from controllers attached to the chipset, and not in the silicon itself. The mobile processors will have TBT integrated, but the desktop processor does not.This slide also mentions Intel Thread Director, which we want to address on the next page before we get to the microarchitecture analysis.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16881/a-deep-dive-into-intels-alder-lake-microarchitectures\n",
      "Title: The \"Smartphone for Snapdragon Insiders\" vs ROG5 Preview: Branded vs Original\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-08-16T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/16867/the-smartphone-for-snapdragon-insiders-review\n",
      "Content: Today we’re reviewing a rather unusual device, the new ASUS “Smartphone for Snapdragon Insiders”. The device had been firstannounced a month agoby Qualcomm and ASUS, and represents a sort of weird kind of collaboration between the two companies. The phone follows a push by Qualcomm in social media and marketing to try to promote their Snapdragon brand, and a program called the Snapdragon Insiders initiative.The “Smartphone for Snapdragon Insiders” – a rather unintuitive name which we’ll shorthand as SSI from here on, is weird in that while it’s primarily pushed by Qualcomm marketing, it’s actually an ASUS designed and manufactured product. The initial announcement of the device was a bit confusing in that one would have thought it’s a Qualcomm developed phone, but the relationship is rather a bit more blurred and complex. For the time being, you can think of the phone as simply being another ASUS device, with Qualcomm Snapdragon branding.For today’s review (which we’re calling a preview for the time being, more on this later), there’s one extremely notable parallel which we can’t help making, and that is the comparison to ASUS’ own ROG Phone 5 phone which was announced and released earlier in the year. As both devices are designed and manufactured by the same party, there’s going to be some extreme similarities between the phones – so comparing them and pitting them against each other is an inevitability in my eyes.Disclosure: Qualcomm has sent us a free review unit of the \"Smartphone for Snapdragon Insiders\" for review. The review is an independent evaluation of the product.ASUS Big PhonesROG Phone 5\"Smartphone for Snapdragon Insiders\"SoCQualcomm Snapdragon 8881xCortex-X1@ 2.84GHz3xCortex-A78@ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 660 @ 840MHzDRAM8/16 GB LPDDR516 GB LPDDR5Storage128/256GB UFS 3.1512GB UFS 3.1Display6.78\"AMOLED2448 x 1080 (20:9)144Hz240Hz TouchSizeHeight173.00 mm173.15 mmWidth77.00 mm77.25 mmDepth9.90 mm9.55 mmWeight239 grams210 gramsBattery Capacity5770 typ (2x 2885) 22.33Wh\"6000mAh\" design65W charging (PD3.0/QC5.0)\"4000mAh\"65W charging (PD3.0/QC5.0)Wireless Charging-Rear CamerasMain64MP IMX6860.8µm pixels (1.6µm 4:1 16MP)f/1.8OIS(SSI only)26.6mm eq.Telephoto-8MP3x opticalf/2.4 w/OIS80mm eq.Wide12MP IMX3631.4µm pixels Dual PDAFf/2.214.3mm eq.Extra5MP Macro-Front Camera24MPf/2.45I/OUSB-C3.5mm headphoneUSB-CWireless (local)802.11ax WiFi-6EBluetooth 5.2 LE + NFCOther FeaturesIn-display fingerprint sensorRear fingerprint sensorGlobal 5G (sub-6 + mmWave)Dual-SIMDual nanoSIMLaunch Price(16/256GB)$999(16/512GB)$1499Starting off with the hardware, the SSI is powered by Qualcomm’s Snapdragon 888 SoC. Qualcomm had noted that the phone doesn’t contain the newer Snapdragon 888+ because the phone was already in design and production before the S888+ came to market, so they opted to use that platform for the device and avoid delaying it further.What’s a bit special about the Smartphone for Snapdragon insiders is a few changes in the internal hardware. First off, the phone only comes in a single configuration, with a whopping 16GB of LPDDR5 and 512GB of UFS 3.1 storage. Generally, this is already one of the aspects which puts the phone at a higher price tier compares to other devices, so it’s important to keep this in mind.The other important aspect of the SSI is its cellular network compatibility and radio front-end and antenna design: To keep it simple, this phone supports every single 5G frequency band in the world – including full mmWave support, something that is unique to the phone and can’t be found in any other device in the market. Generally, the inclusion of radio front-ends to support every single frequency band for every cellular market in the world is a bit unusual as most vendors attempt to optimise the internal hardware of the devices to simply support the frequency bands available in the target market, and simply develop a few different models that target certain markets (North America, EMEA, “Global” models, China, Japan). In the case of the SSI, it’s certainly quite a flex in terms of hardware capabilities – however, unless you’re an extremely international traveller and have according cell plans, it will be something for the very vast majority of users won’t be of practical advantage.The phone features the same display and general form-factor as the ROG Phone 5. The 6.78” AMOLED feature a 2448 x 1080 resolution and goes up to 144Hz refresh rate, along with a 240Hz touch input sampling rate.As noted in our announcement article, it really does seem here that the panel and the whole front glass assembly for the SSI is identical to that of the ROG Phone 5.When face-on, the only real distinguishing factor between the two devices is the fact that the SSI doesn’t have the bottom front speaker slit, and instead uses a bottom-firing speaker. What’s interesting here is that there’s still a glass cut-out where the speaker slit is supposed to be, it’s just that it serves no purpose.The form-factor of the phones is mostly identical; however, the SSI is 0.4mm thinner and 29g lighter than the ROG Phone 5. The reason for the smaller weight is a much smaller battery capacity – instead of the giant 6000mAh of the ROG 5, the SSI only features “a mere” 4000mAh. This is actually a 33% reduction – or better said, the ROG Phone 5 has a 50% larger battery. This will be quite worrying for the battery performance of the phone as we’ll see later.The top of the SSI is practically identical to the ROG Phone 5 with zero differences alongside the glass face.One difference in design between the phones is that the ROG Phone 5 employs a chamfer below the plastic “gasket” along the display glass, whereas the SSI avoids this and simply immediately starts the rounded metal frame of the device.The backside of the phones is characterised by ASUS’s LED elements – this time around it swaps the ROG logo for a 3D Snapdragon logo – it’s actually a three-dimensional element that’s around 2mm deep below the back glass panel, and which lights up.The internal hardware designs of these phones have the motherboard PCB actually placed in the very middle of the phone frame in a horizontal fashion – ASUS had designed this with the Aero Cooler accessory in mind, along with the secondary USB-C port to the side of the ROG Phones. These accessories of course don’t exist for the SSI – placing the Snapdragon logo here is a bit odd in terms of the thermals of the phone, which we’ll dwell deeper later.Also notable on the backside of the SSI is the fact that there’s a fingerprint sensor. This is Qualcomm’s latest ultrasonic fingerprint sensor, however the fact that it’s on the backside of the phone seems a bit odd, and generally counterintuitive to the point of the sensor technology.Qualcomm explains here that the phone uses a solid glass OLED substrate rather than the flexible OLED type, and this former generally cheaper panel type is not compatible with Qualcomm’s ultrasonic fingerprint sensors. This is a bit of a tough situation for the SSI – on one hand it would have been weird if a Snapdragon branded phone had to use a regular optical under-display fingerprint sensor from a regular provider, but this implementation of the ultra-sonic Qualcomm sensor is also generally a bit pointless and definitely not the best showcase of the technology.At the bottom of the phone, there quite notable differences between the phones. We mentioned the bottom-firing speaker – generally the implementation here is not as good as what we have on the ROG Phone 5, and the audio isn’t as even. I’m not really sure why there was a need to change the layout.Furthermore, what’s missing on the SSI is the 3.5mm headphone jack. ASUS is one of the rare few hardware vendors to still provide the 3.5mm headphone jack in their phones nowadays and actually pride themselves in the feature – which is good. The Snapdragon phone losing it is rather odd, but generally in line with what we’ve seen from OEMs over the years, instead opting to promote wireless alternatives.In terms of camera setup, the SSI uses the same main sensor and ultra-wide as the ROG 5- however the main sensor gains OIS, and instead of a 5MP macro module we see the inclusion of a 8MP 3x optical zoom – the same hardware that’s included in the Pro models of the Zenfone 7 and the Zenfone 8 Flip. What’s interesting here is that it seems while the UI interface of the camera is identical to other ASUS phones, Qualcomm had taken up the task to calibrate and optimise the camera processing for the device – which is one of the critical differences to other ASUS phones.The Smartphone for Snapdragon Insiders is rather special in terms of the wireless audio options – the phone comes along with a pair of Snapdragon branded Master & Dynamic MW08 (MW08S in this case) ANC earbuds. The non-branded variant of these TWS actually come at a MSRP of $299, so it’s actually a very substantial value proposition of the $1499 package of the SSI.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16867/the-smartphone-for-snapdragon-insiders-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: HONOR Announces Magic 3 Series: With Snapdragon & Google, A new Start\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-08-12T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/16868/honor-announces-magic3-series-with-snapdragon-google-a-new-start\n",
      "Content: Today, we see HONOR announce the company’s new flagship Magic 3 series of devices. We’ve covered the company formany years now, but to date that was in the context of the company being a subsidiary brand of Huawei. Today’s event also comes from HONOR, but this time around from a spun off company that’s supposed to have cut ties with the trade embargoed and restricted Huawei of past. In a sense, it’s mostly the same people behind the scenes, but also at the same time, it’s also a clear cut-off from past devices, most notably in the form of the adoption of Qualcomm’s Snapdragon chipsets, the return of Google’s application suite and ecosystem, as well as a fresh start from a software OS perspective.The Magic series in the past were rather special devices, showcases of special technology, such as for example one ofthe first bezel-lessdevices on the market with a sliding screen design. HONOR has readopted the Magic series in what appears to be their line of flagship devices – the latter which is also somewhat of a novelty for the company and team as this segment was never a focus-point before.The new Magic 3 series consists of 3 new phones, the “regular” Magic 3, the Magic 3 Pro, and the Magic 3 Pro+. The product segmentation here follows past Huawei devices’ tactics in terms of equipping the phones with increasingly better specifications the higher you go in the range, and in particular the new Magic 3 Pro+ is pushing things quite far when it comes to aspects such as the camera system.HONOR Magic 3 SeriesMagic 3Magic 3 ProMagic 3 Pro+SoCQualcomm Snapdragon 8881xX1@ 2.84GHz3xA78@ 2.42GHz4x A55 @ 1.80GHzAdreno 660 @ 840MHzQualcomm Snapdragon 888+1xCortex-X1@ 2.995GHz3xCortex-A78@ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 660 @ 840MHzDRAM8GBLPDDR5-64008/12GBLPDDR5-640012GBLPDDR5-6400Primary Display6.76\" AMOLED2772 x 1344120Hz RefreshHDR10+SizeHeight162.8mmWidth74.9mmDepth8.9mm (glass)9.5mm (v-leather)9.94mmWeight203g213g236gBattery Capacity4600mAh (Typical)66WWireless Charging-50WRear CamerasMain50MP 1.0µmIMX7662x2 OCL PDAFf/1.923mm eq.50MP 1.22µmIMX700Octa PDf/1.9OIS23mm eq.ExtraMain64MP Monochromef/1.8Telephoto-64MP 0.7µmf/3.5OIS3.5x / 90mm eq.-Ultra-Wide13MP 1µmf/2.2120° FoV / 13mm eq.64MP 0.7µmf/2.2126° FoV / 11mm eq.ExtraLaser Focus8x8 dToFFront Camera13MPf/2.413MPf/2.4+dToFStorage128 / 256GBUFS 3.1256 / 512GBUFS 3.1512GBUFS 3.1I/OUSB-CWireless (local)802.11ax (Wifi 6)Cellular4G + 5G NR NSA+SA Sub-6GHzSpecial FeaturesUnder-screen fingerprint sensorStereo speakersUnder-screen fingerprint sensorStereo speakersUnder-screen fingerprint sensorStereo speakersSplash, Water, Dust ResistanceIP54IP68Dual-SIMnano-SIM + eSIM2x nano-SIM + eSIM2x nano-SIM + eSIMLaunch OSAndroid 11 w/ Magic UI 5.0Launch Price8+128GB: ? €8+256GB: 899€12+256GB: 1099 €12+512GB: ? €12+512GB: 1499€Starting off at the heart of the phone, the new Magic 3 series adopt Qualcomm’s Snapdragon 888 chipset – that is, at least the regular version does. The Magic 3 Pro and Pro+ are amongst one of the first devices to adopt the higher binned Snapdragon 888+, which increases the clock speeds of the X1 cores to up to 3GHz, as well as improving the ML performance of the chip.For a few months now, we’ve heard from Qualcomm executives that they’ve been extremely excited to have HONOR as a new customer, representing a lot of growth opportunity as Huawei had its market share eradicated over the past year.HONOR during the presentation showcased some familiar marketing names, such as GPU Turbo X. We don’t know for sure how these new mechanisms relate toGPU Turbo of years past, but the company also presented some other rather dubious and over-marketed mechanisms such as “OS Turbo X”, “Low-lateny Engine”, “Anti-aging Engine”, “Smart Memory Engine”, “LINK Turbo X” – over the past few years we’ve seen many of the Asian vendors give such marketed names to extremely generic and common operating system mechanisms, so it always sheds a lot of doubt on their legitimacy and if it even does anything.One hardware aspect of the Magic 3 series that HONOR is promoting is the introduction of hexagonal graphene sheets for improve thermal dissipation from the SoC. The new structure here likely helps the graphene sheets to dissipate the heat into multiple directions, and the company is promising a “50%” improvement in heat dissipation from the SoC compared to previous implementations. HONOR went on so far to claim that the Magic 3 series outperforms Apple’s iPhonr 12 Pro in performance and thermals in games such as Genshin Impact – which is quite interesting as a claim, but possible if the phone is really able to sustain quite high average power levels of the SoC.At the front of the phone, all the Magic 3 series devices are supposed to look the same, and feature the exact same screen panel. The 6.76” OLED features a resolution of 2772 x 1344, and also supports high refresh rate up to 120Hz. HDR10+ is advertised, but the company couldn’t confirm if the display features any further advanced features such as variable refresh rate – so my initial suspicion is that it’s a more generic design.HONOR put a lot of emphasis on the extreme curvature of the display – 89° to the sides of the phone. We had seen similar implementationssuch as on the Mate 30 Pro in the past, however I find that while it was an interesting experiment to make, that this design element was not well received by many, including myself, so I think it will remain a very divisive element of the phone. The verdict here is still open until we can get a hands-on with the phone and see if the ergonomics work out or not.The front display also features a pill-shaped notch, featuring a 13MP front camera and a dToF sensor for secure face unlock. This latter module is advertised as only being present on the Pro models – I’m actually not sure how the display cut-out is supposed to look on the regular model and HONOR hasn’t shared a single image of the front of that specific device – maybe it has a simple hole-punch design?The back of the regular Magic 3 reminds us a lot of Huawei’s Mate 30 series – although HONOR here calls the design “the eye of muse”. One difference is that the main camera is located in the centre of the design which remains fairly symmetric.The phone comes in either glass or vegan leather options – the vegan leather variant is a little bit thicker at 9.5mm rather than 8.9mm for the glass, and weighs 1g less than the glass variant.The phone’s footprint is equal in all variants at 162.8 x 74.9mm – meaning it’s very much on the larger side of devices. HONOR here also opted for a larger front curvature than back panel curvature, meaning it might feel larger in hand than other devices which focus on the back panel curvature.Magic 3 Pro Camera InternalsThe main camera module on the Magic 3 and 3 Pro is the IMX766 – a 1/1.56” format sensor with 1.0µm pixels and 50MP resolution, binning down to 12.5MP 2µm pixels for regular shots. HONOR advertises a focal length of 23mm which is quite wide and I would be quite disappointed to see if whether they are doing the same cropping to 10MP / 26mm equivalent cropping as Huawei did in recent devices as this generally defeats the purpose of the resolution and optics of the module. Optics are f/1.9 without OIS, which is concerning.As an ultra-wide module, the Magic 3 and 3 Pro also feature a 13MP 1µm pixel sensor with f/2.2 optics at 13mm equivalent, or 120° FoV.What’s extremely odd in HONOR’s pre-briefing and general product presentation, is that the team only passingly noted that the phone also has a 64MP monochrome sensor at f/1.8. The company made absolutely zero mention of what this module is supposed to achieve – Huawei had presented a similar feature on their P50 series phone which was recently announced, and had talked about how it’s being used to improve dynamic range and resolution through image fusion.I had asked HONOR about their image processing, and the company admitted that they essentially have to start from scratch – not only because of their divorce from Huawei, but also because it’s a whole new SoC and ISP system, on new sensors. Although the team here mentioned that they have done their best efforts, and that the end result should be judged, it felt as if they weren’t quite as confident. For example, they wouldn’t confirm if they had managed to develop a computational photography night mode capture mechanism, so it’s definitely a big question mark remaining as to the phone’s camera prowess.The Magic 3 Pro features a different camera layout, with a larger circular element on the back of the phone. The main camera and ultra-wide are the same, along with the monochrome sensor, but what’s added is a periscope telephoto module.This module is also essentially identical to what we’ve seen announced on Huawei’s P50 series, a 64MP sensor with 0.7µm pixels (binning to 16MP 1.4µm), in a 3.5x optical magnification / 90mm equivalent focal length periscope optics module with f/3.5 aperture.This periscope module is actually extremely smart – at 64MP and 90mm focal length, the resolving power of the camera goes up to 8.84 or 17.67 arcseconds per pixel, which is comparable and competitive to the8.21 arcsecond per pixel periscope of the S21 Ultra, or 7.45 and 14.9 arcsecond per pixel module of the Mi 11 Ultra. The only real issue here is that with an aperture of f/3.5, the diffraction limit andairy diskor the optics is quite a lot larger than the tiny pixel size of the sensor, so it remains to be seen how the quality compares.Alongside the Magic 3 and Magic 3 Pro, we have also the Magic 3 Pro+. This unit differentiates itself from the Pro by adopting a different main camera sensor, a 1/1.28” IMX700 with 1.22µm pixels at 50MP resolution. This is a very large sensor and amongst the largest out there, though it’s a bit smaller than the gargantuan module of the Mi 11 Ultra.The Pro+ also upgrades the ultra-wide camera from 13MP to 64MP – though the sensor here is only a 1/2.0” format one with 0.7µm pixels, binning down to 16MP 1.4µm pixels. What’s also remarkable is the optics, though only f/2.2, it features a 126°FoV or 11mm equivalent diagonal focal length, which would make it the widest ultra-wide of any phone out there.The Pro+ has a ceramic back, which HONOR refers to as “Nano Ceramic”. While ceramic bodies are more scratch resistant, they’re also heavier, and the Pro+ comes in at 237g which is quite a bit more than the 213g of the Magic 3 Pro, in turn also heavier than the regular 203g Magic 3.All three phones feature a 4600mAh battery and support 66W charging, while 50W wireless charging isn’t present on the regular model. Further differences to the regular Magic 3 model is that it’s only IP54 rated, while the Pro models have IP68, and the regular model features Laser autofocus, while the Pro models have a rear 8x8 dToF sensor for low-light focusing.Price: 899 - 1099 - 1499€In terms of pricing, HONOR announced the Magic 3 at 8+256GB at 899€, the Magic 3 Pro at 12+256GB at 1099€, and the Magic 3 Pro+ at 12+512GB at 1499€. These are extremely high prices for the phones, particularly the regular Magic 3 which lacks a lot of the more advanced features of its Pro siblings. Admittedly the Pro+ pricing is quite absurd, but it also features a camera configuration that is unique. Still, HONOR here is pricing themselves above Samsung’s S21 Ultra and the Mi 11 Ultra, which is a very large gamble to make given the company’s essentially restart from scratch.Particularly what worries me is the software maturity and how the cameras will compete – and if they will be competitive from an image processing standpoint. The western models of the phones will now ship with Google services and most importantly Google Play, so they are much better value devices than what Huawei was able to offer in recent troubled years.Availability in European markets wasn’t concretely announced, but should start in the September timeframe. We’ll be reviewing the Magic 3 Pro+ once we receive a review sample, and looking forward to put the device through its paces.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16868/honor-announces-magic3-series-with-snapdragon-google-a-new-start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Samsung Unpacked 2021 Part 2: Galaxy Z Flip 3 & Z Fold 3 Announced\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-08-11T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/16869/samsung-unpacked-2021-part-2-galaxy-z-flip-3-z-fold-3-announced\n",
      "Content: Today Samsung is holding its second Mobile Unpacked event for the year, announcing the new Galaxy Z Flip 3 and the new Galaxy Z Fold 3. It’s a bit of an unusual Unpacked event compared to past years as this time around and for this year we won’t be seeing a new Galaxy Note device.The new Galaxy Z devices follow as iterations to their predecessors –the original Z Flip announced in February last yearand Z Flip 5G later in August, and theZ Fold 2which was also announced around the same time in 2020.Both devices are obvious characterised by their folding form-factor designs, which Samsung this year has iterated on, included new hardware internals, and generally just polished their functionality compared to their predecessors. Samsung’s focus towards foldables seems to result as the market generally is finding it hard to differentiate oneself in-between competitors.Galaxy Z Phones 2021Galaxy Z Flip 3Galaxy Z Fold 3SoCQualcomm Snapdragon 8881xCortex-X1@ 2.84GHz3xCortex-A78@ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 660 @ 840MHzDRAM8GB LPDDR5-640012GB LPDDR5-6400Primary Display6.7\" AMOLED2640 x 1080120Hz Refresh7.6\" AMOLED Foldable2208 x 1768120Hz RefreshSecondary Display1.9\"260 x 5126.2\" AMOLED2260 x 832120Hz RefreshSizeHeight166mm (unfolded)158.2mmWidth72.2mm67.1mm (folded)128.1mm (unfolded)Depth15.9 - 17.1mm (folded)6.9mm (unfolded)14.4 - 16.0mm (folded)6.4mm (unfolded)Weight183g271gBattery Capacity3300mAh (Typical)15W4400mAh (Typical)25WWireless Charging10W10WRear CamerasMain12MP 1.4µm DP PDAFf/1.8OIS26.8mm eq.12MP 1.8µm DP PDAFf/1.8OIS26.8mm eq.Telephoto-12MP 1.0µmf/2.4OIS51mm eq.ExtraTelephoto--Ultra-Wide12MP 1.12µmf/2.2123° FoV / 13mm eq.Extra--Front CameraHold-Punch (Unfolded)10MP 1.22µmf/2.225.7mm eq.Under-Display (unfolded)4MP 2.0µmf/1.8Hole-Punch (folded)10MP 1.22µmf/2.225.7mm eq.Storage128 / 256GBUFS 3.1256 / 512GBUFS 3.1I/OUSB-CWireless (local)802.11ax (Wifi 6),Bluetooth 5.1802.11ax (Wifi 6E),Bluetooth 5.2Cellular4G + 5G NR NSA+SA Sub-6GHz + mmWaveSpecial FeaturesUnder-screen fingerprint sensorStereo speakersIt flipsUnder-screen fingerprint sensorStereo speakersIt foldsSplash, Water, Dust ResistanceIPx8IPx8Dual-SIMnano-SIM + eSIM2x nano-SIM + eSIMLaunch OSAndroid 11 w/ OneUI 3.1Launch Price8+128GB: $999 / 1059€8+256GB: $ / 1109€12+256GB: $1799 / 1799€12+512GB: $ / 1899€Starting off with the hardware, the new Galaxy Z devices are now powered by Qualcomm’s new Snapdragon 888 chipset for all global variants of the phones, which should give the phones excellent performance. It’s to be noted that these foldable phones have likely less thermal dissipation capabilities as the more traditional “bar” style devices, so it’ll be interesting to see how Samsung handles the increased power envelope of the new SoC compared to last year’s Snapdragon 865.The phones come with 8GB of RAM on the Z Flip 3 and 12GB on the Z Fold 3. Storage options for the Flip are 128 or 256GB, while the Fold doubles it to 256GB or 512GB – both UFS 3.1.The Z Fold 3’s overall design hasn’t changed all that much from the Z Fold 2 – the internal screen is still a 7.6” 2208 x 1768 plastic AMOLED panel with the notable characteristic that it is flexible and foldable. It also continues the 120Hz refresh rate.The larger change on the panel isn’t in regards to the panel itself but rather to the front camera, which is now using Samsung’s first ever iteration of an under-screen camera design. The camera sits underneath the screen panel and “sees through” the pixel matrix, which is now partially transparent besides for the actual pixel emitters. Samsung here only uses 4MP camera module, but with large 2µm pixels. The pixel density of the area above the camera module does not appear as dense as what we’ve seen presented by Xiaomi on the MIX4, however Samsung is deploying this technology on a foldable panel first so there might be larger differences and technical limitations to the various implementations.What’s novel on the Fold 3 is the fact that Samsung has now also introduced a digitizer into the panel, allowing it to work alongside the S-Pen. Samsung has also introduced a new S-Pen with a rubber nib and spring pressure mechanism that is much gentler to the plastic screen compared to the traditional S-Pens for glass screens.On the cover side of the Z Fold 3 we again also see a similar 6.2” display, although this year this panel has gained 120Hz refresh rate capabilities. The camera employed here is of the more traditional hole-punch design.Z Fold 3 & Z Flip 3 foldedThe overall dimensions of the Z Fold 3 are similar to its predecessor, a 158.2mm device height, and a folded with of 67.1mm that opens up to 128.1mm when unfolded. The device thickness is 6.4mm when unfolded, or 14.4mm when folded. The phone still remains extremely heavy at 271g, but does shave off a few grams compared to its predecessor. Included in that weight is a 4400mAh battery.Samsung has introduced IPx8 rating for the new Z devices, meaning they’re now water resistant – which is actually quite a feat for a foldable device. Samsung has managed this to employ corrosion resistant mechanical elements in the hinge design, and enforcing water resistant rubber between the electrical elements of the two phone halves.In terms of cameras, the Z Fold 3 does not appear to change anything in regards to the actual hardware: it’s still a 12MP 1.8µm sensor with f/1.8 as the main module, the same as we’ve seen on the S20 and S21 series. There’s a 2x optical 12MP 1.0µm f/2.4 with OIS telephoto module, and a 12MP 1.12µm ultra-wide with 13mm/123° FoV optics.The Z Flip 3 differs from the Fold in that it folds in the vertical direction of a more classical flip-phone, rather than horizontally as its larger sibling. Similar to the Z Fold 3, the generational changes this year are rather minor besides the internal hardware upgrade.In terms of design and the display, it’s again still a 6.7” 2640 x 1080 panel, but this year does gain 120Hz refresh rate.Z Fold 3 & Z Flip 3 foldedThe secondary small display on the rear of the phone has grown from 1.1” to 1.9” and is of a 260 x 512 resolution now, and in general the whole rear part of the phone here is quite different from the Z Flip 2 as Samsung has rearranged the two rear cameras in a vertical positioning alongside the larger secondary rear display.Unfortunately, the camera hardware doesn’t seem to have seen upgrades, the main module is still a 12MP 1.4µm f/1.8 module (Galaxy S10 class generation), and the same 12MP 1.12µm specifications on the ultra-wide sensor.Rather conservative upgradesIn general, this year’s Galaxy Z devices are really only evolutionary upgrades over last year’s options. While the Z Flip 3 has seen what I would call larger differentiation through the larger secondary display and camera rearrangement, the Z Fold 3, beyond the camera island redesign, can also only be really differentiated through the new under-screen inner front camera module.While the Z Fold 3 remains extremely expensive starting at $1799, the Z Flip 3 starts a lower $999, $200 less than its predecessor, which might open up the phone to a larger potential audience.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16869/samsung-unpacked-2021-part-2-galaxy-z-flip-3-z-fold-3-announced\n",
      "Title: Open Compute Project: An Interview with Intel's Rebecca Weekly\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-08-09T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16872/open-compute-project-an-interview-with-intels-rebecca-weekly\n",
      "Content: We all know that we put processors into servers, servers into racks, racks into data centers, and then they 'do stuff'. Whether that's a hyperscaler managing internal infrastructure, offering outside services, businesses processing workflows, high-performance machines working on the latest weather or nuclear simulations, social media companies scaling out their services to billions of users, or smaller startups needing scalable resources for their new monetizable idea, there's always a data center and enterprise backbone.The need for lots of computing resources comes with a number of fundamental issues, the chief among which is likely going to be standardization. Without a consistent size, depth, or definition to the size of a server, a deployment can easily end up as a hodge-podge of malformed hardware with no discernable high-level design methodology. While the silicon vendors or the OEM partners building the systems could have their own way of doing things, without a collaborative effort to define standards, we would still be in the 1970s or 1980s where systems end up unique for one particular customer. On top of this, there is an important overriding drive in the 21st century to ensure that enterprise deployments are power efficient as well.When Facebook was scaling its technologies and pivoting to completely public use in the late 2010s, it started an internal project around data efficiency and scalability. The goal was to end up with a solution that provided scalable resources, efficient compute, and enabled cost savings. In 2011, combined with Intel and Rackspace, the Open Compute Project was launched to enable a set of open standards that could benefit all major industry enterprise players. OCP is also a fluid organization, providing its community a structure that is designed to enable close collaboration on these evolving standards, pushing for 'commodity hardware that is more efficient, flexible and scalable, throwing off the shackles of proprietary one-size-fits-all gear'. OCP also has a certified partner program, allowing external customers to be part of the ecosystem that covers data center facilities, hardware, networking, open firmware, power, security, storage, telecommunications, and future technologies.While the initial founders included Intel and Facebook, other companies involved include ASUS, Arm, IBM, Google, Microsoft, Dell, HPE, NVIDIA, Cisco, Lenovo, and Alibaba. An example of how to think about OCP is that an OCP Rack is 21 inches wide, rather than a standard 19 inches, allowing for more airflow, but the racks are also taller, accommodating more units. Parts of the rack use dedicated high voltage power unit shelves that supply power to the rest of the servers in the rack, rather than relying on each system to have its own power supply. This also allows each server to fit more, such as a 2U six-blade design, or a 30 drive 2U design for storage that allows the drives to sit flat, rather than vertical. The OAM form factor for high-power graphics accelerators comes from the words (OCP Accelerator Module), coming out of the group. Two years ago we reported on Facebook's Zion Unified Training Platform, built to OCP specifications, using Intel's Cooper Lake processors.Rebecca WeeklyOpen Compute ProjectDr. Ian CutressAnandTechIn this interview today we have Rebecca Weekly, who not only sits as the VP and GM of Intel's Hyperscale and Strategy Execution, but is also an Intel Senior Principal Engineer. However, today we are speaking to her in her role as Chairperson and President of the Board of the Open Compute Project, being promoted on July 1st of 2021. When the press relations team sent around the news that Rebecca was taking the role, I reached out and asked if we could interview to get a deeper insight into OCP.Ian Cutress: You’ve been elected as chairperson of the Open Compute Project - how long have you been involved with OCP? And what does your position as chairperson entail exactly?Rebecca Weekly: Great question! I have been on the board of OCP since September 2020. I started (for lack of a better term) shadowing the previous person in the role for Intel back in July, but I took the position in September. But I've been involved in projects with OCP for a long time!At Intel I work with hyperscale customers, and three of those hyperscale cloud providers are on the board of OCP. I’ve worked with many OCP projects, whether it's Mount Olympus, which was donated [to OCP] in conjunction with Microsoft, or any of Yosemite v1/v2/v3, which were donated from the Facebook partnership. Those projects have been things we've been working on forever. With those systems we have firmware support packages, things like OpenBMC. I mean, there are so many projects from a management (and modular design) point of view that revolve around my day job - to work with customers to make sure that they have the kind of modular compute systems that are vanity-free and are ready to go in the ecosystem.It has always has been a part of my day job since I came to Intel six years ago. It felt very natural [to be a part of OCP]. But the board is very different! It's a different way to think. When you come in, you look at the open-source ecosystem and your contribution strategy to that open source ecosystem - from a specific company, you think about the base components that have to be able to work together, and how we enable that while keeping our special sauce unique, right? That's our job at OCP. That's our responsibility to our various stakeholders. When you're sitting on a board, you're thinking about the future of that industry and the community you serve - what needs to happen in that industry given the big trends that are happening.It has been a whirlwind - first of all, just being on a board with Andy Bechtolsheim (who I look up to) is great. Everybody who I have the opportunity to serve with, such as Mark [Roenigk, Facebook], Partha [Ranganathan, Google], Zaid [Kahn, Microsoft], and Jim [Hawkins, Rackspace] - they're all phenomenal humans, who really think about both the future of the industry and the communities they serve, and they wear those hats with a lot of grace. I’m finding that opportunity to see what I am supposed to do for my day job - what am I supposed to do for Intel, but also then what I am supposed to do for this community. It’s about making sure that all can be synergistic, but recognizing the task. I'm here in this capacity - That's the hat I'm wearing and that's what I got to do.Cascade Lake OCP ServerIC: At times they must conflict? If different parts of the board want to do different things than what Intel wants, it’s about the industry going in a given direction?RW: Something I did very early on was that I listed out all the different working groups at OCP. I was totally explicit with my partners on the board which groups Intel plans to ignore - as in, it's not our job to contribute in specific areas. We think we have a lot of special sauce for our key areas, and we're there to participate, and hopefully we will help ensure that everyone’s key contributions are involved. Anything we can contribute from the sense of experience though, we’ve got experience there, we’ve done that, perhaps don’t go down that path!But in general, from the Intel side of things, we're observing most areas versus others where we're trying to lead and where we think it's critical to the future [of Intel]. So I went through OCP’s working groups, took note of all the projects, like what the status was, and how it really worked. Because there are so many different parts of Intel that do contributions to OCP, from tools, to software, firmware, BIOS, and everything that's happening on the system side for systems components, whether it's a network add-in card, or something that's happening at the switch silicon space, versus what's happening on a motherboard. So there are lots of different areas where people can contribute, and we’re trying to get everyone on the same page, with truth and transparency. [We always ask] where are we at - and then share that. So that if a topic came up, I have to say ‘I can't really talk about that, it's not something I'm either empowered to talk about, or it's just not something that we’re going to contribute to'. I can speak as myself, as chairperson of the board, but not in my capacity as working for Intel.IC: OCP is the Open Compute Project, and it is very focused on the enterprise hyperscale industry. It's not going to be for people considering their home system or home networks! But how would you define OCP to people who have never heard about it before?RW: Sure! OCP, as you said, is the Open Compute Project, and it's really a community at first and foremost. It's a community, and it's ‘by engineers, for engineers’, which is probably why I love it so much! It started in 2011, and fundamentally it was about efficient server, storage, data centers, and hardware designs. It's one of the few communities I know of, or in fact the only community I know of, that doesn't just focus on a single element, such as thermal or electrical protocol layers for interconnect in some capacity. For those we have JEDEC and PCI-SIG. [OCP] is about systems, about implementations.[For the others], it's great to talk to Hardware Root of Trust in isolation, but if you want everybody who's participating in your supply chain to have an implementation of a Root of Trust that is consistent, you got to go somewhere and force an implementation spec for it as well as a compliance body in some sense to make sure that happens. So OCP is really the only community I know of that does that work.If you think back to 2011, you still had SGI, you still had all these crazy pseudo companies doing MIPS, which was part of SGI at the time. But they were doing specific individual implementations of very fancy systems - do you remember walking through the data centers with all the LEDs, and they were just so perfect? At the time companies were manufacturing their own screws as if that was important!IC: They were focusing on bespoke designs per customer?RW: Exactly. Then this community got together and said ‘we don't care if it's plywood, it does not matter’ - because conceptually - they cared about vanity-free hardware with consistency. [The community asks itself] ‘how do we drive convergence on vanity-free components to increase supply, to decrease cost, to improve PUE*?. The community asked about everything in the domain space that was really important for the data center to take off.It is, to your point, very hyperscaler led. But actually, if you look at the contributions of the marketplace that people adopt, 58% of adoptions from OCP marketplace are telcos and network operators. The community has changed so much over the last 10 years, and there's a lot of change that will continue to happen. I think, as you know, we're becoming more heterogeneous especially as data is more disaggregated, and everything that we're dealing with just as a community means there are changes afoot.*PUE = Power Usage Effectiveness, a measure of how much energy input into a data center is used in the servers. The best PUE values are 1.06 or lower, meaning for every 106 W going in, 100 W is being used. An average PUE is 1.4-2.0, bad PUE is 2.5+.IC: I've noticed that over the timespan that OCP has existed, it shifts based on need. The biggest thing that's currently in the market right now is AI, and the move to make solutions that are more AI-focused for everybody to use. To your point, the shift towards 5G deployments and telcos, that seems to be a really big focus right now?RW: In that sense, there are some interesting things happening with network anomaly detection, and more of a software perspective for using AI, obviously. But in OCP, we have OAM, or OCP Accelerator Module [which is a unified form factor for high-powered compute cards]. As part of OCP, we think about the form factors we can help create so that people can choose to, for example, take a Cerebras chip, or take a different AI accelerator or whatever, you know the newest, latest, and greatest, and will be and be able to take advantage of the system's footprint and validation footprint that's already in the ecosystem.IC: I’ve noticed Intel has acquired a few AI companies recently, and they've all gone towards that sort of OAM interface!RW: Yup!OCP CommunityIC: So if we look at the companies listed in OCP, we've got ASUS, ARM, IBM, Google, Microsoft, Dell, HPE, NVIDIA, Intel, Cisco, Lenovo, Alibaba. That's a lot of the industry, and you said you work with three out of seven hyperscalers in your role at Intel. Is OCP in the situation of growing its membership, or is it at a healthy level, or are there other people that need to be involved that currently aren't?RW: The OCP Foundation right now has around 250 corporate members - it started as six. So there is definitely a huge number of growing participants. There are over 5,000 engineers, 16,000 participants, and there are 29 active projects across the world. We move as the domain spaces keep shifting and growing. So obviously we have security and operations specifically for security, there are advanced cooling projects, areas in connectivity solutions, testing, validation, enabling. There is so much, for example, there have been so many great papers recently written from OCP members about the complexity of test and validation, or consistency as we have more heterogeneity in systems.One of the awesome and amazing projects that has been a focus in the last year is sustainability, and looking at sustainable practices, because there is zero consistency on reporting emissions as it pertains to ICT equipment. There are also zero standards around reporting, no sort of best practices for operations. [At the base level], it’s different to how your laptop works, such as going into suspend mode and whatever, which has standards - it is really different than if you're trying to operate a public cloud infrastructure. For that, you have to have a certain commitment from an SLA perspective [for your customers] for speeding up latency, but you're actually not fully utilized most of the time. It means you operate with an ‘always-on’ mindset, but [the task is to] not burn power if unnecessary. I’m a total hippie so I get really excited at the prospect of us bringing that community together!But also, there are companies out there making all sorts of claims. There's nothing standard to compare it against - their claim is ‘as measured by’ a hired company with a methodology that has not been validated [or standardized]. Governments haven't necessarily stepped up in this domain space, either. But I think this is a domain space where open source communities really can make a difference - at least they can start, and then others can take notice.IC: One of the recent groups that I think most of our audience is probably interested in is the Open Domain Specific Architecture, sort of an ‘Open Chiplet Marketplace’. Because Intel is moving into that, and Intel’s competitors are moving in that direction, and even when I spoke about AI chips, some of these designs are essentially chiplets in a big infrastructure. This marketplace was announced around two years ago - do you have any insight into what's going on with the sort of chiplet standards now, with OCP?RW: So much, and not nearly enough! But you know, I think that you made an excellent point which is that everybody is getting into this domain space. Whether it's 2D or 3D, there are a lot of interesting things happening with 3D technologies and 2D technology. I think it's fair to say we have probably three, or at least definitely two, main things that are occurring.ODSA itself as a working group has been working to create sort of a test chip mentality, where we can actually give effectively a reference implementation of two different 2D stacking technologies with two different fabs on either side of the wire. That's one OCP project, and it's really about trying to create [combined products]. Whether it's a Bunch Of Wires is used [that’s a technical term, BoW], or an Advanced Interface Bus (AIB) is used, there are lots of things in this domain space.We're all evaluating these technologies based on throughput, but also based on really simple things, like the right to license and utilize. These are things that just require communities to come together, have the debate, and have the discussion. So that's really where that team is focused and that project group is focused.When I look at what's happening in the ecosystem, there were some really interesting conversations that happened in November of last year, at the Virtual OCP Summit. They started talking about this concept of an Open Chiplet Marketplace. This is kind of a brainchild out of Google, where companies are coming together and bringing people together to talk not just about the thermal or the electrical, but how and when we actually produce these things, as well as the software layer, creating consistency, and how do we create a security model, [especially] when chiplets are made anywhere? [We ask whether] we have some aspects of composability and manageability, those sorts of things.So, you probably remember when NVIDIA cards started coming to the public cloud. It was a nightmare. You couldn't chop them up for virtual machines, and if something went wrong in the device, all the visibility went from the virtual machine. So the vendor who's providing it as hardware vendors, we never really thought about the world where customers manage the hardware. But issues like this still happen! I've been focusing on this area for six years at Intel.But we're coming up on the 15 year anniversary of AWS. Even now, we’re still trying to understand fundamentally what it takes to operate a public cloud set of hardware. It's very different.With chiplets, there is so much of the domain space of the actual interconnect in the marketplace, and how that could function to have consistent software and composability. I think there's a ton to go figure out in terms of validation, ensuring it works, and creating it at scale. I don't know where it will go - I mean, I'm so excited to see where it goes. This is like my typical day you know - there are some parts of my world where I know the goal is to get to a place where you just have the robots and they're composing the CPU, or XPU, with all these pre-validated chiplets. Suddenly, boom! You run a couple of burn-in tests on it, and you now have a whole new XPU at scale that is exactly what the end-user needed! Will we get there? It's decades away, but I think that's what those kinds of technologies start getting us towards.IC: I think it's really important that when we start moving the chiplet/tile architectures, if you're buying a third-party chip, you want to verify that it's actually doing what it says, and it has a defined secure supply chain. Because at some point there's going to be, as you say, robots designing these chips. You have got to make sure the robots are making it in a very secure way, such that you can validate start to finish, and the results you get out or right. But that sounds like the right path through!RW: Just getting these things to do 2D stacking together from two different foundries on two different sides of the wire, to do a different and unique use case is a start. I think that getting there would never happen if people don't come together.IC: So in OCP, the O stands for Open. But to become part of the OCP, you have to pay to be a member. But all the meetings for everything seem to be online, so anybody can watch them, and they’re all listed on the OCP website. So what exactly does ‘Open’ mean?RW: It's a good question. To me, ‘Open’ is about open hardware, right? It's about creating specifications that anybody can pick up, whether it's Quanta, Foxconn, Wiwynn - anybody can pick up and produce. [Open hardware is] helping ensure that there's more consistency from the modularity of the computer itself. But even with ‘at scale’ operations, we have to think about management and security solutions in that domain space. So to me, Open is about the specifications that are contributed, in comparison to the participation.Now the Foundation does have to pay for and administer the working groups, and ensures that those happen. Wearing my OCP hat, I would say it's a pretty nominal fee for what it does in the community! We also have summits and all our different collaborative events. But again, that monetary part isn't specifically about the community it serves which is driving open technology and solutions. I mean anybody can get involved and anybody can learn from it - I think the [cost] opportunity is to become a voting member of the community, and be elected to be part of the various working groups or incubation committees. Those things require membership. That membership really is about contributing financially but also contributing with your time - members actually have to make contributions [as part of the fee]. It’s done in order to ensure that you are, in fact, listened to, but also committed to driving these contributions.IC: It sounds like you can't really be a passive member in that sense - you definitely have to be involved.RW: Involved somewhere! There are definitely particular working groups that may not be interesting to you, but the expectation is that it's a very active community. When I joined the board, I still remember Mark Roenigk, the previous chairperson, telling me that we are a working board, and the expectation is you're going to get things done. He told me here's how to set up a support network, to be able to do this for this long - and I took notes, I cannot do it on my own, there's no way.IC: So with respect to what I think most of our readers might interact with OCP is the standard sort of OCP server rack design. I think you mentioned Olympus earlier, this sort of drive to having more efficient servers, helping with cooling and density. The specifications of an OpenRack OCP server right now, I think version three, are wider than a standard server, and the racks are slightly taller. This is completely different to how most of the time we look at enterprise systems! So why hasn't the industry moved to what OCP suggests, and why has it kind of stayed in its lane? As I say this, as you're somebody who works for Intel, and Intel's partners sell a lot of those regular systems!RW: So OpenRack started that design form factor, as the 21 inches instead of the 19, for a very specific kind of single socket, but super dense design. It's a phenomenal design, and OpenRack continues in the ecosystem. But it's not the only OCP-certified rack size. So if you look at Mount Olympus, it was a 19-inch form factor and a standard server configuration. You'll see both, and companies like Google make contributions that are 19-inch as well. So the OpenRack form factor is a compliant standard, but it's not necessarily true that every contribution in the marketplace has to adhere to that original OCP form factor. It was originally about Facebook creating a very unique, very low PUE, high dense design. Facebook was one of the OCP Founding members, and I love what they did there.There are a lot of different people in OCP, and the Open 19 standard started because people were confused why Facebook did this weird 21-inch size thing. There are all sorts of conversations about it. OCP then decided to embrace 19-inch too. So you know I think It's been an interesting journey because so much of the original designs were contributions from hyperscalers, and in partnership for their specific environment. But then as the community has grown into Telcos, into more enterprises, and as the board membership has changed, such as with Rackspace, people are thinking about it differently. So there's more of an expansion in the form factors that are available and that will evolve as the industry evolves.IC: Does OCP actively search out for technologies it feels that it should incorporate into some of its open standard designs? Or do you rely on the companies with those technologies to become part of OCP?RW: It's a great question. When I came on to the board, I had a very similar question - ‘so how does this work? I know what we do, I know what we think about, and it's a pretty proactive process from our lens. But how does new membership work? How do new initiatives work?’. So as a board actually, we sat down and spent days and days together (virtually, because it was during the pandemic), to come up with answers to where the industry is headed. We asked ourselves and our members what do we see as the future initiatives of the world - computation is becoming increasingly heterogeneous, data is increasingly disaggregated, and sustainability is incredibly underreported. We kind of tried to analyze both the pain points - our various lenses on the industry, as well as the changes in opportunity. We then reported to each other.Out of that process, we came up with the OCP 2.0 framework, which we're in the process of rolling out. To meet the market of today, that's traditional OCP. It’s what we've always done, with modularity at scale operations, increasing sustainability, and it's a top pillar of meeting the market today. But I think we're already late, frankly.Then in the integrated solution space, if you think about how much money and time various vendors spend just to certify different solutions, such as SAP certifications or ensuring that vSphere runs correctly in all different configurations - it’s a huge amount of time for the industry. OCP 2.0 is about how we can do that better, faster, and stronger.Then the other aspect of it is, again, understanding the future. The future will be more heterogeneous, more disaggregated, and all these different technologies need to be in place. To take one example - optics needs to be developed, not just from a network switch perspective which is well covered, but at a level of silicon and integrated photonics for node-level integration. When does copper (the established solution) lose? There is a point at which we're going to have to make those changes, and it's going to be chiplet integration. We ask how we going to make sure the modules for optics can go into advanced cooling systems - light has different behavior, so how do we make sure that we're building something that actually will work?You know, in all those different environments, optics was a big area of focus for Intel, and we're seeding future innovation as well as open silicon initiatives. Understanding all the different dynamics that are happening in the industry, whether it's our IDM 2.0, whether it's key acquisitions that have happened to meet the ecosystem, or general consolidation, fewer foundries available across the world, or all the different AI chiplets that you've mentioned earlier - these domains are requiring more and more partnership at all the different layers. At Intel we focus on a lot of layers, but not all! Then we produce a product, someone else produces a product, and we all use standards like PCIe or OAM to work together. I mean the new CXL standard is about having a fundamental understanding of the TLB on a device that is not my own!So, I mentioned optics with cooling, but cooling anywhere - it’s about how we think about cooling at the edge in these tiny colocation facilities, versus cooling in a more hyperscale situation. If we go back to the beginning - HPC invented this stuff. There were fish tank designs, and cold plates, and all sorts of cool things that came together. OCP is about how we make sure that every single one of those things is not a new research project that takes two and a half years, but can become a commodity way we think about it. I don't know about you, but I grew up loving Fry's Electronics, and I still remember that old, gaming system with an immersion cooling solution that was really kind of a cold plate solution, where it went through the front, and it was so cool, and it lit up, and I just thought it was the coolest thing I've ever seen. We need something like a server equivalent, maybe without the lights (Ian: RGB?), but it needs to be easy for us to take advantage of those technologies, because fundamentally that will make our overall power footprint in this industry more efficient and effective for serving the incredible demand that exists. So that's feeding future innovation, that's what we want to do. We have a lot of aspirations, you know, we had to get it done.IC: I was going to talk about the Future Technologies Initiative, but I think you've basically covered it. Fantastic!RW: What's interesting about that one is that that was a community-led initiative - the Future Technology Symposium started with the community, and then the board made it official. Now we've done a good mapping between what was already in the Future Technologies Initiative workstreams, because that started in late 2019, and what the board decided [should be other features under that umbrella]. But Future Technologies is interesting because they are more about service models, like organizing the cloud service model, or an AI hardware model, and a software code design mode. It’s about recognizing the domain space of heterogeneous compute exists, because in the future you will no longer be able to run a generic general-purpose computational solution that isn’t deeply aware of the software running on top. It also has to meet the needs of AI right? We also have Software Defined memory, and I already hinted about CXL and view of what's going to happen there in terms of device models. It’s a completely different device model than anything that we sort of grew up with on assumptions for IO.The industry is just, you know, it's amazing. I mean that's why I'm at it!Many thanks to Rebecca and her team for their time.Also thanks to Gavin for the transcription.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16872/open-compute-project-an-interview-with-intels-rebecca-weekly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The AMD Ryzen 7 5700G, Ryzen 5 5600G, and Ryzen 3 5300G Review\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-08-04T17:45:00Z\n",
      "URL: https://www.anandtech.com/show/16824/amd-ryzen-7-5700g-and-ryzen-5-5600g-apu-review\n",
      "Content: Earlier this year, AMD launched its Zen 3 based desktop processor solutions with integrated graphics. Marketed as the Ryzen 5000G family, these processors are the latest offering to combine AMD’s high-performing Zen 3 cores with tried-and-tested Vega 8 graphics, all built on TSMC’s 7nm process. As desktop processors, AMD made them available to system builders for a few months, allowing the company to keep track of stock levels and have a consistent demand during a high-demand phase for silicon. But on August 5th, they will be made available to buy at retail, and we’ve got the most important models to test.The AMD Ryzen 5000G APUs: Cezanne SiliconAMD actually came to market with its Zen 3-based integrated graphics silicon back in December 2020. The Cezanne silicon, with eight Zen 3 cores and up to Vega 8 graphics, was first earmarked for laptop use. We saw it come to market in that fashion for the 15 W and 45 W segments, and tested it in the ASUS ROG Flow X13 laptop in a 35 W variation, paired with a discrete graphics card.AMD Ryzen 9 5980HS Cezanne Review: Ryzen 5000 Mobile TestedAs a mobile chip in both low power and high power segments, it had to do duty as both a processor and graphics at 15 W, but mainly as a processor at 45 W powering a discrete graphics card. For the desktop processors, AMD cranks up the power even more to 65 W, where it is expected to perform either as a good CPU powering a GPU, or as an integrated graphics solution and do everything.The Ryzen 5000G processor series is an upgrade over last year’s Ryzen 4000G processor series. AMD co-designed both of these processor families to re-use key parts of the chip design, enabling a fast time-to-market and quicker upgrade cycle for AMD’s partners (OEMs), like Dell, HP, Lenovo, and others. The biggest re-use between the two families is the graphics, which has Vega 8 graphics on both, and although there’s a hefty frequency bump for 5000G in mobile, the desktop parts actually see a decline. We expect AMD to have its latest RDNA 2 graphics in its G processors next time around, but for now it stays the same because that helps expedite the design for these processors.AMD CEO, Dr. Lisa Su, with Cezanne siliconThe major difference between the 4000G and 5000G processors is that the new hardware uses eight of AMD’s latest Zen 3 CPU cores, which is an upgrade over the eight Zen 2 cores from last year. The highlight is the +19% raw performance uplift when comparing the two at the same frequency. The second major difference is that the 4000G processors never technically came to retail (but we reviewed them anyway), while AMD is making the Ryzen 7 5700G and Ryzen 5 5600G as individual products that customers can buy.Under the hood, there are a few more key changes that enthusiasts will be interested in. The 8-core Zen 3 design doubles the L3 cache per core, but also combines it into a single 16 MB L3 cache structure. This enables any of the eight cores to access the full cache, reducing latency to main memory (from 4 MB to 16 MB). The previous design had two clusters of four Zen 2 cores, so while it still had 8 cores, each cluster only had access to 4 MB of L3 cache. This is of sizable importance when it comes to workloads that sit in the 4 MB to 16 MB memory space, such as integrated graphics gaming and discrete graphics.The new processor is 180 mm2 in size, compared to 156 mm2 of the last generation, but still fits into the same socket. It contains 10.7 billion transistors, which is up from 9.8 billion. This means an effective decrease in transistor density, although we know that Zen 3 cores are slightly larger than Zen 2 cores, and some additional security measures have been added.There are six desktop processors in this family, and two of them are coming to store shelves.AMD Ryzen 5000G Series APUsAnandTechCore /ThreadBaseFreqTurboFreqGPUCUsGPUFreqPCIe*TDPRyzen 5000GRyzen 7 5700G8 / 16380046008200016+4+465 WRyzen 7 5700GE8 / 16320046008200016+4+435 WRyzen 5 5600G6 / 12390044007190016+4+465 WRyzen 5 5600GE6 / 12340044007190016+4+435 WRyzen 3 5300G4 / 8400042006170016+4+465 WRyzen 3 5300GE4 / 8360042006170016+4+435 W*PCIe lanes on the SoC are listed in 16xGFX + 4xChipset + 4 for NVMeThe top part is a Ryzen 7 5700G, featuring eight cores and sixteen threads, with a base frequency of 3.8 GHz and a turbo frequency of 4.6 GHz. The Vega 8 graphics runs at 2000 MHz, and we get sixteen lanes of PCIe 3.0 for graphics, plus another four for storage and four for the chipset.. TDP of the chip is rated at 65 W, although in most motherboards the Package Power Tracking will bump power up to 88 W. The Ryzen 7 5700G will have an MSRP of $359.The second part is a Ryzen 5 5600G, featuring six cores and twelve threads, with a base frequency of 3.9 GHz and a turbo frequency of 4.6 GHz. It drops down in graphics to Vega 7, running at 1900 MHz, but has the same PCIe 3.0 and TDP settings as the Ryzen 7. AMD has indicated that the 5600G should retail for $259.We’re also testing a third part in this review, the Ryzen 3 5300G, which sits near the bottom of the stack. With only four cores and eight threads, up to 4.2 GHz turbo and Vega 6 graphics running at 1700 MHz, this processor contains only half the L3 cache (8 MB total) of the other two. It sounds like it would make a nice $150 processor if it came to retail, and users can pick this processor up on eBay, but it currently it sits at $272 plus shipping, making it more expensive than the 5600G. Nonetheless, as we had tested Ryzen 7/5/3 from the 4000G series, we wanted to compare up against the 5000G to see if this is a line AMD might consider going beyond OEM deployment.This is ultimately why a staggered launch from laptop to desktop over the course of eight months allows AMD to pitch where its desktop integrated graphics processors should sit in the marketplace. These Cezanne processors use Zen 3 CPU cores, for example, whereas the older ones had Zen 2, Zen+, and Zen before it. What makes these ones different this time around is that AMD is cutting the Ryzen 3 from retail, but the Ryzen 7 at the high-end is now available at retail. The only one that has been consistent is Ryzen 5, and we can compare the Ryzen 5 processors over the years:Ryzen 5 APUs (65W)AnandTechCoresBaseFreqTurboFreqL3MBPCIeGPUDDR4PriceZen37nmRyzen 5 5600G6 / 1239004400163.0 x24Vega83200$259Zen27nmRyzen 5 4650G6 / 12370042004+43.0 x24Vega83200OEMZen+12nmRyzen 5 3400G4 / 83700420043.0 x8Vega112933$149Zen14nmRyzen 5 2400G4 / 83600390043.0 x8Vega112933$169AMD has kept the Vega graphics through all four generations, but moved down from the silicon having Vega 11 on 12nm to Vega 8 on 7nm – AMD said that this was because of density increases and finding the right balance, but also the uplift in frequency and power efficiencies the new process node provided.As it stands, these two new processors at retail fill out AMD's retail offerings, at least down to $259. One of the key benefits is that these two new processors are cheaper than the existing CPU-only offerings, but also both come with appropriate coolers when they are run at their default power modes. Otherwise, the most notable differences between AMD's Ryzen 5000 CPUs and 5000G APUs comes down to the APUs only have half as much L3 cache, and of course, the APUs also come with integrated graphics. Given that modern graphics cards don’t even need PCIe 4.0 levels of bandwidth, we have to see if the cache difference and any CPU frequency differences are worth the price difference.AMD Ryzen 5000 CPU vs APU ComparisonsAnandTechCore /ThreadBaseFreqTurboFreqGPUCUsGPUFreqPCIeL3MBTDPSEPRyzen 7Ryzen 7 5800X8 / 1638004700--4.0 x2432105 W$449Ryzen 7 5700G8 / 1638004600820003.0 x241665 W$359Ryzen 5Ryzen 5 5600X6 / 1237004600--4.0 x243265 W$299Ryzen 5 5600G6 / 1239004400719003.0 x241665 W$259The key thing with the Ryzen 7 comparison is the TDP difference – why the frequency might only be 100 MHz change, at 105 W TDP (or 120W PPT), it will keep its turbo better.Chipset SupportAMD has confirmed that X570, B550, and A520 motherboards will support the new 5000G processors. X470 and B450 motherboards might also be supported, but that depends on the motherboard manufacturer. AMD recommends a BIOS with AGESA version 1203b for full performance.This ReviewIn this article, we will be testing the Ryzen 7 5700G, Ryzen 5 5600G, and Ryzen 3 5300G on our test suite, covering raw CPU performance, integrated graphics performance, but also performance when paired with a discrete GPU.AnandTechExampleProcessorsMotherboardDRAMPSUSSDAMDZen3 APURyzen 7 5700GGIGABYTEX570 Aorus IPro (F34)ADATA32 GBDDR4-3200CorsairAX860iCrucialMX5002 TBRyzen 5 5600GRyzen 3 5300GZen2 APURyzen 7 4750GGIGABYTEX570 Aorus IPro (F30a)ADATA64 GBDDR4-3200CorsairAX860iCrucialMX5002 TBRyzen 5 4650GRyzen 3 4350GZen+ APURyzen 5 3400GGIGABYTEX570 Aorus IPro (F30a)ADATA64 GBDDR4-2933CorsairAX860iCrucialMX5002 TBZen APURyzen 5 2400GGIGABYTEX570 Aorus IPro (F30a)ADATA64 GBDDR4-2933CorsairAX860iCrucialMX5002 TBIntelRocket LakeCore i7-11700KASUS MaximusXIII HeroBIOS0610CorsairAX1600iCrucialMX5002TBBroadwellCore i7-5775CGIGABYTEZ97X-UD5H(F10)Geil Veloce16 GBDDR3-1600Antec HCP1250WCrucialMX5002 TBCore i5-5675CTiger LakeCore i7-1185G7IntelReference32 GBLPDDR4XIntegratedSamsungPCIe 3.0Many thanks to...We must thank the following companies for kindly providing hardware for our multiple testbeds. Some of this hardware is not in this testbed specifically, but is used in other testing.Hardware Providers for CPU and Motherboard ReviewsSapphireRX 460 NitroNVIDIARTX 2080 TiCrucial SSDsCorsair PSUsG.Skill DDR4ADATA DDR4SilverstoneCoolersNoctuaCoolersA big thanks to ADATA for the ​AD4U3200716G22-SGN modulesfor this review. They're currently the backbone of our AMD testing.Read on for the full review.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16824/amd-ryzen-7-5700g-and-ryzen-5-5600g-apu-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The ZTE Axon 30 Ultra Review - Something Surprisingly Different\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-07-30T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/16838/the-zte-axon-30-ultra-review\n",
      "Content: Today, we’re taking a closer look at a somewhat particular device, the new ZTE Axon 30 Ultra. The phone had beenlaunched in the US back in early June, a fact that is alone somewhat remarkable for the relatively more restrictive North American market. While ZTE over the last few years had taken a more subdued position in the mobile market, in particular having alarge set-back with US regulators back in 2018, this year it seems the company is back on track and providing products as interesting as ever.The Axon 30 Ultra is quite a departure from past ZTE devices: particularly on the product positioning, design, and features of the phone. Featuring a new flagship Snapdragon 888 chip, it’s a larger step-up from the premium range Axon 20 5G. Featuring the “Ultra” denomination ZTE here is also trying to mimic the Samsung flagship device naming, although while the Axon 30 Ultra doesn’t really feature every single specification deemed worthy of an “Ultra” name, it also comes at a vastly more reduced price point of $749 – a price point that is particularly more interesting for US readers given the sheer lack of alternatives in the market.ZTE Axon 30 UltraAxon 30 UltraSoCQualcomm Snapdragon 8881xCortex-X1@ 2.84GHz3xCortex-A78@ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 660 @ 840MHzDRAM8/12 GB LPDDR5Storage128/256GB UFS 3.1Display6.67\" AMOLED2400 x 1080 (20:9)144Hz300Hz TouchSizeHeight161.53 mmWidth72.96 mmDepth8.0 mmWeight188 gramsBattery Capacity4600mAh65W charging (PD3.0)Wireless Charging-Rear CamerasWide64MP IMX6860.8µm pixels (1.6µm 4:1 16MP)f/1.6 w/OIS26mm eq.Main64MP GW30.7µm pixels (1.4µm 4:1 16MP)f/1.935mm eq.Ultra-wide64MP GW30.7µm pixels (1.4µm 4:1 16MP)f/2.213mm eq.Telephoto8MPf/3.4 w/OIS(Periscope design)120mm eq.Front Camera16MPf/2.5I/OUSB-C 3.1Wireless (local)802.11ax WiFi-6EBluetooth 5.2 LE + NFCOther FeaturesDual SpeakersUnder-screen optical fingerprint sensorDual-SIMDual nanoSIMLaunch Price8GB+128GB: $749 / £649 / €74912GB+256GB: $849 / £739 / €849In terms of specifications, the Axon 30 Ultra features Qualcomm’s Snapdragon 888 which puts the phone in the top-performing flagship category, and we expect the device to largely perform the same as many of the other alternatives in the market. It comes with either 8 or 12GB of LPDDR5 RAM and either 128GB or 256GB of UFS 3.1 storage options – there is no expandable storage available.The display of the Axon 30 Ultra is both high-performance, but also not quite top of the line as we’ve seen from other vendors. ZTE chose to remain with a lower 2400 x 1080 resolution with the 6.67” display – which is a step below that of other 1440p alternatives such as from Samsung, Xiaomi or OnePlus/OPPO. ZTE however does include high refresh rate and manages to push things up to 144Hz, along with a 300Hz touch sampling rate. It’s somewhat of a similar approach ASUS takes on the ROG phones, and the lower resolution will be more efficient given that it doesn’t feature any newer advanced display technologies.At 6.67” and a body footprint of 161.5 x 72.96mm, the Axon 30 Ultra is considerably smaller than other “Ultra” phones in the market, and actually fits within what I consider myself a sweet-spot between large screen and still good one-handed device handling.The phone actually still features a 4600mAh battery which is just a little less than other big phones, however ZTE managed to keep the weight down at a reasonable 188g, again something that I really appreciate and is immediately noticeable when coming from other contemporary phones.The phone features both curved front display glass as well as a curved back glass which makes for very good ergonomics and the phone feels smaller than it is because of it. It’s still a very “edgy” phone in that the metal frame sticks out a bit on the sides of the phone, and the top and bottom faces are flat – it’s a design aspect that sometimes works and sometimes doesn’t, but in this case, I think it was well implemented and gives the phone a better solid grip.ZTE Axon 30 Ultra CamerasOpticsSensor35mmeq. FLFoV(H/V/D)ApertureOISResolutionPixelPitchPixelRes.SensorSizeUltra-Wide13.85102.6°86.4°114.8°f/2.2✗64.2M native(9248 x 6944)16.0M binned(4624 x 3472)0.7µm1.4µm40.0″79.9″1 / 1.98\"6.47mm x 4.86mm31.46mm²Main (Wide)26.1167.1°52.9°79.3°f/1.6✓64.2M native(9248 x 6944)16.0M binned(4624 x 3472)0.8µm1.6µm26.1″52.2″1 / 1.73\"7.39mm x 5.55mm41.09mm²2nd Main31.4557.7°44.9°69.1°f/1.9✗64.2M native(9248 x 6944)16.0M binned(4624 x 3472)0.7µm1.4µm22.4″44.9″1 / 1.98\"6.47mm x 4.86mm31.46mm²2nd Telephoto123.116.01°12.04°19.94°f/4.9✓7.99M native(3264 x 2448)16.0M scaled(4624 x 3472)1.0µm17.7″1 / 3.92\"3.26mm x 2.44mm7.99mm²The camera setup on the Axon 30 Ultra quite interesting, with a unique camera setup: the ultra-wide, main (wide), and a S21-like secondary wide-angle unit featuring all 64MP sensor, alongside a small periscope telephoto module.The ultra-wide and the secondary wide-angle feature 64.2MP quad-Bayer sensors that bin down to 16MP in regular shots. These are 0.7µm pixels and thus the sensors aren’t actually that large in terms of their 1/1.98” formats, and we’d generally want to keep them in their binned down mode most of the time. The ultra-wide has 13.85mm eq. optics with f/2.2, while the secondary wide has 31.45mm eq. f/1.9 optics. The latter is quite unusual – it’s similar to the S21’s secondary, but due to the quad-Bayer setup and the lack of OIS, isn’t quite as useful. ZTE calls this a portrait lens, though it’s a quite shorter focal length than what we’re used to for portrait photography.The main camera is also 64.2MP, though this is a larger sensor with 0.8µm pixels and a 1/1.73” format. It still bins down to 16MP for regular shots, and features OIS with 26.11mm eq. optics with f/1.6 aperture.Finally, the telephoto module is a tiny 8MP sensor with 1.0µm pixels. As a 1/3.92” format as only 7.99mm² area it’s quite small, but the optics have a long 123.1mm focal length, although the aperture is also dark at f/4.9.Usually we see vendors employ “dummy” sensors in order to bloat up their camera module count, but here ZTE does employ four functional modules, and that second wide-angle module is definitely interesting in what it tries to achieve.Generally, the ZTE Axon 30 Ultra comes with excellent build quality that’s exceptionally solid and premium feeling, along with a design that, while not out of the ordinary, is very ergonomic and works well for the phone. Generally it’s also the “smaller” form-factor which makes the device stand out, being a more reasonable 73mm width along with being lighter than the competition.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16838/the-zte-axon-30-ultra-review\n",
      "Title: Intel's Process Roadmap to 2025: with 4nm, 3nm, 20A and 18A?!\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-07-26T21:00:00Z\n",
      "URL: https://www.anandtech.com/show/16823/intel-accelerated-offensive-process-roadmap-updates-to-10nm-7nm-4nm-3nm-20a-18a-packaging-foundry-emib-foveros\n",
      "Content: In today’s Intel Accelerated event, the company is driving a stake into the ground regarding where it wants to be by 2025. CEO Pat Gelsinger earlier this year stated that Intel would be returning to product leadership in 2025, but hasn’t yet explained how this is coming about – that is until today, where Intel has disclosed its roadmap for its next five generations of process node technology leading to 2025. Intel believes it can follow an aggressive strategy to match and pass its foundry rivals, while at the same time developing new packaging offerings and starting a foundry business for external customers. On top of all this, Intel has renamed its process nodes.The Short Answer:If you only take one thing away from this article, I'm going to put it here front and center. Here is what we're seeing for Intel's roadmaps, based on their disclosures today.As always, there is a difference between when a technology ramps for production and comes to retail; Intel spoke about some technologies as 'being ready', while others were 'ramping', so this timeline is simply those dates as mentioned. As you might imagine, each process node is likely to exist for several years, this graph is simply showcasing the leading technology from Intel at any given time.If you want the details on this graph, then read on.Intel's Defines a Strong Future: Is TSMC at Risk?Earlier this year, CEO Pat Gelsinger announced Intel’s new IDM 2.0 strategy, consisting of three elements:Build (7nm)Expand (TSMC)Productize (Intel Foundry Services)The goal here is to continue to work on Intel’s process node technology development, going beyond the current 10nm designs in production today, but simultaneously using other foundry services from partners (or competitors) to regain/retain Intel’s position in its processors that drive a lot of the company revenue. The third element is IFS, Intel’s Foundry Services, where Intel is committing in a big way to opening up its manufacturing facilities to external semiconductor business.Underpinning (1) and (3) is how Intel executes on its own process node development. While in Intel’s recent Q3 2021 earnings call CEO Gelsinger confirmed that Intel is now producing more 10nm wafers in a day than 14nm wafers, marking a shift in confidence between the two designs, it is no secret that Intel has had difficulty in transitioning from its 14nm process to its 10nm process. On June 29ththis year, Intel also stated that itsnext generation 10nm productrequires additional validation time to streamline deployment on enterprise systems for 2022. Note that at the same time, TSMC has surpassed Intel by shipping at capacity with its equivalent designs (called 7nm) and its leading edge (5nm) designs that surpass Intel’s performance.As with the previous announcement in March, Intel is reaffirming that it intends to return to leadership performance in semiconductors in 2025. This will enable both the company to compete better as it builds its own products (1) but also offer a wider portfolio of performance and technologies for its future IFS customers (3). To do this, it is realigning the roadmap for its future process node technologies to be more aggressive with improvements, yet at the same time more modular with its technology to enable faster transitions.Leading up this plan is Dr. Ann B Kelleher, who was named SVP and GM of the Technology Development division at Intel last year. This division is where all the research and development of Intel’s future process node technologies and enhancements comes from – it used to be part of Intel’s System Architecture Group, however it was split in July 2020 to re-establish a focus purely on Technology Development. Dr. Kelleher’s background involves process research in academia, followed by 26 years at Intel as a process engineer, moving up to managing Fab 24 in Ireland, Fab 12 in Arizona, Fab 11X in Rio Rancho, before landing in HQ in Oregon as the GM of Manufacturing and Operations.Her experience covering both fab-scale production and process node research is going to be critical for Intel’s future plans. In discussing with Kelleher ahead of today’s announcements, she stated that she has implemented fundamental changes when it comes to supplier approach, ecosystem learnings, organizational changes, modular design strategies, contingency plans, and realigning the Technology Development Team into a more streamlined outfit ready to execute. These include key personnel such asSanjay Natarajanas SVP and GM of Logic Development (one ofIntel’s recent rehires) andBabak Sabias CVP and GM of Assembly/Test DevelopmentIntel is today defining ‘technology leadership by 2025’ as defined by the metric of performance per watt. We asked Intel is a pre-briefing what that means for peak performance, which is often a metric we care about for end product design, and the answer was that \"peak performance remains a key part of Intel’s strategic development\".Intel Renames The Nodes: ‘Mine is Smaller’The problem with simply posting Intel’s roadmap here is that the news is two-fold. Not only is Intel disclosing the state of its technology for the next several years, but the names of the technology are changing to better align with common industry norms.It is no secret that having \"Intel 10nm\" being equivalent to \"TSMC 7nm\", even though the numbers actually have nothing to do with the physical implementation, has ground at Intel for a while. A lot of the industry, for whatever reason, hasn’t learned that these numbers aren’t actually a physical measurement. They used to be, but when we moved from 2D planar transistors to 3D FinFET transistors, the numbers became nothing more than a marketing tool. Despite this, every time there’s an article about the technology, people get confused. We’ve been talking about it for half a decade, but the confusion still remains.To that end, Intel is renaming its future process nodes. Here’s the roadmap image, but I’ll be breaking it down piece by piece.2020, Intel 10nm SuperFin (10SF): Current generation technology in use with Tiger Lake and Intel’s Xe-LP discrete graphics solutions (SG1, DG1). The name stays the same.2021 H2, Intel 7: Previously known as 10nm Enhanced Super Fin or 10ESF. Alder Lake and Sapphire Rapids will now be known as Intel 7nm products, showcasing a 10-15% performance per watt gain over 10SF due to transistor optimizations. Alder Lake is currently in volume production. Intel’s Xe-HP will now be known as an Intel 7 product.2022 H2, Intel 4: Previously known as Intel 7nm. Intel earlier this year stated that its Meteor Lake processor will use a compute tile based on this process node technology, and the silicon is now back in the lab being tested. Intel expects a 20% performance per watt gain over the previous generation, and the technology uses more EUV, mostly in the BEOL. Intel’s next Xeon Scalable product, Granite Rapids, will also use a compute tile based on Intel 4.2023 H2, Intel 3:Previously known as Intel 7+.Increased use of EUV and new high density libraries. This is where Intel’s strategy becomes more modular – Intel 3 will share some features of Intel 4, but enough will be new enough to describe this a new full node, in particular new high performance libraries. Nonetheless, a fast follow on is expected. Another step up in EUV use, Intel expects a manufacturing ramp in the second half of 2023 with an 18% performance per watt gain over Intel 4.2024, Intel 20A: Previously known as Intel 5nm. Moving to double digit naming, with the A standing for Ångström, or 10A is equal to 1nm. Few details, but this is where Intel will move from FinFETs to its version of Gate-All-Around (GAA) transistors called RibbonFETs. Also Intel will debut a new PowerVia technology, described below.2025, Intel 18A: Not listed on the diagram above, but Intel is expecting to have an 18A process in 2025. 18A will be using ASML’s latest EUV machines, known as High-NA machines, which are capable of more accurate photolithography. Intel has stated to us that it is ASML’s lead partner when it comes to High-NA, and is set toreceive the first production model of a High-NA machine. ASML recently announced High-NA was being delayed- when asked if this was an issue, Intel said no, as the timelines for High-NA and 18A are where Intel expects to intersect and have unquestioned leadership.Intel has confirmed to us that Intel 3 and Intel 20A will be offered to foundry customers (but hasn’t stated if Intel 4 or Intel 7 will be).To bring this altogether in a single table, with known products, we have the following:Intel's Process Node TechnologyOld NameNew NameRoadmapProductsFeatures10SF10SFTodayTiger LakeSG1DG1Xe-HPC Base TileAgilex-F/I FPGASuperMIMThin Film BarrierVolume 10nmOn sale today10ESFIntel 72021 H2 productsAlder Lake (21)Raptor Lake (22)?Sapphire Rapids (22)Xe-HPXe-HPC IO Tile10-15% PPWUpgraded FinFETADL in Ramp today7nmIntel 42022 H2 ramp2023 H1 productsMeteor Compute TileGranite Compute Tile20% PPW vs 7More EUVSilicon in Lab7+Intel 32023 H2 products-18% PPW vs 4Area SavingsMore EUVNew Perf LibrariesFaster Follow On5nmIntel 20A2024-RibbonFETPowerVia5+Intel 18A2025Unquestioned Leadership2nd Gen RibbonHigh NA EUVOne of the issues here is the difference between a process node being ready, ramping production for product launches, and actually being made available. For example, Alder Lake (now on Intel 7nm) is due to come out this year, but Sapphire Rapids is going to be more of a 2022 product. Similarly, there are reports of Raptor Lake on Intel 7 coming out in 2022 to replace Alder Lake with the tiled Meteor Lake on Intel 4 in 2023. While Intel is happy to discuss process node development time frames, product timeframes are not as open (as no doubt customers would get frustrated if the time stated is missed).Why The Nodes Were RenamedSo as stated before, one element of renaming the nodes is due to matching parity with other foundry offerings. Both TSMC and Samsung, competitors to Intel, were using smaller numbers to compare similar density processes. With Intel now renaming itself, it gets more in-line with the industry. That being said, perhaps sneakily, Intel’s 4nm might be on par with TSMC’s 5nm, reversing the tables. By 3nm we expect there to be a good parity point, however that will depend on Intel matching TSMC’s release schedule.Rather than throw process node names everywhere, it is typical to refer to peak quoted transistor densities instead. Here is the table we published in ourrecent IBM 2nm news post, but with an updated shift on Intel’s naming.2021 Peak Quoted Transistor Densities (MTr/mm2)AnandTechProcess NameIBMTSMCIntelSamsung22nm16.5016nm/14nm28.8844.6733.3210nm52.51100.7651.827nm91.20100.7695.085/4nm171.30~200*126.893nm292.21*2nm / 20A333.33Data from Wikichip, Different Fabs may have different counting methodologies* Estimated Logic DensityExactly where Intel’s new 4nm and below will end up is yet to be disclosed, as numbers with stars alongside are based on estimates by the respective companies.It has been expected for a while that Intel would be realigning its process node naming. Behind closed doors, I personally have been lobbying for it for a while, and I know that a few other journalists and analysts have been suggesting it to Intel as well. Some responses we received were related to apathy – one executive told me that \"our customers that care about this actually know the difference\", which is true for sure, but what we’re talking about here is more about perception in the wider ecosystem for enthusiasts and financial analysts who might not be up to speed. It is more or less a branding exercise, and I also told Intel that they are going to have to expect a mixed response – some voices might interpret the move as Intel trying to pull one over on the market, for example. But they’re going to have to live with it, as these are the new names.Meanwhile, despite Intel’s struggles with 10nm, it is still a process node in production and in volume production, in use for both consumer and enterprise devices, and it's coming to desktops very soon. Even though it has some stiff competition from other players, it is still an offering in the market, and for those that want to compare process node densities using these names, it should have a moniker to avoid confusion. I am applauding that Intel is doing it sooner rather than later.One key point to note is that the new Intel 7 node, which was formerly the 10ESF node, is not necessarily a \"full\" node update as we typically understand it. This node is derived as an update from 10SF, and as the diagram above states, will have ‘transistor optimizations’. Moving from 10nm to 10SF, that meantSuperMIMand new thin-film designs giving an extra 1 GHz+, however the exact details from 10SF to the new Intel 7 is unclear at this point. Intel has however stated that moving from Intel 7 to Intel 4 will be a regular full node jump, with Intel 3 using modular parts of Intel 4 with new high-performance libraries and silicon improvements for another jump in performance.We asked Intel if these process nodes will have additional optimization points, and were told that they will – whether any of them will be explicitly productized will depend on the features. Individual optimizations may account for an additional 5-10% performance per watt, and we were told that even 10SF (which keeps its name) has had several additional optimization points that haven’t necessarily been publicized. So whether these updates get marketed as 7+ or 7SF or 4HP is not known, but as with any manufacturing process as updates occur to help improve performance/power/yield, they get applied assuming the design adheres to the same rules.\"Isn't Intel Just Trying To Pull The Wool Over Our Eyes?\"No.The problem here is that there is no consistent node naming between foundries. Intel has been saving any number change for major advances in its node manufacturing technology, instead using +/++ to signify improvements. If we compare this to TSMC and Samsung, both of whom have been happy to give half-node jumps new numbers entirely.For example, Samsung's 7LPP is a major node, however 6LPP, 5LPE and 4LPE are all iterative efforts on the same design (arguably also iterative of 8LPP), with 3GAE being the next major jump. Compare this to Intel, who was planning 10nm to 7nm to 5nm as major process node jumps – so while Samsung had one jump planned and 4 sub-variants (or more), Intel had two major jumps. Similarly, TSMC's 10nm was a half-node jump over 16nm, while 16nm to 7nm was the full node – Intel made 14 to 10 to 7 as full nodes.Intel stuck to its guns a long while, and delays to 10nm effectively hurt it in a multiplicative fashion. For example, if Intel had labeled 14+ as 13nm, and 14++ as 12nm, perhaps it wouldn't be so bad. I mean, yes Intel should expect some hurt for 10nm being late, but when other foundries were showcasing smaller steps as full number jumps, it became a marketing and media nightmare. 14++++ became an industry joke, and coupled with how every time when they talked about future process nodes they had to cite the equivalent TSMC of Samsung process, it got a bit too much. It had to be explained every time, as new people come into the industry.I've lobbied Intel to adjust its naming for a while, and I know other peers have as well. When we refer to Intel 7 from now on, we can draw equivalents to TSMC 7nm (even if TSMC is shipping 5nm in volume) without having to extensively explain differences in a simple name. This isn't Intel pulling the wool over your eyes, or trying to hide a bad situation.This is Intel catching up to the rest of the industry in how these processes are named.To add to this, it's a good thing that Intel is only renaming future nodes that haven't reached the market yet.This is a multi-page article!Click the dropdown below for more pages, includingThis Page, New Node NamesA Sidebar on Intel EUV and becoming ASML Lead PartnerNew for 2024: RibbonFETs and PowerViasNext Gen EMIB and Foveros PackagingCustomers Customers Customers\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16823/intel-accelerated-offensive-process-roadmap-updates-to-10nm-7nm-4nm-3nm-20a-18a-packaging-foundry-emib-foveros\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: PlasticArm: Get Your Next CPU, Made Without Silicon\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-07-22T12:01:00Z\n",
      "URL: https://www.anandtech.com/show/16837/plasticarm-get-your-next-cpu-without-silicon\n",
      "Content: Known for its core design IP that ends up in everything from IoT to smartphones to servers, Arm is now presenting that it has enabled one of its key microcontrollers in a new form factor: rather than using silicon as a base, the company has enabled a processor core in plastic. The technology has been in the works for almost a decade, but Arm has been waiting on the fabrication methods to create a fully working core. Now the company has something working in a tangible medium and the research has been published in Nature.Creating a Plastic CPU'Plastic' or flexible electronics have been with us for a long while, and usually involve large yet simple designs for electronics flow, or basic 8-bit adders, all the way up to displays. What we're seeing now is something a little different - the key news as published today is that Arm, in association with PragmatIC, has produced a fully functional non-silicon version of one of Arm’s most popular microcontrollers, the M0.This M0 core sits right at the bottom of Arm’s core product stack, however the minimalist design is a popular one for silicon processors due to its low die area and power requirements for simple microcontroller tasks. So while it won’t be powering your next big device any time soon, lots of integrated electronics that you own will likely already be relying on M0 cores for fundamental control tasks.PlasticArm, as it is now called, recreates the M0 core in a flexible plastic medium. This is important in two factors – first, the ability to enable processors or microcontrollers in something other than silicon will allow some amount of programmability in packaging, clothing, medical bandages, and others. Paired with a particle sensor, for example, it might allow for food packaging to determine when what is inside is no longer fit for human consumption due to spoilage or contamination. The second factor is cost, with flexible processing at scale being orders of magnitude cheaper than equivalent silicon designs. To Arm’s credit, the new M0 design here is reported to be 12x more powerful than current state-of-the-art plastic compute designs.Details on the Plastic M0In Arm’s press release, the company states that the Plastic M0 design has 128 bytes of RAM and 456 bytes of ROM, while also supporting a 32-bit Arm microarchitecture.Inside theresearch paper published at Nature, we get fine-grained details.The processor is built with a polyimide substrate and is formed through thin-film metal-oxide transistors, such as IGZO TFTs. This means that this is still technically a photolithography process, using spin-coating and photoresist techniques, ending up with the processor having 13 material layers and 4 routable metal layers. However as TFT designs have been widespread since the use of IGZO displays, the cost of production is still quite low.The core supports the ARMv6-M architecture, with a 16-bit Thumb ISA combined with a sub-set of 32-bit Thumb. As with the regular M0, data and address widths are 32-bit, the in-order design is a 2-stage pipeline, and the core supports 86 instructions. The main difference to a silicon M0 core is that the register file, rather than being inside the CPU, is mapped to the 128-byte bank of DRAM. This is because the TFT design is better supported through a memory mapping technique. Despite this, the Plastic M0 core is binary compatible with all other Cortex M0 cores.A typical die size for a silicon Cortex M0 using TSMC’s 90nm process is 0.04 mm2, whereas PlasticArm is using an equivalent 800nm TFT process and the core size is 59.2 square millimeters (7.536 mm x 7.856 mm). This makes the Plastic M0 core about 1500x the size of a standard IoT implementation. The other big difference is in frequency – the research paper states that the Plastic M0 runs at around 20-29 kilohertz with a 3V input; an M0 on a 180nm Ultra-Low Leakage process optimized for power rather than frequency, in Arms own design documents, can run at 50 MHz. That’s a 1600-2500x difference in frequency.PlasticArm: the Plastic M0Process NodeFlexIC 800nmn-type IGZO TFT200nm polyimide waferDie Size59.2 mm2 (core only)(7.536 mm x 7.856 mm)Thicknessunder 30 micronISAARMv6-M16-bit Thumb + subset of 32-bitFrequency20-29 kilohertzPower21 milliWattsPin Count28 pinsMaterial Layers13 layersRoutable Metal Layers4 layersDevices5634039157 n-type TFT + 17183 resistorsThe Plastic M0 design uses 56340 devices, which is a mix of 39157 thin-film n-type transistors and 17183 resistors. Because the goal of this design was to not have any physically added resistors, the paper documents that implementing resistors at a TFT level within the layers involves using photolithography materials with higher resistance to enable a smaller size. Overall the paper predicts an equivalent silicon design of 18334 NAND2 gates. Overall power for the Plastic M0 core at 29 kHz is listed at 21 mW, 99% of which is static power (45% core, 33% memory, 22% IO). The 28 pins on the processor allow for clock signal generation, reset, GPIO, power, and debug.Research ScopeIn its press release, Arm states that one of the main barriers to production was down to technology and fabrication limitations – the project started in 2013 and even a prototype circuit was shown at Arm TechCon in 2015 using ring oscillators, counters, and shift register arrays. However a number of key questions were still left unsolved, mostly down to cell libraries for all the different components of a modern processor, along with tool flow and production. Over time Arm’s partner PragmatIC, through other projects it was working on, was able to build a range of cell libraries congruent with what was needed for an M0 processor. The first PlasticArm manufacture and validation was reportedly performed in October 2020.Arm’s research points to cell library production being a key to unlocking further designs in the future. As microcontrollers and processors get more complex more elements (and different types) are needed to create an end-to-end useable product. Going beyond M0 thus requires research into enabling individual libraries in a TFT design. Beyond this, the research paper also states that low-power libraries are needed to enable scale. Because most of the power consumption on Plastic M0 is static power, driving that down through design and manufacturing is going to be a direction of research. There is also the angle of manufacturing – this was all done on a photolithography process using 200nm polyimide wafers using deposition techniques. An ultimate goal of plastic processors is that size is less of a limitation, and they can be ‘printed’ using conventional ink techniques. We’re not there yet, but this is certainly a step in that direction.So while we can’t buy an Apple M1 built-in plastic just yet, there seems to be plenty of future potential in the technology.Arm Blog PostRelated ReadingArm Announces Mobile Armv9 CPU Microarchitectures: Cortex-X2, Cortex-A710 & Cortex-A510Arm Announces New Mali-G710, G610, G510 & G310 Mobile GPU FamiliesArm Announces Neoverse V1, N2 Platforms & CPUs, CMN-700 Mesh: More Performance, More Cores, More FlexibilityArm Announces Armv9 Architecture: SVE2, Security, and the Next DecadeThe Ampere Altra Review: 2x 80 Cores Arm Server Performance Monster\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16837/plasticarm-get-your-next-cpu-without-silicon\n",
      "Title: The Xiaomi Mi 11 Ultra Review: Big and Fast, but Inefficient\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-07-20T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16830/the-xiaomi-mi-11-ultra-review\n",
      "Content: Amongst of the newest trend of having a new tier of “Ultra” Android flagship devices that are specced and priced at a notably higher tier than the traditional high-end designs, we saw Xiaomi jump onto the fray with the new Mi 11 Ultra.Announced back in March, the phone took a few months to get to market with any reasonable availability and a bit more for more reasonable pricing, however the device is now fleshed out, and afterour in-depth camera review a few weeks ago, deserves an accompanying device review.The Mi 11 Ultra in a sense is a siblingto the Mi 11– and it is clear they are of the same generation family, having the same display specifications and the same device footprint, however where the Ultra differentiates itself is a much more massive and technically superior camera array. Let’s go over the specifications:Xiaomi Mi 11 SeriesMi 11 UltraMi 11SoCQualcomm Snapdragon 8881xCortex-X1@ 2.84GHz3xCortex-A78@ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 660 @ 840MHzDRAM12GB LPDDR5-64008/12GB LPDDR5-6400Display6.81\" AMOLED3200 x 1440120Hz Refresh480Hz TouchHDR10+ /10-bit panelSizeHeight164.3mm164.3mmWidth74.6mm74.6mmDepth8.38mm8.06mmWeight234g196gBattery Capacity5000mAh (Typical)67W Charging4600mAh (Typical)55W ChargingWireless Charging67W50WRear CamerasMain50MP GN2 1/1.12\" 1.4µm4:1 Binning to 12.5MP / 2.8µmf/1.95 w/OIS24mm eq.108MP HMX 1/1.3\" 0.8µm4:1 Binning to 27MP / 1.6µmf/1.85 w/OIS24mm eq.Telephoto5x optical telephoto48MP IMX586 1/2.0\" 0.8µm4:1 Binning to 12MP 1.6µmf/4.1120mm eq.5MP (Macro only)f/2.248mm eq.ExtraTelephoto--Ultra-Wide48MP IMX586 1/2.0\" 0.8µm4:1 Binning to 12MP 1.6µmf/2.2128° FoV13MPf/2.4123° FoVExtradToF Sensor-Front Camera20MP 0.8µm4:1 Binning 5MP 1.6µmf/2.2Storage256GBUFS 3.1128 / 256GBUFS 3.1I/OUSB-CWireless (local)802.11ax(Wifi6E),Bluetooth 5.2802.11ax(Wifi 6),Bluetooth 5.2Cellular4G + 5G NR NSA+SA Sub-6GHzSpecial FeaturesUnder-screen fingerprint sensorFull-range stereo speakersIR BlasterSecond 1.1\" 126 x 294Rear DisplayUnder-screen fingerprint sensorFull-range stereo speakersIR BlasterSplash, Water, Dust ResistanceIP68No ratingDual-SIM2x nano-SIMLaunch OSAndroid 11 w/ MIUIAndroid 11 w/ MIUILaunch Price12+256GB: 1199€8+128GB: 749€8+256GB: 799€The Mi 11 Ultra shares the same Snapdragon 888 as the Mi 11 and many other flagship devices in 2021. We’ve covered the SoC plenty this year in the various implementations, including Xiaomi – so there’s nothing particular in terms of standout features for the Ultra in this regard. Xiaomi does deliver the Mi 11 Ultra only in a 12GB DRAM and 256GB storage configuration which corresponds to the higher-end option on the Mi 11.From the front, the Mi 11 Ultra is nearly indistinguishable from the Mi 11 as it features the exact same screen and front facing design. The 6.81” AMOLED display comes with a resolution of 3200 x 1440, and features 120Hz refresh rate in a 10-bit colour panel, and Xiaomi advertises a touch input sample rate of up to 480Hz which is incredibly high in the market. Much like on the Mi 11, what’s lacking here is any more advanced technology to help with the power consumption of the panel – while there’s some coarse software refresh rate switching, there’s no real VRR and the display seems to be generally technically equivalent to what we’ve seen last year with the Galaxy S20 series devices. Keep this in mind throughout the review as it ends up being one of the Mi 11 Ultra’s key weaknesses.Design-wise, I really like Xiaomi’s flagship line-up this year as they combine aesthetics with extremely good ergonomics and handling. It’s to be noted though that at 74.6mm device width, it’s on the larger side of the device spectrum, although the rounded frame of the phone helps it feels smaller than comparative sized devices with more squarish edges.On the back of the phone, we find the phone’s very distinguishing feature, and that’s a massive camera setup that attempts to be outright unapologetic about how big and noticeable it is. Xiaomi had to make the camera bump large because it houses the largest camera sensor of any current generation phone on the market – a 50MP 1/1.12” main module that’s pretty much gargantuan for a smartphone.Xiaomi Mi 11 Ultra CamerasOpticsSensor35mmeq. FLFoV(H/V/D)ApertureOISResolutionPixelPitchPixelRes.SensorSizeUltra-Wide12.99106.3°90.0°118.0°f/2.2✗48.0M native(8000 x 6000)12.0MP binned(4000 x 3000)0.8µm1.6µm47.8″95.6″1 / 2.00\"6.40mm x 4.8mm30.72mm²Main (Wide)23.0173.9°58.9°86.5°f/1.95✓49.9M native(8160 x 6120)12.5MP binned(4080 x 3060)1.4µm2.8µm32.6″65.2″1 / 1.12\"11.42mm x 8.56mm97.88mm²Telephoto119.0716.55°12.45°20.60°f/4.1✓48.0M native(8000 x 6000)12.0MP binned(4000 x 3000)0.8µm1.6µm7.45″14.9″1 / 2.00\"6.40mm x 4.8mm30.72mm²Although the camera bump seems humongous, it’s not really that complicated when it comes to the actual camera module configuration as the phone “only” features three units. The aforementioned 50MP main module is what characterises the device as special from the competition, using Samsung’s GN2 sensor at 1/1.12” optical format with f/1.95 optics and OIS, and a relatively wide 23mm equivalent focal length. This is accompanied by a 48MP ultra-wide module with 13mm equivalent focal length, and because it’s an Ultra phone, Xiaomi also included a periscope telephoto module with the same sized sensor as on the ultra-wide at 48MP, and a native focal length of 119mm equivalent – or 5x magnification in relation to the main sensor.Xiaomi’s three cameras appear simpler than the competition which may have more modules, but Xiaomi’s implementation is actually the technically superior one as it takes advantage of the dual-resolution nature of the quad-Bayer sensors on the main camera and the telephoto, which switch over from binning to their native resolutions, allowing for loss-less 12MP cropping, essentially adding in a virtual 2x telephoto as well as a 10x telephoto module in terms of picture quality, allowing the Mi 11 to be the most versatile phone camera on the market right now when it comes to various focal lengths and quality steps.Alongside the three cameras, the Mi 11 Ultra is also characterised by having a tiny rear OLED screen alongside the camera setup, which is actually why the camera bump is the size of most of the width of the phone. The secondary rear display can act as a clock face, a custom text label, or custom image – with displaying secondary information such as battery status or a notification light (no actual notification info). When using the camera, the rear display can also be enabled to act as a preview screen which allows for high quality selfie pictures using the main cameras. It also serves as a general preview for subjects, however in 4:3 view it really becomes quite small so the subject can’t be too far away as it’s otherwise hard to make out what’s on the quite small screen.In general, in my usage, I was relatively ambivalent about the rear display – it’s useful for quickly glancing at the time – which can be set up to light on when you pick up the phone, or when you double tap the rear screen. Beyond it delivering a relatively gimmicky feature, what Xiaomi’s design does is to bring symmetry to the camera bump which I came to realise is actually quite important when you have a camera bump of this size. It is very stable when lying on surfaces, and the bump is high enough on the phone that you generally do not notice it in everyday portrait usage. It’s still a characteristic which you will either be fine with, or hate – so generally I’d recommend people to attempt to get a hands-on experience before committing to it.The bottom and top of the phone are characterised by dual-speaker setups, although Xiaomi’s design here can be a bit misleading as the top speaker grill isn’t actually an outlet where sound comes out, it’s the earpiece slit which is still serves as the top speaker output. There’s still some internal connection between the grill and the speaker as it does reduce distortions, but covering it up doesn’t do much in terms of audible volume difference. In general Xiaomi’s audio quality here is good, however clarity, soundstage and L/R balance is better on a S21 Ultra for example.As with the Mi 11, the screen is curved both on all sides, left/right and top/bottom, with the corners towards the metal frame being flatter, essentially making the frame corner protrude out more. In general, I think the design decision here was made with durability in mind as if the device drops it would have a heightened chance to drop onto the metal frame rather than on the display glass.The phone is IP68 water resistant which is a feature the Mi 11 doesn’t have, along with added in WiFi 6E capability, but what’s somewhat weird for the phone is that Xiaomi for some reason still only enables USB connectivity over 2.0 data-rates, which makes it notably slower than other 3.0 capable devices on the market.Overall, the design and ergonomics of the Mi 11 Ultra are good – at least if you keep in mind that it’s still a large phone, and at 234g, it’s also one of the heaviest in the market right now, so definitely not something which will cater to everybody’s tastes.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16830/the-xiaomi-mi-11-ultra-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD Threadripper Pro Review: An Upgrade Over Regular Threadripper?\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-07-14T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16805/amd-threadripper-pro-review-an-upgrade-over-regular-threadripper\n",
      "Content: Since the launch of AMD’s Threadripper Pro platform, the desire to see what eight channels of memory brings to compute over the regular quad-channel Threadripper has been an intriguing prospect. Threadripper Pro is effectively a faster version of AMD’s EPYC, limited for single CPU workstation use, but also heralds a full 280 W TDP to match the frequencies of the standard Threadripper line. There is a 37% price premium from Threadripper to Threadripper Pro, which allows for ECC memory support, double the PCIe lanes, and double the memory bandwidth. In this review, we’re comparing every member of both platforms that is commercially available.Threadripper Pro: Born of NeedWhen AMD embarked upon its journey with the new Ryzen portfolio, the delineation of where each product sat in the traditional market has not always been entirely clear. The first generation Ryzen was earmarked for standard consumers, however the top of the line Ryzen 7 1800X, with eight cores, competed against Intel’s high-end desktop market. The Zen 2-based portfolio saw the mainstream Ryzen go to 16 cores, pushing past Intel’s best 18-core HEDT processor at the time in most tests. That Zen 2-based Ryzen 9 3950X was still classified as a ‘mainstream platform’ processor, as it only had 24 PCIe lanes and dual-channel memory, sufficient for mainstream users but not enough for workstation markets. These mainstream processors were also limited to 105W TDP.At the other end of the scale was AMD EPYC, with the first generation EPYC 7601 having 32 cores, and the second generation EPYC 7742 having 64 cores, up to 225W TDP. These share the same LGA4094 socket, have eight channels of memory, full ECC support, and 128 PCIe lanes (first PCIe 3.0, then PCIe 4.0), with dual-socket support. For workstation users interested in EPYC, AMD launched single socket ‘P’ versions. These offered the same features, at around 200 TDP, losing some performance to the regular non-P versions.AMD then launched Threadripper, a high-end desktop version of EPYC that went all the way up to 280 W for peak frequency and performance. Threadripper sat above Ryzen with 64 PCIe lanes and quad channel memory, enabling mainstream users that wanted a bit more to get a bit more. However workstation users noted that while 280 W was great, it lacked official ECC memory support, and compared to EPYC, sometimes the reduced memory channel support and reduced PCIe compared to EPYC stopped Threadripper being adopted.So enter Threadripper Pro, which sits between Threadripper and EPYC, and in this instance, very much more on the EPYC side. Threadripper Pro has almost all the features of AMD’s EPYC platform, but in a 280W thermal envelope. It has eight channels of memory support, all 128 PCIe 4.0 lanes, and can support ECC. The only downside to EPYC is that it can only be used in single socket systems, and the peak memory support is halved (from 4 TB to 2 TB). Threadripper Pro also comes at a small price premium as well.AMD ComparisonAnandTechRyzenThreadripperThreadripperProEnterpriseEPYCCores6-1632-6412-6416-64ArchitectureZen 3Zen 2Zen 2Zen 31P FlagshipR95950XTR3990XTR Pro 3995WXEPYC7713PMSRP$799$3990$5490$5010TDP105 W280 W280 W225 WBase Freq3400 MHz2900 MHz2700 MHz2000 MHzTurbo Freq4900 MHz4300 MHz4200 MHz3675 MHzSocketAM4sTRX40sTRX4: WRX80SP3L3 Cache64 MB256 MB256 MB256 MBDRAM2 x DDR4-32004 x DDR4-32008 x DDR4-32008 x DDR4-3200DRAM Capacity128 GB256 GB2 TB, ECC4 TB, ECCPCIe4.0 x20 +chipset4.0 x56 + chipset4.0 x120 + chipset4.0 x128Pro FeaturesNoNoYesYesOne of the biggest pulls for Threadripper and Threadripper Pro has been any market that typically uses high-speed workstations and can scale their workloads. Speaking to a local OEM, the demand for Threadripper and Threadripper Pro from the visual effects industry has been off the charts, where these companies are ripping out their old infrastructure and replacing anew with AMD. This has also been spurned by the recent pandemic, where these studios want to keep the expensive hardware onsite and allow their artists to work from home via remote access.Threadripper Pro CPUs: Four Models, Three at RetailWhen TR Pro launched in 2020, the processors were a Lenovo exclusive for the P620 workstation. The deal between Lenovo and AMD was not disclosed, however it would appear that the exclusivity deal ran for six months, from September to February, with the processors being made retail available on March 2nd.During that time, we were sampled one of these workstations for review, and it still remains one of the best modular systems I’ve ever tested:Lenovo ThinkStation P620 Review: A Vehicle for Threadripper ProAMD’s first Threadripper Pro platform has four processors in it, ranging from 12 cores to 64 cores, mimicking their equivalents in Threadripper 3000 and EPYC 77x2 but at 280W.AMD Ryzen Threadripper ProAnandTechCoresBaseFreqTurboFreqChipletsL3CacheTDPPriceSEP3995WX64 / 128270042008 + 1256 MB280 W$54903975WX32 / 64350042004 + 1128 MB280 W$27503955WX16 / 32390043002 + 164 MB280 W$11503945WX12 / 24400043002 + 164 MB280 WOEMSitting at the top is the 64-core Threadripper Pro 3995WX, with a 2.7 GHz base frequency and a 4.2 GHz turbo frequency. This processor is the only one in the family to have all 256 MB of L3 cache, as it has all eight chiplets fully active. The $5490 price is a full 37.5% increase over the Threadripper 3990X at $3990.AMD 64-Core Zen 2 ComparisonAnandTechThreadripper3990XThreadripperPro 3995WXEPYC7702PMSRP$3990$5490$4425TDP280 W280 W200 WBase Freq2900 MHz2700 MHz2000 MHzTurbo Freq4300 MHz4200 MHz3350 MHzL3 Cache256 MB256 MB256 MBDRAM4 x DDR4-32008 x DDR4-32008 x DDR4-3200DRAM Capacity256 GB2 TB, ECC4 TB, ECCPCIe4.0 x56 + chipset4.0 x120 + chipset4.0 x128Pro FeaturesNoYesYesMiddle of the line is the 32-core Threadripper Pro 3975WX, with a 3.5 GHz base frequency and a 4.2 GHz turbo frequency. AMD decided to make this processor use four chiplets with all eight cores on each chiplet, leading to 128 MB of L3 cache total. At $2750, it is also 37.5% more expensive than the equivalent 32-core Threadripper 3970X.AMD 32-Core Zen 2 ComparisonAnandTechThreadripper3970XThreadripperPro 3975WXEPYC7501PMSRP$3990$2750$2300TDP280 W280 W180 WBase Freq3700 MHz3500 MHz2500 MHzTurbo Freq4500 MHz4200 MHz3350 MHzL3 Cache128 MB128 MB128 MBDRAM4 x DDR4-32008 x DDR4-32008 x DDR4-3200DRAM Capacity256 GB2 TB, ECC4 TB, ECCPCIe4.0 x56 + chipset4.0 x120 + chipset4.0 x128Pro FeaturesNoYesYesThe following two processors have no Threadripper equivalents, but also represent a slightly different scenario that we’ll explore in this review. Both the 3955WX and 3945WX, despite being part of the big Threadripper Pro family, only use two chiplets in their design: 8 core per chipet for the 3955 WX and 6 core per chiplet for the 3945WX. This means these processors only have 64 MB of L3 cache, making them somewhat identical to the Ryzen 9 3950X and Ryzen 9 3900X, except the IO die means there is eight channels of memory and 128 PCIe lanes here.AMD 16-Core Zen 2/3 ComparisonAnandTechRyzen 93950XThreadripperPro 3955WXRyzen 95950XMSRP$749$1150$799TDP105 W280 W105 WBase Freq3500 MHz3900 MHz3400 MHzTurbo Freq4700 MHz4300 MHz4900 MHzL3 Cache64 MB64 MB64 MBDRAM2 x DDR4-32008 x DDR4-32002 x DDR4-3200DRAM Capacity128 GB2 TB, ECC128 GBPCIe4.0 x20+ chipset4.0 x120+ chipset4.0 x20+ chipsetPro FeaturesNoYesNoMotherboard Cost--+++--The 3955WX has a higher base frequency, but the 3950X has the higher turbo frequency. The 3950X is also cheaper, and motherboards are cheaper! It might be worth partitioning these out into a separate comparison review.The final Threadripper Pro processor, the 3945WX, does not have a price, because AMD is not making it available at retail. This part is for selected OEM customers only it seems; perhaps the limited substrate resources in the market right now makes it unappealing to make too many of these? Hard to say.Motherboards: Beware!Despite being based on the same LGA4094 socket as both Threadripper and EPYC, Threadripper Pro has its own unique WRX80 platform that has to be used instead. Only select vendors seem to have access/licenses to make WRX80 motherboards, and your main options are:ASUS Pro WS WRX80E-SAGE SE WiFi($1000)Supermicro M12SWA-TF(~$750)GIGABYTE WRX80 SU8-IPMI($790)All three boards use a transposed LGA4094 socket, eight DDR4 memory slots, and 6-7 PCIe 4.0 slots.Though beware! There is an option of finding an old/refurbished Lenovo P620 motherboard. It is worth noting that Lenovo is exercising an AMD feature for OEMs: processors used in that Lenovo motherboard will be locked to Lenovo forever. This is part of AMD’s guaranteed supply chain process, allowing OEMs to hard lock processors into certain vendors for supply chain end-to-end security that is requested by specific customers. In that instance, if you might ever want to break down your system to upgrade and sell off parts, it is not recommended you find a Lenovo TR Pro system unless you buy/sell it as a whole.This ReviewThe main goal of this review is to test all of the Threadripper Pro 3000 hardware and compare against the equivalent Threadripper 3000 to get a sense of how much performance is gained by the increased memory bandwidth, or lost due to the slight core frequency differences. We are also including Intel’s best HEDT/workstation processor for comparison, the W-3175X, as well as the top consumer-grade processors on the market. All systems are tested at JEDEC specifications.Test SetupAMDTR Pro3995WX3975WX3955WXASUS Pro WSWRX80E-SAGESE WiFiBIOS0405IceGiantThermosiphonKingston8x16 GBDDR4-3200 ECCAMDTRTR 3990XTR 3970XTR 3960XASRockTRX40TaichiBIOSP1.70IceGiantThermosiphonADATA4x32 GBDDR4-3200AMDRyzenR9 5950XGIGABYTEX570 I AorusProBIOSF31LNoctuaNH-U12SADATA4x32 GBDDR4-3200IntelCorei9-11900KASUSMaximusXIII HeroBIOS0703ThermalrightTRUECopper*ADATA4x32 GBDDR4-3200IntelXeonXeon W-3175XASUS ROGDominusExtremeBIOS 0601Asetek690LX-PNDDR4-2666ECCGPUSapphire RX 460 2GB (CPU Tests)PSUVarious (inc. Corsair AX860i)SSDCrucial MX500 2TB*Silverstone SST-FHP141-VF 173 CFM fans also used. Nice and loud.Many thanks to Kingston for supplying a full set of KSM32RD8/16MEI - 16x16 GB of DDR4-3200 ECC RDIMMs for enterprise testing in systems like Threadripper Pro.As part of this review, we are also showcasing the 64 core processors in 128T mode as well as 64T mode. This is being done to showcase how some processors can get better performance by having better memory bandwidth per thread - one of the issues with these high core count processors is the limited amount of memory bandwidth each thread can access. Also, some operating systems (such as Windows) struggle above 64 threads due to the use of thread groups.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16805/amd-threadripper-pro-review-an-upgrade-over-regular-threadripper\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ASUS Announces \"Smartphone for Snapdragon Insiders\" - A Real Product, or Just A Marketing Showcase?\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-07-08T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/16811/asus-announces-smartphone-for-snapdragon-insiders\n",
      "Content: Today we’re covering a quite unusual device announcement a little unlike what we’ve seen before. ASUS and Qualcomm – well, mostly ASUS, are announcing a new device called the “Smartphone for Snapdragon Insiders” – yes that’s the actual name of the phone. The phone follows Qualcomm’s social media initiative called “Snapdragon Insiders” where the company is has opened up more community channels on various social media platforms to closer interaction with Snapdragon fans, attempting to build a closer relationship with its users.The “Smartphone for Snapdragon Insiders”, or let’s just call it the SSI from here on, is a product of the collaboration between Qualcomm and ASUS – though here things become a bit complex and unintuitive, as Qualcomm strictly refers to the SSI as being a product strictly designed and made by ASUS. It’s a weird kind of relationship and marketing exercise that can maybe seen more as a Snapdragon branded phone by ASUS.ASUS Big PhonesROG Phone 5\"Smartphone for Snapdragon Insiders\"SoCQualcomm Snapdragon 8881xCortex-X1@ 2.84GHz3xCortex-A78@ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 660 @ 840MHzDRAM8/16 GB LPDDR516 GB LPDDR5Storage128/256GB UFS 3.1512GB UFS 3.1Display6.78\"AMOLED2448 x 1080 (20:9)144Hz240Hz TouchSizeHeight173.00 mm173.15 mmWidth77.00 mm77.25 mmDepth9.90 mm9.55 mmWeight239 grams210 gramsBattery Capacity5770 typ (2x 2885) 22.33Wh\"6000mAh\" design65W charging (PD3.0/QC5.0)\"4000mAh\"65W charging (PD3.0/QC5.0)Wireless Charging-Rear CamerasMain64MP IMX6860.8µm pixels (1.6µm 4:1 16MP)f/1.8 w/OIS (SSI)26.6mm eq.Telephoto-8MP3x optical zoomf/2.4 w/OIS80mm eq.Wide12MP IMX3631.4µm pixels Dual PDAFf/2.214.3mm eq.Extra5MP Macro-Front Camera24MPf/2.45I/OUSB-C3.5mm headphoneUSB-CWireless (local)802.11ax WiFi-6EBluetooth 5.2 LE + NFCOther FeaturesIn-display fingerprint sensorRear fingerprint sensorDual-SIMDual nanoSIMLaunch Price(16/256GB)$999(16/512GB)$1499Starting off with the internals of the phone, there’s a large sense of familiarity with the specifications, dimensions, and just overall design of the phone that comes from the juxtaposition between ASUS’s ROG Phone 5 and the SSI. Both phones have similar form-factors, similarly internal components and somewhat similar external designs, and even within Qualcomm’s presentation slides during our pre-briefing, showcasing an internal layout design that’s eerily similar to the ROG Phone 5.Both phones are powered by the Snapdragon 888 – it’s actually a bit surprising that the newer SSI doesn’t come with therecently announced Snapdragon 888+, but this was explained to us that the SSI design was already well underway and in design before the 888+ would be available.Given what appears to be the same chassis and thermal design between the ROG 5 and the SSI – one could assume that the general performance of both phones to be extremely similar to each other.The SSI does more notably differentiate itself in terms of RAM and storage – well at least compared to the regular ROG 5, as it only comes with a large 16GB LPDDR5 and 512GB UFS 3.1 storage capacity option, which is more in line with the premium ROG Phone 5 Ultimate that sees lower availability.The display of the phone is essentially identical to that of the ROG Phone 5 – a 6.78” 2448 x 1080 AMOLED panel from Samsung that is able to run up to 144Hz refresh rate. We hadn’t exactly confirmed if the touch sample rate is the same 300Hz as on the ROG 5, however given that the specification sheet on the phone we got from Qualcomm had the display section essentially copy-paste from ASUS’ own ROG 5 press release contents, I would assume this to be the case.Design wise, the SSI is very similar to the ROG 5, however it’s ever so slightly downsized. The fundamental form is still there, but the one big change is that the SSI is 0.35mm thinner and 29g lighter. This doesn’t sound like too much, but it has a huge impact on the battery capacity which has shrunk 33% down from 6000mAh on the ROG Phone 5 to 4000mAh here on the SSI. It still features the same 65W USB-PD 3.0 (“QC 5.0”) charging capabilities.Another large physical change is that the phone has a rear fingerprint sensor, which is a bit weird as the ROG 5 had an under-screen unit. This rear sensor is actually Qualcomm’s 2ndgen sonic sensor, but the issue is that these kinds of sensors are only able to be integrated into flexible OLED panels, and the phone here uses a solid glass substrate variant. This actually begs the question of what the point is to have a sonic sensor at all if it’s not integrated into the screen, as it diminishes the advantages over a traditional capacitive unit.Below the fingerprint reader, we see an illuminated Snapdragon logo instead of the ROG logo. This can also be configured to light up or pulse in a rhythm, or simply turned off.On the bottom of the phone there’s more design changes over the ROG 5 – the dual nano-SIM moves from the very bottom left side of the phone to the left bottom side, next to the USB-C connector. What’s very odd here is that the 3.5mm jack has disappeared, and the bottom speaker is no longer front-facing, but bottom facing.Qualcomm sent us along with the press materials an extract of DXOMark Audio review of the SSI, and even though the phone is supposed to be a showcase of a “Snapdragon Sound” experience, it literally performed worse than the ROG 5 in every speaker audio playback measurement test.The camera setup is also quite similar to ASUS’ phones. On the main unit, we find the 64MP IMX686 with an f/1.8 lens with OIS. The OIS here is different to the ROG 5, but otherwise appears to be the same camera module. The ultra-wide angle appears to be identical with an IMX363 and f/2.2 optics with 14.3mm equivalent focal length.Instead of a 5MP macro, the SSI does feature the 3x optical zoom / 80mm equivalent 8MP telephoto module as the third camera, with f/2.4 optics and OIS.The really large issue I have with the camera setup is that even though this would have been an opportunity for Qualcomm to showcase their own expertise with their own SoC’s camera pipeline, there was very little mention in regards to who exactly is responsible for the software. Qualcomm reiterated that this is an ASUS product, and ASUS’ representatives just noted that the phone features “stock Android” – and it’s unclear exactly what kind of ASUS features or aspects such as camera processing will be integrated into the device.The one feature that does really distinguish the Smartphone for Snapdragon Insiders is its network and cellular connectivity. The big advantage over the ROG Phone 5 is the fact that this is a mmWave device, with Qualcomm claiming that it is the device with the most comprehensive support for all global frequency bands, making it a truly universal phone in that regard.This fact is actually quite unusual as it means that the phone has extensive internal RF front-end and antenna designs, and could somewhat explain where a lot of the internal device space went, given the smaller batteries compared to the ROG 5.Finally, the accessory that comes with every SSI is a pair of Master & Dynamic Snapdragon branded wireless earbuds that in combination with the SSI, represent the first “Snapdragon Sound” experience. Snapdragon sound here in general refers to a plethora of software audio stacks and audio components that allow for higher bitrate and lower latency audio experiences for wireless audio devices. These appear to just be re-branded variants ofMaster & Dynamic’s MW08, which gotaverage reviewsand come at an MSRP of $299.In the end, we come to the price. At $1499 USD, the Smartphone for Snapdragon Insiders is a pretty unattractive value proposition. Even accounting for the fact that you’re getting $299 earphones included into the package, that leaves you with a device price of $1200, and for that price, beyond the added mmWave capability and telephoto module, you get very little added value compared to a ROG Phone 5 – if anything, you’re losing out features such as a 50% larger battery, better front-facing speaker setups, 3.5mm headphone jack and the gaming accessory ecosystem that the ROG Phone 5 allows for.Beyond that, again, ignoring the $299 value earphones in the package, the device literally is destroyed in feature-set and speculations by the likes of a Galaxy S21 Ultra, which is now available for $999 in the 256GB variant.The phone is just a weird combination of specifications – losing out on the very features that made the ROG Phone 5 really distinguish itself from other phones, and appears to me to present itself as nothing more than just a marketing exercise to push the Snapdragon Insiders “brand”. The very few positives of the design, such as the extensive cellular compatibility of the device, don’t make up for just the rather absurd and pointless package of features the phone presents itself as.The Smartphone for Snapdradon Insiders will beavailable in August in ASUS’ eShopin select countries.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16811/asus-announces-smartphone-for-snapdragon-insiders\n",
      "Title: Cristiano Amon Takes over as Qualcomm CEO: Reiterates Focus on Custom CPUs\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-07-02T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/16803/cristiano-amon-takes-over-as-qualcomm-ceo-reiterates-focus-on-custom-cpus\n",
      "Content: As of June 30th, Qualcomm’s Cristiano Amon has taken over as the company’s CEO, replacing his predecessor Steve Mollenkopf, who has now retired. Prior to the appointment, Amon had a long history and tenure at Qualcomm filling engineering roles, and previously filling the role of president of QCT (The company’s semiconductor business).In statements to Reuters, Amon had made comments regarding the company’s future CPU roadmap, which come to further contextualise the company’scompleted acquisition of NUVIA last March.\"We needed to have the leading performance for a battery-powered device,\" Amon said. \"If Arm, which we've had a relationship with for years, eventually develops a CPU that's better than what we can build ourselves, then we always have the option to license from Arm.\"The wording here is again very bullish on Qualcomm’s part, reinforcing the idea that the company is extremely confident in NUVIA’s CPU microarchitecture and that it will have no issue in differentiating itself in terms of performance compared to what Arm has available in terms of CPU IP. Last March, the company had noted that work on integrating NUVIA’s custom CPU core into a laptop-oriented Snapdragon SoC would be an immediate focus, with Amon now stating that they are planning on bringing such a design to market in 2022.In terms of timeline and against which Arm core the NUVIA design might compete against depends on when exactly in 2022 the new chip might make it to market – if it’s in the first half, then we’ll see it compete againstthe already announced Cortex-X2 coresfrom Arm. If it’s in the latter half, it’s possible it will be positioned against Arm’s next-gen Sophia cores. In either case, Qualcomm seems confident in terms of beating the Arm Cortex designs, which bodes well for next-gen Snapdragons.Amon’s comment that if Arm is able to build a better CPU than Qualcomm’s own designs is also reminiscent of the company’s previous generation custom CPU endeavours: the last time the company had employed a custom microarchitecture was in the 2016 Snapdragon 820 with its Kryo cores. Competing Cortex cores had been faster and more power efficient in a smaller area footprint, which lead the company to use those designs instead, and eventually leading to Qualcomm dissolving its CPU design teams – a decision which later ended up with no in-house design capabilities up until the recent NUVIA purchase.Related Reading:Qualcomm Completes Acquisition of NUVIA: Immediate focus on Laptops (Updated)Qualcomm to Acquire NUVIA: A CPU Magnitude ShiftArm Announces Mobile Armv9 CPU Microarchitectures: Cortex-X2, Cortex-A710 & Cortex-A510\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16803/cristiano-amon-takes-over-as-qualcomm-ceo-reiterates-focus-on-custom-cpus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ASUS Launches Zenfone 8 in US: Starting at $599\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-07-02T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16802/asus-launches-zenfone-8-in-us-starting-at-599\n",
      "Content: Following the phone’s initial announcement mid-May, this week ASUS has officially launched their new ZenFone 8 in the US starting at a price of $599. The ZenFone 8 is a bit unusual in the market as it is trying to fit itself in the niche of a small flagship device – at least in terms of performance.ASUS ZenFone 8 SeriesZenFone 8 FlipZenFone 8SoCQualcomm Snapdragon 8881xCortex-X1@ 2.84GHz3xCortex-A78@ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 660 @ 840MHzDRAM6 GB LPDDR56/8/16 GB LPDDR5Storage128GB UFS 3.1+ microSD128/256GBUFS 3.1Display6.67\" AMOLED2400 x 1080 (20:9)90Hz200Hz Touch5.9\"AMOLED2400 x 1080 (20:9)120Hz240Hz TouchSizeHeight165.08 mm148.0 mmWidth77.28 mm68.5 mmDepth9.6 mm8.9 mmWeight230 grams169 gramsBattery Capacity5000mAh30W charging (PD3.0)4000mAh30W charging (PD3.0)Wireless Charging-Rear CamerasMain64MP IMX6860.8µm pixels (1.6µm 4:1 16MP)f/1.764MP IMX6860.8µm pixels (1.6µm 4:1 16MP)f/1.7w/OISTelephoto8MP3x optical zoomf/2.4n/aWide12MP IMX3631.4µm pixels Dual PDAF113° FoV ultra-widef/2.2Extra-Front CameraFlip-camera DesignFront cameras = Rear cameras12MP IMX6631.22µmI/OUSB-CUSB-C3.5mm headphoneWireless (local)802.11ax WiFi-6Bluetooth 5.1 LE + NFCOther FeaturesTriple-function Power Button w/ Capacitive Fingerprint SensorIP68Dual SpeakersUnder-screen fingerprint sensorDual-SIMDual nanoSIMLaunch Price21,999 TWD(USD~748, EUR~626)starting $/€599Today’s US launch now also confirms the pricing of $599 for the 128GB variant, and $699 for the 256GB variant, lining up with the European prices (tax normalised) that were disclosed a few weeks back. ASUS’ launch window and availability seems to have improved compared to past generation ZenFones, which usually had taken several months to see wider spread availability.We had briefly reviewed the ZenFone 8 back in May, and refer to the article for more in-depth coverage:The ASUS Zenfone 8 Hands-On Review: A New Compact DirectionWe found the ZenFone 8 to be an interesting device which strengths as well as drawbacks. The performance of the phone is great given the Snapdragon 888 chipset as well as the 120Hz refresh rate screen, both packing a punch for a device of this smaller form-factor. Possible deal-breakers for users are the lacklustre camera quality which hasn’t seen much improvements versus its predecessor. While the ZenFone 8 is a step-up for ASUS’s mobile portfolio, it still has troubles in terms of representing itself as a good value alternative to devices such as the $699 Galaxy S21, even at a $100 lower price point.The official US variant ZenFone 8can be ordered off ASUS’s own online store.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16802/asus-launches-zenfone-8-in-us-starting-at-599\n",
      "Title: Intel to Launch Next-Gen Sapphire Rapids Xeon with High Bandwidth Memory\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-06-28T16:00:00Z\n",
      "URL: https://www.anandtech.com/show/16795/intel-to-launch-next-gen-sapphire-rapids-xeon-with-high-bandwidth-memory\n",
      "Content: As part of today’s International Supercomputing 2021 (ISC) announcements, Intel is showcasing that it will be launching a version of its upcoming Sapphire Rapids (SPR) Xeon Scalable processor with high-bandwidth memory (HBM). This version of SPR-HBM will come later in 2022, after the main launch of Sapphire Rapids, and Intel has stated that it will be part of its general availability offering to all, rather than a vendor-specific implementation.Hitting a Memory Bandwidth LimitAs core counts have increased in the server processor space, the designers of these processors have to ensure that there is enough data for the cores to enable peak performance. This means developing large fast caches per core so enough data is close by at high speed, there are high bandwidth interconnects inside the processor to shuttle data around, and there is enough main memory bandwidth from data stores located off the processor.Our Ice Lake Xeon Review system with 32 DDR4-3200 SlotsHere at AnandTech, we have been asking processor vendors about this last point, about main memory, for a while. There is only so much bandwidth that can be achieved by continually adding DDR4 (and soon to be DDR5) memory channels. Current eight-channel DDR4-3200 memory designs, for example, have a theoretical maximum of 204.8 gigabytes per second, which pales in comparison to GPUs which quote 1000 gigabytes per second or more. GPUs are able to achieve higher bandwidths because they use GDDR, soldered onto the board, which allows for tighter tolerances at the expense of a modular design. Very few main processors for servers have ever had main memory be integrated at such a level.Intel Xeon Phi 'KNL' with 8 MCDRAM Pads in 2015One of the processors that used to be built with integrated memory was Intel’s Xeon Phi, a product discontinued a couple of years ago. The basis of the Xeon Phi design was lots of vector compute, controlled by up to 72 basic cores, but paired with 8-16 GB of on-board ‘MCDRAM’, connected via 4-8 on-board chiplets in the package. This allowed for 400 gigabytes per second of cache or addressable memory, paired with 384 GB of main memory at 102 gigabytes per second. However, since Xeon Phi was discontinued, no main server processor (at least for x86) announced to the public has had this sort of configuration.New Sapphire Rapids with High-Bandwidth MemoryUntil next year, that is. Intel’s new Sapphire Rapids Xeon Scalable with High-Bandwidth Memory (SPR-HBM) will be coming to market. Rather than hide it away for use with one particular hyperscaler, Intel has stated toAnandTechthat they are committed to making HBM-enabled Sapphire Rapids available to all enterprise customers and server vendors as well. These versions will come out after the main Sapphire Rapids launch, and entertain some interesting configurations. We understand that this means SPR-HBM will be available in a socketed configuration.Intel states that SPR-HBM can be used with standard DDR5, offering an additional tier in memory caching. The HBM can be addressed directly or left as an automatic cache we understand, which would be very similar to how Intel's Xeon Phi processors could access their high bandwidth memory.Alternatively, SPR-HBM can work without any DDR5 at all. This reduces the physical footprint of the processor, allowing for a denser design in compute-dense servers that do not rely much on memory capacity (these customers were already asking for quad-channel design optimizations anyway).The amount of memory was not disclosed, nor the bandwidth or the technology. At the very least, we expect the equivalent of up to 8-Hi stacks of HBM2e, up to 16GB each, with 1-4 stacks onboard leading to 64 GB of HBM. At a theoretical top speed of 460 GB/s per stack, this would mean 1840 GB/s of bandwidth, although we can imagine something more akin to 1 TB/s for yield and power which would still give a sizeable uplift. Depending on demand, Intel may fill out different versions of the memory into different processor options.One of the key elements to consider here is that on-package memory will have an associated power cost within the package. So for every watt that the HBM requires inside the package, that is one less watt for computational performance on the CPU cores. That being said, server processors often do not push the boundaries on peak frequencies, instead opting for a more efficient power/frequency point and scaling the cores. However HBM in this regard is a tradeoff - if HBM were to take 10-20W per stack, four stacks would easily eat into the power budget for the processor (and that power budget has to be managed with additional controllers and power delivery, adding complexity and cost).One thing that was confusing about Intel’s presentation, and I asked about this but my question was ignored during the virtual briefing, is that Intel keeps putting out different package images of Sapphire Rapids. In the briefing deck for this announcement, there was already two variants. The one above (which actually looks like an elongated Xe-HP package that someone put a logo on) and this one (which is more square and has different notches):There have been some unconfirmed leaks online showcasing SPR in a third different package, making it all confusing.Sapphire Rapids: What We KnowIntel has been teasing Sapphire Rapids for almost two years as the successor to its Ice Lake Xeon Scalable family of processors. Built on 10nm Enhanced SuperFin, SPR will be Intel’s first processors to use DDR5 memory, have PCIe 5 connectivity, and support CXL 1.1 for next-generation connections. Also on memory, Intel has stated that Sapphire Rapids will support Crow Pass, the next generation of Intel Optane memory.For core technology, Intel (re)confirmed that Sapphire Rapids will be using Golden Cove cores as part of its design. Golden Cove will be central to Intel's Alder Lake consumer processor later this year, however Intel was quick to point out that Sapphire Rapids will offer a ‘server-optimized’ configuration of the core. Intel has done this in the past with both its Skylake Xeon and Ice Lake Xeon processors wherein the server variant often has a different L2/L3 cache structure than the consumer processors, as well as a different interconnect (ring vs mesh, mesh on servers).Sapphire Rapids will be the core processor at the heart of the Aurora supercomputer at Argonne National Labs, where two SPR processors will be paired with six Intel Ponte Vecchio accelerators, which will also be new to the market. Today's announcement confirms that Aurora will be using the SPR-HBM version of Sapphire Rapids.As part of this announcement today, Intel also stated that Ponte Vecchio will be widely available, in OAM and 4x dense form factors:Sapphire Rapids will also be the first Intel processors to support Advanced Matrix Extensions (AMX), which we understand to help accelerate matrix heavy workflows such as machine learning alongside also having BFloat16 support. This will be paired with updates to Intel’s DL Boost software and OneAPI support. As Intel processors are still very popular for machine learning, especially training, Intel wants to capitalize on any future growth in this market with Sapphire Rapids. SPR will also be updated with Intel’s latest hardware based security.It is highly anticipated that Sapphire Rapids will also be Intel’s first multi compute-die Xeon where the silicon is designed to be integrated (we’re not counting Cascade Lake-AP Hybrids), and there are unconfirmed leaks to suggest this is the case, however nothing that Intel has yet verified.The Aurora supercomputer is expected to be delivered by the end of 2021, and is anticipated to not only be the first official deployment of Sapphire Rapids, but also SPR-HBM. We expect a full launch of the platform sometime in the first half of 2022, with general availability soon after. The exact launch of SPR-HBM beyond HPC workloads is unknown, however given those time frames, Q4 2022 seems fairly reasonable depending on how aggressive Intel wants to attack the launch in light of any competition from other x86 vendors or Arm vendors. Even with SPR-HBM being offered to everyone, Intel may decide to prioritize key HPC customers over general availability.Related ReadingSuperComputing 15: Intel’s Knights Landing / Xeon Phi Silicon on DisplayA Few Notes on Intel’s Knights Landing and MCDRAM Modes from SC15Intel Announces Knights Mill: A Xeon Phi For Deep LearningIntel Begins EOL Plan for Xeon Phi 7200-Series ‘Knights Landing’ Host ProcessorsKnights Mill Spotted at SupercomputingThe Larrabee Chapter Closes: Intel's Final Xeon Phi Processors Now in EOLIntel’s 2021 Exascale Vision in Aurora: Two Sapphire Rapids CPUs with Six Ponte Vecchio GPUsIntel’s Xeon & Xe Compute Accelerators to Power Aurora Exascale SupercomputerHot Chips 33 (2021) Schedule Announced: Alder Lake, IBM Z, Sapphire Rapids, Ponte VecchioIntel’s Full Enterprise Portfolio: An Interview with VP of Xeon, Lisa SpelmanWhat Products Use Intel 10nm? SuperFin and 10++ DemystifiedIntel 3rd Gen Xeon Scalable (Ice Lake SP) Review: Generationally Big, Competitively Small\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16795/intel-to-launch-next-gen-sapphire-rapids-xeon-with-high-bandwidth-memory\n",
      "Title: Marvell Announces OCTEON 10 DPU Family: First to 5nm with N2 CPUs\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-06-28T10:30:00Z\n",
      "URL: https://www.anandtech.com/show/16790/marvell-announces-octeon-10-dpu-family-first-to-5nm-with-n2-cpus\n",
      "Content: It’s been a little over a year since we coveredMarvell’s OCTEON TX2 infrastructure processors, and since then, the ecosystem has been evolving in an extremely fast manner – both within Marvell and outside. Today, we’re covering the new generation OCTEON 10 family of DPUs, a whole new family of SoCs, built upon TSMC’s 5nm process node and also for the featuring for the first time Arm’s new Neoverse N2 processors.Starting off with a bit of history and nomenclature, Marvell is adopting the “DPU” term for this class of chip and accelerator type. The previous generation OCTEON TX and OCTEON TX2 already were DPUs in everything but name, previously simply being referenced as “infrastructure processors”. With the recent industry rising popularity of the term as well as competitor solutions being propped up, it seems we’re seeing the DPU term now being widely accepted nomenclature for this type of versatile chip design, defined by the fact that it’s an entity that helps process and move data while it travels through the network.Starting with an overview, the new OCTEON 10 generally features the same versatile array of building blocks we’ve seen in the previous generation, this time upgraded to the new state of the art IP blocks, and also introducing some new features such as integrated machine learning inference engine, new inline and crypto processors as well as vector packet processors, all able to operated in a virtualised manner.This is also Marvell’s first TSMC N5P silicon design, actually the first DPU of its kind on the new process, and also the first publicly announced Neoverse N2 implementation, featuring the newest PCIe 5.0 I/O capabilities as well as DDR5 support.Starting off with what Marvell views as an important addition to the DPU, is a new in-house ML engine. Marvell had stated that the design for the IP had actually been originally created for a dedicated inference accelerator, and actually had been completed last year, but with Marvell opting to not bring it to market due to the extremely crowded competitive landscape. Instead, Marvell has opted to integrate the ML accelerator into their OCTEON DPU chips. Marvell here states that having the inference accelerator on the same monolithic silicon chip, directly integrated into the data pipeline is extremely important in achieving the low latency for higher throughput processing required for these kinds of data stream use-cases.Essentially Marvell here is offering a competitor solution toNvidia’s next-gen BlueField-3 DPUin terms of AI processing capabilities well ahead in terms of product generation, as the first OCTEON 10 solutions are expected to be sampling by end of this year while Nvidia projected BF3 to be arriving in 2022.Also, a new capability of the new OCTEON 10 family is the introduction of vector packet processing engines, which are able to vastly augment the packet processing throughput by a factor of 5x compared to the current generation scalar processing engines.As noted, the new OCTEON 10 DPU family is the first publicly announced silicon design featuringArm’s newest Neoverse N2 infrastructure CPU IP. We had covered the N2 and its HPC V1 sibling a couple of months ago – the jist of it is that the new generation core is the first Armv9 core from Arm and promises large 40% IPC gains in comparison to the current N1 core seen in Arm server CPUs such as the Amazon Graviton2 or Ampere Altra.For Marvell, the performance improvements are even more significant as the company is switching over from the company’s previous in-house “TX2” CPU IP for the N2 core, promising a massive 3x higher single-threaded performance uplift. Late last year, Marvell had announced that it had stopped its own CPU IP in favour of Arm’s Neoverse cores, and today reiterated that the company is planning to stick to Arm’s roadmap for the foreseeable future, a large endorsement of Arm’s new IP which comes at bit of a contrast to other industry playerssuch as AmpereorQualcomm.Important for DPU use-cases is the fact that this is a Armv9 CPU which also has SVE2 support, containing new important instructions that help data-processing and machine learning capabilities. This actually would be a large IP advantage over Nvidia’s BlueField3 DPU design that still “only” features Cortex-A78 cores which are Armv8.2+.Marvell uses the full cache configuration options for their N2 implementations, meaning 64KB L1I and L1D caches, as well as the full 1MB of L2. The company’s integration into the SoC however continues to use their own internal mesh network solution – on a very high level this still looks similar in terms of basic specs, with 256bit datapaths in the mesh, and also a shared L3 containing 2MB cache slices, scaling up in number along with the core count.In terms of switch integration and network throughput, Marvell integrated a 1 Tb/s switch with up to 16 x 50G MACs – it’s not be noted though that the capabilities here are going to vary a lot based on the actual SKU and chip design in the family.In terms of use-cases, the OCTEON 10 family covers a wide range of applications from the 4G/5G RAN Digital Units or Central Units, Front Haul Gateways or even vRAN Offload processors. In the cloud and datacentre, the solutions can offer a wide array of versatility in terms of compute and network throughput performance, while for enterprise use-cases, the family offers deeply integrated packet processing and security acceleration features.The first OCTEON 10 product and samples will be based on the CN106XX design with 24 N2 cores and 2x 100GbE QSFP56 ports on a PCIe 5.0 form-factor, available for Q4.In terms of specifications, Marvell gives a breakdown of the various OCTEON 10 family designs:Slide note: DDR5 controllers in this context refers to 40-bit channels (32+8bit ECC). Marvell also states that it still uses SPECint2006 due to its historical importance in regards to comparing to previous generation, and competitor solutions – it will publish 2017 estimates once the first silicon is ready.The CN106XX is the first chip design of the OCTEON 10 family, taped out and expected to sample in the latter half of this year. Beyond this first chip, Marvell has 3 other OCTEON 10 designs in the form of the lower-end CN103XX with just 8 N2 cores and low TDPs of 10-25W, and two higher-end CN106XXS with improved network connectivity, and finally the DPU400 flagship with up to a massive 36 N2 cores and featuring the maximum amount of processing power and network connectivity throughput. What’s very exciting to see is that even with the largest implementations, the TDP only reaches 60W, which is far below the current generation CN98XX Octeon TX2 flagship implementation which lands in at 80-120W. These additional parts are yet to be taped out, and are planned to be sampled throughout 2022.Marvell states that it’s been the industry leader in terms of DPU shipments, and is prevalent in all large datacentre deployments. This new Octeon 10 generation certainly seems extremely aggressive from a technology standpoint, featuring leading edge IP as well as manufacturing processes, which should give Marvell a notable advantage in terms of performance and power efficiency over the competition in the fast-evolving DPU market.Related Reading:Marvell Announces OCTEON Fusion and OCTEON TX2 5G Infrastructure ProcessorsArm Announces Neoverse V1, N2 Platforms & CPUs, CMN-700 Mesh: More Performance, More Cores, More Flexibility\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16790/marvell-announces-octeon-10-dpu-family-first-to-5nm-with-n2-cpus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qualcomm Announces Snapdragon 888+ 5G Speed Bin at 3GHz\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-06-28T10:15:00Z\n",
      "URL: https://www.anandtech.com/show/16789/qualcomm-announces-snapdragon-888-plus-5g-speed-bin-at-3ghz\n",
      "Content: Today for the first day of Mobile World Congress, Qualcomm is announcing its usual yearly mini-refresh of its flagship Snapdragon SoC in the form of the new Snapdragon 888+. As in the previous few generations, right around the summer period, Qualcomm is taking advantage of the completed spring device cycle and shifting focus onto newer devices in the second half of the year with, and a new SoC that’s slightly boosts performance.Qualcomm Snapdragon Flagship SoCs 2021SoCSnapdragon 888+Snapdragon 888CPU1xCortex-X1@2.995GHz 1x1024KB pL23xCortex-A78@ 2.42GHz 3x512KB pL24x Cortex-A55@ 1.80GHz 4x128KB pL24MB sL31xCortex-X1@ 2.84GHz 1x1024KB pL23xCortex-A78@ 2.42GHz 3x512KB pL24x Cortex-A55@ 1.80GHz 4x128KB pL24MB sL3GPUAdreno 660 @ 840MHzDSP / NPUHexagon 78032 TOPS AI(Total CPU+GPU+HVX+Tensor)Hexagon 78026 TOPS AI(Total CPU+GPU+HVX+Tensor)MemoryController4x 16-bit CH@3200MHz LPDDR5/ 51.2GB/s3MB system level cacheISP/CameraTriple 14-bit Spectra 580 ISP1x 200MP or 84MP with ZSLor64+25MP with ZSLor3x 28MP with ZSL4K video & 64MP burst captureEncode/Decode8K30 / 4K120 10-bit H.265Dolby Vision, HDR10+, HDR10, HLG720p960 infinite recordingIntegrated ModemX60 integrated(LTE Category 24/22)DL = 2500 Mbps7x20MHz CA, 1024-QAMUL = 316 Mbps3x20MHz CA, 256-QAM(5G NR Sub-6 + mmWave)DL = 7500 MbpsUL = 3000 MbpsMfc. ProcessSamsung5nm (5LPE)The new Snapdragon 888+ is a speed bin of the Snapdragon 888 that we’ve seen earlier in the year. What’s changed in the new unit is that Qualcomm is upgrading the CPU frequencies from the original 2.84GHz to 2.995GHz – a 5.2% performance upgrade. It’s not too much of a major upgrade, and I’m curious to see how it plays out in terms of power consumption as the X1 cores in the 888 were already quite power hungry.Besides the CPU uplift, the AI engine has also seen a combined performance uplift through frequency bumps as well as software optimisations that increases the AI throughput from 26 TOPS up to 32 TOPS. As a reminder, this is an aggregate figure across all of the SoC’s computational blocks of CPU, GPU, and DSP/NPU.The one thing that’s not been upgraded this year is the GPU performance, which is a bit unusual for a “+” part as normally in the past few years we’ve also seen frequency increases on this SoC block. Generally, this can be explained through the fact that the Adreno 660 in the Snapdragon 888 is already running at extremely high frequencies and corresponding high-power draw, and most devices today are not able to sustain those peak performance states – further increasing frequencies would have little benefits.ASUS, HONOR, Vivo and Xiaomi were partner vendors which endorsed the new Snapdragon 888+ and are working on devices featuring the chipset. We should be expecting the first Snapdragon 888+ devices to start being announced in the third quarter.Related Reading:The Snapdragon 888 vs The Exynos 2100: Cortex-X1 & 5nm - Who Does It Better?Qualcomm Discloses Snapdragon 888 Benchmarks: Promising PerformanceQualcomm Details The Snapdragon 888: 3rd Gen 5G & Cortex-X1 on 5nm\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16789/qualcomm-announces-snapdragon-888-plus-5g-speed-bin-at-3ghz\n",
      "Title: AMD EPYC Milan Review Part 2: Testing 8 to 64 Cores in a Production Platform\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-06-25T13:30:00Z\n",
      "URL: https://www.anandtech.com/show/16778/amd-epyc-milan-review-part-2\n",
      "Content: It’s been a few months since AMD first announced their new third generation EPYC Milan server CPU line-up. We had initially reviewed the first SKUS back in March, covering the core density optimised 64-core EPYC 7763, EPYC 7713 and the core-performance optimised 32-core EPYC 75F3. Since then, we’ve ben able to get our hands on several new mid and lower end SKUs in the form of the new 24-core EPYC 7443, the 16-core 7343, as well as the very curious 8-core EPYC 72F3 which we’ll be reviewing today.What’s also changed since our initial review back in March, is the release of Intel’s newer 3rdgeneration Xeon Scalable processors (Ice Lake SP) with our review of the 40-core Xeon 8330 and 28-core Xeon 6330.Today’s review will be focused around the new performance numbers of AMD’s EPYC CPUs, for a more comprehensive platform and architecture overview I highly recommend reading our respective initial reviews which go into more detail of the current server CPU landscape:(April 6th)Intel 3rd Gen Xeon Scalable (Ice Lake SP) Review: Generationally Big, Competitively Small(March 15th)AMD 3rd Gen EPYC Milan Review: A Peak vs Per Core Performance Balance(December 18th)The Ampere Altra Review: 2x 80 Cores Arm Server Performance MonsterWhat's New: EPYC 7443, 7343, 72F3 Low Core Count SKUsIn terms of new SKUs that we’re testing today, as mentioned, we’ll be looking at AMD new EPYC 7443, 7343 as well as the 72F3, mid- to low core-count SKUs that come at much more affordable price tags compared to the flagship units we had initially reviewed back in March. As part of the new platform switch, we’ll cover in a bit, we’re also re-reviewing the 64-core EPYC 7763 and the 32-core EPYC 75F3 – resulting in a few surprises and resolving some of the issues we’ve identified with 3rdgeneration Milan in our first review.AMD EPYC 7003 ProcessorsCore Performance OptimizedCoresThreadsBaseFreqTurboFreqL3(MB)TDPPriceF-SeriesEPYC 75F332 / 6429504000256MB280 W$4860EPYC 74F324 / 4832004000240 W$2900EPYC 73F316 / 3235004000240 W$3521EPYC 72F38 / 1637004100180 W$2468Starting off with probably the weirdest CPU in AMD’s EPYC 7003 line-up, the new 72F3 is quite the speciality part in the form of it being an 8-core server CPU, yet still featuring the maximum available platform capabilities as well as the full 256MB of L3 cache. AMD achieves this by essentially populating the part with 8 chiplet dies with each a full 32MB of L3 cache, but only one core enabled per die. This enables the part (for a server part) relatively high base frequency of 3.7GHz, boosting up to 4.1GHz and landing with a TDP of 180W, with the part costing $2468.The unit is a quite extreme case of SKU segmentation and focuses on deployments where per-core performance is paramount, or also use-cases where per-core software licenses vastly outweigh the cost of the actual hardware. We’re also re-reviewing the 32-core 75F3 in this core-performance optimised family, featuring up to 32 cores, but going for much higher 280W TDPs.AMD EPYC 7003 ProcessorsCore Density OptimizedCoresThreadsBaseFreqTurboFreqL3(MB)TDPPriceEPYC 776364 / 12824503400256MB280 W$7890EPYC 771364 / 12820003675225 W$7060EPYC 766356 / 11220003500240 W$6366EPYC 764348 / 9623003600225 W$4995P-Series (Single Socket Only)EPYC 7713P64 / 12820003675256225 W$5010In the core-density optimised series, we’re continuing on using the 64-core EPYC 7763 flagship SKU which lands in at 280W TDP and a high cost of $7890 MSRP. Unfortunately, we no longer have access to the EPYC 7713 so we couldn’t re-review this part, and benchmark numbers from this SKU in this review will carry forward our older scores, also being aptly labelled as such in our graphs.AMD EPYC 7003 ProcessorsCoresThreadsBaseFreqTurboFreqL3(MB)TDPPriceEPYC 754332 / 6428003700256 MB225 W$3761EPYC 751332 / 6426003650128 MB200 W$2840EPYC 745328 / 562750345064 MB225 W$1570EPYC 744324 / 4828504000128MB200 W$2010EPYC 741324 / 4826503600180 W$1825EPYC 734316 / 3232003900190 W$1565EPYC 731316 / 3230003700155 W$1083P-Series (Single Socket Only)EPYC 7543P32 / 6428003700256 MB225 W$2730EPYC 7443P24 / 4828504000128 MB200 W$1337EPYC 7313P16 / 3230003700155 W$913Finally, the most interesting parts of today’s evaluation are AMD’s mid- to low-core count EPYC 7443 and EPYC 7343 CPUs. At 24- and 16-core, the chips feature a fraction of the maximum theoretical core counts of the platform, but also come at much more affordable price points. These parts should especially be interesting for deployments that plan on using the platform’s full memory or I/O capabilities, but don’t require the raw processing power of the higher-end parts.These two parts are also defined by having only 128MB of L3 cache, meaning the chips are running only 4 active chiplets, with respectively only 6 and 4 cores per chiplet active. The TDPs are also more reasonable at 200W and 190W, with also respectively lower pricing of $2010 and $1565.Following Intel’s 3rdgeneration Xeon Ice Lake SP and our testing of the Xeon 28-core 6330 which lands in at an MSRP of $1894, it’s here where we’ll be seeing the most interesting performance and value comparison for today’s review.Test Platform Change - Production Milan Board from GIGABYTE: MZ72-HB0 (rev. 3.0)In our initial Milan review, we unfortunately had to work with AMD to remotely test newest Milan parts within the company’s local datacentre, as our own Daytona reference server platform encountered an unrecoverable hardware failure.In general, if possible, we also prefer to test things on production systems as they represent a more mature and representative firmware stack.A few weeks ago, at Computex, GIGABYTE had revealed their newest revision of the company’s dual-socket EPYC board, the E-ATX MZ72-HB0 rev.3.0, which now comes with out-of-box support for the newest 3rdgeneration Milan parts (The prior rev.1.0 boards don’t support the new CPUs).The E-ATX form-factor allows for more test-bench setups and noiseless operation (Thanks to Noctua’s massive NH-U14S TR4-SP3 coolers) in more conventional workstation setups.The platform change away from AMD’s Daytona reference server to the GIGABYTE system also have some significant impacts in regards to the 3rdgeneration Milan SKUs’ performance, behaving notably different in terms of power characteristics than what we saw on AMD’s system, allowing the chips to achieve even higher performance than what we had tested and published in our initial review.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16778/amd-epyc-milan-review-part-2\n",
      "Title: Intel Licenses SiFive’s Portfolio for Intel Foundry Services on 7nm\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-06-22T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16777/intel-licenses-sifives-portfolio-for-intel-foundry-services-on-7nm\n",
      "Content: Today’s announcement from SiFive comes in two parts; this part is significant as it recognizes that Intel will be enabling SiFive’s IP portfolio on its 7nm manufacturing process for upcoming foundry customers. We are expecting Intel to offer a wide variety of its own IP, such as some of the x86 cores, memory controllers, PCIe controllers, and accelerators, however the depth of its third party IP support has not been fully established at this point. SiFive’s IP is the first (we believe) official confirmation of specific IP that will be supported.Intel Foundry ServicesAnnounced earlier this year by Pat Gelsinger, Intel Foundry Services (or IFS) is one prong of Intel’s strategy to realign itself with the current and future semiconductor market. Despite having attempted to become a foundry player in the past, whereby they build chips under contract for their customers, it hasn’t really worked out that well – however IFS is a new reinvigoration of that idea, this time with more emphasis on getting it right and expanding the scope.To that end, Intel has been reorganizing its manufacturing and operations hierarchy into its own vertical group under Keyvan Esfarjani, with IFS led by Randhir Thakur as President reporting to this group.As reported by CRNin the past week, Intel has recently hired a number of key talents from its foundry competition to aid this transition, including Hong Hao, the former head of Samsung’s North American foundry business, to ‘formulate business strategies and execute customer engagement plans’. Intel has also hired Bob Brennan, a former 22 year Intel employee most recently at Micron, as Vice President of Customer Design Enablement at IFS, assisting with end-to-end contract-to-product solutions for IFS customers. This includes engagement with Intel’s Design Engineering Group, putting customers on the right track with Intel roadmaps.Beyond Intel IPSimply offering Intel’s portfolio of technologies alone to customers isn’t going to entice many customers, and in order to compete with other foundry competition, Intel needs to engage with other IP holders to provide design kits and tools for other technologies. For example, if an Intel Foundry Services customer wants to use an Intel core paired with a third party memory controller on Intel’s 14nm process, that third party memory controller design has to be validated at Intel. Similarly, if a customer wants to combine Arm cores or other accelerators, that IP has to be validated. Intel has to create a structure of supported IP aside from their own, and today’s announcement is one of the initial steps to that.Today’s announcement from SiFive is the launch of a new family of ‘Performance’ cores, debuting with the P270 and the P550. Both cores are Linux capable, with full support for the RISC-V vector extension v1.0rc, and the P550 is SiFive’s highest performance core ever with a reported SPEC2006 int score of 8.65 per GHz. As part of that announcement, Intel is the lead development partner of the P550 core on Intel’s own 7nm process. That’s what the press release says.In our emails with PR however, the wording was a little loose, and stated thatIntel recently selected SiFive to work with the Intel Foundry Services (IFS) businessto offer IPto Foundry customers. As part of the growth and development of the IFS business, IFS is creating … the P550 on the Intel 7nm.The first sentence is critical here; it broadens the scope of the initial press release beyond simple co-operation of a singular IP core (the P550) into something more encompassing. The first step on such an endeavor would obviously be a proof point, such as enabling the P550, but then going beyond to other elements of SiFive’s portfolio. How wide the scope is beyond the P550 has not been commented on at this time, however if all goes well, we could easily imagine a wide remit for SiFive solutions at any foundry.It should be noted that some musings have been made that Intel might be trying to acquire SiFive, however nothing that AnandTech could confirm or get comment on – the fact that the two companies are putting this announcement out today might dampen those rumors a little.The fact that the press release also mentions 7nm is an interesting angle. We’re covering what Intel is making with SiFive P550 in a separate news article, as it is big enough news to get its own title.Related ReadingSamsung to Use SiFive RISC-V Cores for SoCs, Automotive, 5G ApplicationsSiFive Announces First RISC-V OoO CPU Core: The U8-Series Processor IPSiFive Acquires USB 2.0 and 3.x IP Portfolio to Strengthen RISC-V SoCsSiFive Unveils Freedom Platforms for RISC-V-Based Semi-Custom ChipsWestern Digital’s RISC-V \"SweRV\" Core Design Released For Free\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16777/intel-licenses-sifives-portfolio-for-intel-foundry-services-on-7nm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: An AnandTech Interview with Jim Keller: 'The Laziest Person at Tesla'\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-06-17T16:20:00Z\n",
      "URL: https://www.anandtech.com/show/16762/an-anandtech-interview-with-jim-keller-laziest-person-at-tesla\n",
      "Content: I'vespoken about Jim Keller many times on AnandTech. In the world of semiconductor design, his name draws attention, simply by the number of large successful projects he has worked on, or led, that have created billions of dollars of revenue for those respective companies. His career spans DEC, AMD, SiByte, Broadcom, PA Semi, Apple, AMD (again), Tesla, Intel, and now he is at Tenstorrent as CTO, developing the next generation of scalable AI hardware. Jim's work ethic has often been described as 'enjoying a challenge', and over the years when I've spoken to him, he always wants to make sure that what he is doing is both that challenge, but also important for who he is working for. More recently that means working on the most exciting semiconductor direction of the day, either high-performance compute, self-driving, or AI.Jim KellerCTO TenstorrentIan CutressAnandTechI have recentlyinterviewed Tenstorrent's CEO, Ljubisa Bajic, alongside Jimdiscussing the next generation of AI semiconductors. Today we're publishing a transcript of a recent chat with Jim, now five months into his role at Tenstorrent, but moreso to talk about Jim the person, rather than simply Jim the engineer.Jim Keller: Work ExperienceAnandTechCompanyTitleImportantProduct1980s1998DECArchitectAlpha19981999AMDLead ArchitectK7, K8v1HyperTransport19992000SiByteChief ArchitectMIPS Networking20002004BroadcomChief ArchitectMIPS Networking20042008P.A. SemiVP EngineeringLow Power Mobile20082012AppleVP EngineeringA4 / A5 Mobile8/20129/2015AMDCorp VP andChief Cores ArchitectSkybridge / K12(+ Zen)1/20164/2018TeslaVP AutopilotHardware EngineeringFully Self-Driving(FSD) Chip4/20186/2020IntelSenior VPSilicon Engineering?2021TenstorrentPresident and CTOTBDTopics CoveredAMD, Zen, and Project SkybridgeManaging 10000 People at IntelThe Future with TenstorrentEngineers and People SkillsArm vs x86 vs RISC-VLiving a Life of AbstractionThoughts on Moore's LawEngineering the Right TeamIdols, Maturity, and the Human ExperienceNature vs NurturePushing Everyone To Be The BestSecurity, Ethics, and Group BeliefChips Made by AI, and Beyond SiliconAMD, Zen, and Project SkybridgeIan Cutress: Most of the audience questions are focused on your time at AMD, so let’s start there. You worked at AMD on Zen, and on the Skybridge platform - AMD is now gaining market share with the Zen product line, and you're off on to bigger and better things. But there has been a lot of confusion as to your exact role at AMD during that project. Some people believe you were integral in nailing down Zen’s design, then Zen 2 and Zen 3 high-level microarchitecture. Others believe that you put the people in place, signed off at high level, and then went to focus on the Arm version of Skybridge, K12. Can you give us any clarity as to your role there, how deep you went with Zen versus K12, or your involvement in things like Infinity Fabric?Jim Keller:Yeah, it was a complicated project, right? At AMD when I joined, they had Bulldozer and Jaguar, and they both had some charming features but they weren't successful in the market. The roadmaps weren't aggressive, they were falling behind Intel, and so that's not a good thing to do if you're already behind - you better be catching up, not falling behind. So I took the role, and I was president of the CPU team which I think when I joined was 500 people. Then over the next three years the SoC team, the Fabric team, and some IP teams joined my little gang. I think when I left, it was 2400 people I was told. So I was a VP with a staff. I had senior directors reporting to me, and the senior fellows, and my staff was 15 people. So I was hardly writing RTL!That said we did a whole bunch of things. I'm a computer architect, I’m not really a manager. I wanted the management role, which was the biggest management role I'd had at the time. Up to that point I'd been the VP of a start-up, but that was 50 people, and we all got along - this was a fairly different play for me. I knew that the technical changes we had to make would involve getting people aligned to it. I didn't want to be the architect on the side arguing with the VP about why somebody could or couldn’t do the job, or why this was the right or wrong decision. I spoke to Mark Papermaster, I told him my theory, and he said ‘okay, we'll give it a try’, and it worked out pretty good.With that I had direct authority as it were - but people don't really do what they're told to do, right? They do what they're inspired to do. So you have to lay out a plan, and part of it was finding out who were the right people to do these different things, and sometimes somebody is really good, but people get very invested in what they did last time, or they believe things can't be changed, and I would say my view was things were so bad that almost everything had to change. So I went in with that as a default. Does that make sense? Now, it wasn't that we didn't find a whole bunch of stuff that was good to use. But you had to prove that the old thing was good, as opposed to prove the new thing was good, so we changed that mindset.Architecturally, I had a pretty good idea what I wanted to build and why. I found people inside the company, such asMike Clark,Leslie Barnes,Jay Fleischman, and others. There are quite a few really great people that once we describe what we wanted to do, they were like, ‘yeah, we want to do that’. Architecturally, I had some input. There was often decisions and analysis, and people have different opinions, so I was fairly hands-on doing that. But I wasn't doing block diagrams or writing RTL. We had multiple projects going on - there was Zen, there was the Arm cousin of that, the follow-on, and some new SoC methodology. But we did more than just CPU design - we did methodology design, IP refactoring, very large organizational changes. I was hands-on top to bottom with all that stuff, so it makes sense.IC: A few people consider you 'The Father of Zen', do you think you’d scribe to that position? Or should that go to somebody else?JK:Perhaps one of the uncles. There were a lot of really great people on Zen. There was a methodology team that was worldwide, the SoC team was partly in Austin and partly in India, the floating-point cache was done in Colorado, the core execution front end was in Austin, the Arm front end was in Sunnyvale, and we had good technical leaders. I was in daily communication for a while withSuzanne Plummerand Steve Hale, who kind of built the front end of the Zen core, and the Colorado team. It was really good people. Mike Clark's a great architect, so we had a lot of fun, and success. Success has a lot of authors - failure has one. So that was a success. Then some teams stepped up - we moved Excavator to the Boston team, where they took over finishing the design and the physical stuff,Harry Fairand his guys did a great job on that. So there were some fairly stressful organizational changes that we did, going through that. The team all came together, so I think there was a lot of camaraderie in it. So I won't claim to be the ‘father’ - I was brought in, you know, as the instigator and the chief nudge, but part architect part transformational leader. That was fun.IC: Is everything that you worked on now out at AMD, or is there still, kind of roadmap stuff still to come out, do you think from the ideas that you helped propagate?JK:So when you build a new computer, and Zen was a new computer, there was already work underway. You build in basically a roadmap, so I was thinking about what we were going to do for five years, chip after chip. We did this at Apple too when we built the first big core at Apple - we built big bones [into the design]. When you make a computer faster, there's two ways to do it - you make the fundamental structure bigger, or you tweak features, and Zen had a big structure. Then there were obvious things to do for several generations to follow. They've been following through on that.So at some point, they will have to do another big rewrite and change. I don't know if they started that yet. What we had planned for the architectural performance improvements were fairly large, over a couple of years, and they seem to be doing a great job of executing to that. But I've been out of there for a while - four or five years now.IC: Yeah, I think they said that Zen 3, the last one that just came out was a rewrite. So I think some people are thinking that was still under your direction.JK:Yeah, it's hard to say. Even when we did Zen, we did a from-scratch design - a clean design at the top. But then when they built it, there was a whole bunch of pieces of RTL that came from Bulldozer, and Jaguar, which were perfectly good to use. They just had to be modified and built into the new Zen structure. So hardware guys are super good at using code when it's good.So when they say they did a big rewrite, they probably took some pieces and re-architected them at the top, but when they built the code, it wouldn't surprise me if somewhere between 20% and 80% of the code was the same stuff, or mildly modified, but that's pretty normal. The key is to get the structure right, and then reuse code as needed, as opposed to taking something that's complicated and trying to tweak it to get somewhere. So if they did a rewrite, they probably fixed the structure.Managing 10000 People at IntelIC: I know it’s still kind of fresh, so I’m not sure what kind of NDAs you are still under, but your work at Intel - was that more of a clean slate? Can you go into any detail about what you did there?JK:I can’t talk too much, obviously. The role I had was Senior Vice President of Silicon Engineering Group, and the team was 10,000 people. They're doing so many different things, it's just amazing. It was something like 60 or 70 SoCs is in flight at a time, literally from design to prototyping, debugging, and in production. So it was a fairly diverse group, and there my staff was vice presidents and senior fellows, so it was a big organizational thing.I had thought I was going there because there was a bunch of new technology to go build. I spent most of my time working with the team about both organizational and methodology transformation, like new CAD tools, new methodologies, new ways to build chips. A couple of years before I joined, they started what's called the SoC IP view of building chips, versus Intel's historic monolithic view. That to be honest wasn't going well, because they took the monolithic chips, they took the great client and server parts, and simply broke it into pieces. You can't just break it into pieces - you have to actually rebuild those pieces and some of the methodology goes with it.We found a bunch of people [internally] who were really excited about working on that, and I also spent a lot of time on IP quality, IP density, libraries, characterization, process technology. You name it, I was on it. My days were kind of wild - some days I’d have 14 different meanings in one day. It was just click, click, click, click, so many things going on.IC: All those meetings, how did you get anything done?JK:I don't get anything done technically! I got told I was the senior vice president - it's evaluation, set direction, make judgment calls, or let’s say try some organizational change, or people change. That adds up after a while. Know that the key thing about getting somewhere is to know where you are going, and then put an organization in place that knows how to do that - that takes a lot of work. So I didn't write much code, but I did send a lot of text messages.IC: Now Intel has a new engineering-focused CEO in Pat Gelsinger. Would you ever consider going back if the right opportunity came up?JK:I don't know. I have a really fun job now, and in a really explosive growth market. So I wish him the best. I think it was a good choice [for Pat as CEO], and I hope it's a good choice, but we'll see what happens. He definitely cares a lot about Intel, and he's had real success in the past. He’s definitely going to bring a lot more technical focus to the company. But I liked working with Bob Swan just fine, so we'll see what happens.The Future with TenstorrentIC: You are now several companies on from AMD, at a company called Tenstorrent, with an old friend in Ljubisa Bajic. You’ve been jumping from company to company to company for basically your whole career. You’re always finding another project, another opportunity, another angle. Not to be too blunt, but is Tenstorrent going to be a forever home?JK:First, I was at Digital (DEC) for 15 years, right! Now that was a different career because I was in the mid-range group where we built computers out ofECL- these were refrigerator-sized boxes. I was in the DEC Alpha team where we built little microprocessors, little teeny things, which at the time we thought were huge. These were 300 square millimeters at 50 watts, which blew everybody's mind.So I was there for a while, and I went to AMD right during the internet rush, and we did a whole bunch of stuff in a couple of years. We started Opteron, HyperTransport, 2P servers - it was kind of a whirlwind of a place. But I got sucked up or caught up in the enthusiasm of the internet, and I went to SiByte, which got bought by Broadcom, and I was there for four years total. We delivered several generations of products.I was then at P.A Semi, and we delivered a great product, but they didn't really want to sell the product for some reason, or they thought they were going to sell it to Apple. I actually went to Apple, and then Apple bought P.A Semi, and then I worked for that team, so you know I was between P.A Semi and Apple. That was seven years, so I don't really feel like that was jumping around too much.Then I jumped to AMD I guess, and that was fun for a while. Then I went to Tesla where we delivered Hardware 3 (Tesla Autopilot). So that was kind of phenomenal. From a standing start to driving a car in 18 months - I don't think that's ever been done before, and that product shipped really successfully. They built a million of them last year. Tesla and Intel were a different kind of a whirlwind, so you could say I jumped in and jumped out. I sure had a lot of fun.So yeah, I've been around a little bit. I like to think I mostly get done what I set out to accomplish. My success right there is pretty high in terms of delivering products that have lasting value. I'm not the guy to tweak things in production – it’s either a clean piece of paper or a complete disaster. That seems to be the things I do best at. It's good to know yourself - I'm not an operational manager. So Tenstorrent is more the clean piece of paper. The AI space is exploding. The company itself is already many years old, but we're building a new generation of parts and going to market and starting to sell stuff. I'm CTO and president, have a big stake in the company, both financially and also a commitment to my friends there, so I plan on being here for a while.IC: I think you said before that going beyond the sort of matrix, you end up with massive graph structures, especially for AI and ML, and the whole point about Tenstorrent, it’s a graph compiler and a graph compute engine, not just a simple matrix multiply.JK:From old math, and I'm not a mathematician, so mathematicians are going to cringe a little bit, but there was scalar math, like A = B + C x D. When you had a small number of transistors, that's the math you could do. Now we have more transistors you could say ‘I can do a vector of those’, like an equation properly in a step. Then we got more transistors, we could do a matrix multiply. Then as we got more transistors, you wanted to take those big operations and break them up, because if you make your matrix multiplier too big, the power of just getting across the unit is a waste of energy.So you find you want to build this optimal size block that’s not too small, like a thread in a GPU, but it's not too big, like covering the whole chip with one matrix multiplier. That would be a really dumb idea from a power perspective. So then you get this array of medium size processors, where medium is something like four TOPs. That is still hilarious to me, because I remember when that was a really big number. Once you break that up, now you have to take the big operations and map them to the array of processors and AI looks like a graph of very big operations. It’s still a graph, and then the big operations are factored down into smaller graphs. Now you have to lay that out on a chip with lots of processors, and have the data flow around it.This is a very different kind of computing than running a vector or a matrix program. So we sometimes call it a scalar vector matrix. Raja used to call it spatial compute, which would probably be a better word.IC: Alongside the Tensix cores, Tenstorrent is also adding in vector engines into your cores for the next generation? How does that fit in?JK:Remember the general-purpose CPUs that have vector engines on them – it turns out that when you're running AI programs, there is some general-purpose computing you just want to have. There are also some times in the graph where you want to run a C program on the result of an AI operation, and so having that compute be tightly coupled is nice. [By keeping] it on the same chip, the latency is super low, and the power to get back and forth is reasonable. So yeah, we're working on an interesting roadmap for that. That's a little computer architectural research area, like, what's the right mix with accelerated computing and total purpose computing and how are people using it. Then how do you build it in a way programmers can actually use it? That's the trick, which we're working on.Engineers and People SkillsIC: If I go through your career, you’ve gone between high-performance computing and low-powered efficient computing. Now you’re in the world of AI acceleration. Has it ever got boring?JK:No, and it's really weird! Well it's changed, and it's changed so much, but at some level it doesn't change at all. Computers at the bottom, they just add ones and zeros together. It's pretty easy. 011011100, it's not that complicated.But I worked on the VAX 8800 where we built it out of gate arrays that had 200 OR gates in each chip. Like 200, right? Now at Tenstorrent, our little computers, we call them Tensix cores, are four trillion operations per second per core, and there's 100 of them in a chip. So the building block has shifted from 200 gates to four Tera Ops. That's kind of a wild transformation.Then the tools are way better than they used to be. What you can do now - you can't build more complicated things unless the abstraction levels change and the tools change. There have been so many changes on that kind of stuff. When I was a kid, I used to think I had to do everything myself - and I worked like a maniac and coded all the time. Now I know how to work with people and organizations and listen. Stuff like that. People skills. I probably would have a pretty uneven scorecard on the people skills! I do have a few.IC: Would you say that engineers need more people skills these days? Because everything is complex, everything has separate abstraction layers, and if you want to work between them you have to have the fundamentals down.JK:Now here’s the fundamental truth, people aren't getting any smarter. So people can't continue to work across more and more things - that's just dumb. But you do have to build tools and organizations that support people's ability to do complicated things. The VAX 8800 team was 150 people. But the team that built the first or second processor at Apple, the first big custom core, was 150 people. Now, the CAD tools are unbelievably better, and we use 1000s of computers to do simulations, plus we have tools that could place and route 2 million gates versus 200. So something has changed radically, but the number of people an engineer might talk to in a given day didn't change at all. If you have an engineer talk to more than five people a day, they'll lose their mind. So, some things are really constant.CPU Instruction Sets: Arm vs x86 vs RISC-VIC: You’ve spoken about CPU instruction sets in the past, and one of the biggest requests for this interview I got was around your opinion about CPU instruction sets. Specifically questions came in about how we should deal with fundamental limits on them, how we pivot to better ones, and what your skin in the game is in terms of ARM versus x86 versus RISC V. I think at one point, you said most compute happens on a couple of dozen op-codes. Am I remembering that correctly?JK:[Arguing about instruction sets] is a very sad story. It's not even a couple of dozen [op-codes] - 80% of core execution is only six instructions - you know, load, store, add, subtract, compare and branch. With those you have pretty much covered it. If you're writing in Perl or something, maybe call and return are more important than compare and branch. But instruction sets only matter a little bit - you can lose 10%, or 20%, [of performance] because you're missing instructions.For a while we thought variable-length instructions were really hard to decode. But we keep figuring out how to do that. You basically predict where all the instructions are in tables, and once you have good predictors, you can predict that stuff well enough. So fixed-length instructions seem really nice when you're building little baby computers, but if you're building a really big computer, to predict or to figure out where all the instructions are, it isn't dominating the die. So it doesn't matter that much.When RISC first came out, x86 was half microcode. So if you look at the die, half the chip is a ROM, or maybe a third or something. And the RISC guys could say that there is no ROM on a RISC chip, so we get more performance. But now the ROM is so small, you can't find it. Actually, the adder is so small, you can hardly find it? What limits computer performance today is predictability, and the two big ones are instruction/branch predictability, and data locality.Now the new predictors are really good at that. They're big - two predictors are way bigger than the adder. That's where you get into the CPU versus GPU (or AI engine) debate. The GPU guys will say ‘look there's no branch predictor because we do everything in parallel’. So the chip has way more adders and subtractors, and that's true if that's the problem you have. But they're crap at running C programs.GPUs were built to run shader programs on pixels, so if you're given 8 million pixels, and the big GPUs now have 6000 threads, you can cover all the pixels with each one of them running 1000 programs per frame. But it's sort of like an army of ants carrying around grains of sand, whereas big AI computers, they have really big matrix multipliers. They like a much smaller number of threads that do a lot more math because the problem is inherently big. Whereas the shader problem was that the problems were inherently small because there are so many pixels.There are genuinely three different kinds of computers: CPUs, GPUs, and AI. NVIDIA is kind of doing the ‘inbetweener’ thing where they're using a GPU to run AI, and they're trying to enhance it. Some of that is obviously working pretty well, and some of it is obviously fairly complicated. What's interesting, and this happens a lot, is that general-purpose CPUs when they saw the vector performance of GPUs, added vector units. Sometimes that was great, because you only had a little bit of vector computing to do, but if you had a lot, a GPU might be a better solution.IC: So going back to ISA question - many people were asking about what do you think about Arm versus x86? Which one has the legs, which one has the performance? Do you care much, if at all?JK:I care a little. Here's what happened - so when x86 first came out, it was super simple and clean, right? Then at the time, there were multiple 8-bit architectures: x86, the 6800, the 6502. I programmed probably all of them way back in the day. Then x86, oddly enough, was the open version. They licensed that to seven different companies. Then that gave people opportunity, but Intel surprisingly licensed it. Then they went to 16 bits and 32 bits, and then they added virtual memory, virtualization, security, then 64 bits and more features. So what happens to an architecture as you add stuff, you keep the old stuff so it's compatible.So when Arm first came out, it was a clean 32-bit computer. Compared to x86, it just looked way simpler and easier to build. Then they added a 16-bit mode and the IT (if then) instruction, which is awful. Then [they added] a weird floating-point vector extension set with overlays in a register file, and then 64-bit, which partly cleaned it up. There was some special stuff for security and booting, and so it has only got more complicated.Now RISC-V shows up and it's the shiny new cousin, right? Because there's no legacy. It's actually an open instruction set architecture, and people build it in universities where they don’t have time or interest to add too much junk, like some architectures have. So relatively speaking, just because of its pedigree, and age, it's early in the life cycle of complexity. It's a pretty good instruction set, they did a fine job. So if I was just going to say if I want to build a computer really fast today, and I want it to go fast, RISC-V is the easiest one to choose. It’s the simplest one, it has got all the right features, it has got the right top eight instructions that you actually need to optimize for, and it doesn't have too much junk.IC: So modern instruction sets have too much bloat, especially the old ones. Legacy baggage and such?JK:Instructions that have been iterated on, and added to, have too much bloat.That's what always happens. As you keep adding things, the engineers have the struggle. You can have this really good design, there are 10 features, and so you add some features to it. The features all make it better, but they also make it more complicated. As you go along, every new feature added gets harder to do, because the interaction for that feature, and everything else, gets terrible.The marketing guys, and the old customers, will say ‘don't delete anything’, but in the meantime they are all playing with the new fresh thing that only does 70% of what the old one does, but it does it way better because it doesn't have all these problems. I've talked about diminishing return curves, and there's a bunch of reasons for diminishing returns, but one of them is the complexity of the interactions of things. They slow you down to the point where something simpler that did less would actually be faster. That has happened many times, and it's some result of complexity theory and you know, human nefariousness I think.IC: So did you ever see a situation where x86 gets broken down and something just gets reinvented? Or will it just remain sort of legacy, and then just new things will pop up like RISC-V to kind of fill the void when needed?JK:x86-64 was a fairly clean slate, but obviously it had to carry all the old baggage for this and that. They deprecated a lot of the old 16-bit modes. There's a whole bunch of gunk that disappeared, and sometimes if you're careful, you can say ‘I need to support this legacy, but it doesn't have to be performant, and I can isolate it from the rest’. You either emulate it or support it.We used to build computers such that you had a front end, a fetch, a dispatch, an execute, a load store, an L2 cache. If you looked at the boundaries between them, you'd see 100 wires doing random things that were dependent on exactly what cycle or what phase of the clock it was. Now these interfaces tend to look less like instruction boundaries – if I send an instruction from here to there, now I have a protocol. So the computer inside doesn't look like a big mess of stuff connected together, it looks like eight computers hooked together that do different things. There’s a fetch computer and a dispatch computer, an execution computer, and a floating-point computer. If you do that properly, you can change the floating-point without touching anything else.That's less of an instruction set thing – it’s more ‘what was your design principle when you build it’, and then how did you do it. The thing is, if you get to a problem, you could say ‘if I could just have these five wires between these two boxes, I could get rid of this problem’. But every time you do that, every time you violate the abstraction layer, you've created a problem for future Jim. I've done that so many times, and like if you solve it properly, it would still be clean, but at some point if you hack it a little bit, then that kills you over time.Living a Life of AbstractionIC: I've seen a number of talks where you speak about the concept of abstraction layers in not only a lot of aspects of engineering, but also life as well. This concept that you can independently upgrade different layers without affecting those above and below, and providing new platforms to build upon. At what point in your life did that kind of ethos click, and what happened in your life to make it that a pervasive element of your personality?JK:Pervasive element of my personality? That's pretty funny! I know I repeat it a lot, maybe I'm trying to convince myself.Like, when we built EV 6, Dirk Meyer was the other architect. We had a couple other strong people. We divided the design into a few pieces, we wrote a very simple performance model, got it, but when we built the thing, it was a relatively short pipe for an out-of-order machine, because we were still a little weak on predictors. There were a lot of interactions between things, and it was a difficult design we built. We also built it with the custom design methodology Digital had at the time. So we had 22 different flip-flops, and people could/would roll their own flip flop. We frequently built large structures out of transistors. I remember somebody asked me what elements were in our library, and I said, both of them! N-devices and P-devices, right? Then I went to AMD, and K7 was built with a cell library.Now, the engineers there were really good at laying down the cell libraries in a way they got good performance. They only had two flip flops - a big one and a little one, and they had a clean cell library. They had an abstraction layer between the transistors and the designers. This was before the age of really good place-and-route tools, and that was way better.Then on the interface that we built on EV6, which was later called the S2K bus, we listened to AMD. We originally had a lot of complicated transactions to do snoops, and loads, and stores, and reads, and writes, and all kinds of stuff. A friend of mine, who was at Digital Research Lab, I explained how it worked to him one day - he listened to me and he just shook his head. He said ‘Jim, that's not the way you do this’. He explained how virtual channels worked, and how you could have separate abstract channels of information. You get that right before you start encoding commands. As a result of that educational seminar/ass-kicking, was HyperTransport. It has a lot of the S2K protocol, but it was built in a much more abstract way. So I would say that my move from AMD, from Digital to AMD, was where we had the ideas of how to build high-performance computing, but the methodologies were integrated, so from transistor up to architecture it couldn't be the same person.At AMD, there’s Mike Clark, the architects, the microarchitects, and the RTL people who write Verilog, but they literally translated to the gate libraries, to the gate people, and it was much more of a layered approach. K7 was quite a fast processor, and our first swing at K8, we kind of went backwards. My favorite circuit partner at the time - he and I could talk about big designs, and we saw this as transistors, but that's a complicated way to build computers. Since then, I've been more convinced that the abstraction layers were right. You don't overstep human capability - that's the biggest problem. If you want to build something bigger and more complicated, you better solve the abstraction layers, because people aren't getting smarter. If you put more than 100 people on it, it'll slow down, not speed up, and so you have to solve that problem.IC: If you have more than 100 people, you need to split into two abstraction layers?JK:Exactly. There are reasons for that, like human beings are really good at tracking. Your inner circle of friends is like 10-20 people, it's like a close family, and then there is this kind of 50 to 100 depending on how it's organized, that you can keep track of. But above that, you read everybody outside your group of 100 people as semi-strangers. So you have to have some different contracts about how you do it. Like when we built Zen, we had 200 people, and half the team at the front end and half the team at the back end. The interface between them was defined, and they didn't really have to talk to each other about the details behind the contract. That was important. Now they got along pretty good and they worked together, but they didn't constantly have to go back and forth across that boundary.Thoughts on Moore's LawIC: You've said on stage, and in interviews in the past, that you're not worried about Moore's Law. You’re not worried on the process node side, about the evolution of semiconductors, and it will eventually get worked out by someone, somewhere. Would you say your attitude towards Moore's law is apathetic?JK:I’m super proactive. That’s not apathetic at all. Like, I know a lot of details about it. People conflate a few things, like when Intel's 10-nanometer slipped. People said that Moore's law is dead, but TSMC’s roadmap didn’t slip at all.Some of that is because TSMC’s roadmap aligned to the EUV machine availability. So when they went from 16nm, to 10nm, to 7nm, they did something that TSMC has been really good at - doing these half steps. So they did 7nm without EUV, and that 7nm with EUV, then 5nm without, and 5+nm with EUV, and they tweaked stuff. Then with the EUV machines, for a while people weren't sure if they're going to work. But now ASML’s market cap is twice that of Intel's (it’s actually about even now, on 21stJune).Then there's a funny thing - I realized that at the locus of innovation, we tend to think of TSMC, Samsung, and Intel as the process leaders. But a lot of the leadership is actually in the equipment manufacturers like ASML, and in materials. If you look at who is building the innovative stuff, and the EUV worldwide sales, the number is something like TSMC is going to buy like 150 EUV machines by 2023 or something like that. The numbers are phenomenal because even a few years ago not many people were even sure that EUV was going to work. But now there's X-ray lithography coming up, and again, you can say it's impossible, but bloody everything has been impossible! The fine print, this what Richard Feynman said - he's kind of smart. He said ‘there's lots of room at the bottom’, and I personally can count, and if you look at how many atoms are across transistors, there's a lot. If you look at how many transistors you actually need to make a junction, without too many quantum effects, there are only 10. So there is room there.There's also this funny thing - there's a belief system when everybody believes technology is moving at this pace and the whole world is oriented towards it. But technology isn't one thing. There are people who figure out how to build transistors, like what the process designers do at like Intel, or TSMC, or Samsung. They use equipment which can do features, but then the features actually interact, and then there's a really interesting trade-off between, like, how should this be deposited and etched, how tall should it be, how wide, in what space. They are the craftsman using the tools, so the tools have to be super sharp, and the craftsmen have to be super knowledgeable. That's a complicated play. There's lots of interaction and at some level, because the machines themselves are complicated, you have this little complexity combination where the machine manufacturers are doing different pieces, but they don't always coordinate perfectly, or they coordinate through the machine integration guys who designed the process, and that's complicated. It can slow things down. But it's not due to physics fundamentals - we're making good progress on physics fundamentals.IC: In your scaled ML talk, the one that you have in Comic Sans, you had the printed X slide. About it you say that as time goes on the way you print the X, because of the laws of physics, there are still several more steps to go in EUV. Also High NA EUV is coming in a couple of years, but now you mention X-rays. What's the timeline for that? It's not even on my radar yet.JK:Typically when a technology comes along, they use it for one thing. First, when EUV was first used in DRAMs, it was literally for one step, maybe two. So I'm trying to remember – perhaps 2023/2024? It's not that far away. That means they're already up and running, and people are playing with it. Then the wild thing is, when they went from optical light to EUV, it was about a 10x reduction in wavelength? So they while they had crazy multi-patterning and interference kind of stuff that you saw those pictures of DUV, when it came to EUV, they could just print direct. But actually [as you go smaller] they can use the same tricks on EUV. So EUV is going to multi-patterning, I think in 3nm. Then there are so many tricks you can do with that. So yeah, the physics is really interesting. Then along with the physics, the optics stuff, and then there's the purity of the materials, which is super important, then temperature control, so things don't move around too much. Everywhere you look there are interesting physics problems, and so there's lots to do. There are hundreds of thousands of people working on it, and there’s more than enough innovation bandwidth.Engineering the Right TeamIC: So pivoting to a popular question we’ve had. One of the things that we've noted you doing, as you go from company to company, is the topic of building a team. As teams are built by others, we've seen some people take engineers from a team they've built at previous companies to the next company. Have you ever got any insights into how you build your teams? Have there been any different approaches at the companies that you work for on this?JK:The first thing you have to realize is if you are building the team, or finding one. So there's a great museum in Venice, the David Museum, and the front of the museum, there's these huge blocks of marble. 20 by 20 by 20. How they move them, I don't know. The block of marble sitting there, and Michelangelo could see this beautiful sculpture in it. It was already there, right? The problem was removing the excess marble.So if you go into companies with 1000 employees, I guarantee you, there's a good team there. You don't have to hire anybody. When I was at AMD, I hardly hired anybody. We moved people around, we re-deployed people [elsewhere], but there were plenty of great people there. When I went to Tesla, we had to build the team from scratch, because there was nobody at Tesla that was building chips. I hired people that I knew, but then we hired a bunch of people that I didn't know at some point, and this is one of those interesting things.I've seen leaders go from one company to another and they bring their 20 people, and then they start trying to reproduce what they had before. That's a bad idea, because although 20 people is enough to reproduce [what you had], it alienates what you want [in that new team]. When you build a new team, ideally, you get people you really like, either you just met them, or you work with them, but you want some differences in approach and thinking because everybody gets into a local minimum. So the new team has this opportunity to make something new together. Some of that is because if you had ten really great teams all working really well, and then you made a new team with one person from each of those teams: that may well be better, because they will re-select which the best ideas were.But every team has pluses and minuses, and so you have to think about if you're building the team or finding a team, and then what's the dynamic you're trying to create that gives it space for people to have new ideas. Or, if some people get stuck on one idea, they then work with new people and they’ll start doing this incredible thing, and you think they're great, even though they used to be not so great, so what happened? Well, they were carrying some idea around that wasn't great, and then they met somebody who challenged them or the environment forced them, and all of a sudden they're doing a great job. I've seen that happen so many times.Ken Olson at Digital (DEC) said there are no bad employees, there are just bad employee job matches. When I was younger, I thought that was stupid. But as I've worked with more people, I've seen that happen so many bloody times that I've even fired people who went on to be really successful. All because they weren't doing a good job and they were stuck, emotionally, and they felt committed to something that wasn't working. The act of moving them to a different place freed them up. [Needless to say] I don't get a thank you. (laughs)IC: So how much of that also comes down to company culture? I mean, when you're looking for the person for the right position, or whether you're hiring in for the new position, do you try and get something that goes against the company grain? Or goes with the company grain? Do you have any tactics here or are you just looking for someone with spark?JK:If you're trying to do something really innovative, it's probably mostly going against [the grain]. If you have a project that's going really well, bringing in instigators is going to slow everybody down, because you're already doing well. You have to read the group in the environment. Then there are some people who are really good, and they're really flexible to go on this project, they fit in and just push, but on the next project, you can see they have been building their network and the team, and on the next project they’re ready to do a pivot and everybody's willing to work. Trust is a funny thing, right? You know, if somebody walks up and says to jump off this bridge but you'll be fine, you're likely to call bullshit - but if you had already been through a whole bunch of stuff with them, and they said ‘look, trust me, then jump - you're going to be fine; it's going to suck, but it's going to be fine’, you'll do it, right? Teams that trust each other are way more effective than ones that have to do everything with contracts, negotiation, and politics.So that's probably one thing - if you're building or finding a team, and you start seeing people doing politics, which means manipulating the environment for their own benefit, they have got to go. Unless you're the boss! Then you have got to see if they deliver. Some people are very political, but they really think their political strength comes from delivering. But people randomly in an organization that are political just cause lots of stress.IC: Do you recommend that early or mid-career engineers should bounce around regularly from project to project, just so they don’t get stuck in a hole? It sounds like that’s a common thing.JK:You learn fastest when you're doing something new, and working for somebody that knows way more than you. So if you're relatively early in your career and you're not learning a lot or, you know, the people that you're working for aren't inspiring you, then yeah you should probably change. There are some careers where I've seen people bounce around three times because they're getting experience and they end up being good at nothing. They would have been better staying where they were, and really getting deep at something. So you know, creative tension - there's creative tension between those two ideas.Idols, Maturity, and the Human ExperienceIC: So that kind of leads into a good question, actually, because I wanted to ask about you and your mentors going through your early career. Who did you look up to for leadership or knowledge or skills? Is there anyone you idolize?JK:Oh, yeah, lots of people. Well it started out with my parents. Like, I was really lucky. My father was an engineer, and my mom was super smart, kind of more verbally and linguistically. The weird thing was that when I grew up, I was sort of more like her, you know, thinking-wise, but I was dyslexic - I couldn't read. My father was an engineer, so I grew up thinking I was like him, but I was actually intellectually more like my mother. They were both smart people. Now they came out of the 50s, and my mom raised family, so she didn't start her career as a therapist until later in life. But they were pretty interesting people.Then, when I first started at Digital, I worked for a guy named Bob Stewart, who was a great computer architect. He did the PDP-11/44, PDP-11/70, VAX 780, VAX 8800, and the CI interconnect. Somebody said that every project that he had ever worked on earned a billion dollars, back when that was a huge number. So I worked for him and he was great, but there were half a dozen other really great computer architects there. I was at DEC and DEC had DEC Research Labs, and I got to meet guys like Butler Lampson and Chuck Thacker and Neil Wilhelm. Nancy Kronenberg was one of my mentors when I was a little kid, and she’s one of the chief people on the VMS operating system. So that was kind of lucky.So did I idolize them? Well, they were both daunting and not, because I was a little bit of a, you know. I didn't quite realize who they were at the time. I was more a little oblivious to what was going on. Like, my first week at Digital, we got trained on this drawing system called Valid, which is kind of before the Matrox graphics era. So this guy walked in, and he was asking us questions and telling us about hierarchical design. I explained to him why that was partly good idea and partly stupid, and so we had an hour debate about it, then he walked off. Somebody said that was Gordon Bell. I asked ‘Who's that? He’s the CTO of Digital? Really? Well he's wrong about half the stuff he just said - I hope I straightened him out.’ But you know, I think that's just some serotonin activation or something. That's more of a mental problem with me than a feature, I think!IC: So would you say you’ve matured?JK:Not a bit!IC: Is that where the fun is?JK:I mean, there's a whole bunch of stuff. When I was young, it was like I get nervous when I give a talk, and I realized I had to understand the people around me better. But you know, I wasn't always quite convinced. [At the time] I rather they just do the right thing or something. So there's a bunch of stuff that has changed. Now I'm really interested in what people think and why they think it, and I have a lot of experience with that. Every once a while you can really help debug somebody, or get the group to work better. I don't mind giving public talks at all. I just decided that the energy I got from being nervous was fun. I still remember walking out on stage at Intel at some conference, like 2000 people. I was like I should have been really nervous, but instead I was just really excited about it. So some of that kind of stuff changed, but that's partly conscious, and partly just practice. I still get excited around like computer design and stuff. I had a friend of mine’s wife ask what they put in the water, because all we ever do is talk about computers. It's really fun, you know. Changing the world. It's great.IC: It sounds like you have spent a lot more time, in a way, studying the human experience. If you understand how people think, how people operate, that’s different compared to mouthing at Gordon Bell for an hour.JK:It's funny. People occasionally ask me like, or I tell people, that I read books. You learn a lot from books. Books are fun by the way - if you know how a book works. Somebody who lives 20 years, then passionately writes their best ideas (and there are lots of those books), and then you go on Amazon and find the best ones. It's hilarious, right? Like a really condensed experience in a book, written, and you can select the better books, like who knew, right? But I've been reading a lot of books for a long time.It's hard to say, ‘read these four books, it'll change your life’. Sometimes a [single] book will change your life. But reading 1000 books will [certainly] change your life that's for damn sure. There's so much human experience that's useful. Who knew Shakespeare would be really useful for engineering management, right? But like, what are all those stories - power politics, devious guys, the minions doing all the work and the occasional hero saving the day? How does that all play out? You're always placed 500 years ago, but it applies to corporate America every single day of the week. So if you don't know Shakespeare or Machiavelli, you don’t know nothing.IC: I think I remember you saying that before you went into your big first management role, you read 20 books about management techniques, and how you ended up realizing that you'd read 19 more than anybody else.JK:Yeah, pretty much. I actually contacted Venkat (Venkatesh) Rao, who's famous for the Ribbonfarm blog and a few other things to figure [stuff] out. I really liked his thinking about organization from his blog, and he had a little thing at the bottom where it says to click here to buy him a cup of coffee, or get a consulting or a consult, so I sent him an email. So we started yakking, and we spent a lot of time talking before I joined AMD. He said I should read these books and I did. I thought everybody who’s in a big management job did that, but nobody does. You know it was hilarious - like 19 is generous. I read 20 more management books than most managers have ever read. Or they read some superficial thing like Good to Great, which has some nice stories in it, but it's not that deep a book management-wise. You'd be better off readingCarl Jungthan Good to Great if you want to understand management.IC: Do you find yourself reading more fiction or nonfiction?JK:As a kid, I read all the nonfiction books. Then my parents had a book club. I didn't really learn to read until I was in fourth grade, but somewhere around seventh or eighth grade, I had read all the books in the house. They hadJohn Updike, andJohn Barthwas one of my favorite authors when I was a kid. So there were a whole bunch of stories. ThenDoris Lessing. Doris Lessing wrote a series of science fiction books that were also psychological inquiries, and I read that, and I just, I couldn't believe it. Every once a while stuff like that kind of blows your mind. And it happened, obviously, at the right time. But now I read all kinds of stuff. I like history and anthropology and psychology, and mysticism, and there are so many different things. I’ve probably read fewer fiction books in the last 10 years. But when I was younger, I read probably mostly fiction.IC: I did get a few particular comments from the audience in advance of this interview about comments you made when you were being interviewed by Lex Fridman. You said that you read two books a week. You’re also very adept at quoting from key engineers and futurists. I'm sure if you started tweeting what book you’re reading when you start a new one, you'll get a very large following. A sort of a passive Jim Keller book club!JK:I would say I read two books a week. Now, I read a lot, but it tends to be blogs and all kinds of crazy stuff. I don't know - like doing Lex [Lex’s Podcast] is super fun, but I don't know that I have the attention span for social media to do anything like that. I'd forget about it for weeks at a time.IC: How do you make sure that you're absorbing what you're reading, rather than having your brain diverting about some other problem that you might be worrying about?JK:I don't really care about that. I know people that read books, and they are really worried if they're going to remember them. They spend all this time highlighting and analyzing. I read for interest, right? What I really remember is that people have to write 250-page books, because that's like a publisher rule. It doesn't matter if you have 50 pages of ideas, or 500, but you can tell pretty fast. I've read some really good books that are only 50 pages, because that's all they had. You can also read 50 pages, and you think, ‘wow, it's really great!’, but then the next 50 pages is the same shit. Then you realize it’s just been fleshed out – at that point I wish they just published a shorter book.But that is what it is. But if the ideas are interesting, that's good. I meditate regularly, and then I think about what I'm thinking about, which is sometimes related to what I'm reading. Then if it's interesting, it gets incorporated. But your brain is this kind of weird thing - you don't actually have access to all the ideas and thoughts and things you've read, but your personality seems to be well informed by it, and I trust that process. So I don't worry if I can't remember somebody's name [in a book], because their idea may have changed, and who I was and I don't remember what book it came from. I don't care about that stuff.IC: As long as you have passively absorb it at some level?JK:Yeah. Well, there's a combination of passive and active. I told Lex that a lot of times when I'm working on problems, I prep my dreams for it. It's really useful. That's a fairly straightforward thing to do. Before you fall asleep, you call up your mind, on what you're really working on and thinking about. Then my personal experiences sometimes, I really do work on that, and sometimes that's just a problem in the way of what I actually need to think about, and I'll dream about something else. I'll wake up well, and one way or the other it was really interesting.Nature vs NurtureIC: So on the topic of time, here we are discussing personal health, study, meditation, and family, but also how you execute professionally. Are you one of these people who only needs four hours of sleep a night?JK:Nah, I need like seven. Well, I added it up one day that my ideal day would have like 34 hours in it. Because I like to work out, spend time with my kids, I like to sleep and eat, and you know I like to work. I like to read too, so I don't know. Work is the weird one, because that can fill in lots more time than you want to spend on it. But I also really like working, so it's a challenge to kind of stamp it down.IC: When there's a deadline, what gets pushed out the way first? You've worked at companies where getting the product out, and time to market, has been a key element of what you're doing.JK:For about the last six years, the key thing for me is that once I have too much to do, I find somebody that wants to do it more than me. I mostly work on unsolved problems. You know I was the laziest person at Tesla. Tesla had a culture of working 12 hours a day to make it look like you're working, and I worked, you know, 9 to 7, which was a lot of hours. But I also went running at lunch, and a workout. They had a weightlifting room.Deer Creekwas right next to the big machine shop, so I would go down there for an hour to work out and to eat.At AMD and Intel, they're big, big organizations, and I had a really good staff. So I'd find myself spending way too much time on presentations, or working on some particular thing. Then I'd find some people who wanted to work on it, so I’d give it to them and, you know, go on vacation.IC: Or speaking to press people like me, and taking up your time! What is your feeling about doing these sorts of press interviews, and you know, more the sort of marketing and corporate and discussion? These aren't really necessarily related to actually pushing the envelope, it's just talk.JK:It’s not just talk. I’ve worked on some really interesting stuff, so I like to talk about it. When I was in Intel, I realized it was one of the ways to influence the Intel engineers. Like everybody thought Moore's Law was dead, and I thought ‘holy crap, it's the Moore's Law company!’. It was really a drag if [as an engineer] your main thing was that [Moore’s Law is dead], because I thought it wasn't. So I talked to various people, then they amplified what I said and debated it, and it went back inside. You know, I actually reached more people inside of Intel by doing external talks. So that was useful to me, because I had a mission to build faster computers. That's what I like to do. So when I talked to people, they always bring all kinds of stuff up, like how the work we do impacts people. Guys like you, and think really hard about it, and you talk to each other. Then I talk to you, and you ask all these questions, and it's kind of stimulating. It's fun. If you can explain something really clearly, you probably know it. There are a lot of times you think you know it, and then you go to explain it, but you're stumbling all around. I did some public talks where they were hard to do, like the talk actually seems simple, but to get to the simple part you have to get your ideas out and reorganize them and then throw out the BS. It's a useful thing to talk.IC: Is it Feynman or Sagan that said ‘if you can’t explain the concept to at first-year college level, then you don’t really understand it’?JK:Yeah, that sounds probably like Feynman. He did that really well, like with his lecture series on physics. It was quite interesting. Feynman’s problem was that he had such a brilliant intuition for the math, that his idea of simple was often not that simple! Like he just saw it, and you could tell. Like he could calculate some orbital geometry in five ‘simple’ steps, and he was so excited about how simple it was. But I think he was the only person in the room that thought it was simple.IC: I presume he had the ability to visualize things in his head and manipulate them. I remember you saying at one point, that when it comes down to circuit-level design, that's the sort of thing you can do.JK:Yeah. If I had one superpower, I feel like I can visualize how a computer actually runs. So when I do performance modeling and stuff like that, I can see the whole thing in my head and I'm just writing the code down. It is a really useful skill, but you know I probably partly was born with it. Partly developed and partly something that came out of my late adult diagnosis of dyslexia.IC: I was going to ask how much of that is nature versus nurture?JK:It's hard. There's this funny thing that with super-smart people, often things are so easy for them, that they can go a really long way without having to work hard. So I'm not that smart. So persistence, and what they call grit, is super useful, especially in computer design. When lots of stuff takes a lot of tweaking, you have to believe you can get there. But a lot of times, there's a whole bunch of subtle iterations to do, and practice with that actually really works. So yeah, everybody's a combination. But if you don't have any talent, it's pretty hard to get anywhere, but sometimes really talented people don't learn how to work, so they get stuck with just doing the things that are obvious, not the things that take that persistence through the mess.IC: Also identifying that talent is critical as well, especially if you don’t know you have it?JK:Yeah, but on the flip side, you may have enough talent, but you just haven't worked hard, and some people give up too soon. You’ve got to do something, something you're really interested in. When people are struggling, like if they want to be an engineer or in marketing or this or that, [ask yourself] what do you like? This is especially true for people who want to be engineers, but their parents or somebody wants me to be a manager. You're going to have a tough life, because you're not chasing your dream, you're chasing somebody else's. The odds that you will be excited about somebody else’s dream are low. So if you're not excited, you're not going to put the energy in. or learn. That's a tough loop in the end.Pushing Everyone To Be The BestIC: To what extent do you spend your time mentoring others, either inside organizations, or externally with previous coworkers or students? Do you ever envision yourself doing something on a more serious basis, like the ‘Jim Keller School of Semiconductor Design’?JK:Nah. So it's funny because I'm mostly mission driven. Like, ‘we're going to build Zen!’, or ‘we're going to build Autopilot!’, and then there are people that work for me. Then as soon as they start working for me, I start figuring out who they are, and then some of them are fine, and some of them have big problems that need to be, let's say, dealt with one way or the other. So then I'll tell them what I want, sometimes I'll give them some pointed advice. Sometimes I'll do stuff, and you can tell some people are really good at learning by following. Then people later on are telling me that I was mentoring them, but I'm thinking that I thought I was kicking your ass? It's a funny experience.There are quite a few people that said I impacted their life in some way, but for some of those, I went after them about their health or diet, because I thought they looked not energized by life. You can make really big improvements there. It's worth doing by the way. It was either that, or they were doing the wrong thing, and they were just not excited about it. [At that point] you can tell they should be doing something else. So they either have to figure out why they are not excited or get excited, and then a lot of people start fussing with themselves or with other people about their status or something. The best way to have status is to do something great, and then everybody thinks you're great. Having status by trying to claw your way up is terrible, because everybody thinks you're a climber, and sometimes they don’t have the competence or skill to make the right choice there. It mostly comes out of being mission driven.I do care about people, at least I try to, and then I see the results. I mean, it's really gratifying to get a big complicated project done. You know where it was when you started, and then you know where it was when it was done, and then people when they work on successful things associate the leadership and the team they're working with as being part of that. So that's really great, but it doesn't always happen. I have a hard time doing quote ‘mentoring people’, because what's the mission? Like, somebody comes to you and says ‘I want to get better’. Well, better at what? Then if that's like wanting to be better at you playing violin, well I'm not good at that.Whereas when I say ‘hey, we're going to build the world's fastest autopilot chip’, then everybody working on it needs to get better at doing that. It turns out three-quarters of their problems are actually personal, not technical. So to get the autopilot chip, you have to go debug all that stuff, and there are all kinds of personal problems - health problems, parental childhood problems, partner problems, workplace problems, and career stall problems. The list is so bloody long, and we take them all seriously. As it turns out, everybody thinks their own problems are really important, right? You may not think their problems are important, but I tell you, they do, and they have a list. Ask anybody – what are your top five problems. They can probably tell you. Or even weirder, they give you the wrong five, because that happens too.IC: But did they give you the five they think you want to hear rather than the actual five?JK:Yeah. People also have no-fly zones, so their biggest problem may be something they don’t want to talk about. But if you help them solve that, then the project will go better, and then at some point, they'll appreciate you. Then they'll say you're a mentor, and you're thinking, kinda, I don’t know.IC: So you mentioned about your project succeeding, and you know, people being proud of their products. Do you have a 'proudest moment' of your career, project, or accolade? Any specific moments in time?JK:I have, and there's a whole bunch of them. I worked withBecky Loopat Intel, and we were debugging some quality things. It turns out there was a whole bunch of layers of stuff. We were going back and forth on how to analyze it, how to present it, and I was frustrated with the data and what was going on. One day she came up with this picture, and it was just perfect. I was really excited for her because she'd gotten to the bottom of it. We actually saw a line of sight to fix and stuff. But that kind of stuff happens a lot.IC: An epiphany?JK:Yeah. Well sometimes working with a group of people, going into it is like a mess, but then it gets better. The Tesla Autopilot thing was wild, and Zen’s success has been fantastic. Everybody thought that the AMD team couldn't shoot straight, and I was very intrigued with the possibility of building a really great computer with the team that everybody thought was out of it. Like nobody thought AMD had a great CPU design team. But you know, the people who built Zen, they had 25 to 30 years work history at AMD. That was insane.IC: I mean Mike Clark and Leslie Barnes, they’ve been there for 25 to 30 years.JK:Steve Hale, Suzanne Plummer.IC: The Lifers?JK:Yeah, they're kind of lifers, but they had done many great projects there. They all had good track records. But what did we do different? We set some really clear goals, and then we reorganized to hit the goals. We did some really thorough talent analysis of where we were, and there were a couple people that had really checked out because they were frustrated that they could never do the right thing. You know I listened to them - whoa Jesus, I love to listen to people.We had this really fun meeting, and it was one of the best experiences of my life. Suzanne called me up and said that people on the Zen team don't believe they can do it. I said, ‘great - I'll drive to the airport, I’m in California, and I'll see you there tomorrow morning, eight o'clock. Make sure you have a big room with lots of whiteboards’. It was like 30 angry people ready to tell me all the reasons why it wouldn't work. So I just wrote all of the reasons down on a whiteboard, and we spent two days solving them. It was wild because it started with me defending against the gang, but people started to jump in. I was like, whenever possible, when somebody would say ‘I know how we fix that’, I would give them the pen and they would get up on the board and explain it. It worked out really good. The thing was, the honesty of what they did, was great. Here are all the problems that we don't know how to solve, and so we're putting them on the table. They didn't give you 2 reasons but hold back 10 and say ‘you solve those two’. There was none of that kind of bullshit kind of stuff. They were serious people that had real problems, and they'd been through projects where people said they could solve these problems, and they couldn't. So they were probably calling me out, but like I’m just not a bullshitter. I’m not a bullshitter, but I told them how some we can do, some I don't know. But I remember, Mike Clark was there and he said we could solve all these problems. You know I walked out when our thing is pretty good, and people walked out of the room feeling okay, but two days later problems all pop back up. So you know, like how often do you have to go convince somebody? But that’s why they got through it. It wasn’t just me hectoring them from the sidelines, there were lots of people and lots of parts of the team that really said, they’re willing to really put some energy into this, which is great.IC: At some point I’d love to interview some of them, but AMD keeps them under lock and key from the likes of us.JK:That’s probably smart!IC: Is there somebody in your career that you consider like a silent hero, that hasn’t got enough credit for the work that they’ve done?JK:A person?IC: Yeah.JK:Most engineers. There are so many of them, it’s unbelievable. You know engineers, they don’t really get it. Compared to lawyers that are making 800 bucks an hour in Silicon Valley, engineers so often want to be left alone and do their work and crank out stuff. There are so many of those people that are just bloody great. I've talked to people who say stuff like ‘this is my eighth-generation memory controller’, and they're just proud as hell because it works and there are no bugs in it, and the RTL is clean, and the commits are perfect. Engineers like that are all over the place, I really like that scenario.IC: But they don’t self-promote, or the company doesn’t?JK:Engineers are more introverted, and conscientious. The introverted tend not to be the people who self-promote.IC: But aren’t you a little like me, you’ve learned how to be more extroverted as you’ve grown?JK:Well, I decided I wanted to build bigger projects, and to do that, you have to pretend to be an extrovert, and you have to promote yourself, because there's a whole bunch of people who are decision-makers who don't do the work to find out who the best architect is. They're going to pick who the person that everybody says is the best architect, or the loudest, or the capable. So at some level, if you want to succeed above ‘principal engineer’, you have to understand how to work in the environment of people who play it. Some people are super good at that naturally, so they get pretty high in organizations without much talent, sometimes without much hard work. Then the group of people, Director and above, that you have to deal with have a way different skill set than most of the engineers. So if you want to be part of that gang, even if you're an engineer, you have to learn how that rolls. It's not that complicated. Read Shakespeare, Young, a couple of books, Machiavelli, you know, you can learn a lot from that.Security, Ethics, and Group BeliefIC: One of the future aspects of computing is security, and we've had a wake of side-channel vulnerabilities. This is a potential can of worms, attacking the tricks that we use to make fast computers. To what extent do you approach those security aspects when you're designing silicon these days? Are you proactive? Do you find yourself specifically proactive or reactive?JK:So the market is sort of dictating needs. The funny thing about security first of all is you know it only has to be secure if somebody cares about it. For years, security in an operating system was virtual memory – for a particular process, its virtual memory couldn't look into another process's virtual memory. But the code underneath it in the operating system was so complicated that you could trick the operating system into doing something. So basically you started from security by correct software, but once you couldn't prove the software correct, they started putting additional hardware barriers in there. Now we're building computers where the operating system can't see the data the user has, and vice versa. So we're trying to put these extra boundaries in, but every time you do, you've made it a little more complicated.At some level security worldwide is mostly for security by obscurity, right? Nobody cares about you, in particular, because you're just one out of 7 billion people. Like somebody could crack your iPhone, but they mostly don't care about it. There's a funny arms race going on about this, but it's definitely kind of incremental. They discovered side-channel attacks, and they weren't that hard to fix. But there'll be some other things, and, you know, I'm not a security expert. The overhead of building security features is mostly low. The hard part is thinking that out and deciding what to do. Every once in a while somebody will say something like ‘this is secure, because the software does x’, and I always think, ‘yeah, just wait 10 minutes, and the software will get more complicated, which will introduce a gap in it’. So there needs to be real hardware boundaries in there.There are lots of computers that are secure, because they don't talk to anything. Like there are boatloads of places where the computers are usually behind a hard firewall, or literally disconnected from anything. So only physical attacks work, and then they have physical guards. So now, it's going to be interesting, but it's not super high in my thinking, I mostly follow what's going on, and then we'll just do the right thing. But I have no faith in security by software, let's say, because that always kind of grows to the point where it kind of violates its own premises. It's happened many times.IC: So you've worked at Tesla, and when you designed a product specifically for Tesla. You have also worked at companies that sell products for a wide array of uses. Beyond that sort of customer workload analysis, do you consider the myriad of possibilities of what the product you are building will be used for? Do you consider the ethics behind what it might be used for? Or are you just there to solve the problem of building the chip?JK:The funny thing about general-purpose computing is it can really be used for anything. So the ethics is more if the net good is better than the net bad. For the most part I think the net good is better than the possible downsides. But people do have serious concerns about this. There's all a big movement around ethics in AI, and to be honest, the AI capabilities have so far outstripped the thinking around the aspects of that. I don't know what to think about it.What the current systems can do is already has stripped us bare, it knows what we think, and what we want, and what we're doing. Then the question is how many people have that one reason to build a lower-cost AI and programmable AI. We're talking to quite a large number of AI software startups, that want AI hardware and computing in more people's hands, because then you have a little mutual standoff situation, as opposed to one winner take all. But the modern tech world has been sort of a winner take all. There are literally several dozen very large companies that have a competitive relationship with each other. So, that's kind of complicated. I think about it some, but I don't have anything you know, really good to say, besides, you know the net benefit so far has been a positive. Having technology in more people's hands rather than a concentrated few seems better, but we'll see how it plays out.IC: You've worked for a number of big personalities. You know, Elon Musk, Steve Jobs to name two. It appears you still have a strong contact with Elon. Your presence at theNeuralink demolast year with Lex, was not unnoticed. What’s your relationship with Elon now, and was he the one to invite you?JK:I was invited by somebody in the Neuralink team. I mean Elon, I would say I don’t have a lot of contact with him at the moment. I like the development team there, so I went over to talk to those guys. It was fun.IC: So you don’t stay in touch with Elon?JK:No, I haven’t talked to him recently, no.IC: It was very much a professional, not a personal relationship when you worked for Tesla then?JK:Yeah.IC: Because I was going ask about the fact that Elon is a big believer in Cryptocurrency. He regularly discusses it as it pertains to demands of computing and resources, for something that has no intrinsic value. Do you have any opinions as it comes to Cryptocurrency?JK:Not much. Not really. I mean humans are really weird where they can put value in something like gold, or money, or cryptocurrency, and you know that's a shared belief contract. What it's based on, the best I can tell, hasn't mattered much. I mean the thing the crypto guys like is that it appears to be out of the hands of some central government. Whether that's true or not, I couldn't say. Jow that's going to impact stuff, I have no idea. But as a human, you know, group beliefs are really interesting, because when you're building things, if you don't have a group belief that makes sense then you're not going to get anything done. Group beliefs are super powerful, and they move currencies, politics, companies, technologies, philosophies, self-fulfillment. You name it. So that's a super interesting topic, but as for the details of Cryptocurrency, I don't care much about it, except as a manifestation of some kind of psychological phenomena about group beliefs, which is actually interesting. But it seems to be more of a symptom, or a random example let's say.Chips Made by AI, and Beyond SiliconIC: In terms of processor design, currently with EDA tools there is some amount of automation in there. Advances in AI and Machine Learning are being expanded into processor design - do you ever envision a time where an AI model can design a purposeful multi-million device or chip that will be unfathomable to human engineers? Would that occur in our lifetime, do you think?JK:Yeah, and it’s coming pretty fast. So already the complexity of a high-end AMD, Intel, or Apple chip, is almost unfathomable that any one person. But if you actually go down into details today, you can mostly read the RTL or look at the cell libraries and say, ‘I know what they do’, right? But if you go look inside a neural network that's been trained and say, why is this weight 0.015843? Nobody knows.IC: Isn’t that more data than design, though?JK:Well, somebody told me this. Scientists, traditionally, do a bunch of observations and they go, ‘hey, when I drop a rock, it accelerates like this’. They then calculate how fast it accelerated and then they curve fit, and they realize ‘holy crap, there's this equation’. Physicists for years have come up with all these equations, and then when they got to relativity, they had to bend space and quantum mechanics, and they had to introduce probability. But still there are mostly understandable equations.There's a phenomenon now that a machine learning thing can learn, and predict. Physics is some equation, put inputs, equation outputs, or function output, right? But if there's a black box there, where the AI networks as inputs, a black box of AI outputs, and you if you looked in the box, you can't tell what it means. There's no equation. So now you could say that the design of the neurons is obvious, you know - the little processors, little four teraflop computers, but the design of the weights is not obvious. That's where the thing is. Now, let’s go use an AI computer to go build an AI calculator, what if you go look inside the AI calculator? You can't tell why it's getting a value, and you don't understand the weight. You don't understand the math or the circuits underneath them. That's possible. So now you have two levels of things you don't understand. But what result do you desire? You might still be designed in the human experience.Computer designers used to design things with transistors, and now we design things with high-level languages. So those AI things will be building blocks in the future. But it's pretty weird that there's going to be parts of science where the function is not intelligible. There used to be physics by explanation, such as if I was Aristotle, 1500 years ago - he was wrong about a whole bunch of stuff. Then there was physics by equation, like Newton, Copernicus, and people like that. Stephen Wolfram says there’s now going to be physics by, by program. There are very few programs that you can write in one equation. Theorems are complicated, and he says, why isn’t physics like that? Well, protein folding in the computing world now we have programmed by AI, which has no intelligible equations, or statements, so why isn’t physics going to do the same thing?IC: It's going to be those abstraction layers, down to the transistor. Eventually, each of those layers will be replaced by AI, by some unintelligible black box.JK:The thing that assembles the transistors will make things that we don’t even understand as devices. It’s like people have been staring at the brain for how many years, they still can't tell you exactly why the brain does anything.IC: It’s 20 Watts of fat and salt.JK:Yeah and they see chemicals go back and forth, and electrical signals move around, and, you know, they're finding more stuff, but, it's fairly sophisticated.IC: I wanted to ask you about going beyond silicon. We've been working on silicon now for 50+ years, and the silicon paradigm has been continually optimized. Do you ever think about what’s going to happen beyond silicon, if we ever reach a theoretical limit within our lifetime? Or will anything get there, because it won’t have 50 years of catch-up optimization?JK:Oh yeah. Computers started, you know, with Abacuses, right? Then mechanical relays. Then vacuum tubes, transistors, and integrated circuits. Now the way we build transistors, it's like a 12th generation transistor. They're amazing, and there's more to do. The optical guys have been actually making some progress, because they can direct light through polysilicon, and do some really interesting switching things. But that's sort of been 10 years away for 20 years. But they actually seem to be making progress.It’s like the economics of biology. It’s 100 million times cheaper to make a complicated molecule than it is to make a transistor. The economics are amazing. Once you have something that can replicate proteins - I know a company that makes proteins for a living, and we did the math, and it was literally 100 million times less capital per molecule than we spent on transistors. So when you print transistors it’s something interesting because they're organized and connected in very sophisticated ways and in arrays. But our bodies are self-organizing - they get the proteins exactly where they need to be. So there's something amazing about that. There's so much room, as Feynman said, at the bottom, of how chemicals are made and organized, and how they’re convinced to go a certain way.I was talking to some guys who were looking at doing a quantum computing startup, and they were using lasers to quiet down atoms, and hold them in 3D grids. It was super cool. So I think we've barely scratched the surface on what's possible. Physics is so complicated and apparently arbitrary that who the hell knows what we're going to build out of it. So yeah, I think about it. It could be that we need an AI kind of computation in order to organize the atoms in ways that takes us to that next level. But the possibilities are so unbelievable, it's literally crazy. Yeah I think about that.Many thanks to Jim Keller and his team for their time.Many thanks also to Gavin Bonshor for assistance in transcription,\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16762/an-anandtech-interview-with-jim-keller-laziest-person-at-tesla\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Google Announces AMD Milan-based Cloud Instances - Out with SMT vCPUs?\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-06-17T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/16765/google-announces-amd-milanbased-cloud-instances-out-with-smt-vcpus\n",
      "Content: Today, Google announced the planned introduction of their new set of “Tau” VMs, or T2D, in their Google Compute Engine VM offerings. The hardware consists ofAMD’s new Milan processors– which is a welcome addition to Google’s offerings.The biggest news of today’s announcement however was not Milan, but the fact of what Google is doing in terms of vCPUs, how this impacts performance, and the consequences it has in the cloud provider space – particularly in context of the new Arm server CPU competition.Starting off with the most important data-point Google is presenting today, is that the new GCP Tau VMs showcase a staggering performance advantage over the competitor offerings from AWS and Azure. Thecomparison VM details are published here:Google’s SPECrate2017_int methodology largely mimics our own internal usage of the test suite in terms of flags (A few differences like LTO and allocator linkage), but the most important figure comes down from the disclosure of the compilers, with Google stating that the +56% performance advantage over AWS’s Graviton2 comes from an AOCC run. They further disclose that a GCC run achieving a +25% performance advantage, which clarifies some aspects:Note that we also tested with GCC using -O3, but we saw better performance with -Ofast on all machines tested. An interesting note is that while we saw a 56% estimated SPECrate®2017_int_base performance uplift on the t2d-standard-32 over the m6g.8xlarge when we used AMD's optimizing compiler, which could take advantage of the AMD architecture, we also saw a 25% performance uplift on the t2d-standard-32 over the m6g.8xlarge when using GCC 11.1 with the above flags for both machines.Having this 25% figure in mind, we can fall back to our own internally tested data of the Graviton2 as well as the more recently tested AMD Milan flagship for a rough positioning of where things stand:Google doesn’t disclose any details of what kind of SKU they are testing, however we do have 64-core and 32-core vCPU data on Graviton2, scoring estimated scores of 169.9 and 97.8 with per-thread scores of 2.65 and 2.16. Our internal numbers of an AMD EPYC 7763 (64 core 280W) CPU showcase an estimated score of 255 rate and 1.99 per thread with SMT, and 219 rate and 3.43 per thread for respectively 128 threads and 64 thread runs per socket. Scaling the scores down based on a thread count of 32 – based on what Google states here as vCPUs for the T2D instance, would get us to scores of either 63.8 with SMT, or 109.8 without SMT. The SMT run with 32 threads would be notably underperforming the Graviton2, however the non-SMT run would be +12 higher performance. We estimate that the actual scores in a 32-vCPU environment with less load on the rest of the SoC would be notably higher, and this would roughly match up with the company’s quoted +25 performance advantage.And here lies the big surprise of today’s announcement: for Google's new Milan performance figures to make sense, itmustmean that they are using instances with vCPU counts that actually match the physical core count – which has large implications on benchmarking and performance comparisons between instances of an equal vCPU count.Notably, because Google is focusing on the Graviton2 comparison at AWS, I see this as a direct attack and response to Amazon’s and Arm’s cloud performance metric claims in regards to VMs with a given number of vCPUs. Indeed, even when we reviewed the Graviton2 last year, we made note of this discrepancy that when comparing cloud VM offerings to x86 cloud offerings which have SMT, and where a vCPU essentially just means you’re getting a logical core instead of a physical core, in contrast to the newer Arm-based Graviton2 instances. In effect, we had been benchmarking Arm CPUs with double the core counts vs the x86 incumbents at the same instance sizes. Actually, this is still what Google is doing today when comparing a 32vCPU Milan Tau VM against a Azure 32vCPU Cascade Lake VM – it’s a 32 core vs 16 core comparison, just the latter has SMT enabled.Because Google is now essentially levelling the playing field against the Arm-based Graviton2 VM instances at equal vCPU count, by actually having the same number of physical cores available, it means that it has no issues to compete in terms of performance with the Arm competitor, and naturally it also outperforms other cloud provider options where a vCPU is still only a logical SMT CPU.Google is offering a 32vCPU T2D instance with 128GB of RAM at USD 1.35 per hour, compared to a comparable AWS instance of m6g.8xlarge with also 32vCPUs and 128GB of RAM at USD 1.23 per hour. While Google’s usage of AOCC to get to the higher performance figures compared to our GCC numbers play some role, and Milan’s performance is great, it’s really the fact that we seem to now be comparing physical cores to physical cores that really makes the new Tau VM instances special compared to the AWS and Azure offerings (physical to logical in the latter case).In general, I applaud Google for the initiative here, as being offered only part of a core as a vCPU until now was a complete rip-off. In a sense, we also have to thank the new Arm competition in finally moving the ecosystem into bringing about what appears to be the beginning of the end of such questionable vCPU practices and VM offerings. It also wouldn’t have been possible without AMD’s new large core count CPU offerings. It will be interesting to see how AWS and Azure will respond in the future, as I feel Google is up-ending the cloud market in terms of pricing and value.Related Reading:Amazon's Arm-based Graviton2 Against AMD and Intel: Comparing Cloud ComputeAMD 3rd Gen EPYC Milan Review: A Peak vs Per Core Performance BalanceIntel 3rd Gen Xeon Scalable (Ice Lake SP) Review: Generationally Big, Competitively Small\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16765/google-announces-amd-milanbased-cloud-instances-out-with-smt-vcpus\n",
      "Title: Sponsored Post: Keep Your App’s Memory Safe with Arm Memory Tagging Extension (MTE)\n",
      "Author: Sponsored Post\n",
      "Date Published: 2021-06-14T16:30:00Z\n",
      "URL: https://www.anandtech.com/show/16759/sponsored-post-keep-your-apps-memory-safe-with-arm-memory-tagging-extension-mte\n",
      "Content: Subtle memory bugs, including buffer overruns and pointer errors, create ticking time bombs inside your applications. Malicious actors can exploit these bugs to execute unauthorized code, take over systems to add them to malware botnets, or simply cause applications and systems to crash. The notorious Morris Worm of 1988 was one of the earliest examples of a malicious application exploiting a buffer overflow. Announcements of memory safety issues creating potential exploits arrive with alarming frequency, either from security researchers or found loose in the wild.The impact on users can be substantial. Rogue applications can take advantage of unsafe memory in order to gain access to sniff out sensitive data, such as user credentials and passwords, enabling access to higher levels of privilege in the system. This allows bad actors to gain access to confidential data or make the system part of a larger botnet. It’s not always outside forces that cause problems – sometimes unsafe memory results in unpredictable system crashes due to memory leaks and related issues, frustrating users. It’s estimated that two-thirds of all Android vulnerabilities happen due to unsafe memory practices.Arm Memory Tagging ExtensionSoftware-based solutions, including Address Sanitizer (Asan), help mitigate these memory issues by integrating memory corruption detection into modern compilers. However, Asan requires adding software instrumentation to application code, which can significantly slow down app runtime and increase memory usage, particularly problematic in mobile and embedded systems.What’s needed is a solution to detect and minimize memory bugs with minimal impact on performance and memory use. Properly implementing a hardware-based method for detecting potentially unsafe memory usage results in smaller memory usage and better performance, while improving system reliability and security.Arm introduced its memory tagging extension as a part of the Armv8.5 instruction set. MTE is now built into Armv9 compliant CPUs recently announced by Arm, such as the Cortex-X2, Cortex-A710, and Cortex-A510. Future CPUs based on Armv9 will also integrate MTE. These all include memory tagging as a basic part of the architecture.The idea behind memory tagging is pretty simple: add a small set of bits to chunks of memory to identify them as safe for application usage. Arm implements memory tagging as a two-phase system, known as the lock and the key:Address tagging. This adds four bits to the top of every pointer in the process. Address tagging only works with 64-bit applications since it uses top-byte-ignore, which is an Arm 64-bit feature. Address tags act as a virtual “key.”Memory tagging. Memory tags also consist of four bits, but are linked with every aligned 16-byte region in the application’s memory space. Arm refers to these 16-byte regions astag granules.These four bits aren’t used for application data and are stored separately. The memory tag is the “lock”.A virtual address tag (key) must match the memory tag (lock). Otherwise, an error occurs.Figure 1.Shows an example of lock and key access to memorySince the address tag must match the memory tag, the first thing you might notice is that 4-bits is only 16 variations. This makes MTE a stochastic process, which means that it is possible for a key to incorrectly match up to a different lock. The likelihood of this happening is less than 8%, according to Arm.Since address and memory tags are created and destroyed on the fly frequently, memory allocation units work to make sure that sequential memory tags always differ. MTE supports random tag generation as well. The combination of the memory allocator understanding that sequential tags must be different plus the random tag generation feature means the actual frequency of tag clashes is quite low. Furthermore, running MTE across a fleet of millions (or billions) of devices can provide robust error detection for system and application software.Underlying ArchitectureArmv8.5 and v9 implement a new memory type, which Arm dubs Normal Tagged Memory. The CPU can determine the safety of a memory access, by comparing an address tag to the corresponding memory tag. Developers can choose whether or not a tag mismatch results in a synchronous exception or reported asynchronously, which allows the application to continue. Figure 2 shows how MTE is implemented in ARM CPU designs.Figure 2.Arm Total Compute Solution (Armv9)Asynchronous mismatch details accumulate in a system register. This means the OS can isolate mismatches to specific execution threads and make decisions based on ongoing operations.Synchronous exceptions can directly identify the specific load or store instruction causing tag mismatches. Arm added a variety of new instructions to the instruction set to manipulate tags, handle pointer and stack tagging, and for low-level system use.Implementing Arm MTEMTE is handled in hardware; load and store instructions have been modified to verify that the address tag matches the memory tag, and hardware memory allocation ensures the randomization of address and memory tag creation. This has differing implications for OS developers and end-user application programmers.Arm enhanced its AMBA 5 coherent interconnect to support MTE. Tag check logic is typically built into the system-level cache, with tag checking and tag caching occurring ahead of the DRAM interface. Figure 3 shows an example block diagram.Figure 3: Example block diagram showing how MTE might be implemented in an SoC design. (Source: Arm)Operating systems must be modified in order to fully support MTE. Arm initially prototyped MTE by creating a version of the Linux kernel which implemented tags. Google has expressed its intent to add MTE to Android and is working with SoC developers to ensure compatibility.End-user application developers have it a bit easier assuming operating system support for MTE. Since MTE occurs behind the scenes in the OS and hardware, applications require no source code modifications. MTE tagging for heap memory requires no extra effort. However, tagging memory on existing runtimes using stack memory requires compiler support, so existing binaries need to be recompiled. This is straightforward since mobile app developers frequently push out updates anyway. Figure 4 shows the software development timeline when implementing MTE.Figure 4:Software development timeline with MTEEnsuring memory is protected may require aligning memory objects to the Tag Granule (16-byte alignment). This can increase stack and memory utilization, though the impact seems to be fairly minimal.Why Use Arm MTE?MTE offers several quality-of-life improvements for developers. MTE allows programmers to find memory-related bugs quickly, speeding up the application debugging and development process. Since memory bugs can be found and quashed sooner, issues such as memory leaks, memory race conditions, and other memory-related crashes become more infrequent. This in turn improves the end-user experience.Memory safety bugs account for about two-thirds of all common vulnerabilities and exposure (CVE) bugs, so MTE allows companies to ship applications faster with fewer bugs. End users may often be reluctant to upgrade to new hardware or operating system software, but MTE gives them tangible reasons to upgrade, including improved stability and overall security.Further InformationYou can find more detailed information on Arm’s memory tagging extensions in a variety of sources.Arm’s white paperdescribing its implementation of MTE.Google’s Konstantin Serebryany’s detailed paper on howMTE improves memory safety with C/C++.How Armimplemented MTE in the Linux kernel.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16759/sponsored-post-keep-your-apps-memory-safe-with-arm-memory-tagging-extension-mte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Xilinx Expands Versal AI to the Edge: Helping Solve the Silicon Shortage\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-06-09T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16750/xilinx-expands-versal-ai-to-the-edge-helping-solve-the-silicon-shortage\n",
      "Content: Today Xilinx is announcing an expansion to its Versal family, focused specifically on low power and edge devices. Xilinx Versal is the productization of a combination of many different processor technologies: programmable logic gates (FPGAs), Arm cores, fast memory, AI engines, programmable DSPs, hardened memory controllers, and IO – the benefits of all these technologies means that Versal can scale from the high end Premium (launched in 2020), and now down to edge-class devices, all built on TSMC’s 7nm processes. Xilinx’s new Versal AI Edge processors start at 6 W, all the way up to 75 W.Going for the ACAPA couple of years ago, Xilinx saw a change in its customer requirements – despite being an FPGA vendor, customers wanted something more akin to a regular processor, but with the flexibility with an FPGA. In 2018, the company introduced the concept of an ACAP, an Adaptive Computing Acceleration Platform that offered hardened compute, memory, and IO like a traditional processor, but also substantial programmable logic and acceleration engines from an FPGA. The first high-end ACAP processors, built on TSMC N7, were showcased in 2020 and featured large premium silicon, some with HBM, for high performance workloads.So rather than having a design that was 100% FPGA, by transferring some of that die area to hardened logic like processor cores or memory, Xilinx’s ACAP design allows for a full range of dedicated standardized IP blocks at lower power and smaller die area, while still retaining a good portion of the silicon for FPGA allowing customers to deploy custom logic solutions. This has been important in the advancement of AI, as algorithms are evolving, new frameworks are taking shape, or different compute networks require different balances of resources. Having an FPGA on die, coupled with standard hardened IP, allows a single product install to last for many years as algorithms rebalance and get updated.Xilinx Versal AI Edge: Next GenerationOn that final point about having an installed product for a decade and having to update the algorithms, in no area is that more true than with traditional ‘edge’ devices. At the ‘edge’, we’re talking sensors, cameras, industrial systems, commercial systems – equipment that has to last over its long install lifetime with whatever hardware it has in it. There are edge systems today built on pre-2000 hardware, to give you a scope of this market. As a result, there is always a push to make edge equipment more malleable as needs and use cases change. This is what Xilinx is targeting with its new Versal AI Edge portfolio – the ability to continually update ‘smart’ functionality in equipment such as cameras, robotics, automation, medical, and other markets.Xilinx’s traditional Versal device contains a number of scalar engines (Arm A72 cores for applications, Arm R5 core for real-time), intelligent engines (AI blocks, DSPs), adaptable engines (FPGA), and IO (PCIe, DDR, Ethernet, MIPI). For the biggest Versal products, these are large and powerful, facilitated by a programmable network on chip. For Versal’s AI Edge platform, there are two new features into the mix.First is the use of Accelerator SRAM placed very close to the scalar engines. Rather than traditional caches, this is a dedicated configurable scratchpad with dense SRAM that the engines can access at low latency rather than traversing across the memory bus. Traditional caches use predictive algorithms to pull data from main memory, but if the programmer knows the workload, they can ensure that data needed at the most latency critical points can already be placed close to the processor before the predictors know what to do. This 4 MB block has a deterministic latency, enabling the real-time R5 to get involved as well, and offers 12.8 GB/s of bandwidth to the R5. It also has 35 GB/s bandwidth to the AI engines for data that needs to get processed in that direction.The other update is in the AI Engines themselves. The original Xilinx Versal hardware enabled both types of machine learning: training and inference. These two workloads have different optimization points for compute and memory, and while it was important on the big chips to support both, these Edge processors will almost exclusively be used for inference. As a result, Xilinx has reconfigured the core, and is calling these new engines ‘AIE-ML’.The simplest AIE-ML configuration, on the 6W processor, has 8 AIE-ML engines, while the largest has 304. What makes them different to the usual engines is by having double the local data cache per engine, additional memory tiles for global SRAM access, and native support for inference specific data types, such as INT4 and BF16. Beyond this, the multipliers are also doubled, enabling double INT8 performance.The combination of these two features means that Xilinx is claiming 4x performance per watt against traditional GPU solutions (vs AGX Xavier), 10x the compute density (vs Zynq Ultrascale), and more adaptability as AI workloads change. Coupled to this will be additional validation with support for multiple security standards in many of the industrial verticals.Through our briefing with Xilinx, there was one particular comment that stood out to me in light of the current global demand for semiconductors. It all boils down to one slide, where Xilinx compared its own current automotive solutions for Level 3 driving to its new solution.In this situation, to enable Level 3 driving, the current solution uses three processors, totalling 1259 mm2 of silicon, and then beyond that memory for each processor and such. The new Versal AI Edge solution replaces all three Zynq FPGAs, reducing 3 processors down to 1, going down to 529 mm2 of silicon for the same power, but also with 4x the compute capabilities. Even if an automobile manufacturer doubled up for redundancy, the new solution is still less die area than the previous one.This is going to be a key feature of processor solutions as we go forward – how much silicon is needed to actually get a platform to work. Less silicon usually means less cost and less strain on the semiconductor supply chain, enabling more units to be processed in a fixed amount of time. The trade-off is that large silicon might not yield as well, or it might not be the optimal configuration of process nodes for power (and cost in that regard), however if the industry is eventually limited on silicon throughput and packaging, it is a consideration worth taking into account.However, as is usual in the land of FPGAs (or ACAPs), announcements happen earlier and progress moves a little slower. Xilinx’s announcement today corresponds only to the fact that documentation is available today, with sample silicon available in the first half of 2022. A full testing and evaluation kit is coming in the second half of 2022. Xilinx is suggesting that customers interested in the AI Edge platform can start prototyping today with the Versal AI ACAP VCK190 Eval Kit, and migrate.Full specifications of the AI Edge processors are in the slide below. The new accelerator SRAM is on the first four processors, while AIE-ML is on all 2000-series parts. Xilinx has indicated that all AI Edge processors will be built on TSMC's N7+ process.Related ReadingAMD in $35 Billion All-Stock Acquisition of XilinxXilinx Announces World Largest FPGA: Virtex Ultrascale+ VU19P with 9m CellsHot Chips 31 Live Blogs: Xilinx Versal AI EngineHot Chips 2018: Xilinx 7nm ACAP Live BlogXilinx Acquires DEEPhi Tech ML StartupXilinx Announces Project Everest: The 7nm FPGA SoC HybridNVIDIA Details DRIVE AGX Orin: A Herculean Arm Automotive SoC For 2022Marvell to Acquire Aquantia, Eying Automotive Networking MarketArm Announces Cortex-A65AE for Automotive: First SMT CPU Core\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16750/xilinx-expands-versal-ai-to-the-edge-helping-solve-the-silicon-shortage\n",
      "Title: The Apple WWDC 2021 Keynote Live Blog (Starts at 10am PT/17:00 UTC)\n",
      "Author: Ryan Smith\n",
      "Date Published: 2021-06-07T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/16745/the-apple-wwdc-2021-keynote-live-blog\n",
      "Content: As things slowly get back to normal, Apple's annual World Wide Developers Conference is taking virtual place this week in its traditional early-June slot. As always, Apple kicks off WWDC with their big keynote event, which though aimed first and foremost at developers, is also used as a venue to announce new products and ecosystem strategies. The keynote starts at 10am Pacific (17:00 UTC) today, and AnandTech will be offering live blog coverage of Apple's event.A rapid-fire, two-hour run through Apple's ecosystem, WWDC keynotes cover everything from macOS and iOS to individual Apple applications and more. On the hardware side of matters, last year we saw the official announcement of Apple's shift from x86 processors to Arm processors for their venerable Mac lineup of computers, and while it's unlikely Apple is going to have anything to top that for WWDC21, the company is not even half-way through its transition to Arm SoCs. So this year's WWDC gives Apple ample opportunity to reflect on the Arm transition thus far, as well as what's coming next for the company's more powerful Macs.12:57PM EDT- Welcome to AnandTech and to another year of Apple's World Wide Developers Conference12:58PM EDT- Joining me this morning is our resident Apple Silicon guru, Andrei Frumusanu12:59PM EDT- Apple's event is once again virtual this year, and based on prior Apple events I'm expecting it to move extremely quickly. With no need to take time to pause for applause and gaffs, Apple's speakers can move at a rapid rate01:00PM EDT- So I expect Apple will fill this presentation with as much as possible01:00PM EDT- And hopefully (for us), some hardware news!01:00PM EDT- And here we go01:01PM EDT- Starting with a typical Apple-style opening video01:02PM EDT- Apple's gone meta this year - a video about what the Apple WWDC video should be01:03PM EDT- Tim and Craig as rockstars? Sure, why not?01:04PM EDT- And now on (virtual) stage: Tim Cook01:04PM EDT- Last year's WWDC was the most watched ever, with 25mil viewers01:05PM EDT- This year's event offers more than 200 sessions, labs with Apple engineers, and more01:05PM EDT- All for free01:06PM EDT- (I wonder if Apple will ever go back to charging?)01:06PM EDT- With Craig on stage01:06PM EDT- iOS 1501:07PM EDT- First subject: Facetime01:07PM EDT- Which has seen particular importance in the past year given the pandemic and associated lockdowns01:07PM EDT- Apple is looking to make Facetime calls more natural and lifelike01:08PM EDT- Spatial audio is coming to Facetime to make them sound more like they're in the room01:08PM EDT- Voice isolation is being added as well01:08PM EDT- Using machine learning to isolate the user's voice (i.e. filter out background noise)01:09PM EDT- Also offers a \"wide spectrum\" mode to pick up everything as it is now01:09PM EDT- Portrait mode is being added to Facetime01:09PM EDT- Blur the background (ala a lot of today's video conferencing apps)01:10PM EDT- Facetime links: generate URLs to join Facetime calls01:10PM EDT- URLs work on Android?!01:10PM EDT- It sounds like Apple is adding a web version of Facetime?01:11PM EDT- SharePlay: add audio/video playback and screen sharing to Facetime01:11PM EDT- (Apple's half-way to turning Facetime into a video conferencing app, it would seem)01:12PM EDT- Picture-in-picture-in-picture01:13PM EDT- SharePlay will have an API to allow devs to integrate their apps into Facetime so that more than just Apple AV services can be shared01:13PM EDT- Screen sharing works across Apple devices01:14PM EDT- Now on to Messages01:14PM EDT- Now on stage: Mindy Borovsky01:14PM EDT- Demoing messages01:15PM EDT- \"Shared with you\": articles linked via Messages are collated into Apple News01:16PM EDT- There are shared with you sections in other apps as well, including Music and Photos, collating music and photos from Messages respectively01:16PM EDT- And that's Messages01:17PM EDT- Now on to the subject of focusing01:17PM EDT- Notifications are being revised01:18PM EDT- Introducing \"Notification Summary\"01:18PM EDT- Intelligent collation of notifications from apps to try to deliver the most relevant ones at the right time01:19PM EDT- Do Not Disturb mode will notify Messages users that someone is in DND - and offer a way to override that as necessary01:19PM EDT- This is part of a larger new feature/app that Apple is calling Focus01:20PM EDT- Focus rules are shared among Apple devices01:21PM EDT- Now on to Live Text01:21PM EDT- Auto text recognition and OCR in the Camera and Photos apps01:22PM EDT- Directly select text from photos/images01:22PM EDT- Works with photos \"across the system\" including photos from the web01:23PM EDT- Understands 7 languages, including Simplified Chinese01:23PM EDT- Even Spotlight can search photos by text01:24PM EDT- Photos Memories01:24PM EDT- Apple is adding Apple Music support to their existing Memories feature01:25PM EDT- So playing a specific song alongside a Memories-generated video01:25PM EDT- Matching the pace of the photos to the music beat01:26PM EDT- Sounds like this relies on metadata Apple has attached to the songs in the Apple Music collection01:27PM EDT- So AM has identified the style, beat, etc01:27PM EDT- Now to Apple Wallet01:28PM EDT- Wallet will add support for car keys using Ultra Wideband (UWB)01:28PM EDT- Support for corporate badges01:28PM EDT- And hotel keys; Hyatt will be launching a program this year01:29PM EDT- Identity cards are coming to Wallet01:29PM EDT- Can store state ID cards01:29PM EDT- Will require the participation of the states01:29PM EDT- Apple has also been working with the TSA to get the service approved & used01:29PM EDT- Now on to the Weather app01:30PM EDT- Apple is adding weather maps to the app01:30PM EDT- Now to Apple Maps01:31PM EDT- Recapping the updated base map and what countries it's been rolled out to01:31PM EDT- Maps now offers a globe view01:32PM EDT- Maps in iOS 15 will include details like elevation, landmarks, and more for major cities01:32PM EDT- Additional road details for driving as well, such as turn lanes, medians, and bus/taxi lanes01:33PM EDT- (The turn lanes will be especially useful, in my experience)01:33PM EDT- Improvements for transit as well01:33PM EDT- Follows yout transit route to let you know when to get off and where to go once you get off01:34PM EDT- Including an augmented reality view to help users figure out where to go on the streets01:35PM EDT- And that's iOS 1501:35PM EDT- With more in iOS 15 than Apple had time to get to in this keynote01:35PM EDT- Now on to Apple's AirPods01:35PM EDT- Conversation Boost01:36PM EDT- Use ML and filtering to help people better hear conversations01:36PM EDT- (Sounds like a quasi-hearing aid mode)01:36PM EDT- AirPods can be located via FindMy in iOS 1501:37PM EDT- And separating alerts to inform users if they're leaving their AirPods behind01:37PM EDT- Spatial Audio is coming to tvOS01:37PM EDT- And macOS on M1-powered Macs01:38PM EDT- Now on to iPadOS01:39PM EDT- iPadOS 15 will be making a big update to widgets01:40PM EDT- New widgets like FindMy01:40PM EDT- Adding a new larger format for widgets designed for iPad's larger display01:41PM EDT- App Library is coming to the iPad01:41PM EDT- Access to it is built into the Docks01:41PM EDT- (Craig casually tossing an iPad)01:42PM EDT- Showing off app split view on iPad01:43PM EDT- A new multi-tasking area Apple is calling the Shelf01:43PM EDT- Minimize windows to the Shelf for later use01:44PM EDT- A new set of keyboard shortcuts are being added to aid these features01:44PM EDT- Now on to iPadOS Notes01:44PM EDT- Shared notes and an activity view01:44PM EDT- Adding hash tag support to notes to categorize them01:45PM EDT- New Feature: Quick Note01:45PM EDT- Drag from the corner of the screen to create a note window01:45PM EDT- Quick Notes is aware of the active application01:46PM EDT- Access your saved notes in the Notes app01:46PM EDT- Quick Notes can be created on macOS as well. And accessed from iOS01:47PM EDT- The Translate app is coming to iPad01:47PM EDT- Integration with handwriting recognition01:47PM EDT- And auto translate with voice translation01:47PM EDT- It's available system-wide. macOS, iOS, and iPadOS01:48PM EDT- Now on to Swift Playgrounds01:48PM EDT- Apple is adding the ability to build Playgrounds apps on the iPad01:48PM EDT- Improved code completion01:48PM EDT- And a guide to creating your first app01:49PM EDT- Can now build iOS/iPadOS apps right on an iPad01:49PM EDT- Now on to privacy01:50PM EDT- (Apple's been big on privacy; if nothing else, it helps to better set them apart from Google)01:50PM EDT- (It has not made them very popular with Facebook)01:51PM EDT- New feature: Mail Privacy Protection01:51PM EDT- Hides IP address to keep mail senders using tracking pixels and the like from tracking readers01:51PM EDT- App Privacy Report01:52PM EDT- A section that tracks how apps handle the user's privacy01:52PM EDT- Now on to Siri01:52PM EDT- Siri is up to 600mil users, and Apple is adding some new voices01:53PM EDT- As for privacy, Apple is moving to on-device speech recognition for Siri01:53PM EDT- So no more reaching out to Apple's servers to handle voice recog01:53PM EDT- Going on-device also means that Siri voice recog is usable even without an internet connection, and can respond more quickly01:54PM EDT- (Moving to on-device is a big deal from a resources standpoint)01:54PM EDT- Now on to iCloud01:54PM EDT- Apple is adding a new Account Recovery feature01:55PM EDT- You can use selected friends/family to get them to vouch for you and get you access codes01:55PM EDT- Paid subscription for iCloud is now iCloud+01:55PM EDT- And iCloud+ is getting additional features01:56PM EDT- New service: Private Relay01:56PM EDT- Apple is doing their own take on a VPN service, going through two relays. Apparently Apple can't decrypt this traffic01:57PM EDT- Generate disposable email addresses01:57PM EDT- Pricing for iCloud+ will be the same as it is today01:57PM EDT- Now on to health01:59PM EDT- Apple's rolling a video right now02:01PM EDT- Three new features02:01PM EDT- 1) Mobility02:02PM EDT- Walking Steadiness; have an iPhone analyze a user's fall risk02:03PM EDT- Notify the user if they're at increased risk of falling02:03PM EDT- 2) Lab results02:04PM EDT- The Health app will include detailed descriptions to help users better understand their data, and what expected ranges should be02:04PM EDT- 3) Trends02:04PM EDT- Insights into long-term changes on certain monitored biological data02:05PM EDT- Apple is adding the ability to share health data from Apple devices with medical providers02:06PM EDT- Health sharing for families as well02:06PM EDT- Keep an eye on your kids and your parents02:07PM EDT- Apple doesn't have access to any of this shared information02:08PM EDT- Now on to watchOS02:09PM EDT- Breathe app has been improved for watchOS 802:09PM EDT- And a feature called \"Reflect\" to encourage mindfulness02:09PM EDT- Via the new Mindfulness app02:10PM EDT- WatchOS 8 is adding new workout types02:10PM EDT- Starting with tai chi02:10PM EDT- And pilates02:11PM EDT- Artist spotlight series for workouts02:12PM EDT- WatchOS 8 is adding a portraits watchface02:13PM EDT- Photos on watchOS has been redesigned with several new features such as mosaic layouts02:13PM EDT- And sharing photos via Messages and Mail right on an Apple Watch02:14PM EDT- And a \"scribble\" view for writing messages02:15PM EDT- Now back to Craig for home products02:16PM EDT- At the heart of all of this is HomeKit02:16PM EDT- Unlock doors with Wallet's keys feature and supported electronic locks02:17PM EDT- AppleTV will have a Shared With You mode as well02:17PM EDT- And movie suggestions based on profile compatibility with multiple selected users02:18PM EDT- HomePod Mini availability is being expanded to additional countries02:18PM EDT- And voice recognition support in all countries where the Mini is available02:18PM EDT- Siri is coming to third party devices02:19PM EDT- Siri requests from third-party devices are routed through the HomePod, rather than third-party servers02:19PM EDT- Support for Apple's Matter smart home comms standard is finally launching in iOS 1502:20PM EDT- Adding package detection to HomeKit video02:20PM EDT- And that's HomeKit02:21PM EDT- Now on to macOS02:22PM EDT- Say hello to macOS Monterey02:22PM EDT- Many of the new iOS/iPadOS capabilities are coming to macOS02:22PM EDT- So Focus, Facetime, Notes, and more02:23PM EDT- Apple is updating its Continuity feature02:23PM EDT- New feature called Universal Control02:23PM EDT- Use a single mouse and keyboard to move between your Mac and iPad02:24PM EDT- Using the iPad as a second screen for a Mac02:24PM EDT- Or control the iPad via the Mac02:24PM EDT- Drag & drop files between devices02:24PM EDT- Universal Control works with more than 2 devices02:25PM EDT- (Okay, that's actually pretty impressive)02:26PM EDT- New feature: AirPlay to Mac02:26PM EDT- Using Macs as an AirPlay receiver02:27PM EDT- iOS shortcuts are coming to the Mac02:27PM EDT- So Macs can be automated in the same fashion as iOS02:28PM EDT- \"Shortcuts is the future of automation on the Mac\"02:28PM EDT- Automator will continue to be supported02:28PM EDT- Now talking Safari02:29PM EDT- Apple has further minimized the browser02:29PM EDT- And added tab groups02:30PM EDT- The URL bar shifts location depending on the open tab02:31PM EDT- Tab groups can be shared02:32PM EDT- These Safari changes are also coming in various forms to iPadOS and iOS02:32PM EDT- iOS gets a new grid view of tabs02:33PM EDT- And a big emphasis on syncing Safari settings and preferences between Apple devices02:33PM EDT- Web Extension support are coming to iOS and iPadOS02:33PM EDT- So Web Extensions are becoming the de-facto standard for web browser extensions. Now all the major vendors support the standard02:34PM EDT- And that's macOS02:34PM EDT- Now on to Developer Technologies02:35PM EDT- Discussing new Apple APIs coming this year02:36PM EDT- New API: Object Capture02:36PM EDT- Use photogrammetry to create 3D models of objects02:36PM EDT- Take several photos of an object on an Apple device and then, in this example, import it into Cinema 4D as an object02:37PM EDT- Object Capture is built on top of Swift02:37PM EDT- And on that note, here's an update on Swift02:38PM EDT- Swift is adding concurrency support02:38PM EDT- Making it far easier to write apps that use multiple cores02:38PM EDT- Using async-await pattern02:39PM EDT- Along with actors02:39PM EDT- And now the App Store02:40PM EDT- The App Store is up to 600mil viewers per week02:40PM EDT- Adding app product pages02:40PM EDT- Devs can create multiple different product pages for their apps02:41PM EDT- New feature: In-App Events02:41PM EDT- Improving the visibility of time-limited events going on within apps02:42PM EDT- And Apple will be curating the best and biggest events02:42PM EDT- (The line between apps and services is getting very blurred here)02:42PM EDT- Introducing Xcode Cloud02:43PM EDT- An Apple cloud service for app development02:43PM EDT- Team app development, cloud app building, cloud testing02:43PM EDT- And distribution to human testers via TestFlight02:44PM EDT- Xcode Cloud only stores the products of a build02:44PM EDT- Meanwhile TestFlight is coming to the Mac02:44PM EDT- Xcode Cloud available to all developers next year02:45PM EDT- Now back to Tim to recap things02:45PM EDT- Dev betas for OSes available today02:45PM EDT- Public betas in July02:45PM EDT- With final releases in the fall02:46PM EDT- And that's a wrap on the keynote02:46PM EDT- Apple will have the more dev-focused Platform State of the Union this afternoon02:47PM EDT- Meanwhile it looks like we've struck out on hardware this year02:47PM EDT- Thank you for joining us\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16745/the-apple-wwdc-2021-keynote-live-blog\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD confirms Ray-Tracing and VRS in Samsung Exynos RDNA GPU IP\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-06-01T10:00:00Z\n",
      "URL: https://www.anandtech.com/show/16728/amd-samsung-exynos-rt-vrs\n",
      "Content: At this year’s AMD Computex 2021 keynote event, CEO Lisa Su,among a series of various new product announcements and technology disclosures, has teased some new details on the company’s cooperation with Samsung in regards to the new RDNA GPU IP that’s been licensed out and the two companies have been working on to deliver in the next-generation Exynos SoCs.The original licensing and cooperation agreement had beenfirst announced in June of 2019, and seemingly presented itself as a quite unique IP licencing deal between AMD and Samsung LSI, as it seemingly went beyond the usual IP block licensing deal. While we still don’t have too many official details on the exact agreement structure, the deal is said to be more of a collaborative effort between the two parties in terms of developing the IP and adapting and optimising it for low-power mobile SoC usages. This comes at contrast with the usual IP block licensing habit in the industry where the IP provider has full, and sole control over the design.Earlier this year, Samsung LSI’s VP and GM Dr. Inyup Kanghad announcedthat the division is planning on integrating the new AMD RDNA-based GPU in the next-generation Exynos flagship SoC.At the event today, Dr. Lisa Su further commented on a few new details about the GPU IP:The next place you’ll find RDNA 2 will be the high-performance mobile phone market. AMD has partnered with industry leader Samsung to accelerate graphics innovation in the mobile market, and we’re happy to announce we will bring custom graphics IP to Samsung’s next flagship SoC, with ray-tracing and variable rate shading capabilities. We’re really looking forward to Samsung providing more details later this year.The new details being divulged here is the fact that Samsung’s implementation of the RDNA GPU will continue to carry over ray-tracing and variable-rate shading capabilities from its larger AMD PC and console GPU architecture siblings. These are two important features coming into the mobile market; while the introduction of VRS wouldn’t be the first on the market – notably being implemented in this year’sAdreno 660 GPU in the Qualcomm Snapdragon 888, it would be the second such confirmed implemented mobile GPU, and the only one from a GPU IP vendor, which AMD now seemingly is.The addition of ray-tracing capabilities would also be a first in the mobile market. While it’s not clear how this would translate over in actual game titles on smartphones and tablets, having RT capabilities is certainly an important feature set that Samsung LSI will undoubtedly take advantage of in their marketing of the next-gen SoCs. Again, this would be the first and only RT-capable GPU IP that we have confirmation on being implemented in the industry – at least in the mobile market.Imagination’s next-gen C-Series GPU IPis also advertised to have RT, however the company in recent years has had no mobile IP wins, and AMD and Samsung will seemingly beat them to market. Meanwhile, Arm has also teased thatthey are working on VRS and RT GPU IP, howeverthe recently announced Mali-G710 does not contain these features, and as such we won’t be expecting any compatible mobile Mali GPU implementation till 2023 at minimum.The one thing that was odd about today’s announcement was the fact that AMD and Dr. Lisa Su was partly referring to the GPU IP as RDNA 2. I think this was a confounding of IP generations for the keynote and general public’s sake – RDNA 2 as a graphics IP is a 2020 matter and Samsung’s next-gen Exynos SoC coming out at the end of 2021 for 2022 flagship devices would very much indicate that this is an RDNA 3 IP generation. AMD not having officially announced or detailed RDNA 3 as of yet, and Dr. Lisa Su also more specifically referring to the use of “custom graphics IP”, also makes me feel that the mention of RDNA 2 in the marketing materials to be more of a matter of colloquialism. AMD would want to promote Samsung’s IP usage, but also not put the cart before the horse in terms of RDNA 3 announcements.In general, today’s announcements lines up with our expectations of the AMD x Samsung collaboration. We’ll be seeing the new Exynos 2100 successor being revealed sometime towards the end of the year, and largely expect it to be employed in the Galaxy S22 generation of devices in early 2022.Related Reading:Samsung Confirms AMD RDNA GPU In Next Exynos FlagshipAMD and Samsung's GPU Licensing Deal: A New Era of Collaboration?AMD To License Out Radeon GPU IP to Samsung For Use In SLSI Mobile GPUs\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16728/amd-samsung-exynos-rt-vrs\n",
      "Title: Computex 2021: AMD's Keynote, a Live Blog (10pm ET)\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-06-01T01:22:00Z\n",
      "URL: https://www.anandtech.com/show/16722/computex-2021-amds-keynote-a-live-blog-10pm-et\n",
      "Content: 09:47PM EDT- When the big trade shows roll around, this is the time for the big companies in our sphere to announce their next biggest hardware, or update us on what is to some. AMD had some really big launches at the top of the year, with Ryzen 5000 for desktop and mobile, Radeon for desktop, and then a bit later we saw EPYC on Zen 3 come to market. This year at Computex, CEO Dr. Lisa Su heads up AMD's keynote presentation, and we're here ready to live blog the announcements as they come in. Join us at 10pm ET (10am Taiwan local time)!09:55PM EDT- Four minutes to go09:58PM EDT- Here's the AMD Youtube stream, for anyone who wants to watch along with the live blog:https://www.youtube.com/watch?v=gqAYMx34euU10:00PM EDT- Two minute countdown10:02PM EDT- Intro video10:03PM EDT- Share new products10:03PM EDT- Starting with the pandemic10:03PM EDT- Positive impact with vaccination10:04PM EDT- Crowd in attendance, AMD employees10:04PM EDT- High performance computing10:04PM EDT- We are in a megacycle10:04PM EDT- transformation of devices, ai10:04PM EDT- Push the envelope in High Perf Computing10:05PM EDT- Preview of advanced technology at the end10:05PM EDT- March was 3rd Gen AMD EPYC Milan, based on Zen 310:05PM EDT- Deep partnerships across the server ecosystem10:05PM EDT- 100+ Epyc server platforms, 400+ Epyc instances by end of year10:05PM EDT- AMD-powered cloud services10:06PM EDT- Microsoft 365, teams, twitter, zoom, tencent meeting10:06PM EDT- Momentum with 3rd Gen EPYC10:06PM EDT- 220+ world records on Milan10:06PM EDT- WR in Int, FP, Java, DB, Analytics10:07PM EDT- Doubled solutions compared to previous gen10:07PM EDT- HCI, converged infrastructure, HPC, analytics10:07PM EDT- Since launch, compare against Ice Lake10:07PM EDT- e-commerce perf10:07PM EDT- This is java10:07PM EDT- With SLA10:07PM EDT- 2P 7763 vs 2P 838010:08PM EDT- POS requests, online purchases, analytics10:08PM EDT- 200k vs 300k perf10:08PM EDT- No mention of Arm / Ampere Altra10:09PM EDT- 50% better TCO on this workload10:10PM EDT- Now the new architectures10:11PM EDT- New AMD APUs into the market10:11PM EDT- 5700G and 5600G10:11PM EDT- 8-core and 6-core10:11PM EDT- 65W10:12PM EDT- Performs above the competition10:12PM EDT- Both models available August 5th10:12PM EDT- $359 and $259, worldwide launch10:12PM EDT- Now RDNA210:13PM EDT- First cards in Q4 with 6900XT, 6800XT, and 680010:13PM EDT- 6700 was early this year10:13PM EDT- RDNA2 in the consoles10:13PM EDT- Just getting started with RDNA210:13PM EDT- RDNA2 coming to Tesla!10:14PM EDT- Embedded ryzen APU in the infotainment10:14PM EDT- discrete RDNA2 that kicks in 10 Teraflops when AAA is needed10:14PM EDT- Tesla is the next platform for triple A gaming10:14PM EDT- High perf mobile RDNA210:14PM EDT- Custom Graphics IP to Samsung with Ray Tracing and Variable Rate Shading10:15PM EDT- Samsung to provide details later this year10:15PM EDT- Now notebooks10:15PM EDT- Scott Herkelman to the stage10:15PM EDT- Now that's a smug face10:16PM EDT- +27% YoY gaming laptop growth10:16PM EDT- Looking at Gen on Gen perf10:17PM EDT- Up to 50% more perf at the same power10:17PM EDT- 43% lower power at same perf10:17PM EDT- +77% more perf Gen on Gen off the battery10:17PM EDT- *on the battery10:17PM EDT- announcing RX 6000M series10:18PM EDT- 6800M - 2300 MHz, 12GB GDDR610:18PM EDT- ROG Strix 1510:19PM EDT- 120 FPS on RE:Village at 1440p Max10:19PM EDT- gaming on battery leadership10:19PM EDT- AAA and eSports10:20PM EDT- Ultimate gaming on the go experience, best in class battery perfomance10:20PM EDT- Radeon Chill for longer gaming10:20PM EDT- Also two other products10:20PM EDT- RX 6700M10:20PM EDT- 100FPS at 1440p max10:21PM EDT- RX 6600M10:21PM EDT- HP Omen 16 shipping later this month10:21PM EDT- AMD FidelityFX Super Sampling10:22PM EDT- 10s of millions of gamers on laptops need the best experiences10:22PM EDT- high performance upscaling required10:22PM EDT- AMD FSX for all gamers10:22PM EDT- *FSR10:22PM EDT- spatial upscaling10:22PM EDT- FSR enables broad adoption10:23PM EDT- 4 different quality settings10:23PM EDT- Supported on all Vega or higher GPUs10:24PM EDT- Available for the entire industry to use on GPUOpen10:24PM EDT- Works on NVIDIA hardware as well10:24PM EDT- works on all 10-series NVIDIA cards and newer10:25PM EDT- Support in 10 game engines already10:25PM EDT- support on 100 GPUs and CPUs and competitor products10:25PM EDT- First games support with FSR on June 22nd10:25PM EDT- Frank Azor to the stage10:27PM EDT- Laptops have no room for error10:27PM EDT- A great gaming laptop isn't easy10:27PM EDT- It goes beyond CPU+GPU10:27PM EDT- power, cooling, display, storage, IO, thermals, size10:28PM EDT- AMD Advantage10:29PM EDT- AMD Advantage laptop buyers know it was designed for a great gaming experience10:29PM EDT- Requires AMD Ryzen 5000 CPU, RX6000M graphics, Radeon Software10:29PM EDT- Exclusive AMD smart technologies10:29PM EDT- AMD Smart Access Memory10:30PM EDT- SAM comes to advantage with RX 6000M10:30PM EDT- Also AMD Smartshift10:30PM EDT- shifts power between CPU/grapics10:30PM EDT- +11% gaming performance10:31PM EDT- All AMD Advantage laptops have 144 Hz+ displays, 300 Nits+, IPS/OLED10:31PM EDT- All about the gamers10:32PM EDT- All AMD Advantage will have at least one NVMe x4 drive10:32PM EDT- low temps on the WASD and arrow keys10:32PM EDT- First AMD A+A is ROG Strix G1510:33PM EDT- 5900HX, 300 Hz display, RX 6800M10:33PM EDT- Select countries in June10:33PM EDT- HP Omen, 16-inch 5900HX + 6600M, 165 Hz10:33PM EDT- 1080p gaming10:34PM EDT- More models coming soon10:35PM EDT- FSR available on GPUOpen on MIT license10:35PM EDT- Now advanced technology10:35PM EDT- X3D packaging10:36PM EDT- roadmap to 5nm with first Zen 4 products next year10:36PM EDT- TSMC 3d fabric10:37PM EDT- First application is 3D vertical cache10:37PM EDT- stacking 64 MB on top of a zen 3 chiplet10:37PM EDT- TSVs10:37PM EDT- 2 TB/sec bandwidth10:38PM EDT- 192 MB L3 in a single package10:38PM EDT- 200x interconnect density10:39PM EDT- Denser connection with IP10:39PM EDT- No solder bumps, TSVs only10:39PM EDT- 3D V-Cache technology10:39PM EDT- 184 fps vs 206 FPS10:40PM EDT- 4 GHz clock speed fixed10:40PM EDT- +12 %10:40PM EDT- 12 core / 24 thread10:40PM EDT- +15% faster gaming on average10:40PM EDT- One entire architecture gain just from vcache10:41PM EDT- Starting production end of the year10:42PM EDT- That's a wrap. That was unexpected.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16722/computex-2021-amds-keynote-a-live-blog-10pm-et\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Announces New Mali-G710, G610, G510 & G310 Mobile GPU Families\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-05-25T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/16694/arm-announces-new-malig710-g610-g510-g310-mobile-gpu-families\n",
      "Content: Alongside with today’s extremely large and comprehensive CPU line-up announcement including the new Cortex-X2, Cortex-A710, Cortex-A510, new DSU-110 and new interconnects, we’re seeing the announcement of Arm’s newest Mali GPU line-up. Similar to the CPU family, we’re also seeing an extensive line-up announcement with the new Mali-G710 flagship series, the G510 middle-range, and the new ultra-area efficient Mali-G310.The new GPU series follows up in the same Valhall GPU family that wasstarted back in 2019 with the Mali-G77and seen minor improvements with theMali-G78 in last year’sannouncements and seen silicon adoption in this year’s SoC’s such as the Kirin 9000, Exynos 2100 or the new MediaTek Dimensity SoCs.At the high-end, the Mali-G710 is a direct successor to the Mali-G78 and is a relatively straightforward generational improvement in terms of what it’s aiming for: the highest possible performance that Arm’s architects can achieve in a Mali GPU. The Mali-G610 is a branding exercise that differentiates the same microarchitecture as the G710 at lower core counts, aiming to aid partners to better differentiate flagship products from the “premium” segment.The Mali-G510 is a successor to the 2019 Mali-G57 and is a major upgrade to Arm’s mid-range portfolio, bringing extremely large generational performance boosts as well as power efficiency gains over the predecessor.Finally, the new Mali-G310 is a new Valhall based low-end entry that represents a multi-generation architectural bump over the old-in-the-tooth Bifrost based Mali-G31 and targets the low-end area-efficiency focused market where we see hundred of billions of low-cost devices and other embedded markets such as smart TVs.As gross overview, the highlight today for most readers will be focused around the new Mali-G710 flagship GPU. The improvements that the company is promising is roughly a +20% boost in performance in a ISO-process node GPU configuration compared to a comparable Mali-G78 GPU. Similarly, at similar performance, the new GPU design promises a -20% reduction in power consumption and thus also energy efficiency gain.Recently, Arm has also made a focus on Machine Learning on the GPU and here the new design is promising a larger +35% boost in performance.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16694/arm-announces-new-malig710-g610-g510-g310-mobile-gpu-families\n",
      "Title: Arm Announces Mobile Armv9 CPU Microarchitectures: Cortex-X2, Cortex-A710 & Cortex-A510\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-05-25T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16693/arm-announces-mobile-armv9-cpu-microarchitectures-cortexx2-cortexa710-cortexa510\n",
      "Content: It’s that time of the year again, and after last month’s unveiling of Arm’snewest infrastructure Neoverse V1 and Neoverse N2CPU IPs, it’s now time to cover the client and mobile side of things. This year, things Arm is shaking things up quite a bit more than usual as we’re seeing three new generation microarchitectures for mobile and client: The flagship Cortex-X2 core, a new A78 successor in the form of the Cortex-A710, and for the first time in years, a brand-new little core with the new Cortex-A510. The three new CPUs form a new trio of Armv9 compatible designs that aim to mark a larger architectural/ISA shift that comes very seldomly in the industry.Alongside the new CPU cores, we’re also seeing a new L3 and cluster design with the DSU-110, and Arm is also making a big upgrade in its interconnect IP with the new cache coherent CI-700 mesh network and NI-700 network-on-chip IPs.The Cortex-X2, A710 and A510 follow up on last year's X1, A78 and A55. For the new Cortex-X2 and A710 in particular, these are direct microarchitectural successors to their predecessors. These parts, while iterating on generational improvements in IPC and efficiency, also incorporate brand-new architectural features in the form of Armv9 and new extensions such as SVE2.The Cortex-A510, Arm's new little core, is a larger microarchitectural jump, as it represents a new clean-sheet CPU design from Arm’s Cambridge CPU design team. A510 brings large IPC improvements while still having a continued focus on power efficiency, and, perhaps most interestingly, retains its characteristic in-order microarchitectural.An Armv9 CPU Family – AArch64 only for all practical purposes*The new CPU family marks one of the largest architectural jumps we’ve had in years, as the company is now baselining all three new CPU IPs on Armv9.0. We've extensivelycovered the details of the new Arm architectureback in late March. Cornerstone features of the new ISA include the new enrollment of prior optional/missing Armv8.2+ features that weren’t guaranteed in mobile and client designs (mostly due to the older A55 cores), and the introduction of new SVE2 SIMD and vector extensions.One big change we’ve been expecting for quite some time now is that we’ll be seeing a deprecation of the 32-bit AArch32 execution mode in upcoming Arm Cortex-A mobile cores. The clock has been ticking for 32-bit apps ever sinceGoogle’s announcedin 2019 that the Google Play store will require for 64-bit app uploads, and the company will stop serving 32-bit applications to 64-bit compatible devices later this summerWhile Arm is declaring that shift to happen in 2023, for all intents and purposes it’s already happening next year for most global users. Both the Cortex-X2 flagship core and the Cortex-A510 little cores are AArch64-only microarchitectures that are no longer able to execute AArch32 code.With that said, sharp readers will note that two out of three CPUs isn't acompleteshift, and the reason for that is because the Cortex-A710 actually still supports AArch32. Arm states that the reason for this is primarily to meet the needs of the Chinese mobile market, which lacks the homogeneous ecosystem capabilities of the global Play Store markets, and Chinese vendors and their domestic app market require a little more time to facilitate the shift towards 64-bit only. This means we’ll have an odd scenario next year of having SoCs on which only the middle cores are able to execute 32-bit applications, with those apps being relegated to the middle A710 cores and missing out on the little A510 cores’ power efficiency or the X2 cores’ performance.On the big core side, the new Cortex-X2 and Cortex-A710 are successors to the Cortex-X1 and Cortex-A78. Both designs are mostly designed by Arm’s Austin design team, and represent the 4thgeneration of this microarchitecture family, which hadstarted off with the Cortex-A76 several years ago. These cores should be the last of this microarchitecture family before Arm hands things off to a completely new design with next year’s new Sophia cores.In terms of design philosophy, the X2 and A710 generally keep the same overarching goals the X1 and A78 had defined: The X-series continues to focus on advancing performance by increasing microarchitectural structures and by Arm being willing to make compromises on power within reasonable limits. Meanwhile the A710 continues to focus on advancing performance and efficiency through smarter design and with a large focus on maximizing the power, performance, and area (PPA) balance of the IP.One point Arm makes in the above slide is having optimized critical paths and physical design for sustained voltage operations – this is more of a goal the company is striving for in the next generations of “middle” cores rather than something that’s specifically reflected in the Cortex-A710.This year, we are also finally seeing a new little core.We had covered the Cortex-A55 back in 2017, and since then we haven’t had seen any updates to Arm’s little cores, to the point of it being seen as large weakness of last few generations of mobile SoCs.The new Cortex-A510 is a clean-sheet design from Arm’s Cambridge design team, leveraging a lot of the technologies that had been employed in the company’s larger cores, but implemented into a new in-order little microarchitecture. Yes – we’re still talking about an in-order core, and Arm still sees this to be the best choice in terms of extracting the best efficiency and “Days of use” of mobile devices.Even though it’s a in-order core, Arm made a comparison that the new design is extremely similar to a flagship core of 2017 – namely the Cortex-A73, achieving very similar IPC and frequency capabilities whilst consuming a lot less power.The new design also comes with a very interesting shared complex approach and shares the L2 and FP/SIMD pipelines with a second core, a design approach Arm calls “merged core” and undoubtedly will remind readers of AMD’s CMT approach in Bulldozer cores 10 years ago, even though there are quite important differences in the approaches.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16693/arm-announces-mobile-armv9-cpu-microarchitectures-cortexx2-cortexa710-cortexa510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qualcomm Announces Snapdragon 7c Gen 2: Entry-Level PC and Chromebook Refresh\n",
      "Author: Ryan Smith\n",
      "Date Published: 2021-05-24T15:01:00Z\n",
      "URL: https://www.anandtech.com/show/16696/qualcomm-announces-snapdragon-7c-gen-2-entrylevel-pc-and-chromebook-refresh\n",
      "Content: As part of their Scaling the Mobile Compute Ecosystem presentation, Qualcomm this morning is announcing a refreshed version of their Snapdragon 7c for laptops. Aptly named Snapdragon 7c Gen 2, the updated chip for entry-level Windows PCs and Chromebooks sports a slight clockspeed boost, bumping the frequency of the two Kryo 468 (Cortex-A76) cores up to 2.55GHz. Spec bumps aside, Qualcomm’s target market for the 7c family hasn’t changed, with the updated SoC designed to serve as an anchor for sub-$400 “always-on” devices.Theoriginal Snapdragon 7c was announced back at Qualcomm’s 2019 tech summit, as part of Qualcomm’s efforts to flesh out their lineup of Snapdragon Compute SoCs aimed at Arm-powered PCs and other laptops. At the time, Qualcomm was (and still is) gunning heavily for making a market of always-on Windows devices, taking advantage of the low power consumption of Qualcomm’s chips and integrated LTE modem to deliver a mobile-like platform for Windows PCs. Those efforts, though not a smash hit, have at least proven fruitful enough for Qualcomm to refresh parts of the Snapdragon Compute lineup while better bringing Chromebooks into the fold as well.To that end, the Snapdragon 7c is getting what amounts to a mid-generation face lift. Like the Gen 2 update for the 8cx last year, Qualcomm isn’t rolling out a new silicon design for the 7c Gen 2, but they are reaping the benefits of general yield improvements and better binning to ratchet up the clockspeeds. For the 7c Gen 2, the clockspeed on the chip’s two Cortex-A76 cores is being bumped from 2.4GHz to 2.55GHz.Qualcomm Snapdragon Compute SoCsSoCSnapdragon 8cx Gen 2Snapdragon 8cSnapdragon 7c Gen 2CPU4x Kryo 495 Gold (CA76)4x Kryo 495 Silver (CA55)Up to 3.15 GHz4x Kryo 490 Gold (CA76)4x Kryo 490 Silver (CA55)Up to 2.45 GHz2x Kryo 468 Gold (CA76)6x Kryo 468 Silver (CA55)Up to 2.55 GHzGPUAdreno 680Adreno 675Adreno 618DSP / NPUHexagon 690Hexagon 690Hexagon 692AI Perf Combined7 TOPs6 TOPs5 TOPsMemoryController8x 16-bit CHLPDDR4X-426663.58 GB/s4x 16-bit CHLPDDR4X-426631.79 GB/s2 x 16-bit CHLPDDR4X-426615.90 GB/sISP/CameraDual 14-bit Spectra 390 ISP1x 32MP or 2x 16MP14-bit Spectra 2551x 32MP or 2x 16MPDecodeEncode4Kp120 10-bit H.265HDR Support4Kp60 H.265HDR SupportWi-FiWi-Fi 6 + BT 5.1Wi-Fi 5 + BT 5.0Integrated 4G ModemSnapdragon X24 LTE(Category 20)Snapdragon X15 LTE(Category 15/13)External 5G ModemSnapdragon X55 (Optional)--Mfc. Process7nm7nm8nmFeature-wise, the rest of the package is otherwise the same as it was for the original Snapdragon 7c. This includes a dual-channel (32-bit) LPDDR4X memory bus, a Hexagon 692 DSP, a Spectra 255 ISP, and an LTE category 15 Snapdragon X15 modem. The 7c Gen 2 is unabashedly an entry-level platform, with Qualcomm keeping the feature set (and performance) light in order to be price-competitive in the entry-level market.Overall then, the Snapdragon 7c Gen 2 not much of a bump over the original Snapdragon 7c, but then again Qualcomm isn’t pitching the Snapdragon 7c Gen 2 as a true next-generation successor to the original Snapdragon 7c. Instead, the launch of the 7c Gen 2 is being treated as something of a second launch for the 7c platform, while also letting Qualcomm pivot a bit on their device strategy to chase Windows devices and Chromebooks more equally. From Qualcomm’s perspective, they still have a competitive and desirable product, especially as entry-level devices have been selling like hotcakes over the past year due to the pandemic.As with the original 7c, Qualcomm’s biggest strengths with the 7c Gen 2 platform are on battery life and the integrated LTE radio. While the performance of a pair of A76 CPU cores is nothing to write home about these days, they are rather power-efficient by PC standards. Meanwhile an integrated modem not only helps to sell the SoC to cost-conscious OEMs, but also security-minded companies that want their devices reachable the bulk of the time.The competition for this revised SoC first and foremost remains Intel, who is now shipping their Tremont architecture-basedJasper Lakeplatform. Qualcomm is still looking to snag a piece of Intel’s PC pie, especially as the chip crunch has forced Intel to prioritize shipping high-end (high margin) hardware. And though Jasper Lake will undoubtedly make for stiffer competition than the older Gemini Lake platform, Qualcomm is still counting on battery life as well as their audio and video processing capabilities to give them an edge over Intel.Wrapping things up, Qualcomm expects devices based on the Snapdragon 7c Gen 2 to be available this summer. Given the hardware similarities to the original 7c, this should make for a relatively seamless transition for Qualcomm’s OEM partners.Gallery:Qualcomm Snapdragon 7c Slide Deck\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16696/qualcomm-announces-snapdragon-7c-gen-2-entrylevel-pc-and-chromebook-refresh\n",
      "Title: Ampere Roadmap Update: Switching to In-House CPU Designs, 128+ 5nm Cores in 2022\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-05-19T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/16684/ampere-roadmap-full-custom-cores\n",
      "Content: Today we’re covering some news of the more unusual type, and that is a roadmap update from Ampere, and having a closer look what the company is planning in terms of architectural and microarchitectural choices of their upcoming next-generation server CPUs in 2022 and onwards.For people not familiar with Ampere, the company was founded back in 2017 by former Intel president Renée James, notably built upon a group of former Intel engineers who had left along with her to the new adventure. Initially, the company had relied on IP and design talent fromformer AppliedMicro’s X-Gene CPUsand still supporting legacy productssuch as the eMAG line-up.With Arm having starting a more emphasised focus on designing and releasing datacentre and enterprise CPU IP line-ups in the form ofthe new Neoverse core offeringsa few years back, over the last year or so we had finally seen the fruits of these efforts in the form of the release of several implementations of the first generation Neoverse N1 server CPU cores products,such as Amazon’s Graviton2, and more importantly,Ampere’s “Altra Quicksilver” 80-core server CPU.The Altra Q line-up, for which wereviewed the flagship Q80-33 SKUlast winter, was inarguably one of the most impressive Arm server CPU executions in past years, with the chip being able to keep up or beat the best AMD and Intel had to offer, even extending that positioning against the latest generationXeonandEPYCgeneration.Ampere’s next generation \"Mystique\" Altra Max is the next product on the roadmap, and is targeted to be sampling in the next few months and released later this year. The design relies on the same first generation Arm Neoverse N1 cores, at the same maximum 250W TDP as a drop-in replacement on the same platform, however with an optimised implementation that now allows for up to 128 CPU cores – 60% more cores than the first iteration of Altra we have today, and double the amount of cores of competitor systems from AMD or Amazon’s Graviton2.For the future for designs beyond the Altra Max, Ampere is promising that they will be continuing emphasis of what they consider “predictable performance” for workloads with scaling socket load, increasing core counts with a linear increase in performance, and what I found interesting as a metric, to continue to reduce power per core – something to keep in mind as we’re discussing the next big news today:Replacing Neoverse with Full Custom CoresToday’s big reveal comes in regard to the microarchitecture choices that Ampere is going to be using starting in their next generation 2022 “Siryn” design, successor to the Altra Max, and relates to the CPU IP being used:Starting with Siryn, Ampere will be switching over from Arm’s Neoverse cores to their new in-house full custom CPU microarchitecture. This announcement admittedly caught us completely off-guard, as we had largely expected Ampere to continue to be using Arm’s Neoverse cores for the foreseeable future. The switch to a new full custom microarchitecture puts Ampere on a completely different trajectory than we had initially expected from the company.In fact, Ampere explains that what the move towards a full custom microarchitecture core design was actually always the plan for the company since its inception, and their custom CPU design had been in the works for the past 3+ years.In terms of background - the design team leading the effort is lead by Ampere’s CTO Atiq Bajwa, who is also acting as the chief architect on the project. Bajwa and the team surrounding him appear to be mostly comprised of high-profile ex-Intel engineers and veterans which had left the company along with Renée James in 2017, topped-off with talent from a slew of other companies in the industry who joined them in the effort. The pedigree and history of the team is marked by achievements such as working on Intel’s Haswell and Broadwell processors.Ampere’s explanation and rationale for designing a full custom core from the ground up, is that they are claiming they are able to achieve better performance and better power efficiency in datacentre workloads compared to what Arm’s Neoverse “more general purpose” designs are able to achieve. This is quite an interesting claim to make, and contrasts Arm’s projections and goals for their Neoverse cores. The recentNeoverse V1 and N2 cores were unveiled in more detail last monthand are claimed to achieve significant generational IPC gains.For Ampere to relinquish the reliance on Arm’s next-gen cores, and instead to rely on their own design and actually go forward with that switch in the next-gen product, shows a sign of great confidence in their custom microarchitecture design – and at the same time one could interpret it as a sign of no confidence in Arm’s Neoverse IP and roadmap. This comes at a great juxtaposition to what others are doing in the industry: Marvell hasstopped development of their own ThunderX CPU IPin favour of adopting Arm Neoverse cores. On the other hand, not specifically related to the cloud and server market,Qualcomm earlier this year have acquired Nuvia, and their rationale and explanation was similar to Ampere’s in that they’re claiming that the new in-house design capabilities offered performance that otherwise wouldn’t have been possible with Arm’s Cortex CPU IP.In our talks with Jeff Wittich, Ampere’s Chief Product Officer, he explains that today’s announcement should hopefully help paint a better picture of where Ampere is heading as a company – whether they’d continue to be content on “just” being an Arm IP integrator, or if they had plans for more. Jeff was pretty clear that in a few years’ time they’re envisioning and aiming for Ampere to be a top CPU provider for the cloud market and major player in the industry.In terms of technical details as to how Ampere’s CPU microarchitecture will be different in terms of approach and how and why they see it as a superior performer in the cloud, are questions to which we’ll have to be a bit more patient for hearing answers to. The company wouldn’t comment on the exact status of the Siryn design right now – on whether it’s been taped in or taped out yet, but they do retierate that they’re planning customer sampling in early 2022 in accordance to prior roadmap disclosures. By the tone of the discussions, it seems the design is mostly complete, and Ampere is doing the finishing touches on the whole SoC. Jeff mentioned that in due time, they also will be doing microarchitectural disclosures on the new core, explaining their design choices in things like front-end or back-end design, and why they see it as a better fit for the cloud market.Altra Max later this year, more cloud customer disclosuresBeyond the longer-term >2022 plans, today’s roadmap updates also contained a few more performance claim reiterations of Ampere’s upcoming 128-core Altra Max product, which is planned to hit the market later in the second half of the year and customers being sampled in the next few months.The “Mystique” code-named Altra Max design will be characterised in that it’s able to increase the core-count by 60% versus the current generation Altra design, all while remaining at and below the same 250W TDP. The performance slides here are showcasing comparisons and performance claims against what is by now the previous generation competitor products, Ampere here simply explains they haven’t been able to get their hands on more recent Milan or Ice Lake-SP hardware to test. Nevertheless, the relative positioning against the Altra Q80-30 and the EPYC 7742 would indicate that the new chip would easily surpass the performance of even AMD’s latest EPYC 7763.In the slide, Ampere actually discloses the SKU model name being used for the comparison, which is the \"Altra Max M128-30\" – meaning for the first time we have confirmation that all 128 cores are running at up to 3GHz clock speed, which is impressive given that we’re supposed to be seeing the same TDP and power characteristics between it and the Q80-33. We’ll be verifying these figures in the next few months once we get to review the Altra Max.Today’s announcement also comes with an update on Ampere’s customers. Oracle was notably one of thefirst Altra adopters, but today’s disclosure also includes a wider range of cloud providers, with big names such as ByteDance and Tencent Cloud, two of the biggest hyperscalers in China.Microsoft in particular is a big addition to the customer list, and while Ampere’s Jeff Wittich couldn’t comment on whether Microsoft has other internal plans in the works, he said that today’s announcement should give more clarity aroundthe rumours of the Redmond company working on Arm-based servers, reports of which had surfaced back in December. Microsoft’s Azure cloud service is only second to Amazon’s AWS in terms of size and scale, and the company onboarding Altra products is a massive win for Ampere.Taking control of one’s own futureToday’s announcements by Ampere of them deploying their own microarchitecture in future products is a major change in the company’s prospects. The news admittedly took us by surprise, but in the grand scheme of things it makes a lot of sense given that the company aims to be a major industry player in the next few years – taking full control of one’s own product future is critical in terms of assuring that success.While over the years we’ve seen many CPU design teams be disbanded, actually having a new player and microarchitecture pop up is a much welcome change to the industry. While the news is a blow to Arm’s Neoverse IP, the fact that Ampere continues to use the Arm architecture is a further encouragement and win for the Arm ecosystem.Related Reading:The Ampere Altra Review: 2x 80 Cores Arm Server Performance MonsterOracle Announces Upcoming Cloud Compute Instances: Ice Lake and Milan, A100 and AltraNext Generation Arm Server: Ampere’s Altra 80-core N1 SoC for Hyperscalers against Rome and XeonArm Announces Neoverse V1, N2 Platforms & CPUs, CMN-700 Mesh: More Performance, More Cores, More FlexibilityIntel 3rd Gen Xeon Scalable (Ice Lake SP) Review: Generationally Big, Competitively SmallAMD 3rd Gen EPYC Milan Review: A Peak vs Per Core Performance Balance\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16684/ampere-roadmap-full-custom-cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Hot Chips 33 (2021) Schedule Announced: Alder Lake, IBM Z, Sapphire Rapids, Ponte Vecchio\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-05-18T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/16688/hot-chips-33-2021-schedule-announced-alder-lake-ibm-z-sapphire-rapids-ponte-vecchio\n",
      "Content: Once a year the promise of super hot potatoes graces the semiconductor world. Hot Chips in 2021 is set to be held virtually for the second successive year, and the presentation schedule has just been announced. Coming this August, there will be deeper disclosures on next-generation processor architectures, infrastructure compute platforms, new enabling technologies such as processing-in-memory, a number of upcoming AI solutions, as well as a deeper look into custom accelerators.If you thoughtlast year's Hot Chipswas a good conference, this one is a strong competitor. Hot Chips is an annual two-day semiconductor disclosure event where the latest processor technology from around the industry (except Apple*, see at the bottom) is presented by the engineers behind the projects. A number of key players use Hot Chips as the first opportunity to promote key details of their designs in the market for potential customers, and startups with enough backing also get to talk about what makes their new chips unique in a crowded market.A highlight of each event is also the keynotes, with previous years involvingDr. Lisa Su, CEO AMDdiscussing the companies success,Dr. Phillip Wong of TSMCgiving the lay of the land at the leading edge of manufacturing, Jon Masters of Red Hat going through a number of the issues stemming from Spectre and Meltdown, andRaja Koduri covering Intel's visionof a full scalar-vector-matrix-spatial XPU strategy.TSMC Keynote, Hot Chips 31 (2019)Normally Hot Chips is an on-location event, but similar to last year due to restrictions on travel it will be an all-virtual event again. This means that most presentations are pre-recorded, but there was a lot of interaction at the event last year.Anyone can attend, and the virtual prices are low, at most $160, which provides the attendee with live presentations, a chance to ask Q&A, access to all the slide decks, and continued access to the talks for several months before they are made public. Last year's online solution went really well.This year's event will be held August 22-24th, and will run to the Pacific Time Zone. All times below are in PT.Hot Chips 33 (2021) ScheduleAnandTechTime and SessionSession TitleDay 0Aug 22nd08h30Tutorial 1ML Performance14h00Tutorial 2Advanced PackagingDay 1Aug 23rd08h45Session 1Processors11h30Session 2Academic Spinout Chips12h30KeynoteSynopsys14h30Session 3Infrastructure and Data Processors16h00KeynoteSkydio17h30Session 4Enabling TechnologiesDay 2Aug 24th08h30Session 1ML Inference for the Cloud10h00KeynoteDepartment of Energy11h30Session 2ML and Computation Platforms14h30Session 3Graphics and Video17h00Session 4New TechnologiesHere's a quick overview of the Hot Chips 2021 schedule.Day 0: Tutorial DayBecause Hot Chips caters to both professionals and students, the pseudo-first day of the event is typically a chance for attendees to get to grips with new topics in the industry. Of late these sessions have covered topics such as building scaleout systems, quantum computing, new networking paradigms, and security.Hot Chips 33 (2021): Tutorial DayAugust 22nd, SundayAnandTechInfo08h30 - 13h00Machine Learning PerformanceHardware and software co-optimization of the industry-standard MLPerf benchmarks, as well as applications, performance characteristics, key challenges, and considerations for those deploying unique workloads14h00 - 17h15Advanced PackagingHow advanced packaging techniques enable performance and density improvements, covering how current technologies in the market work, how they are used, and the cutting edge of packaging and chip design by the industry leadersThe first tutorial here is an expansion of previous talks by MLCommons, the incorporated industry body behind MLPerf. Over the past year we have seen the benchmark reach afull v1.0 with respect to inference, and much in the same way that the industry-standard SPEC benchmarks are optimized to the n-th degree, this session is here to assist with how companies can optimize their hardware and software stack to get the best MLPerf results.The second tutorial sounds really interesting. Packaging (and interconnect) are the next frontiers of scaled computational resources, with lots of research from the big players already put to use in modern mobile processors to big AI chips. This session is likely to coverTSMC's 3DFabricfamily of packaging methods, along with associated roadmaps that were disclosed last year, but alsoIntel's EMIB, Foveros, and ODI packaging. Other companies with advanced packaging products are also likely to get involved in how they use TSMC's and Intel's designs.Day One: MorningDay One is going to be very busy, and is split into six sessions, from 8:45 am to 7pm PT.As with any conference, the opening minutes are spent detailing the conference, what’s new for the year, and some of the rules (such as no streaming). I suspect there will be a large discussion about how the COVID situation will affect the presentations, what to do if one of the presentations fails, or such. I actually hope that the presentations are pre-recorded so that doesn’t happen.The first session is on Processors.Hot Chips 33 (2021): Day One, Session 1Server ProcessorsAnandTechSpeakerCompanyInfo08h45Opening Remarks09h00Efraim RotemIntelIntel Alder Lake CPU Architectures09h30Mark EversAMDAMD Next Generation Zen 3 Core10h00Christian JacobiIBMThe > 5 GHz next-generation IBM Z processor chip10h30Arijit BiswasIntelNext-Generation Intel Xeon CPUSapphire RapidsSailesh KottapalliThe first official day of presentations always starts with discussing leading-edge processors, and this year looks to be a stellar set of talks.First up isIntel discussing Alder Lake, its second-generation heterogeneous processor architecture (after Lakefield) that is set to be the next-generation processor for both desktops and high-end laptops. We already know that Alder Lake will use both Golden Cove and Gracemont microarchitectures, to the hope here is that Intel will spend time going deep into both. Normally this is the sort of thing they would have disclosed over several hours at an Intel-specific event, and given 30 minutes for the talk I wonder how much will actually be disclosed - it might instead be a talk solely about the SoC and we won't get microarchitecture detail at all.Second is an AMD talk about its latestZen 3 core microarchitecture. As Zen 3 was launched into the market in Q4 last year, with an updated back-end and unified L3 cache structure, I doubt we will see anything new in this talk. The hardware has been thoroughly tested; AMD typically uses Hot Chips to refresh what's already out in the market, and there's an RDNA2 talk on the second day which is expected to be of a similar nature.Third is IBM discussing its next-generation mainframe architecture and product line, the Z processor. We've covered IBM discussing z14 andz15 in previous Hot Chips events, and so this is either a deeper dive into z15 (which was presented last year in lots of detail) or a new look at an upcoming z16 design. The Z mainframe solution usually consists of compute processors and control/cache processors across a unified multi-rack approach - because this talk is titled 'processor chip', I suspect it is more about the compute processor than the solution, but hopefully there will be a slide or two on how it all fits together.The final talk of the session is another Intel talk, this time discussing the upcoming next-generation Sapphire Rapids platform, set to launch either sometime at the end of this year or early next year (Intel has a contract with DoE for theAurora supercomputerit needs to fill by the end of the year with this part, so general availability might be after). Sapphire Rapids is using Intel's 10nm Enhanced SuperFin process, and the same Golden Cove cores mentioned in Alder Lake, though perhaps cache optimized for server use. I suspect this talk will be heavy on the die configuration and new elements, such as PCIe 5.0 and DDR5.After a short break, we get to the Academic section.Hot Chips 33 (2021): Day One, Session 2Academic Spinout ChipsAnandTechSpeakerCompanyInfo11h30Karu SankaralingamUniversity of Wisconsin-MadisonMozart, Designing for Software Maturity and the Next Paradigm for Chip Architectures12h00Todd AustinUniversity of MichiganMorpheus II: A RISC-V Security Extension for Protecting Vulnerable Software and HardwareIt's not often we pay too much attention here given the research nature of the devices, however the second talk on a RISC-V security extension is rather interesting. Coming from the University of Michigan, the Morpheus II core has been reported as being the target for 500+ cybersecurity researchers for 3 months as part of the DARPA red-teaming challenge, and had zero penetrations in that time.Day One: Keynote One12h30Aart de GeusSynopsysSynopsys KeynoteThis year is a little different to most, with the first day having two separate keynotes. First up is an untitled presentation from the CEO of Synopsys, a company well known in the industry for its EDA (electronic design automation) tools. This includes logic synthesis, place and route, static timing analysis, hardware language simulators, and transistor-level circuit simulation. This talk should give the company a chance to discuss its next-generation technologies, especially as we move into an era of 3D design and packaging.When we get the full title of the talk, this segment will be updated.Day One: AfternoonAfter lunch the next series of talks are on non-standard processor designs, typically optimized for infrastructure or data processing.Hot Chips 33 (2021): Day One, Session 3Infrastructure and Data ProcessorsAnandTechSpeakerCompanyInfo14h30Andrea PellegriniArmArm Neoverse N2: Arm's second-generation high-performance infrastructure CPUs and system products15h00Idan BursteinNVIDIANVIDIA Data Center Processing Unit (DPU) Architecture15h30Bradley BurresIntelIntel's Hyperscale-Ready Smart-NIC for Infrastructure ProcessingThe first talk is regarding Arm's Neoverse N2 core, the upgraded model of the N1, which was announced in late April. N2 products aren't due out until next year, however companies working with N2 are likely already designing their SoCs with the core. Arm didn't disclose many of thepipeline details in the April announcement, and so we might see more information along these lines, however there is the potential for it to just be the same announcement as April. We shall wait and see.The second talk from NVIDIA is about its Data Processor Unit architecture, also known as Bluefield. Coming from its acquisition of Mellanox, the Bluefield line of DPUs enables Smart-NIC like network acceleration by integrating a network controller, general-purpose compute cores, and PCIe connectivity into the same device. The latest product is Bluefield-2,however murmurings of Bluefield-3 have been made as to the future roadmap replacement. The title of the talk does not specifically state NVIDIA will talk about current generation architectures or next generation.Third up is the Intel SmartNIC solution which has been announced previously as the C5000X platform. A number of Intel's customers and OEM partners, are already shipping the hardware, based on Intel Altera FPGAs, to key customers such as Baidu. Partners such as Silicom are selling the parts under their own brand. There hasn't been much presentational material about the architecture of the SmartNIC, so this might be an interesting insight into one of Intel's new market pushes.Day One: Keynote TwoIn an interesting turn of events, the first day of Hot Chips has two keynotes.16h00Abraham BachrachSkydioSkydio Autonomy Engine: Enabling the Next Generation of Autonomous FlightThe second keynote of the day comes from Skydio, a company I had not heard of before the announcement, but appears to be on the leading edge of AI-based pilot technology. The Skydio 2 platform for example seems to be powered by an NVIDIA Tegra TX2, and the concept of autonomous flight / drone technology has always been on the cusp in the evolution on AI. Skydio looks set to talk about its next-generation platform, given that its Skydio 2 was released in 2019 and since then it has had another round of VC funding.Day One: Even More TalksTo end the first day, Hot Chips will discuss future technologies. After a long day, I do wonder why they don't extend Hot Chips out into a third day of talks - normally at this point I am truly on information overload. Out of the three talks in this session, the one I'm most familiar with is the final one, by Samsung, about its compute-in-memory solution.Hot Chips 33 (2021): Day One, Session 4Enabling TechnologiesAnandTechSpeakerCompanyInfo17h30Ramanujan VenkatadriInfineonHeterogeneous computing to enable the highest level of safety in automotive systems18h00Sriram RajagopalEdgeQArchitecting an Open RISC-V 5G and AI SoC for Next Generation 5G Open Radio Acess Network18h30Jin Hyun KimSamsungAquabolt-XL: Samsung HBM2-PIM with in-memory processing for machine learning acceleratorsThe Aquabolt-XL was unveiled earlier this year, with Samsung able to add compute cores per memory bank into its HBM2 memory without any hardware modifications on the host side. Aquabolt-XL works by sending commands to specific memory addresses and can perform simple compute tasks on its in-order cores on the data within that memory bank. The idea is that energy will be saved by not having to move data from memory to the core for the simplest operations. At the time, I asked Samsung if this requires extra power, as other compute-in-memory solutions, and Samsung said no - this is very much a simple drop-in replacement for any HBM2 solution today, and requires software modification for use.Day Two: MorningThe second day starts early, at 8:30am, and runs until 7pm. The first session starts with discussing Machine Learning inference processors from some of the big companies looking for cloud deployment.Hot Chips 33 (2021): Day Two, Session 1ML Inference for the CloudAnandTechSpeakerCompanyInfo08h30David DitzelEsperanto TechnologiesAccelerating ML Recommendation with over a Thousand RISC-V Tensor Processors on Esperanto's ET-SoC-1 Chip09h00Ryan LiuEnflame TechnologyAI Compute Chip from EnflameChuang Feng09h30Karam CathaQualcommQualcomm Cloud AI 100: 12 TOPs/W Scalable, High Performance and Low Latency Deep Learning AcceleratorThe first talk in this session is from Esperanto, a startup with $58m+ in funding to create recommendation engine processors for the could. All the major hyperscalers and retailers use recommendation engines - showing users what product would most interest them at any given time. Esperanto's solution seems to be a 1000-core RISC-V solution, with combined tensor cores for inference acceleration. This talk should be the first disclosure of the chip and the architecture underneath.Second is another AI semiconductor startup, this time from China with a large backing from Tencent. The last funding round in January this year was for some $279m+ in Series C, although exactly what Enflame is producing hasn't been announced.Qualcomm's AI 100 solution is third, a product that wasannounced just after Hot Chips last year. This AI inference processor is the top version of a range of AI inference hardware built on the same architecture underneath. The Qualcomm Cloud AI 100 has a reported speed of 400 TOPs for 75 W, although the most efficient version is aiming for 12 TOPs per Watt. I expect this talk to go into the architecture details of the processor family.Day Two: KeynoteThe second day only gets a single keynote earlier than usual, but this one should be interesting as it is from the US Department of Energy.10h00Dimitri KusnezovDoEDoE AI and TechnologyIn this instance, the Deputy Under Secretary for AI and Technology will present an unnamed talk, which could cover a number of talks from the use of AI in current DoE deployed systems, or the DoE approach to adopting AI at scale, in both training and inference. A number of companies at this event have contracts with the DoE in some form, making it an interesting talk from our point of view.Day Two: Home StretchContinuing the machine learning theme, the second session of the day before lunch is about bigger AI chips built more for training, as well as a special surprise.Hot Chips 33 (2021): Day Two, Session 2ML and Computation PlatformsAnandTechSpeakerCompanyInfo11h30Simon KnowlesGraphcoreGraphcore Colossus Mk2 IPU12h00Sean LieCerebras SystemsThe Multi-Million Core, Multi-Wafer AI Cluster12h30Raghu PrabhakarSambaNova Systems IncSambaNova SN10 RDU: Accelerating Software 2.0 with DataflowSumti Jairath13h00J. Adam ButtsD.E. Shaw ResearchThe Anton 3 ASIC: a Fire-Breathing Monster for Molecular Dynamics SimulationsDavid E. ShawThe first talk is from Graphcore, talking about its Mk2 IPU product family. Graphcore has been talking about theMk2 IPU deployment for over a year, including its four-IPU single server 1U solution all the way up to an IPU Pod and beyond. This talk seems to be a recap of the underlying architecture just at a different event, although fingers crossed we see something about a roadmap here as well.The second talk is Cerebras Systems withits second-generation Wafer Scale Engine- a CPU the size of your head with 850,000 cores, and one wafer becomes one chip. Having announced WSE-2 at the beginning of the year, one of the highlights of those presentations was using multiple WSE-2 systems in the same rack to scale out the solution. That seems to be the focus of this talk.Third is SambaNova, one of the AI companies that has recently announced hundreds of millions in their latest rounds of funding. TheCardinal AI solution from SambaNovais targeting AI training, and scaling across many systems in a reconfigurable gate array architecture, very similar to that of an FPGA but geared towards AI. SambaNova has presented its architecture at a few select conferences, however this will be the first time I've seen a talk on the topic.Finally we have the surprise talk of the event, at least from my perspective. Here's a question - what do you do if there isn't any hardware on the market to solve your problem? Simple, build your own! The Anton 3 ASIC is next-generation dedicated hardware for molecular dynamics for all those molecular modeling problems that take too long to complete on conventional hardware. The story behind how the Anton 3 came into being seems fascinating from my short research, so I hope it becomes a part of the talk. The Anton 2 chip was presented at Hot Chips 26, in 2014, before I started covering the event.After another lunch, the next session will excite a number of users, as they breach the topic of graphics disclosures. It looks like we're going to get a couple of good ones this year, despite the current state of the graphics market.Hot Chips 33 (2021): Day Two, Session 3Graphics and VideoAnandTechSpeakerCompanyInfo14h30David BlytheIntelIntel's Ponte Vecchio GPU Architecture15h00Andrew PomianowskiAMDAMD RDNA 2 Graphics Architecture15h30Aki KuuselaGoogleGoogle's Video Coding Unit (VCU) AcceleratorClint Smullen16h00Juanjo NogueraXilinxXilinx 7nm Edge ProcessorsFirst up is Intel discussing its Xe-HPC architecture, or rather specificallyits Ponte Vecchio HPC chipthat uses 47 tiles from multiple process nodes on a single substrate. Ponte Vecchio was initially designed for the Aurora Exascale Supercomputer project, and will be partnered with Sapphire Rapids. As the chip is due to be shipped to production systems later this year, an expose on the architecture is widely welcomed.The second talk will be AMD recapping itsRDNA2 architecture, which is now at the heart of many of its graphics products.The third talk is one that interests me comes from Google - as with all things at Youtube, it has to be done at scale, and making sure that every video on the platform is available at multiple resolutions requires a lot of computational resources. To help speed that up, Google built its own video co-processor, and through 2021 has been disclosing bits of how it works. Currently, the VCU only caters for a couple of the most regular codecs, but hopefully we will get an insight into what future versions might bring with AV1 and newer codecs.Finally for this session,Xilinxwill talk about its 7nm processor designs for the edge which it announced earlier this year.The final session of the event is filed under 'New Technologies'. Everything here isn't strictly to do with processors, but certainly involves Hot Chips.Hot Chips 33 (2021): Day Two, Session 4New TechnologiesAnandTechSpeakerCompanyInfo17h00Michael WiemerMojo VisionMojo Lens - AR Contact Lenses for Real PeopleRanaldi Winoto17h30Sukki YoonSamsungWorld's Largest Mobile Image Sensor with All Directional Phase Detection and Auto Focus Function18h00Hidekuni TakaoKagawa UniversityNew Value Creation by Nano-Tactile Sensor Chip Exceeding our Fingertip Discrimination Ability18h30Christopher MonroeIonQ IncThe IonQ Trapped Ion Quantum Computer Architecture*A good number of engineers from Apple attend the event every year, and ask a lot of questions, however they have never presented a talk. These events are often a collaborative industry disclosure mechanism, and gives a chance for companies to present their best technology as well as quiz their competitors. Apple's engineers are happy to ask lots of questions, but so far they have never had a talk good enough to be presented (I'm pretty sure they've never submitted a talk) or sponsored the event in any capacity.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16688/hot-chips-33-2021-schedule-announced-alder-lake-ibm-z-sapphire-rapids-ponte-vecchio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ZTE Announces Global Launch of Axon 30 Ultra: Starting at $749\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-05-13T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/16675/zte-announces-global-launch-of-axon-30-ultra-starting-at-749\n",
      "Content: Today we’re hearing news from ZTE, as the company is announcing the global upcoming launch and availability of the new Axon 30 Ultra. The device had actually been unveiled back on April 15th, but today actually represents the disclosure global availability information and pricing of the device.ZTE’s actually a relatively larger player in the mobile market, although it doesn’t get as much media attention as other vendors. What sets ZTE apart from other Chinese vendors is the fact that the company is well established in the US and officially releases and supports phones for US carriers, putting them in a rather unique situation in the North American market and why the Axon 30 Ultra is a quite interesting device.ZTE Axon 30 UltraAxon 30 UltraSoCQualcomm Snapdragon 8881xCortex-X1@ 2.84GHz3xCortex-A78@ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 660 @ 840MHzDRAM8/12 GB LPDDR5Storage128/256GB UFS 3.1Display6.67\" AMOLED2400 x 1080 (20:9)144Hz300Hz TouchSizeHeight161.53 mmWidth72.96 mmDepth8.0 mmWeight188 gramsBattery Capacity4600mAh65W charging (PD3.0)Wireless Charging-Rear CamerasWide64MP IMX6860.8µm pixels (1.6µm 4:1 16MP)f/1.6 w/OIS26mm eq.Main64MP GW30.7µm pixels (1.4µm 4:1 16MP)f/1.935mm eq.Ultra-wide64MP GW30.7µm pixels (1.4µm 4:1 16MP)f/2.213mm eq.Telephoto8MPf/3.4 w/OIS(Periscope design)120mm eq.Front Camera16MPf/2.5I/OUSB-C 3.1Wireless (local)802.11ax WiFi-6EBluetooth 5.2 LE + NFCOther FeaturesDual SpeakersUnder-screen optical fingerprint sensorDual-SIMDual nanoSIMLaunch Price8GB+128GB: $749 / £649 / €74912GB+256GB: $849 / £739 / €849The phone is powered by a Snapdragon 888 SoC which is par of the course for a 2021 flagship phone. The company pairs it with either 8 or 12GB of LPDDR5 RAM and either 128 or 256GB of UFS 3.1 storage. There’s no expandable storage but the phone support dual nanoSIMs.The display of the Axon 30 Ultra is interesting as it’s a 6.67” AMOLED with a resolution of 2400 x 1080 – so far that’s not anything extraordinary, but what sets it apart from most other phones out in the market right now is the 144Hz refresh rate. The phone comes with 144, 120, 90 and 60Hz refresh rate options. In my brief hands-on with the phone, I actually figured out that there is a kind of variable refresh rate mechanism on the device which when on “Auto” mode does seem to switch around between the modes more seamlessly. It’s something to dwell into more detail once we have more time with the phone. What’s for certain is that the 144Hz mode is extremely power hungry – besides the refresh rate ZTE also uses a 300Hz input touch sampling rate which is extremely high for a non-gaming branded phone.In terms of design the front of the phone is relatively within the norm – the curved side bezels are relatively within the norm of what we’ve seen of flagships of these past few years, and the display houses a hole-punch front camera with a 16MP f/2.5 module underneath.The back of the phone comes with frosted glass, and also has curved sides. The curvature here is more much pronounced than on the front screen glass, and it allows for excellent ergonomics and in-hand feel.What’s actually more surprising about the Axon 30 Ultra is the fact that it comes with the “Ultra” designation. Usually, you would expect super large and heavy phones such as the S21 Ultra or the Mi 11 Ultra. The Axon 30 Ultra on the other hand is actually and unexpectedly quite reasonable in terms of dimensions and handling. At only 72.96mm width it’s a smaller form-factor than the competitor Ultra devices, and it also only weighs in at 188 grams, vastly below the other usual >220g Ultra phones. So, in that sense, the Axon 30 Ultra is more of a balanced phone in terms of size.The phone gives up a few things to get to that weight and 8.0mm thinness. The battery is ever so slightly smaller at 4600mAh, but most notably it’s lacking wireless charging. It comes with a 65W USB PD PPS power brick included.The top and bottom sides of the frame are flattened out; the top side is bare but for a microphone hole and the bottom includes your typical dual nanoSIM tray, USB 3.1 capable USB-C port, and the bottom speaker grill.The side-buttons are located on the right side of the phone and in general of a nice quality and well positioned. Overall the first impression I got from the phone was that it was of excellent build quality, and given the size, weight, fit in well in my preferred sweet spot for devices.The real “Ultra” denomination of the phones comes from the camera assembly and the modules. It’s a bit unusual as it’s a unique take in the market: the phone features a quad-camera setup with three 64MP modules. The first module, usually referred as the “main” or sometimes “wide” module is a 26mm equivalent unit powered by an IMX686 which features 0.8µm pixels and bins down to 16MP 1.6µm pictures. The aperture is a wide f/1.6 and the optics feature OIS.The unusual thing about the phone is a secondary 35mm equivalent unit, essentially 1.35x optical magnification, also powered by a 64MP sensor but this time it’s a Samsung powered GW3 unit with smaller 0.7µm pixels. ZTE calls this the portrait module but it’s still unusual to see such a focal length being used as most other vendors have traditionally settled for a 50mm equivalent. The optics are f/1.9 and features no OIS.The ultra-wide module features the same GW3 sensor and features f/2.2 optics and 13mm equivalent focal length, capable of capturing a 120° field of view. ZTE promises an ultra-low distortion optics design to avoid fish-eye view and too much software side corrections.Finally, the telephoto module is a 120mm equivalent focal length and represents essentially an around 4.6x optical magnification relative to the main sensor. The optics are f/3.4 and it features OIS, but the sensor is of a smaller 8MP size, keeping the general thickness of the module in check, which is why the camera setup and phone in general is thinner than competitive Ultra devices.How the camera setup performs is also very much influenced by the software, but so far my first experiences with the camera and a few test shots have been actually quite positive, with the phone producing some good images in well-lit scenarios.Starting at $749 / £649 / €749 on June 4thThe key metric of today’s announcement was the pricing: Starting at €/$749 and £649, ZTE is asking for quite a seemingly competitive pricing for the Axon 30 Ultra. What will end up determining the device’s value is whether it holds up in terms of battery life and if the camera setup is indeed competitive.Beyond the fundamentals, the Axon 30 Ultra is also extremely interesting for US readers given that it’s officially sold and supported in North America, giving a much-needed alternative in those markets where the usual flagship options have shrunk down to just Samsung and OnePlus. Given the more attractive pricing, if the phone pans out in terms of its fundamentals, it could very much represent a good alternative in the market.Full review embargo as well as pre-orders for the phone start on May 27th– stay tuned for a more in depth evaluation.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16675/zte-announces-global-launch-of-axon-30-ultra-starting-at-749\n",
      "Title: The ASUS Zenfone 8 Hands-On Review: A New Compact Direction\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-05-12T17:30:00Z\n",
      "URL: https://www.anandtech.com/show/16665/the-asus-zenfone-8-hands-on-review\n",
      "Content: Today ASUS is launching their new mainstream line-up of flagship devices, the Zenfone 8 series. Unlike last year’s iteration of the Zenfone 7 which was defined by the flip-camera design, ASUS is mixing up the formula this year with the new Zenfone 8, a completely different phone in a completely different form-factor, targeting a niche in the market which ASUS sees as an opportunity to differentiate itself in.The Zenfone 8 is defined by its size: with a 5.9” screen and a width and height of 68.5 and 148mm, it’s by far one of the smallest flagship SoC powered devices in the market. It’s an extreme departure from the Zenfone 7 – however ASUS also introduces the Zenfone 8 Flip, essentially a Snapdragon 888 upgrade over what we’ve seen in the Zenfone 7, though this variant of the Zenfone 8 will be limited in terms of market availability, and today’s article will focus on the smaller Zenfone 8.ASUS ZenFone 8 SeriesZenFone 8 FlipZenFone 8SoCQualcomm Snapdragon 8881xCortex-X1@ 2.84GHz3xCortex-A78@ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 660 @ 840MHzDRAM6 GB LPDDR56/8/16 GB LPDDR5Storage128GB UFS 3.1+ microSD128/256GBUFS 3.1Display6.67\" AMOLED2400 x 1080 (20:9)90Hz200Hz Touch5.9\"AMOLED2400 x 1080 (20:9)120Hz240Hz TouchSizeHeight165.08 mm148.0 mmWidth77.28 mm68.5 mmDepth9.6 mm8.9 mmWeight230 grams169 gramsBattery Capacity5000mAh30W charging (PD3.0)4000mAh30W charging (PD3.0)Wireless Charging-Rear CamerasMain64MP IMX6860.8µm pixels (1.6µm 4:1 16MP)f/1.764MP IMX6860.8µm pixels (1.6µm 4:1 16MP)f/1.7w/OISTelephoto8MP3x optical zoomf/2.4n/aWide12MP IMX3631.4µm pixels Dual PDAF113° FoV ultra-widef/2.2Extra-Front CameraFlip-camera DesignFront cameras = Rear cameras12MP IMX6631.22µmI/OUSB-CUSB-C3.5mm headphoneWireless (local)802.11ax WiFi-6Bluetooth 5.1 LE + NFCOther FeaturesTriple-function Power Button w/ Capacitive Fingerprint SensorIP68Dual SpeakersUnder-screen fingerprint sensorDual-SIMDual nanoSIMLaunch Price21,999 TWD(USD~748, EUR~626)starting 599€The Zenfone 8 and 8 Flip are powered by the new Snapdragon 888. We’ve reviewed the SoC quite extensively over the last few months in a wide range of devices from various vendors – the chip is characterised by increased performance coming at a cost of quite higher power usage due to the shift toa regressed 5nm process node.ASUS equips the Zenfone 8 from 6 to 16GB of LPDDR5 RAM depending on the SKU model you chose, and comes equipped with 128 or 256GB of UFS 3.1 storage, with no microSD option this time around.As noted, what really differentiates the Zenfone 8 from its predecessors as well as from other smartphones in the market is its more diminutive stature. At 148 x 68.5 x 8.9mm and only 169g weight, the Zenfone 8 is one of the smallest phones in the market, especially amongst devices powered by the latest flagship hardware internals.ASUS states that this was a deliberate market positioning that they are experimenting with: the company has seen that while there’s tons of competitors in the now regular “larger” form-factor of phones, there’s actually very little options when it comes to smaller devices. Sony’s Xperia 5 series was one of the rare ones out there with a phone width below 70mm, however many people didn’t like Sony’s extremely elongated aspect ratio, and ASUS pointing out that the Zenfone 8 is now also the only option out there with a device height of below 150mm.ASUS’s strategy here is I think excellent, and allows them to fill a niche in the market and compete for customers who are looking for such devices. The ergonomics of the Zenfone 8 are generally excellent due to its smaller size, but ASUS also designed the phone to have good in-hand feel due to the curved back glass design as well as the rounded metal frame of the phone.The build quality of the phone is excellent, and I particularly notice the removal of the plastic “gasket” piece between the phone frame and the display glass that’s usually found on cheaper devices in the market.The screen itself is a 5.9” OLED with 2400 x 1080 resolution, with an upgraded refresh rate of up to 120Hz, and a touch input of up to 240Hz. Unfortunately there is no variable refresh rate here, neither software nor hardware, so anything above 60Hz comes at the cost of battery life.The Zenfone 8 uses a “regular” hole-punch front camera module instead of a mechanical flip mechanism of the rear cameras, which is completely fine due to the design limitations of such a much smaller device. I found it a bit weird that ASUS adopted this metallic ring design around the camera – I’ve seen it used before on other devices and I was not fan of it as it really draws the attention to the camera instead of making it inconspicuous. At least here on the Zenfone 8 it’s centred perfectly within the display hole.The camera setup on the Zenfone 8 is extremely simple: it features the same main camera and ultra-wide module as found on last year’s Zenfone 7 Pro series, meaning a 64MP IMX686 main camera module that bins down to 16MP 1.6µm in regular auto mode pictures and features a f/1.7 optics with OIS, and a 113° UWA module powered by a 12MP IMX363 and f/2.2 aperture with autofocus capabilities. Generally we were not very impressed withthis camera setup on the Zenfone 7 last year, and have similar low expectations of the Zenfone 8 – we’ll quickly check out some samples later in the piece.The bottom of the phone features your typical SIM tray, which this time around does not feature a microSD anymore, USB-C port, as well as a good quality main speaker. Between the USB port and the speaker hole there’s actually also a small LED notification light – something that over the years has seen been deprecated by various vendors in favour on always-on displays. I greatly appreciate this feature as it’s much more power efficient compared to AOD notifications.At the top of the phone, we find the mythical and elusive 3.5mm headphone jack. Over the many years we see countless vendors drop the feature and trying to promote wireless headphones which cost, more, have worse audio quality, and are prone to degradation due to batteries. Sony and ASUS are two vendors who did drop the headphone jack in the past and reintroduced them in subsequent generations due to negative feedback, so I applaud ASUS for also including it here on the Zenfone 8.What’s new for ASUS, is the Zenfone 8 is an IP68 rated device, which was one feature limitation of the mechanical flip-camera design of the Zenfone 7, and continues to be so for the Zenfone 8 Flip.Today’s hand-on review focuses around the Zenfone 8 as that’s what ASUS had sent out as samples, but the company is also launching the Zenfone 8 Flip. This phone is essentially identical to the Zenfone 7 with the exception for the upgrade to a new Snapdragon 888 SoC. Unfortunately, the 8 Flip will see a much more limited release compared to the Zenfone 8, notably with it not launching in the North American market.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16665/the-asus-zenfone-8-hands-on-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Using a PCIe Slot to Install DRAM: New Samsung CXL.mem Expansion Module\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-05-11T08:10:00Z\n",
      "URL: https://www.anandtech.com/show/16670/using-a-pcie-slot-to-install-dram-new-samsung-cxlmem-expansion-module\n",
      "Content: In the computing industry, we’ve lived with PCIe as a standard for a long time. It is used to add any additional features to a system: graphics, storage, USB ports, more storage, networking, add-in cards, storage, sound cards, Wi-Fi, oh did I mention storage? Well the one thing that we haven’t been able to put into a PCIe slot is DRAM – I don’t mean DRAM as a storage device, but memory that actually is added to the system as useable DRAM. Back in 2019 a new CXL standard was introduced, which uses a PCIe 5.0 link as the physical interface. Part of that standard is CXL.memory – the ability to add DRAM into a system through a CXL/PCIe slot. Today Samsung is unveiling the first DRAM module specifically designed in this way.CXL: A RefresherTheoriginal CXL standardstarted off as a research project inside Intel to create an interface that can support accelerators, IO, cache, and memory. It subsequently spun out into its own consortium, with over 50+ members, and support from key players in the industry: Intel, AMD, Arm, IBM, Broadcom, Marvell, NVIDIA, Samsung, SK Hynix, WD, and others. The latest standard is CXL 2.0, finalized in November 2020.The CXL 1.1 standard covers three sets of intrinsics, known as CXL.io, CXL.memory and CXL.cache. These allow for deeper control over the connected devices, as well as an expansion as to what is possible. The CXL consortium sees three main areas for this:The first type is a cache/accelerator, such as an offload engine or a SmartNIC (a smart network controller). With the CXL.io and CXL.cache intrinsics, this would allow the network controller to sort incoming data, analyze it, and filter what is needed directly into the main processors memory.The second type is an accelerator with memory, and direct access to the HBM on the accelerator from the processor (as well as access to DRAM from the accelerator). The idea is a pseudo-heterogeneous compute design allowing for simpler but dense computational solvers.The third type is perhaps the one we’re most interested in today: memory buffers. Using CXL.memory, a memory buffer can be installed over a CXL link and the attached memory can be directly pooled with the system memory. This allows for either increased memory bandwidth, or increased memory expansion, to the order of thousands of gigabytes.CXL 2.0 also introduces CXL.security, support for persistent memory, and switching capabilities.It should be noted that CXL is using the same electrical interface as PCIe. That means any CXL device will have what looks like a PCIe physical connector. Beyond that, CXL uses PCIe in its startup process, so currently any CXL supporting device has to also support a PCIe-to-PCIe link, making any CXL controller also a PCIe controller by default.One of the common questions I’ve seen is what would happen if a CXL-only CPU was made? Because CXL and PCIe are intertwined, a CPU can’t be CXL-only, it would have to support PCIe connections as well. That being said, from the other direction: if we see CXL-based graphics cards for example, they would also have to at least initialize over PCIe, however full working modes might not be possible if CXL isn’t initialized.Intel is set to introduce CXL 1.1 over PCIe 5.0 with its Sapphire Rapids processors.Microchip has announcedPCIe 5.0 and CXL-based retimers for motherboard trace extensions. Samsung today is the third announcement for CXL supported devices. IBM has a similar technology called OMI (OpenCAPI Memory Interface), however that hasn’t seen wide adoption outside of IBM’s own processors.Samsung’s CXL Memory ModuleModern processors rely on memory controllers for attached DRAM access. The top line x86 processors have eight channels of DDR4, while a number of accelerators have gone down the HBM route. One of the limiting factors in scaling up memory bandwidth is the number of controllers, which can also limit capacity, and beyond that memory needs to be validated and trained to work with a system. Most systems are not built to simply add or remove memory the same way you might do with a storage device.Enter CXL, and the ability to add memory like a storage device. Samsung’s unveiling today is of a CXL-attached module packed to the max with DDR5. It uses a full PCIe 5.0 x16 link, allowing for a theoretical bidirectional 32 GT/s, but with multiple TB of memory behind a buffer controller. In much the same way that companies like Samsung pack NAND into a U.2-sized form factor, with sufficient cooling, Samsung does the same here but with DRAM.The DRAM is still a volatile memory, and data is lost if power is lost. (I doubt it is hot swappable either, but weirder things have happened). Persistent memory can be used, but only with CXL 2.0. Samsung hasn't stated if their device supports CXL 2.0, but it should be at least CXL 1.1 as they state it currently is being tested with Intel's Sapphire Rapids platform.It should be noted that a modern DRAM slot is usually rated maximum for ~18W. The only modules in that power window are Intel’s Optane DCPMM, but a 256 GB DDR4 module would be in that ~10+ W range. For a 2 TB add-in CXL module like this, I suspect we are looking at around 70-80 W, and so to add that amount of DRAM through the CXL interface would likely require active cooling as well as the big heatsink that these renders suggest.Samsung doesn’t give any details about the module they are unveiling, except that it is CXL based and has DDR5 in it. Not only that, but the ‘photos’ provided look a lot like renders, so it’s hard to state if they have an aesthetic unit available for photography, or if there’s simply a working controller in a bring-up lab somewhere that has been validated on a system.Update: Samsung has confirmed these are live shots, not renders.As part of the announcement Samsung quoted AMD and Intel, indicating which partners they are more closely working with, and what they have today is being validated on Intel next-gen servers. Intel’s next-gen servers, Sapphire Rapids, are due to launch at the end of the year, in line with the Aurora supercomputing contract set to be initially shipped by year end.Related ReadingCompute eXpress Link 2.0 (CXL 2.0) Finalized: Switching, PMEM, SecurityCXL Consortium Formally Incorporated, Gets New Board Members & CXL 1.1 SpecificationCXL Specification 1.0 Released: New Industry High-Speed Interconnect From IntelIntel Agilex: 10nm FPGAs with PCIe 5.0, DDR5, and CXLSynopsys Demonstrates CXL and CCIX 1.1 over PCIe 5.0: Next-Gen In ActionMicrochip Announces PCIe 5.0 And CXL RetimersDDR5 Memory Specification Released: Setting the Stage for DDR5-6400 And BeyondHere's Some DDR5-4800: Hands-On First Look at Next Gen DRAMInsights into DDR5 Sub-timings and Latencies\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16670/using-a-pcie-slot-to-install-dram-new-samsung-cxlmem-expansion-module\n",
      "Title: Intel’s Integrated Graphics Mini-Review: Is Rocket Lake Core 11th Gen Competitive?\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-05-07T14:20:00Z\n",
      "URL: https://www.anandtech.com/show/16658/intels-integrated-graphics-minireview-is-rocket-lake-core-11th-gen-competitive\n",
      "Content: In the last few months we have tested the latest x86 integrated graphics options on the desktop from AMD, with some surprising results about how performant a platform with integrated graphics can be. In this review, we’re doing a similar test but with Intel’s latest Rocket Lake Core 11thGen processors. These processors feature Intel’s Xe-LP graphics, which were touted as ‘next-generation’ when they launched with Intel’s mobile-focused Tiger Lake platform. However, the version implemented on Rocket Lake has fewer graphics units, slower memory, but a nice healthy power budget to maximize. Lo, Intel set forth for battle.When a CPU meets GPUIntel initially started integrating graphics onto its systems in 1999, by pairing the chipset with some form of video output. In 2010, the company moved from chipset graphics to on-board processor graphics, enabling the graphics hardware to take advantage of a much faster bandwidth to main memory as well as a much lower latency. Intel’s consumer processors now feature integrated graphics as the default configuration, with Intel at times dedicating more of the processor design to graphics than to actual cores.Intel CPUs: IGP as a % of Die AreaAnandTechExampleLaunchedCoresIGPSizeIGP asDie Area %Sandy Bridgei7-2600KJan 20114Gen6GT211%Ivy Bridgei7-3770KApril 20124Gen7GT229%Haswelli7-4770KJune 20134Gen7.5GT229%Broadwelli7-5775CJune 20154Gen8GT3e48%Skylakei7-6700KAug 20154Gen9GT236%Kaby Lakei7-7700KJan 20174Gen9GT236%Coffee Lakei7-8700KSept 20176Gen9GT230%Coffee Lakei9-9900KOct 20188Gen9GT226%Comet Lakei9-10900KApril 202010Gen924 EUs22%Rocket Lakei9-11900KMarch 20218Xe-LP32 EUs21%Mobile CPUsIce Lake-Ui7-1065G7Aug 20194Gen1164 EUs36%Tiger Lake-Ui7-1185G7Sept 20204Xe-LP96 EUs32%All the way from Intel’s first integrated graphics to its 2020 product line, Intel was reliant on its ‘Gen’ design. We saw a number of iterations over the years, with updates to the function and processing ratios, with Gen11 featuring heavily in Intel’s first production 10nm processor, Ice Lake.The latest graphics design however is different. No longer called ‘Gen’, Intel upcycled its design with additional compute, more features, and an extended effort for the design to scale from mobile compute all the way up to supercomputers. This new graphics family, known as Xe, is now the foundation of Intel’s graphics portfolio. It comes in four main flavors:Xe-HPC for High Performance Computing in SupercomputersXe-HP for High Performance and Optimized FP64Xe-HPG for High Performance Gaming with Ray TracingXe-LP for Low Power for Integrated and Entry LevelIntel has initially rolled out its LP designs into the market place, first with its Tiger Lake mobile processors, then with its XeMAX entry level notebook graphics card, and now with Rocket Lake.Xe-LP, A Quick RefresherIntel’s LP improves on the previous Gen11 graphics by reorganizing the base structure of the design. Rather than 7 logic units per execution unit, we now have 8, and LP’s front-end can dispatch up two triangles per clock rather than one. The default design of LP involves 96 execution units, split into a centralized ‘slice’ that has all the geometry features and fixed function hardware, and up to 6 ‘sub-slices’ each with 16 logic units and 64 KiB of L1 cache. Each variant of LP can then have up to 96 execution units in a 6x16 configuration.Execution units now work in pairs, rather than on their own, with a thread scheduler shared between each pair. Even with this change, each individual execution unit has moved to an 8+2 wide design, with the first 8 working on FP/INT and the final two on complex math. Previously we saw something more akin to a 4+4 design, so Intel has rebalanced the math engine while also making in larger per unit. This new 8+2 design actually decreases the potential of some arithmetic directly blocking the FP pipes, improving throughput particularly in graphics and compute workloads.The full Tiger Lake LP solution has all 96 execution units, with six sub-slices each of 16 execution units (6x16), Rocket Lake is neutered by comparison. Rocket Lake has 4 sub-slices, which would suggest a 64 execution unit design, but actually half of those EUs are disabled per sub-slice, and the final result is a 32 EU implementation (4x8). The two lowest Rocket Lake processors have only a 3x8 design. By having only half of each sub-slide active, this should in theory give more cache per thread during operation, and provides less cache pressure. Intel has enabled this flexibility presumably to provide a lift in edge-case graphics workloads for the parts that have fractional sub-slices enabled.Xe-LP also comes with a revamped media engine. Along with a 12-bit end-to-end video pipeline enabling HDR, there is also HEVC coding support and AV1 decode, the latter of which is a royalty-free codec providing reported similar or better quality than HEVC. Intel is the first desktop IGP solution to provide AV1 accelerated decode support.Rocket Lake ComparisonsFor this review, we are using the Core i9-11900K, Core i7-11700K, and Core i5-11600K. These three are the highest power processors in Intel’s Rocket Lake lineup, and as a result they support the highest configuration of LP graphics that Intel provides on Rocket Lake. All three processors have a 4x8 configuration, and a turbo frequency up to 1300 MHz.Intel Integrated GraphicsAnandTechCore i911900KCore i711700KCore i511600KCore i910900KCores8 / 168 / 166 / 1210 / 20Base Freq3500 MHz3600 MHz3900 MHz3700 MHz1T Turbo5300 MHz5000 MHz4900 MHz5300 MHzGPU uArchXe-LPXe-LPXe-LPGen 11GPU EUs32 EUs32 EUs32 EUs24 EUsGPU Base350 MHz350 MHz350 MHz350 MHzGPU Turbo1300 MHz1300 MHz1300 MHz1200 MHzMemoryDDR4-3200DDR4-3200DDR4-3200DDR4-2933Cost (1ku)$539$399$262$488Our comparison points are going to be Intel’s previous generation Gen11 graphics, as tested on the Core i9-10900K which has a 24 Execution Unit design, AMD’s latest desktop processors, a number of Intel’s mobile processors, and a discrete graphics option with the GT1030.In all situations, we will be testing with JEDEC memory. Graphics loves memory bandwidth, and CPU memory controllers are slow by comparison to mobile processors or discrete cards; while a GPU might love 300 GB/s from some GDDR memory, a CPU with two channels of DDR4-3200 will only have 51.2 GB/s. Also, that memory bank needs to be shared between CPU and GPU, making it all the more complex. The use case for most of these processors on integrated graphics will often be in prebuilt systems designed to a price. That being said, if the price of Ethereum keeps increasing, integrated graphics might be the only thing we have left.The goal for our testing comes in two flavors: Best Case and Best Experience. This means for most benchmarks we will be testing at 720p Low and 1080p Max, as this is the area in which integrated graphics is used. If a design can’t perform at 720p Low, then it won’t be going anywhere soon, however if we can achieve good results at 1080p Max in certain games, then integrated graphics lends itself as a competitive option against the basic discrete graphics solutions.If you would like to see the full CPU review of these Rocket Lake processors, please read our review:Intel Rocket Lake (14nm) Review: Core i9-11900K, Core i7-11700K, and Core i5-11600KPages In This ReviewAnalysis and CompetitionIntegrated Graphics GamingConclusions and Final Words\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16658/intels-integrated-graphics-minireview-is-rocket-lake-core-11th-gen-competitive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Announces Neoverse V1, N2 Platforms & CPUs, CMN-700 Mesh: More Performance, More Cores, More Flexibility\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-04-27T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16640/arm-announces-neoverse-v1-n2-platforms-cpus-cmn700-mesh\n",
      "Content: 2020 has been an extremely successful year for Arm’s infrastructure and enterprise endeavours, as it was the year where we’ve seen fruition of the company’s “Neoverse” line of CPU microarchitectures hit the market in the form of Amazon’snew Graviton2 designas well asAmpere’s Altraserver processor. Arm had first introduced the Neoverse N1 back in early 2019 and if you weren’t convinced of the Arm server promise with the Graviton2, the more powerful and super-sized Altra certainly should have turned some heads.Inarguably the first generation of Arm servers that are truly competitive at the top end of performance, Arm is now finally achieving a goal the company has had in their sights for several years now, gaining real market share against the x86 incumbents.Fast-forward to 2021, the Neoverse N1 design today employed in designs such as the Ampere Altra is still competitive, or beating the newest generation AMD or Intel designs – a situation that which a few years ago seemed farfetched. We recommend catching up on these important review pieces over the last 2 years to get an accurate picture of today’s market:Arm Announces Neoverse N1 & E1 Platforms & CPUs: Enabling A Huge Jump In Infrastructure PerformanceAmazon's Arm-based Graviton2 Against AMD and Intel: Comparing Cloud ComputeThe Ampere Altra Review: 2x 80 Cores Arm Server Performance MonsterAMD 3rd Gen EPYC Milan Review: A Peak vs Per Core Performance BalanceIntel 3rd Gen Xeon Scalable (Ice Lake SP) Review: Generationally Big, Competitively Small(Note: Y axis left chart starts at 50%)Arm is very open that their main priority with the Neoverse line of products is gaining cloud footprint deployment market share, and as an example of the new-found success is an estimate into Amazon’s own AWS instance additions throughout 2020, where the new Arm-based Graviton2 is said to be the dominant hardware deployment, picking up the majority of share that’s being lost by Intel.Looking towards 2022 and BeyondToday, we’re pivoting towards the future and the new Neoverse V1 and Neoverse N2 generation of products. Armhad already tested the new products last September, teasing a few characteristics of the new designs, but falling short of disclosing more concrete details about the new microarchitectures. Followinglast month’s announcement of the Armv9 architecture, we’re now finally ready to dive into the two new CPU microarchitectures as well as the new CMN-700 mesh network.As presented back in September, this generation of Neoverse CPU microarchitectures differ themselves in that we’re talking about two quite different products, aimed at different goals and market segments. The Neoverse V1 represents a new line-up for Arm, with a CPU microarchitecture that is aiming itself for more HPC-like workloads and designs oriented towards such markets, while the Neoverse N2 is more of a straight-up successor to the Neoverse N1 and infrastructure and cloud deployments in the same way that the N1 sees itself today in products such as the Graviton or Altra processors.For readers who are familiar with Arm’s mobile CPU microarchitectures, there’s definitely very large similarities between the designs – even though Arm’s marketing seems to be oddly reluctant to make such kind of comparisons, which is why I made the above chart which more clearly tries to depict the similarities between design generations.The original Neoverse N1 as seen in the Graviton2 and Altra Q processors had been a derivative, or better said, a sibling microarchitecture, to theCortex-A76, which had been employed in the 2019 generation of Cortex-A76 mobile SoCs such as theSnapdragon 855. Naturally, the Neoverse designs had server-oriented features and changes that aren’t present in the mobile counterparts.Similarly to how the N1 was related to the A76, the new generation V1 and N2 microarchitectures are related to newer designs in the Cortex-portfolio. The V1 is related to the Cortex-X1 which we’ve seen in this year’s new mobile SoCssuch as the Snapdragon 888 or Exynos 2100. The Neoverse N2 on the other hand is related to an upcoming new Cortex-A microarchitecture which we expect to hear more about in the following few months. Throughout the piece today we’ll make a few more references to this generational disconnect between the V1 and N2, and it’s important to remember that the N2 is a newer design, albeit aimed at different performance and efficiency points.This decoupling of design goals between the V1 and N2 for Arm comes through the company’s attempt to target more specific markets where the end products might have different priorities, much like how in the mobile space the new Cortex-X series prioritises per-core performance while the Cortex-A series continues to focus on the best PPA. Similarly, the V1 focuses on maximised performance at lower efficiency, with features such as wider SIMD units (2x256b SVE), while the N2 continues the scale-out philosophy of having the best power-efficiency while still moving forward performance through generational IPC improvements.In today’s piece, we’ll be diving into the new microarchitectural changes of the V1, N2, as well as Arm’s newest generation mesh interconnect IP, the CMN-700, which is expected to serve as the foundation of the next-generation Arm infrastructure processors.Table of contents:A Successful 2020 for Arm - Looking Towards 2022The Neoverse V1 Microarchitecture: X1 with SVE?The Neoverse V1 Microarchitecture: Platform EnhancementsThe Neoverse N2 Microarchitecture: First Armv9 For EnterpriseThe SVE Factor - More Than Just Vector SizePPA & ISO Performance ProjectionsThe CMN-700 Mesh Network - Bigger, More FlexibleEventual Design Performance ProjectionsFirst Thoughts & End Remarks\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16640/arm-announces-neoverse-v1-n2-platforms-cpus-cmn700-mesh\n",
      "Title: Cerebras Unveils Wafer Scale Engine Two (WSE2): 2.6 Trillion Transistors, 100% Yield\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-04-20T18:00:00Z\n",
      "URL: https://www.anandtech.com/show/16626/cerebras-unveils-wafer-scale-engine-two-wse2-26-trillion-transistors-100-yield\n",
      "Content: The last few years has seen a glut of processors enter the market with the sole purpose of accelerating artificial intelligence and machine learning workloads. Due to the different types of machine learning algorithms possible, these processors are often focused on a few key areas, but one thing limits them all – how big you can make the processor. Two years ago Cerebras unveiled a revolution in silicon design: a processor as big as your head, using as much area on a 12-inch wafer as a rectangular design would allow, built on 16nm, focused on both AI as well as HPC workloads. Today the company is launching its second generation product, built on TSMC 7nm, with more than double the cores and more than double of everything.Second Generation Wafer Scale EngineThe new processor from Cerebras builds on the first by moving to TSMC’s N7 process. This allows the logic to scale down, as well as to some extent the SRAMs, and now the new chip has 850,000 AI cores on board. Basically almost everything about the new chip is over 2x:Cerebras Wafer ScaleAnandTechWafer ScaleEngine Gen1Wafer ScaleEngine Gen2IncreaseAI Cores400,000850,0002.13xManufacturingTSMC 16nmTSMC 7nm-Launch DateAugust 2019Q3 2021-Die Size46225 mm246225 mm2-Transistors1200 billion2600 billion2.17x(Density)25.96 mTr/mm256.246 mTr/mm22.17xOn-board SRAM18 GB40 GB2.22xMemory Bandwidth9 PB/s20 PB/s2.22xFabric Bandwidth100 Pb/s220 Pb/s2.22xCost$2 million+arm+leg‽As with the original processor, known as the Wafer Scale Engine (WSE-1), the new WSE-2 features hundreds of thousands of AI cores across a massive 46225 mm2of silicon. In that space, Cerebras has enabled 2.6 trillion transistors for 850,000 cores - by comparison, the second biggest AI CPU on the market is ~826 mm2, with 0.054 trillion transistors. Cerebras also cites 1000x more onboard memory, with 40 GB of SRAM, compared to 40 MB on the Ampere A100.Me with Wafer Scale Gen1 - looks the same, but with less than half the cores.The cores are connected with a 2D Mesh with FMAC datapaths. Cerebras achieves 100% yield by designing a system in which any manufacturing defect can be bypassed – initially Cerebras had 1.5% extra cores to allow for defects, but we’ve since been told this was way too much as TSMC's process is so mature. Cerebras’ goal with WSE is to provide a single platform, designed through innovative patents, that allowed for bigger processors useful in AI calculations but has also been extended into a wider array of HPC workloads.Building on First Gen WSEA key to the design is the custom graph compiler, that takes pyTorch or TensorFlow and maps each layer to a physical part of the chip, allowing for asynchronous compute as the data flows through. Having such a large processor means the data never has to go off-die and wait in memory, wasting power, and can continually be moved onto the next stage of the calculation in a pipelined fashion. The compiler and processor are also designed with sparsity in mind, allowing high utilization regardless of batch size, or can enable parameter search algorithms to run simultaneously.For Cerebras’ first generation WSE is sold as a complete system called CS-1, and the company has several dozen customers with deployed systems up and running, including a number of research laboratories, pharmaceutical companies, biotechnology research, military, and the oil and gas industries. Lawrence Livermore has a CS-1 paired to its 23 PFLOP ‘Lassen’ Supercomputer. Pittsburgh Supercomputer Center purchased two systems with a $5m grant, and these systems are attached to their Neocortex supercomputer, allowing for simultaneous AI and enhanced compute.Products and PartnershipsCerebras sells complete CS-1 systems today as a 15U box that contains one WSE-1 along with 12x100 GbE, twelve 4 kW power supplies (6 redundant, peak power about 23 kW), and deployments at some institutions are paired with HPE’s SuperDome Flex. The new CS-2 system shares this same configuration, albeit with more than double the cores and double the on-board memory, but still within the same power. Compared to other platforms, these processors are arranged vertically inside the 15U design in order to enable ease of access as well as built-in liquid cooling across such a large processor. It should also be noted that those front doors are machined from a single piece of aluminium.The uniqueness of Cerebras’ design is being able to go beyond the physical manufacturing limits normally presented in manufacturing, known as the reticle limit. Processors are designed with this limit as the maximum size of a chip, as connecting two areas with a cross-reticle connection is difficult. This is part of the secret sauce that Cerebras brings to the table, and the company remains the only one offering a processor on this scale – the same patents that Cerebras developed and were awarded to build these large chips are still in play here, and the second gen WSE will be built into CS-2 systems with a similar design to CS-1 in terms of connectivity and visuals.The same compiler and software packages with updates enable any customer that has been trialling AI workloads with the first system to use the second at the point at which they deploy one. Cerebras has been working on higher-level implementations to enable customers with standardized TensorFlow and PyTorch models very quick assimilation of their existing GPU code by adding three lines of code and using Cerebras’ graph compiler. The compiler then divides the whole 850,000 cores into segments of each layer that allow for data flow in a pipelined fashion without stalls. The silicon can also be used for multiple networks simultaneously for parameter search.Cerebras states that with having such a large single chip solution means that the barrier to distributed training methods across 100s of AI chips is now so much further away that this excess complication is not needed in most scenarios – to that, we’re seeing CS-1 deployments of single systems attached to supercomputers. However, Cerebras is keen to point out that two CS-2 systems will deliver 1.7 million AI cores in a standard 42U rack, or three systems for 2.55 million in a larger 46U rack (assuming there’s sufficient power for all at once!), replacing a dozen racks of alternative compute hardware.At Hot Chips 2020, Chief Hardware Architect Sean Lie stated that one of Cerebras' key benefits to customers was the ability to enable workload simplification that previously required racks of GPU/TPU but instead can run on a single WSE in a computationally relevant fashion.As a company, Cerebras has ~300 staff across Toronto, San Diego, Tokyo, and San Francisco. They have dozens of customers already with CS-1 deployed and a number more already trialling CS-2 remotely as they bring up the commercial systems. Beyond AI, Cerebras is getting a lot of interest from typical commercial high performance compute markets, such as oil-and-gas and genomics, due to the flexibility of the chip is enabling fluid dynamics and other compute simulations. Deployments of CS-2 will occur later this year in Q3, and the price has risen from ~$2-3 million to ‘several’ million.With Godzilla for a size referenceRelated ReadingCerebras Wafer Scale Engine News: DoE Supercomputer Gets 400,000 AI Cores342 Transistors for Every Person In the World: Cerebras 2nd Gen Wafer Scale Engine TeasedCerebras’ Wafer Scale Engine Scores a Sale: $5m Buys Two for PSCHot Chips 2020 Live Blog: Cerebras WSE Programming (3:00pm PT)Hot Chips 2019 Live Blog: Cerebras' 1.2 Trillion Transistor Deep Learning Processor\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16626/cerebras-unveils-wafer-scale-engine-two-wse2-26-trillion-transistors-100-yield\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AI Funding Spree: +$300m for Groq, +$676m for SambaNova\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-04-19T11:00:00Z\n",
      "URL: https://www.anandtech.com/show/16627/ai-funding-spree-300m-for-groq-676m-for-sambanova\n",
      "Content: The growth of AI has seen a resurgence in venture capital funding for silicon start-ups. Designing AI silicon for machine learning, both for training and inference, has become hot property in Silicon Valley, especially as machine learning compute and memory requirements are coalesced into tangible targets for this silicon to go after. A number of these companies are already shipping high performance processors to customers, and are looking for further funding to help support customers, expand the customer base, and develop next generation products until profitability happens, or the company is acquired. The two latest funding rounds for AI silicon were announced in this past week.Groq (Series C, $300m, Tensor Streaming Processor)When Groq’s first product came onto the scene, detailed bythe Microprocessor Reportback in January 2020, it was described as the first PetaOP processor that eschewed traditional many-core designs and instead implemented a single VLIW-like core with hundreds of functional units. In this method, the data is subject to instruction flow, rather than instructions being reliant on data flow, saving time on synchronicity and decode overhead that many-core processors require.The end result is a product that implements 400,000 multiply-accumulate units, but the key marketing metric is the deterministic performance. Using this single core methodology, the Groq Chip 1 will take the same time to inference workload without any quality-of-service requirements. In speaking with CEO Jonathan Ross, Groq’s TSP enables workloads that were previously unusable due to long tail quality of service performance degradation (i.e. worst case results take too long). This is especially important in analysis that requires batch size 1, such as video.The Groq ecosystem also means that distribution across many TSPs simply scales out inferences per second, with multiple Groq Chip 1 parts under the same algorithm all implementing the same deterministic performance.Jonathan stated to us, as the company has stated in the past, that Groq as a company was built on a compiler-first approach. Historically this sort of approach puts a lot of pressure on the compiler doing the optimization (such as Itanium and other VLIW processors), and often leads to concerns about the product as a whole. However, we were told that the team never touched any silicon design until six months into the software and compiler work, allowing the company to lock down the key aspects of the major ML frameworks before even designing the silicon.As part of its funding efforts, Groq reached out to us for a company update. All of Groq’s hardware and software work to date has been achieved through two rounds of VC funding, totaling $67.3m, with about $50m being used so far. In that capital they have designed, built, and deployed the Groq Chip 1 TSP to almost a dozen customers, including the audio/visual industry, datacenter, and government labs. The second generation product is also well underway. This latest Series C funding round of $300m, led by Tiger Global Management and D1 Capital, will allow the company to expand from 120 people to 250 by the end of the year, support current and future customers with bigger teams, and enable a progressive roadmap.Groq stated in our briefing that its second generation product will build on its unique design points, offering alternatives for customers that were interested in the Groq Chip 1 but have other requirements for their workloads. Each generation of Groq’s TSP, according to the company, will have half a dozen unique selling points in the market (some public, some not), with one goal at least to displace as many GPUs as possible with a single TSP in order to give customers the best TCO.SambaNova (Series D, $676m, Cardinal AI)The second company this week is SambaNova, whose Series D funding is a staggering $676 million, led by SoftBank’s Vision Fund 2, with new investors Temasek and GIC, joining existing backers such as BlackRock, Intel Capital, GV (formerly Google Ventures) and others. To date SambaNova has generated over $1.1 billion in investment, enabling a $5 billion valuation.SambaNova’s entry into the AI silicon space is with its Cardinal AI processor. Rather than focusing on machine learning inference workloads, such as trying to identify animals with a known algorithm, the Cardinal AI processor is one of the few dedicated implementations to provide peak training performance. Training is a substantially harder problem than inference, especially as training algorithms are constantly changing and requirements for the biggest datasets are seemingly ever increasing.The Cardinal AI processor has already featured on AnandTech, whenSambaNova announced its eight-socket solutionknown as the ‘DataScale SN10-8R’. In a quarter rack design, an EPYC Rome x86 system is paired with eight Cardinal processors backed by 12 terabytes of DDR4-3200 memory, and SambaNova can scale this to a half-rack or full-rack solution. Each Cardinal AI processor has 1.5 TB of DDR4, with six memory channels for 153 GB/s bandwidth per processor. Within each eight socket configuration, the chips are connected in an all-to-all fashion with 64x PCIe 4.0 lanes to dedicated switching network silicon (like an NVSwitch) for 128 GB/s in each direction to all other processors. The protocol being used over PCIe is custom to SambaNova. The switches also enable system-to-system connectivity that allows SambaNova to scale as required. SambaNova is quoting that a dual-rack solution will outperform an equivalent DGX-A100 deployment by 40% and will be at a much lower power, or enable companies to coalesce a 16-rack 1024 V100 deployment into a single quarter-rack DataScale system.SambaNova’s customers are looking for a mix of private and public cloud options, and as a result the flagship offering is a Dataflow-as-a-Service product line allowing customers a subscription model for AI initiatives without purchasing the hardware outright. These subscription systems can be deployed internally to the company with the subscription, and be managed remotely by SambaNova. The company cites that TensorFlow or PyTorch workloads can be rebuilt using SambaNova’s compiler in less than an hour.SambaNova has not given many more details on its architecture as yet, however they do state that SambaNova can enable AI training that requires large image datasets (50000x50000 pixel images, for example) for astronomy, oil-and-gas, or medical imaging that often require losing resolution/accuracy for other platforms. The Cardinal AI processor can also perform in-the-loop training allowing for model reclassification and optimization of inference-with-training workloads on the fly by enabling a heterogeneous zerocopy-style solution – GPUs instead have to memory dump and/or kernel switch, which can be a significant part of any utilization analysis.The company has now been through four rounds of funding:Series A, $56m, led by Walden International and Google VenturesSeries B, $150m, led by Intel CapitalSeries C, $250m, led by BlackRockSeries D, $676m, led by SoftBankThis puts SambaNova almost at the top of AI chip funding with $1132m, just behind Horizon Robotics ($1600m), but ahead ofGraphCore($460m), Groq ($367m),Nuvia($293m, acquired by Qualcomm),Cambricon($200m), andCerebras($112m).Related ReadingSambaNova Breaks Cover: $450M AI Startup with 8-Socket AI Training Solutions (and more)Cerebras Wafer Scale Engine News: DoE Supercomputer Gets 400,000 AI CoresNVIDIA Unveils Grace: A High-Performance Arm Server CPU For Use In Big AI SystemsIntel’s New eASIC N5X Series: Hardened Security for 5G and AI Through Structured ASICsQualcomm's Cloud AI 100 Now Sampling: Up to 400TOPs at 75WTSMC and Graphcore Prepare for AI Acceleration on 3nmIntel Whittles Down AI Portfolio, Folds Nervana in Favor of HabanaSamsung Kicks Off Mass Production of AI Chip for Baidu: 260 TOPS at 150 W\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16627/ai-funding-spree-300m-for-groq-676m-for-sambanova\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Intel’s Full Enterprise Portfolio: An Interview with VP of Xeon, Lisa Spelman\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-04-15T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/16608/intels-full-enterprise-portfolio-an-interview-with-vp-of-xeon-lisa-spelman\n",
      "Content: With the launch of Intel’s Third Generation Xeon Scalable platform based on 10nm Ice Lake processors, Intel has upgraded a part of the company that makes the BIG money. For the last few years Intel has been pivoting from a CPU-centric company to a Data-centric company, leveraging the fact that more and more of its product lines are built towards the datacenter mindset. With the launch of the new server platform in this past week, Intel is gearing itself up for an enterprise stack built on 10nm, PCIe 4.0, and cryptographic acceleration.Alongside the new 10nm Ice Lake Xeons, Intel also has Optane persistent memory, 200 gigabit Ethernet, Stratix and Agilex FPGAs, high-performance SSDs with 3D NAND, Optane-based storage, AI hardware with Movidius and Habana, Tofino network switching, eASIC technology, and upcoming Xegraphics. This portfolio combined, according to Lisa Spelman, Intel’s CVP and GM of the Xeon and Memory Group, puts the company in a unique position of offering solutions that the competition can’t enable.In this interview, we ask about Intel’s offerings, the scope of new accelerative features, what really matters to Intel’s customers, and how Intel is approaching its roadmap given the fast follow on from Ice Lake to Sapphire Rapids.Lisa SpelmanIntelIan CutressAnandTechLisa Spelman is categorically what I call an ‘Intel Lifer’, having spent almost 20 years in the company. Her role has evolved very much into one of the faces of Intel’s Data Center business and the Xeon product portfolio, discussing technical aspects of the product lines but also business and marketing strategy in detail. Her previous roles have included being an internal analyst, technical advisor to the CIO, the Director of Client Services, and Director of Datacenter Marketing.IC: Ice Lake Xeon is finally here with its Sunny Cove core design. There are optimizations for the server industry, for AI, up to 40 cores, new security features, and higher memory performance. Historically Intel has taken its Xeon Scalable processors to every corner of the market - is this still true with the new Ice Lake Xeon platform?LS:I think you’ve hit on some of those key features and benefits. What we’re doing here is we’re updating the platform, we’re updating the processor, and we’re updating a bunch of our portfolio all at once, which we think delivers a tremendous amount of customer value. To your question about having Intel in every corner of the market - we think it is a differentiator of ours. Where we are really focused starts with hitting edge workloads and delivering through the network, and driving further network transformation onto Xeon [from edge to cloud]. We’re delivering in the core of the datacenter, and we’re delivering in cloud and high performance computing. We’re continuing to seek to expand the services and capabilities that we can offer for customers, and just deliver platform consistency across so many of their use cases.[Note that] I never call Xeon a server product! I worked so hard to get it changed into being called a true datacenter product and now it’s actually even extended out beyond that, to sitting in so many edge and ruggedized environments. So I take great pride and joy in seeing a Xeon on every base station and on every telephone pole!IC: These extra markets that Intel plays in, are they a big opportunity for revenue growth? It’s been a big feature of what previous CEO Bob Swan and what new CEO Pat Gelsinger has been saying recently.LS:It’s a revenue opportunity and it’s a customer opportunity. It allows us to address more of their needs, and actually it allows us to play a more important role in our customer success. We’re not just in the datacenter, we’re spanning all of the ways in which our customers are seeking to drive their own value, their own monetization, and so that’s actually some of the funnest stuff to be part of.IC: A lot of focus over the past generations has specifically been on the core performance on common everyday workloads for server customers. Intel is now seemingly focused on solution delivery, enabled through the other parts of the Scalable ecosystem such as Optane DCPMM 200 series, new 800-series Ethernet, Optane SSD and 3D NAND, Agilex FPGAs, eASICs, and the software stack behind DLBoost and OneAPI. Can you go into how customer demand in the ecosystem is shifting from simply a core purchase to a solution-optimized purchase that goes beyond the core?LS:I still describe the Xeon as the most general purpose of all the general purpose solutions! It can literally run anything, and it’ll give you a good out of the box experience on everything. We have spent a tremendous amount of effort and resources in how we can improve in specific areas, and we do target those higher [market] growth areas. You mentioned artificial intelligence, which is an area where we’re investing both on the hardware side, which is super important, but the software is at least equal (if not more important) to tune for performance. [Enabling] that entire portfolio to deliver a solutions mindset has probably been one of our biggest changes, [especially] how to engage with our customers, and by looking at our [customers] and working with them on much more holistic requirements. So our requirements gathering has definitely improved and become much more comprehensive, and then we’ve built some solutions capabilities and solutions offerings on top of it.We have talked about Market Ready Solutions, especially targeted to the Edge, and we talked about Select Solutions for cloud and enterprise and network and HPC use cases. We actually have similar types of programs that are less focused on the branding for our top cloud service provider customers as well, creating cloud engineering efforts that are focused on pairing those custom CPUs with the Optane performance, or the Ethernet 800 Series performance, and really helping them drive more value out of their infrastructure purchases.An area where we’ve been utilizing the FPGA portion of our portfolio is in the move into smart NICs as well, which is a growing area of interest and gives us an opportunity to really holistically address our customer infrastructure management, as well as the monetization they want to do on top of the core.IC: How important or relevant are raw core performance comparisons in the server industry?LS:I still think there is a core of the audience and the industry that wants to hear that, and wants to see what it looks like. I don’t want us to walk away from utilization of those benchmarks. I’m also trying to drive the team [towards] the market that is pairing the benchmark with the actual real-world result when you use either just a CPU Xeon product or the entirety of the Xeon platform. We want to be able to [voice] that translation because customers are in different spots in their journey around a solution-style view. I think it’s important for Intel to continue to drive performance up, and we will continue to report on and discuss standardized benchmarks, but we’re adding into it a lot more of a holistic view of what the portfolio can do. So I think it’s important to meet our customers and our audiences where they’re at, instead of just dictating where we’re at.IC: Intel’s server products have been using AVX-512 since the days of Xeon Phi, and then it was introduced on Xeon processors from Skylake. We are now also seeing AVX-512 roll out on the consumer desktop processors for the first time. There’s still some confusion as to how pervasive AVX-512 is, especially when it comes to Enterprise and Cloud Service Provider (CSP) code bases. What pick-up trends are you seeing with the feature, and how critical it will be as Intel progresses forward with platforms like Ice Lake.LS:AVX-512 is such a great feature. It has tremendous value for our customers that use it, and sometimes I marvel when I read your audience’s comments about the raucous debate they will have about the value of AVX-512. But I tell you, and no joke, that a week and a half or so before launch I was looking at a list of the deal wins in one of our geographies. 70% of those deal wins, the reason listed by our salesforce for that win was AVX-512. Optimization is real.What happens though, as with AVX-512 so far and with SGX in the future, is that when you launch and announce something new, despite our 10,000+ software engineers and our coverage of the ecosystem, it can be hard to get all of the stack to take advantage of one of those hardware features. It’s really hard to get that completely enabled. It has taken us years [for AVX-512] and there is more work to be done.So has every customer that could benefit from AVX-512 had access to that because their software stack is completely ready? No. But we have a tremendous amount [of more customers] this year that weren’t able to utilize AVX-512 in 2011, or 2012, or 2015, or 2017. Enablement just keeps growing, year on year.If we link it to Software Guard Extensions (SGX), we now have the larger size enclave available on Xeon Scalable. We’ve had SGX in the market on our Xeon-E product line for a few generations, and that has allowed a tremendous amount of industry preparation and adoption. It also allows you to see who is really deeply interested and committed in building that confidential computing foundation. So now we are [moving the feature] into the larger socket, with a larger footprint, a larger enclave size, and people can start to actually scale their deployments pretty quickly because they’re ready.So you look at cloud service providers that already have confidential computing instances, like Microsoft, IBM, Alibaba, Baidu, and then you look at these enterprises, like Royal Bank of Canada, PayPal, VISA, UCSF, German Healthcare - these companies are well underway to deployment because they have had the time and now they can move it into even greater scale.It all leads back to how I think of how I’m working with our team on overall roadmap management. We have seen, and we are driving, a shift over these past few years to focus more concretely on meaningful feature advancement, [rather than simply] a laundry list of features. We are much more focused and prescriptive about looking at our roadmaps. If we add a feature, we have to consider what it would take to have that feature utilized, and how would it benefit a customer, rather than simply what it takes to put it in the hardware. That’s a shift in an Intel mindset and Intel thinking, but I think it will benefit not only ourselves, but our customers as well. They will see more stuff [over time], and it will be more ready in the market when it arrives.IC: SGX, Software Guard Extensions, is something you have had in Xeon-E for a while. Now that the feature is moving up into the wider Xeon stack, Intel has segmentation with enclave size. Some SKUs have 8 GB maximum enclaves, some doing 64 GB, and some doing 512 GB. Can you speak to reasons why this segmentation matters to Intel, but also can you speak about other security elements that the platform is enabling?LS:On the enclave size segmentation, this is about giving customers choice and opportunity. They have the opportunity to think through what type of deployment that they are going to need, what that type of enclave size [they will need], and what type of data they are trying to manage. [It is about] what type of applications are they trying to more robustly secure, and this segmentation gives them that optionally.[It is worth noting that] it is the first time we are bringing that choice into the market, and so we’ll learn. We will get feedback from our customers, and those with which we have worked with in our ecosystem of cloud and OEM partners as we build out what the SKU stack looks like. But now [with the new feature] we will get all of that real-world customer deployment information, and then we will make adjustments as we go forward. You’ve been through enough of these launches with us, and seeing how we launch new features, where we start with an effort and then we refine it over time as we see and address the market adoption. So we think this will give customers a great foundation to start with, and we will see even more customers transition their existing PoCs (proof of concepts) from Xeon E to Xeon scalable deployments. We will also see a new wave of proof of concepts for customers that were waiting for a little bit more of that true market readiness with a two socket system. So I think that it will give us an opportunity to learn and grow in the market together [with our customers]. So that is one of our biggest security advancements - adding that hardware based addition to our customer’s software security portfolio.We are also adding further crypto acceleration, and this is an area that Intel’s been invested in for several years to a decade or more. We do see it as a true differentiator for us. We have a brilliant team that focuses in this space, and works on both the hardware and the cryptographic algorithms as well software support for the hardware. It’s also an area of really tight collaboration with our customers. As you heard me say at [Ice Lake Xeon-SP] launch, the race is on to get as much data as possible encrypted, and keep it under greater security. I think that this is going to continue to build upon very important feature sets, and enable greater capability that our customers benefit from already.IC: Speaking about the customers, historically when Intel comes out with a Xeon platform launch, it scales all the way from one socket to eight sockets. We’ve now got a situation where with 3rd Generation Xeon Scalable, we have a split, where you’ve got Ice Lake for up to two sockets on 10 nm, and Cooper Lake on four to eight sockets on 14 nm. Can you explain reasons for splitting the markets? Are there technical limitations, or is it customer demand, or is there something else in the mix? Is this indicative of what we might see in the future, or will Intel be coalescing back again?LS:I think that we are building an even greater flexibility in our portfolio. You will see a lot of optionality that we create with some of the stuff CEO Pat Gelsinger talked about at Intel Unleashed a few weeks ago. Pat talked about [being able to] meet customer needs that might not be quite so specific. [In discussions with our customers,] we had an opportunity to refresh and bring forth some new capabilities in that four socket and above space ahead of Ice Lake. We made the choice to take advantage of that opportunity, add some new capabilities (like BFLOAT16), and meet some very specific high volume customer requirements that we had in that space ahead of getting Ice Lake out into the market.Given the size of the traditional four socket enterprise market, and the specific customers, doubling our investment in Ice Lake or 3rd Gen Xeon Scalable to enable another four socket platform so quickly seemed like too much to manage for the value that platform would offer. So we had delivered the right capabilities with the first portion of the 3rd Gen portfolio (Cooper Lake) for a refresh to meet a couple of really big customer requirements. [This enables us] to have the ecosystem focus on their Ice Lake two socket systems first, but then moving on to our next generation of Sapphire Rapids which will cover the whole of the stack.So [Cooper Lake] realized an opportunity we had, driven by customer demand. It was not an eternal strategy to separate the [dual socket and quad socket markets], but as we increase our ability and pace to bring new technology to market, we may still look at how often the traditional four socket market needs that level of refresh.IC: Speaking about those select customers, in the past at the Data Centric Innovation Day, I remember a slide which had a pie chart indicating all the Skylake Xeons that Intel makes. More than half of them in that graph were customized to a specific customer needs, especially these big high profile customers, the cloud service providers and the hyperscalers. Would you say that you’re still 50%+ customizing for those sorts of big customers? Can you talk about how that has evolved?LS:It’s still more than 50%, and I would say that you will continue to see that grow. What we’re also trying to do is add a level of customization for all those markets.Across the main roadmap of SKUs you will see the SGX enclave size opportunity, but we also have SKUs that are Cloud optimized for VM utilization, we have Networking and NFV optimized SKUs, the media processing optimized opportunities, and some long life and more ruggedized use case processors for edge workloads. So we’re trying to provide a level of customization even within the main SKU stack for those that have that kind of precision targeting of their infrastructure for their workload.We do still deliver 50%+ customized parts, and it will continue to grow our customized volume. Over time we have grown our capabilities in this space, and I have a separate team for this that we’ve invested in that manages our custom SKUs with our major customers. This allows them to really tune and target for their environments. It started out as a couple of knobs to tune, and now it has grown and we have the capability and ability and willingness to do full on customization up to the IP level.What we do with these SKUs, when we manufacture them, we create them, and as we deploy them, we actually leave it up to our customers for how much of the detail that they want to expose [to the press or customers]. That co-optimization is often a lot of their own significance in how they differentiate themselves against competitors. So we consider that a lot of their ability to determine how much they want to share with the world. For Intel, we focus on the main performance benchmarks, and everything we do is on the main line available to everyone.IC: So it’s interesting that you bring up the new separate SKUs. Beyond the network-focused ones, which you had in the previous generation, you now have a media focused SKU. What’s that all about? What’s that got in it? What is special?LS:If you think about the workload, we’re trying to target the right core count, the right frequency, and then the mix between single-core versus all-core turbos. It’s also about how much cache is in there, and what the thermals look like for those customers.In this case, across the media processing and optimized workloads, we have a team that is focused on this workload and these key customers and as they gather requirements. Requirements come from the CSP operators, online gaming operators, and all of those types of customers. [We want to enable] the most ideal configuration for guaranteeing quality of service and allowing for as much performance to be delivered before moving potentially to offload accelerators. So for those are the hardware configurations we have those knobs we might turn to create that SKU.The second thing we’ll do is we focus our software work for them. So as we do encoding support, software support, we try to make sure they’re updated to take advantage of that specific SKU and that configuration. You will obviously get benefits across any SKU you choose, but we’ll do the greater tuning and optimization for code targeted on this processor. So on a standard benchmark you might get X performance, but if you take the media SKU version against that, you will likely get X plus a performance boost, could be 10%, depending on the exact case.IC: What’s this liquid cooled processor in the product stack all about! Is that for HPC?LS:You never know who is out there that’s going to experiment as well! But yes, definitely focused that one towards HPC. I still haven’t personally had the opportunity to dip my hand in a tank of something and pull out a Xeon, but I’m looking forward to it!IC: Sorry when I say liquid cooled, I thought it meant some sort of like a closed loop liquid cooler. You’re talking about immersion cooling?LS:We have customers that are looking to do all of the above! So we have a customer in every corner of the earth that wants to push the boundaries on all of it on everything. The place where we see the most traction and interest on this is in the high performance computing community. As their capabilities have continued to progress and grow, we have some of our OEMs that have built up a lot of specialization in this space, and use this CPU for their differentiation. We want to support that.IC: As we see the Xeon mainstream server platform moving forward, at the same time Intel is also talking about its enterprise graphics solutions. So is there anything special in Ice Lake here that gets enabled when the two are paired together?LS:We are definitely working on that. I don’t have a ‘3rd Xeon Scalable Better Together’ slogan or anything right now, but as we look out towards Sapphire Rapids and beyond, it is definitely something that we’re working together on. We absolutely want that portfolio value to show up when you put an Intel Xeon with an Intel GPU. One API as you know has a software foundation for it, and then between my team and Jeff McVeigh’s team, and in Raja’s organization, we will work to make sure that we are delivering on that value promise. It’s too big of an opportunity to miss! But I have nothing to reveal for you today!IC: Intel has historically enabled three silicon sizes for its Xeon products: a low core count for entry, a mid-core count for the bulk of the market, and a high core count for compute or cache optimized solutions. Can you describe how Intel is segmenting the Ice Lake silicon this time around, and mention any special segments that customers should be aware of?LS:You know it’s fairly similar in the sense that we obviously have our top highest core count offerings, up to the 40 core, and we have the higher thermal configurations that it can hit. We are driving towards that peak performance, and trying to really drive down latency while improving performance. We also have options that go down to 105 watts. We cover the whole range, like I said, and as you try to hit some of these Edge workloads, you want to offer a whole range of parts. We go down to 8 cores, up to 40 cores, and then like you said, a range of cache sizes in between.I think what we’ll see is we will have customers that will remain on their 2nd gen platforms as well, because they are functioning well, and they might not be as high performance of a buyer as a purchaser. They may value the stability and continuity of staying with the platform instead of moving or refreshing, and we intend to support both of those types of customers in the market. I really think that the way the market has segmented and shifted, the day of that wholesale entire bulk pickup and transition is not happening anymore. But those that ramp the fastest are continuing to ramp really fast, and those customers are a very significant portion of the volume.IC: I’ve done some rough die-size calculations of the biggest Ice Lake chips being offered, making them approximately around 620-660 mm2 - it’s a super large bit of silicon for Intel’s 10nm manufacturing processes. I know you announced 100k+ CPUs have been shipped already before launch, and 200k+ as we went through the launch. But with such a large die, how do you see roll-out of Ice Lake Xeon progressing to OEM partners and specialized customers?LS:I promise I’m not going to be doing like a weekly blog post of the next number! So we actually view this one as going very similar to our other platform transition ramps, as far as rate and pace. We know we have pent up demand to meet [demands in] performance. We know that we have customers that are looking forward to productizing an Intel PCIe Gen 4.0 offering, and we know we have the platform value, with higher memory channels and such. All of that leads to that compelling performance that customers want to go out and get.We’re planning for this to be pretty in line with our traditional ramp, and we have staged our supply to do so. The fact that the Xeon Scalable or an Ice Lake Xeon based die is a big piece of silicon, or the biggest one that Intel produces, is not a feature that’s new to Xeon. Big die sizes are my life! I am very nice to my factory friends, as I ask them for their best yields and highest volume output. What we’re facing right now is worldwide constraints on chip manufacturing and in chip components, so we are trying to use our strategic supply chain capability to address that, and we intend to support the whole of the Ice Lake offering as we ramp. I think we’re actually in a really good position to capitalize on the fact that we are an IDM, and have that IDM advantage.IC: Intel is already making musings about next generation products, especially with the deployment of processors like Sapphire Rapids in the exascale Aurora supercomputer, which is happening at the end of this year or beginning of next. I know you probably won’t talk about Sapphire Rapids, but perhaps you can explain how long-lived the Ice Lake platform will be for customers looking to deploy systems today?LS:I actually see Ice Lake as having an opportunity to live for quite a long time into that Sapphire Rapids generation. I talked about it a little bit earlier, that [it’s not solely about] wholesale moves.If I think back to starting on the cloud service provider journey with these major customers, 10 years ago, at that time they wanted the lowest performance 1U servers. If anything happened, they threw it out and then when we launched the new one, they threw them all out anyway and put all the new ones in. It has changed so much - cloud service providers have moved into true Enterprise-class requirements. They are our highest performance purchasers, they move towards the top of the stack, and they keep products and production longer. [This is true] especially as an ‘Infrastructure as a Service’ provider, where if they have a happy customer, why would they disrupt the happy customer? So they have that need to have a bit of a longer life on the platforms we provide.At the same time, whenever there’s something new that they can use to differentiate [their offering] and create new services, or create value for their customers, they’ll pivot and move towards that. I think what will happen is that a lot of our Ice Lake customers will just continue to offer them now as premium services and fill their stack, then apply Sapphire on top of it. A lot has been made about how close together they are, but this actually is kind of the representation of what it looks like as we move closer towards that four and five quarter cadence that our customers want. So there’s a lot of market chatter as to if they will be too close together, but we really see the Sapphire Rapids timing similar to Ice Lake timing as having that opportunity for customers to ramp one and then start applying the other one on top. So we’ll have to work through that with the industry, but we’ve got a good team focused on it.IC: Ice Lake has launched a lot later than Intel originally planned, there’s no denying that, at a time when there is increased competition in the market from both other x86 designs as well as new Arm designs. While the late launch isn’t ideal, can you describe areas of the supporting portfolio that Intel has been able to accelerate to aggressively support the Ice Lake launch to increase its competitiveness?LS:For a Xeon launch, the important thing is not that you hit a specific date or a marketing moment, it is that you have your ecosystem ready to go. That is what we’ve really been focused on, rather than arbitrary date management. Then again we do intend to capitalize on and continue to build value around the remainder of that portfolio that you mentioned, whether that’s the Optane SSDs, the Optane Persistent Memory, FPGAs.I enjoy quite a bit being with my peers and discussing the deals that are actually being won with Intel, and even held at times with customers based on platform components. So I mentioned ADQ for the Ethernet 800 series - , we have customers that the Xeon Plus ADQ capability and feature is so important to them that even if they were interested or viewing a competitive CPU silicon architecture, they’re sticking and staying with Xeon because that ADQ plus Xeon solution makes such a big difference in their total cost of ownership or their application outcome. We see the same thing with Optane Persistent Memory, and I alluded to some of the work we’re doing with our FPGA based SmartNIC wins. It is such an advantage to have that portfolio, and it’s interesting to see others in the market that are pursuing to do the same. I’m grateful for our multi-year head start in that space, especially as we enter such an incredibly dynamic time in technology and in silicon development. There really is a lot going on right now.IC: Intel is pushing Optane DCPMM 200-series with Ice Lake Xeon as a competitive value addition, however it’s hard to miss that the agreement with Micron has ended, Micron is looking to sell the Leti fab, and Intel’s only commitment to date has been R&D in the Dalian plant in China. Can you speak to Intel’s vision of the future of Optane, especially as it comes to product longevity on Ice Lake and Intel’s future?LS:I think we are on the cusp of so much really great opportunity and continued customer momentum on Optane, and I’ve talked to you before about our proof of concept conversion into revenue. We’re at 80%, and that’s just tremendous for a new-ish technology out in the market, and so I’m really looking forward to getting the 200 series out into customers hands. We’ve got a great pipeline [for Optane], and we’ve got a lot of focus on building that ecosystem, just like we had to do on AVX-512, just like we’ve had to do on SGX, and just like we’re doing here.I know that the Micron stuff was news in the ecosystem, but I have confidence in Intel’s ability to navigate and manage through manufacturing challenges or changes in agreements like that. I guess I’ll just say that particular part of it is not something that I think our customers need to worry about. I think we have a lot of optionality in how to produce and manufacture and deliver the Optane product. So I’m spending like 100% of my Optane time building customer momentum, making sure they’re seen achieving and realizing value from it, rather than working through the challenges on the manufacturing side. I think we’re going to end up in an OK position there.IC: Intel CEO Pat Gelsinger in the recent Intel Unleashed spoke about Intel moving to a tiling concept with some of its product lines. It’s clear that the future is going to be with chiplets, tiles, or just bunches of silicon using advanced 3D packaging, especially for markets that deal with high-performance complex silicon products. Can you dive down into how this is going to affect Intel Xeon, the team, the research, and the future?LS:It is - it’s good, it’s exciting, and it is aligned with our future. Before Pat had finished his first week [at Intel], I already had an opportunity to spend about two and a half hours with him just going over what we’re doing on Xeon - a few things have changed since he was last here! But I had a chance to sit down with him and walk through our long term strategy - where we are at today, what we are working through today, what are we setting ourselves up for in the future, and how are we addressing these customer needs. It was great to get his initial feedback and his reactions. He’s got a customer view that is very valuable to us, coming not only from his silicon background experience, but his last 11 years of really dialing up his software knowledge I think is going to help us so much. The good news was walking out of that meeting. The kind of view from him is that we are on the right track, and we’ve got the right strategy, so now it’s just a desire to push us faster, harder, sooner, better, stronger - all of that good stuff. So I think that Pat’s view of our vision, and this Xeon team that we’ve built over the past couple of years, is really closely coupled.So then to your point, we are going to take advantage of all of those unique and differentiated packaging capabilities in order to deliver in Sapphire Rapids, and deliver in Grand Rapids, and beyond that to some really exciting stuff that we’re working on that allows us to use even more of the cores, even more of the portfolio, together. So it’s definitely put in there.The other thing I’ll say is that over the last year and a half or so, we’ve restructured how we’re building Xeon. It was necessary as we faced some challenges that are well discussed around our 10 nm in the industry. I had an opportunity to move out of more traditional marketing role into having the control of product management and the IP planning and overall roadmap structuring to build this cross company team - the Xeon Leadership team.I’m really excited about this group of leaders, and the people that we have pulled together that I think represent best in class for the industry. The work that we’ve done in the last 18 months to kind of reset the foundation of where we’re going I think will deliver a lot of customer value. It’s going to give us an opportunity to take advantage of EMIB or Foveros or others on the horizon faster. It’s going to make us more competitive across all types of silicon. It’s not just x86 competition or any singular one market, and I look at the way we’ve capitalized on a diversity of thought and a diversity of talent.I’m excited to have leaders like Gloria Leong, VP GM Xeon Performance Group, running all of our silicon design. We have Karin Eibschitz Segal, the VP Design Engineering Group, GM Intel Validation, who’s running our validation. We’ve got Rose Schooler, CVP Data Center Sales and Marketing Group, running all of our sales, and I’ve got Jennifer Huffstatler, GM Data Center Product Management, running strategy for us. Rebecca Weekly, VP and GM of our Hyperscale Strategy and a Senior Principal Engineer of DPG running our cloud for our major cloud customers. We’ve got Niveditha Sundaram, VP Cloud Engineering running our cloud engineering, and Nevine Nassif, Intel Fellow, Sapphire Rapids CPU Lead Architect, who is running our Sapphire Rapids programme. So you might notice a theme with those! I also have wonderful awesome male colleagues that are driving this as well - we’ve got Sailesh Kottapalli, Intel Fellow and Ronak Singhal, Intel Fellow.But this core group of women that I’m really excited about in leadership and technical badass roles out there. They are arm and arm with me and with our partners to crush it and bring so much performance and capability and commitment back to Xeon. I’m excited! So I say to the engineers of the world that if you’re looking for a great team that values diversity of thought and ideas, and experience, and wants to capitalize on everyone’s unique strengths, then Xeon is the place to do amazing and cool work.IC: That almost sounds like a job ad!LS:Almost! I want all the people that have the best ideas that are really looking for a home to make a major impact - I want them to feel welcome here. Having that many amazing diverse technical leaders happens when you’re purposeful about it, and you put them in big roles, and you give them the opportunity and the platform to shine.Many thanks to Lisa Spelman and her team for their time.Many thanks also to Gavin Bonshor for transcription.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16608/intels-full-enterprise-portfolio-an-interview-with-vp-of-xeon-lisa-spelman\n",
      "Title: Xiaomi Mi 11 Lite 5G Performance Report: First Taste of the Snapdragon 780G\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-04-13T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16614/xiaomi-mi-11-lite-5g-performance-report-first-taste-of-the-snapdragon-780\n",
      "Content: It’s only been a couple of weeks since Qualcomm first announced its new “premium” range SoC for 2021: the new Snapdragon 780G. The chip is a successor to the Snapdragon 765 which ended up being a quite successful and popular chip among 2020 mid-range and “premium” segment devices, powering many devices such as for example the Pixel 5 and 4a 5G.Also a couple of weeks ago, Xiaomi had made a flurry of device announcements amongthe Mi 11 Ultraas the new mega-flagship device for 2021. One device we didn’t have time to cover was the new Mi 11 Lite 5G. Beyond the super interesting form-factor of a 6.8mm thick device and weighing in only 159g despite its larger 160.5 x 75.5mm footprint, as well as higher quality OLED display, the phone differentiated itself by being the very first Snapdragon 780G phone in the market.We’ve got our hands on a review sample, and wanted to dedicate a piece focusing a bit more on the SoC, as the new chip will be powering not only the Mi 11 Lite 5G, but undoubtedly a whole lot of other mid-range and premium devices in the coming months.Qualcomm Snapdragon Premium SoCsSoCSnapdragon 765Snapdragon 765GSnapdragon 768GSnapdragon 780GCPU1x Cortex-A76@ 2.3GHz (non-G)@ 2.4GHz (765G)1x Cortex-A76@ 2.2GHz6x Cortex-A55@ 1.8GHz1x Cortex-A76@ 2.8GHz1x Cortex-A76@ 2.4GHz6x Cortex-A55@ 1.8GHz1xCortex-A78@ 2.4GHz3x Cortex-A78@ 2.2GHz4x Cortex-A55@ 1.9GHzGPUAdreno 620Adreno 620+15% perf over 765GAdreno 642+50% perf over 768GDSP / NPUHexagon 696HVX + Tensor5.4TOPS AI(Total CPU+GPU+HVX+Tensor)Hexagon 770Scalar+Tensor+Vector12TOPs AI(Total CPU+GPU+DSP)MemoryController2x 16-bit CH@ 2133MHz LPDDR4X / 17.0GB/sISP/CameraDual 14-bit Spectra 355 ISP1x 192MPor1x 36MP ZSLor2x 22MP with ZSLTriple 14-bit Spectra 570 ISP1x 192MPor1x 84MP ZSLor2x 64+20MP ZSLor3x 25MP ZSLEncode/Decode2160p30, 1080p120H.264 & H.26510-bit HDR pipelinesIntegrated ModemSnapdragon X52Integrated(LTE Category 24/22)DL = 1200 Mbps4x20MHz CA, 256-QAMUL = 210 Mbps2x20MHz CA, 256-QAM(5G NR Sub-6 4x4 100MHz+ mmWave 2x2 400MHz)DL = 3700 MbpsUL = 1600 MbpsSnapdragon X53 Integrated(LTE Category 24/22)DL = 1200 Mbps4x20MHz CA, 256-QAMUL = 210 Mbps2x20MHz CA, 256-QAM(5G NR Sub-6 4x4 100MHz)DL = 3300 MbpsUL = ? MbpsMfc. ProcessSamsung7nm (7LPP)Samsung5nm (5LPE)On paper, the new Snapdragon 780G is a major upgrade from the Snapdragon 765G. The new SoC features not only upgraded CPU microarchitectures, actually jumping from a Cortex-A76 generation to a newer Cortex-A78 generation, but also shifts from a 1+1+4 core setup to a 1+3+4 setup, meaning the new SoC has double the amount of performance CPU cores. The clock frequency remains at a peak 2.4GHz for the fastest core in the configuration.This setup is quite interesting as it’s quite similar to the middle CPU cores of the Snapdragon 888 flagship SoC, which employs the same microarchitecture at roughly the same clocks, and both chips are manufactured on Samsung’s 5nm 5LPE process node. There are differences in the memory setup as the Snapdragon 780G remains on a LPDDR4X interface that’s only 2x16b wide, which is smaller than the LPDDR5 4x16b bus that the flagship SoC gets. There’s also differences in caches such as the L3 cache being much smaller.CPU Performance & EfficiencyComparing the performance and power efficiency of the new chip compared to not only its predecessor, but also its current generation flagship sibling, we should be able to paint a better picture of how the new chip sits in the competitive landscape.In SPEC2006, the new chip – albeit running at the same frequency as the Snapdragon 765G, is showcasing significant performance leaps compared to its predecessor. We’re seeing roughly +34% performance improvements in both integer and floating point test suites,not quite 40% as Qualcomm had proclaimed, but that’s also an “up to” figure depending on the workload.In the performance positioning, we indeed see that the performance core of the Snapdragon 780 roughly matches up to the middle cores of the Snapdragon 888, which is expected given their similar specifications. The S888 wins out ahead due to a stronger memory subsystem.In terms of power and energy efficiency, it’s an interesting situation. In terms of power usage, we see that the performance increases did come with a cost, as the new chip is roughly +20% more power hungry as its predecessor in single-threaded tasks. Due to the performance increase being higher than the power increase however, the energy efficiency of the new chip is actually higher than its predecessor, showcasing 8 to 13% less energy usage to complete a task, which should have direct effects on the battery life of devices.In general, the larger performance increase combined with a reduction in energy usage of the new SoC means that it’s a pretty significant generational leap against the Snapdragon 765.A bit unrelated to the Snapdragon 780, last month Google had finallypushed AArch64 versions of Chrome and the system WebView packages, signifying an important shift away from AArch32 32-bit era. This actually has a larger impact on benchmarks as newer Arm Cortex microarchitectures have more significant differences (relative to past ones) between the two execution modes, with the AArch64 mode being more performant and more efficient. For this reason, we’re starting with a new clean slate of test results with explicitly stated (64b) in the graphs.We’re comparing the Mi 11 Lite 5G to the Pixel 5 here as the Mi 10 Lite 5G with the Snapdragon 765G unfortunately is still being deployed 32-bit packages due to its lower 6GB RAM, and we still want an apples-to-apples comparison.In the JetStream 2 and Speedometer 2.0 browser benchmarks we’re seeing +36-43% performance increases compared to the Snapdragon 765G, which are significant boosts. The Snapdragon 780G still clearly lags behind the performance of the flagship SoCs, signifying that there’s still plenty of performance differentiation to be had between the segments.Excellent User ExperienceIn general, the performance leap of the Snapdragon 780G is quite significant. The new CPU microarchitecture delivers large generational IPC boosts and the fact that the new chip has double the big performance cores also helps out multi-threaded workloads a lot.Truth be told, it’s a vast shift in user experience as the device definitely feels much faster and snappier than Snapdragon 765G devices. It roughly lands in between the Snapdragon 855 and Snapdragon 865 flagship generations in terms of overall device experience, which is not surprising as that’s also exactly where the new chip benchmarks itself at.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16614/xiaomi-mi-11-lite-5g-performance-report-first-taste-of-the-snapdragon-780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NVIDIA Unveils Grace: A High-Performance Arm Server CPU For Use In Big AI Systems\n",
      "Author: Ryan Smith\n",
      "Date Published: 2021-04-12T16:20:00Z\n",
      "URL: https://www.anandtech.com/show/16610/nvidia-unveils-grace-a-highperformance-arm-server-cpu-for-use-in-ai-systems\n",
      "Content: Kicking off another busy Spring GPU Technology Conference for NVIDIA, this morning the graphics and accelerator designer is announcing that they are going to once again design their own Arm-based CPU/SoC. Dubbed Grace – after Grace Hopper, the computer programming pioneer and US Navy rear admiral – the CPU is NVIDIA’s latest stab at more fully vertically integrating their hardware stack by being able to offer a high-performance CPU alongside their regular GPU wares. According to NVIDIA, the chip is being designed specifically for large-scale neural network workloads, and is expected to become available in NVIDIA products in 2023.With two years to go until the chip is ready, NVIDIA is playing things relatively coy at this time. The company is offering only limited details for the chip – it will be based on a future iteration of Arm’s Neoverse cores, for example – as today’s announcement is a bit more focused on NVIDIA’s future workflow model than it is speeds and feeds. If nothing else, the company is making it clear early on that, at least for now, Grace is an internal product for NVIDIA, to be offered as part of their larger server offerings. The company isn’t directly gunning for the Intel Xeon or AMD EPYC server market, but instead they are building their own chip to complement their GPU offerings, creating a specialized chip that can directly connect to their GPUs and help handle enormous, trillion parameter AI models.NVIDIA SoC Specification ComparisonGraceXavierParker(Tegra X2)CPU Cores?82CPU ArchitectureNext-Gen Arm Neoverse(Arm v9?)Carmel(Custom Arm v8.2)Denver 2(Custom Arm v8)Memory Bandwidth>500GB/secLPDDR5X(ECC)137GB/secLPDDR4X60GB/secLPDDR4GPU-to-CPU Interface>900GB/secNVLink 4PCIe 3PCIe 3CPU-to-CPU Interface>600GB/secNVLink 4N/AN/AManufacturing Process?TSMC 12nmTSMC 16nmRelease Year202320182016More broadly speaking, Grace is designed to fill the CPU-sized hole in NVIDIA’s AI server offerings. The company’s GPUs are incredibly well-suited for certain classes of deep learning workloads, but not all workloads are purely GPU-bound, if only because a CPU is needed to keep the GPUs fed. NVIDIA’s current server offerings, in turn, typically rely on AMD’s EPYC processors, which are very fast for general compute purposes, but lack the kind of high-speed I/O and deep learning optimizations that NVIDIA is looking for. In particular, NVIDIA is currently bottlenecked by the use of PCI Express for CPU-GPU connectivity; their GPUs can talk quickly amongst themselves via NVLink, but not back to the host CPU or system RAM.The solution to the problem, as was the case even before Grace, is to use NVLink for CPU-GPU communications. Previously NVIDIA has worked with the OpenPOWER foundation to get NVLink into POWER9 for exactly this reason, however that relationship is seemingly on its way out, both as POWER’s popularity wanes and POWER10 is skipping NVLink. Instead, NVIDIA is going their own way by building an Arm server CPU with the necessary NVLink functionality.The end result, according to NVIDIA, will be a high-performance and high-bandwidth CPU that is designed to work in tandem with a future generation of NVIDIA server GPUs. With NVIDIA talking about pairing each NVIDIA GPU with a Grace CPU on a single board – similar to today’s mezzanine cards – not only does CPU performance and system memory scale up with the number of GPUs, but in a roundabout way, Grace will serve as a co-processor of sorts to NVIDIA’s GPUs. This, if nothing else, is a very NVIDIA solution to the problem, not only improving their performance, but giving them a counter should the more traditionally integrated AMD or Intel try some sort of similar CPU+GPU fusion play.By 2023 NVIDIA will be up to NVLink 4, which will offer at least 900GB/sec of cummulative (up + down) bandwidth between the SoC and GPU, and over 600GB/sec cummulative between Grace SoCs. Critically, this is greater than the memory bandwidth of the SoC, which means that NVIDIA’s GPUs will have a cache coherent link to the CPU that can access the system memory at full bandwidth, and also allowing the entire system to have a single shared memory address space. NVIDIA describes this as balancing the amount of bandwidth available in a system, and they’re not wrong, but there’s more to it. Having an on-package CPU is a major means towards increasing the amount of memory NVIDIA’s GPUs can effectively access and use, as memory capacity continues to be the primary constraining factors for large neural networks – you can only efficiently run a network as big as your local memory pool.CPU & GPU Interconnect BandwidthGraceEPYC 2 + A100EPYC 1 + V100GPU-to-CPU Interface(Cummulative, Both Directions)>900GB/secNVLink 4~64GB/secPCIe 4 x16~32GB/secPCIe 3 x16CPU-to-CPU Interface(Cummulative, Both Directions)>600GB/secNVLink 4304GB/secInfinity Fabric 2152GB/secInfinity FabricAnd this memory-focused strategy is reflected in the memory pool design of Grace, as well. Since NVIDIA is putting the CPU on a shared package with the GPU, they’re going to put the RAM down right next to it. Grace-equipped GPU modules will include a to-be-determined amount of LPDDR5x memory, with NVIDIA targeting at least 500GB/sec of memory bandwidth. Besides being what’s likely to be the highest-bandwidth non-graphics memory option in 2023, NVIDIA is touting the use of LPDDR5x as a gain for energy efficiency, owing to the technology’s mobile-focused roots and very short trace lengths. And, since this is a server part, Grace’s memory will be ECC-enabled, as well.As for CPU performance, this is actually the part where NVIDIA has said the least. The company will be using a future generation of Arm’s Neoverse CPU cores, where the initial N1 design has already beenturning heads. But other than that, all the company is saying is that the cores should break 300 points on the SPECrate2017_int_base throughput benchmark, which would be comparable to some of AMD’s second-generation 64 core EPYC CPUs. The company also isn’t saying much about how the CPUs are configured or what optimizations are being added specifically for neural network processing. But since Grace is meant to support NVIDIA’s GPUs, I would expect it to be stronger where GPUs in general are weaker.Otherwise, as mentioned earlier, NVIDIA big vision goal for Grace is significantly cutting down the time required for the largest neural networking models. NVIDIA is gunning for 10x higher performance on 1 trillion parameter models, and their performance projections for a 64 module Grace+A100 system (with theoretical NVLink 4 support) would be to bring down training such a model from a month to three days. Or alternatively, being able to do real-time inference on a 500 billion parameter model on an 8 module system.Overall, this is NVIDIA’s second real stab at the data center CPU market – and the first that is likely to succeed. NVIDIA’sProject Denver, which was originally announced just over a decade ago, never really panned out as NVIDIA expected. The family of custom Arm cores was never good enough, and never made it out of NVIDIA’s mobile SoCs. Grace, in contrast, is a much safer project for NVIDIA; they’re merely licensing Arm cores rather than building their own, and those cores will be in use by numerous other parties, as well. So NVIDIA’s risk is reduced to largely getting the I/O and memory plumbing right, as well as keeping the final design energy efficient.If all goes according to plan, expect to see Grace in 2023. NVIDIA is already confirming that Grace modules will be available for use in HGX carrier boards, and by extension DGX and all the other systems that use those boards. So while we haven’t seen the full extent of NVIDIA’s Grace plans, it’s clear that they are planning to make it a core part of future server offerings.First Two Supercomputer Customers: CSCS and LANLAnd even though Grace isn’t shipping until 2023, NVIDIA has already lined up their first customers for the hardware – and they’re supercomputer customers, no less. Both the Swiss National Supercomputing Centre (CSCS) and Los Alamos National Laboratory are announcing today that they’ll be ordering supercomputers based on Grace. Both systems will be built by HPE’s Cray group, and are set to come online in 2023.CSCS’s system, dubbed Alps, will be replacing their current Piz Daint system, a Xeon plus NVIDIA P100 cluster. According to the two companies, Alps will offer 20 ExaFLOPS of AI performance, which is presumably a combination of CPU, CUDA core, and tensor core throughput. When it’s launched, Alps should be the fastest AI-focused supercomputer in the world.An artist's rendition of the expected Alps systemInterestingly, however, CSCS’s ambitions for the system go beyond just machine learning workloads. The institute says that they’ll be using Alps as a general purpose system, working on more traditional HPC-type tasks as well as AI-focused tasks. This includes CSCS’s traditional research into weather and the climate, which the pre-AI Piz Daint is already used for as well.As previously mentioned, Alps will be built by HPE, who will be basing on their previously-announced Cray EX architecture. This would make NVIDIA’s Grace the second CPU option for Cray EX, along with AMD’s EPYC processors.Meanwhile Los Alamos’ system is being developed as part of an ongoing collaboration between the lab and NVIDIA, with LANL set to be the first US-based customer to receive a Grace system. LANL is not discussing the expected performance of their system beyond the fact that it’s expected to be “leadership-class,” though the lab is planning on using it for 3D simulations, taking advantage of the largest data set sizes afforded by Grace. The LANL system is set to be delivered in early 2023.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16610/nvidia-unveils-grace-a-highperformance-arm-server-cpu-for-use-in-ai-systems\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The NVIDIA GTC 2021 Keynote Live Blog (Starts at 8:30am PT/15:30 UTC)\n",
      "Author: Ryan Smith\n",
      "Date Published: 2021-04-12T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16611/the-nvidia-gtc-2021-keynote-live-blog-starts-at-830am-pt1630-utc\n",
      "Content: 11:27AM EDT- Thank you for joining us for another year of the NVIDIA GTC keynote live blog11:28AM EDT- Whether physical or virtual, GTC is inevitab...ly a lot of news in a short period of time11:29AM EDT- NVIDIA's revenues have doubled over less than half a decade, and with that so has the number of business they're in11:29AM EDT- Graphics, AI, automotive, HPC, and most recently networking11:30AM EDT- So it's a lot for CEO Jensen Huang to go over in (ideally) less than 2 hours11:30AM EDT- This year will be no exception. With a whole year to prepare, NVIDIA is firing on all cylinders ahead of the show11:30AM EDT- And here we go11:32AM EDT- With the virtual show, this year's keynote is pre-recorded. So it should keep a tight pace. Still, according to YouTube, we're looking at a 1 hour and 48 minute recording11:33AM EDT- Rolling the intro video. \"I am AI\"11:33AM EDT- And here's Jensen11:34AM EDT- Starting right off the bat talking about AI11:34AM EDT- \"AI and 5G are the ingrediants to kickstart the 4th industrial revolution\"11:35AM EDT- Jensen's talk will be in 4 stacks: graphics and omniverse, data center AI and server hardware, edge AI and EGX 5G, and automotive/DRIVE11:37AM EDT- \"With just a GeForce, every student can have a supercomputer\"11:38AM EDT- Now recapping some of the things that NVIDIA's clients have been doing with their hardware11:38AM EDT- By headcount, NVIDIA is primarily a software company (seriously), and there is no shortage of major computer science researchers set to give talks at this year's show11:39AM EDT- \"Let's start where NVIDIA started: computer graphics\"11:40AM EDT- Recapping last year's introduction of second-generation RTX (Ampere) hardware11:40AM EDT- Now rolling some video of some recently-released games and future games in development11:42AM EDT- Suffice it to say, game graphical quality has only continued to get better over the years11:42AM EDT- And NVIDIA wants ray tracing to push that further11:42AM EDT- But games aren't everything. NVIDIA is also focused on productivity use of graphics11:42AM EDT- NVIDIA's Omniverse technology11:43AM EDT- Which was first announced a couple of years back, and went into beta testing late last year11:43AM EDT- Omniverse is essentially a shared group simulation and graphics software package11:44AM EDT- Omniverse is server-hosted, and any RTX client can plug in to see it. Or even resort to streaming for those devices that can't render it locally11:46AM EDT- In other words, shared collaboration and design within a single 3D project. All with an emphasis on high quality physics and rendering11:47AM EDT- One particular focus of Omniverse is \"digital twins\"; creating a virtual copy of a real-world project/location11:48AM EDT- This is one of NVIDIA's big pulls for its traditional professional graphics clients, especially in the movie and TV production industry11:48AM EDT- But also robotics, R&D, and pretty much any other use case you can think of where a shared, real-time interface to a model might be useful11:49AM EDT- (Oh good, someone remembered the teapot. It's not graphics without a Utah teapot!)11:50AM EDT- NVIDIA's Isaac robotics platform can interface with Omniverse as well11:50AM EDT- Which among other things, can be used to train robots using a digital twin of a factory within Omniverse11:51AM EDT- NVIDIA has even created a digital twin of a BMW factory11:52AM EDT- BMW is using this as part of their planning processes11:53AM EDT- Discussing an example of using the model to optimize an assembly line for productivity and safety by quickly adjusting the line and relocating various tools/stations11:53AM EDT- BMW is also deploying logistics robots that are using isaac11:55AM EDT- It all looks impressive. Though I am curious what the required investment is with respect to art. Someone has to create all of these models, items, and their surface textures11:56AM EDT- Omniverse connector SDKs from major software packages are available now, with more on the way11:56AM EDT- Omniverse will be available for commercial use this summer under enterprise licensing11:56AM EDT- Now on to data centers11:57AM EDT- Currently discussing virtualization, and the impact of doing it on CPUs11:58AM EDT- GPUs generate a lot of cross-datacenter traffic. Deep learning added even more to that11:58AM EDT- And thus NVIDIA's networking processors, the Data Processing Unit (DPU)11:59AM EDT- The Bluefield family of DPUs was inherited from Mellanox, and now a core part of NVIDIA's offerings12:00PM EDT- Bluefield is designed to offload a major part of network functions, including all the processing that goes with them, such as SSL and security analysis12:00PM EDT- Today NVIDIA is announcing Bluefield 312:00PM EDT- 400Gbps network processor with 22 billion transistors12:01PM EDT- And NVIDIA is already working on Bluefield 4 for 2024, which will be around 64B transistors, and incorporate NVIDIA's AI acceleration technology12:01PM EDT- \"Software will be written by software running on AI computers\"12:02PM EDT- Now segueing into NVIDIA's DGX server hardware12:03PM EDT- DGX A100 series ranges from a workstation-like DGX Station box, up through DGX A100 servers and DGX SuperPods comprised of many A100 servers12:03PM EDT- Announcing the DGX Station 320G12:04PM EDT- 2.5 PFLOPS, 320GB of VRAM, and all in 1500W12:04PM EDT- This is essentially the most powerful box NVIDIA can build that can safely be plugged into a standard North American 115V/15A circuit12:05PM EDT- (PANAMAX for workstations, if you will)12:05PM EDT- NVIDIA is also updating the DGX SuperPod12:06PM EDT- The latest generation SuperPod has added Bluefield 2 DPUs12:06PM EDT- The 80GB A100, first announced last year, is also an option12:06PM EDT- Pricing starts at 7 million dollars and scales to 60 million depending on the size of the system12:06PM EDT- Now on to the next subject: transformers12:07PM EDT- Natural language transformer machine learning models12:07PM EDT- \"We expect to see multi-trillion parameter models by next year\"12:08PM EDT- Transformer models are growing quickly. The bigger the model, generally the better and more nuanced the results12:08PM EDT- So NVIDIA has developed their own transformer technology: Megatorn12:09PM EDT- Announcing the Megatron Triton DGX server12:09PM EDT- Able to repond to up to 16 simultaneous queries in an instant12:10PM EDT- Now on to NVIDIA's Clara library of machine learning models and technology for medical research12:10PM EDT- NVIDIA is adding 4 new models to the Clara Discovery library12:12PM EDT- Among other tasks, one of the new models can be used to recognize DNA sequences12:12PM EDT- Meanwhile, Jensen is also pitching NVIDIA's hardware and software for drug discovery12:13PM EDT- And if that's not enough, how about quantum physics simulations running on GPUs? IBM's doing it12:14PM EDT- Er, excuse me, quantum computing, not quantum physics12:15PM EDT- NVIDIA is announcing a new software package, cuQuantum, to help research and simulate quantum computers12:15PM EDT- cuQuantum is optimized to run on NVIDIA's DGX hardware12:16PM EDT- Jensen wants cuQuantum to do what cuDNN did for deep learning12:16PM EDT- Now on to data center server architectures12:17PM EDT- \"Processing large amounts of data remains a challenge for computers today\"12:18PM EDT- Discussing the current architecture of GPU server boxes like NVIDIA's DGX: 4 GPUs hooked up to a single CPU via PCI Express12:18PM EDT- PCI Express is the bottleneck12:18PM EDT- NVIDIA has NVLink, but no x86 CPU has NVLink12:18PM EDT- So NVIDIA is making their own data center CPU: Grace12:18PM EDT- Named after Grace Hopper12:19PM EDT- Grace is an Arm-based CPU, specialized in hosting NVIDIA's GPUs for bandwidth and AI throughput reasons12:20PM EDT- \"Amazing increase in system and memory bandwidth\"12:20PM EDT- And we're now deconstructing Jensen's kitchen...12:20PM EDT- Grace in the artist-envisioned flesh12:21PM EDT- NVIDIA has already lined up a customer for Grace: CSCS, who is building their Alps supercomputer12:21PM EDT- Set to come online in 202312:21PM EDT- NVIDIA is now a CPU, GPU, and DPU company12:22PM EDT- Each chip architecture will have a 2 year rhythm, with likely a kicker in-between12:22PM EDT- NVIDIA will not stop supporting x8612:22PM EDT- Instead they'll support both Arm and x8612:24PM EDT- Speaking of Arm, NVIDIA is developing an Arm SDK, in a partnership with Ampere (the company)12:24PM EDT- And jumping subjects again, this time to edge AI12:27PM EDT- Recapping NVIDIA's various AI libraries and toolkits12:27PM EDT- Which NVIDIA simply calls \"NVIDIA AI\"12:27PM EDT- From PCs and laptops to workstations and supercomputers12:28PM EDT- But one segment of the market that NVIDIA has not focused on up until now has been enterprise computing12:28PM EDT- So NVIDIA is announcing their EGX enterprise platform12:28PM EDT- NVIDIA AI runs on VMware12:29PM EDT- So NVIDIA AI is available within virtualized environments12:29PM EDT- \"The missing link is 5G\"12:30PM EDT- NVIDIA is putting together another new hardware platform, which they are calling the Aerial A10012:30PM EDT- An A100 GPU and Bluefield 2 processor on a single PCIe card12:30PM EDT- For use in 5G basestations12:30PM EDT- Software defined, with acceleration of PHY, crypto, packet processing, and more12:31PM EDT- Which will be offered as part of an EGX edge server package12:32PM EDT- Announcing NVIDIA Morpheus: a data center security product12:32PM EDT- This is another DPU-centric product12:32PM EDT- Now rolling an informational video about how NVIDIA is using Morpheus in-house12:33PM EDT- Morpheus flags when it encounters unencrypted data12:33PM EDT- Relying on AI, rather than specific pattern matching12:35PM EDT- And recapping NVIDIA's enterprise hardware offerings, backed by EGX servers12:36PM EDT- Now on to graphics-related AI projects like DLSS and variouos GANs12:37PM EDT- NVIDIA sees the next wave of AI including increasingly plug-and-play use of the technology12:37PM EDT- To that end, NVIDIA is adding even more pre-trained models to their collection for customers12:38PM EDT- Announcing NVIDIA Tao framework12:38PM EDT- And NVIDIA fleet commmand for securely controlling AI edge servers12:39PM EDT- Now rolling a video about a customer using NVIDIA's Tao and Fleet Command products12:40PM EDT- Starting with a pre-trained model, and then using Tao to re-train the model to better accomodate the specific job site12:40PM EDT- All of the models are trained in minutes12:40PM EDT- And the updated models are deployed via Fleet Command12:41PM EDT- Pick a pre-trained model from NGC, optimize it with Tao, and then deploy it via Fleet Command12:41PM EDT- Now on to conversational AIs12:42PM EDT- NVIDIA's Jarvis package is now available for production use12:42PM EDT- Jarvis has 90% recognition accuracy out of the box12:42PM EDT- 5 languages supported today12:43PM EDT- \"No more mechanical talk\"12:43PM EDT- Jensen is focusing on the edge use cases for Jarvis, and where it could be run12:44PM EDT- And NVIDIA is partnering with Mozilla to collect voice samples to better train Jarvis and other future voice AI systems12:44PM EDT- \"I have no idea what I said, but Jarvis recognized it perfectly\"12:45PM EDT- And showing Jarvis doing English to Japanese translations (voice to text to text)12:45PM EDT- And configurable voice options, including intensity and enthusiasm12:46PM EDT- Now on to recommender systems12:46PM EDT- (We're moving at a breakneck pace here. NVIDIA has a lot of subjects to get through)12:46PM EDT- Announcing NVIDIA Merlin, NVIDIA's end-to-end accelerated recommender system12:47PM EDT- (A recommender system is exactly what it sounds like: a system that attempts to figure out what a user would prefer, and thus what they should be recommended)12:47PM EDT- And on to NVIDIA Maxine, NVIDIA's video conferencing technology suites12:48PM EDT- Which incorporates Jarvis voice recognition and translation12:48PM EDT- Also showing off an eye contact faker/correcter12:49PM EDT- A lot of people are videoconferencing these days, to say the least. So NVIDIA is keen on lining up customers in that market with tools to improve the experience12:49PM EDT- Announcing NVIDIA Triton inference server12:50PM EDT- Triton schedules models on to hardware. Any model and framework on to the appropriate hardware12:51PM EDT- And a quick look at biomedical molecule simulations using Triton12:52PM EDT- And now on to talking about what customers have been doing with NVIDIA's AI technologies12:52PM EDT- Best Buy, Spotify, T-Mobile, and more12:53PM EDT- And now on to automotive and DRIVE AV12:53PM EDT- \"AV computing demand is skyrocketing\"12:54PM EDT- Automakers still need more computing power12:54PM EDT- Recapping NVIDIA's Orin SoC, which is set to arrive next year12:56PM EDT- And the possibility of using a single Orin system as a central computer for everything within a car. From autonomous driving to dashes and infotainment, all execution segregated12:56PM EDT- And NVIDIA's next-generation SoC past Orin is already in development12:56PM EDT- DRIVE Atlan12:56PM EDT- 1000 TOPS on a single chip12:57PM EDT- Newly incorporating NVIDIA's AI and DPU technologies on top of the many other existing hardware features12:57PM EDT- Due in 202512:58PM EDT- Now talking about NVIDIA's increasing number of major automotive customers, and what they're doing with NV's tech12:58PM EDT- The big one, of course: robo taxis12:59PM EDT- Driverless trucks, anyone?12:59PM EDT- And now we're reaching the end, and Jensen is looping back to Omniverse12:59PM EDT- Running NVIDIA DRIVE simulations within Omniverse12:59PM EDT- And digital twin opportunities01:00PM EDT- NVIDIA's Drive Sim engine will be available to Omniverse users01:00PM EDT- Now rolling a video01:01PM EDT- Showing Drive Sim in action, inside and outside of a simulated car01:02PM EDT- And now on to the recap01:03PM EDT- Omniverse01:03PM EDT- DGX systems and Grace CPUs01:03PM EDT- Jarvis, Merlin, and edge AI01:04PM EDT- NVIDIA Tao, Fleet Command, and Triton01:04PM EDT- And Drive, Orin, and the new Atlan SoC01:05PM EDT- And that's a wrap. Thanks again for joining us\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16611/the-nvidia-gtc-2021-keynote-live-blog-starts-at-830am-pt1630-utc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD Issues Updated Speculative Spectre Security Status: Predictive Store Forwarding\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-04-08T10:23:00Z\n",
      "URL: https://www.anandtech.com/show/16604/amd-issues-updated-speculative-spectre-security-status-predictive-store-forwarding\n",
      "Content: The mention of Spectre and Meltdown is enough to send chills down any InfoSec spine. A number of these batches of security vulnerabilities deal with speculative execution, and how a processor might leak data while executing code in a speculative manner. This week AMD has pre-empted the security space by detailing a potential security concerns regarding its new Zen 3-based Predictive Store Forwarding feature designed to improve code performance by predicting dependencies between loads and stores. AMD is clear to point out that most users will not need to take any action, as the risk for general consumer use to any breach is low, and no known code is vulnerable.Predictions Create Predilections for DataModern processors use a number of clever techniques to improve performance. A number of those techniques come under the heading of ‘speculation’ – at a high level, when a processor runs code like a simple true/false branch, rather than wait for the result of that true/false check to come in from memory, it will start executing both branches at once. When the true/false result comes back from memory, the branch that had the right answer is kept, and the other is destroyed. Modern processors also predict memory addresses in repetitive loops, or values in a sequence, by learning what code has already been processed. For example, if your loop increments a load address by 1024 bytes every cycle, by the 100thloop, the processor has learned where it expects the next load to come from. It’s all rather clever, and enables a lot of performance.The downside of these techniques, aside from the extra power consumption needed to execute multiple branches, is the fact that data is in flow from both the correct branch and the incorrect branch at once. That incorrect branch could be accessing data it shouldn’t meant to be and storing it in caches, where it can be read or accessed by different threads. A malicious attacker could cause the incorrect branch to access data it shouldn't be accessing. The concept has lots of layers and is a lot more complicated than I’ve presented here, but in any event, speculation for the sake of performance without consideration to security can lead to fast but leaky data.For the most part, the whole industry including AMD, Intel, and Arm, have been susceptible to these sort of side-channel attacks. While Meltdown style attacks are more isolated to Intel microarchitectures, Spectre-type attacks are industry wide, and have the potential to leak user memory even in browser-like scenarios.Predictive Store ForwardingAMD’s document this week is a security analysis on its new Predictive Store Forwarding (PSF) feature inside Zen 3. PSF identifies execution patterns and commonalities in repeated store/load code, known as store-to-load forwarding. PSF enables the thread to speculate on the next store-to-load result before waiting to see if that result is even needed in the first place. If the result is eventually needed, then we haven’t needed to wait, and the prediction/speculation has done its job and enabled extra performance.AMD has identified that its PSF feature could be vulnerable in two ways.First, the pattern of the store-to-load forwarding could change unexpectedly. If the store/load pair is based on a fixed dependency pattern (such as a fixed data stride length using an external multiplier), the PSF feature learns that pattern and continues. If that dependency suddenly changes, or becomes effectively, random, the PSF feature will continue to speculate until it has learned the new dependency pattern. As it continues to speculate during this time, it has the potential to draw unneeded data into the caches which can be probed by external threads, or the access time to that sensitive data will change for external threads, and this can be monitored.Second, PSF can be vulnerable through memory alignment / aliasing of predictions with dependencies. The PSF is designed to work and track data based on a portion of memory address alignment. As a result, when the store-to-load speculation occurs with an alignment, if a dependency is in the mix of that speculation and the dependency ends up not aligning the predicted values, this might result in incorrect speculation. The data is still valid for a speculation that won’t be used, but therein lies the issue – that data might be sensitive or outside the memory bounds of the thread in question.LimitationsPSF only occurs within a singular thread – how PSF learns where the next store/load pair should be is individual to each thread. This means that an attack of this nature relies on the underlying code causing the PSF speculation to venture into unintended memory, and cannot be exploited directly by an incoming thread, even on the same core. This might sound as if it becomes somewhat unattackable, however if you have ever used a code simulator in a web-browser, then your code is running in the same thread as the browser.PSF training is also limited by context – a number of thread-related values (CPL, ASID, PCID, CR3, SMM) define the context and if any one of these is changed, the PSF flushes what it has learned starts a new as an effective new context has been created. Context switching also occurs with system calls, flushing the data as well.AMD lists that in order to exploit PSF, it requires the store-to-load pairs to be close together in the instruction code. Also the PSF is trained through successive correct branch predictions – a complete mis-prediction can cause a pipeline flush between the store and the load, removing any potential harmful data.Effect on Consumers, Users, and EnterpriseAMD (and its security partners) has identified that the impact of PSF exploitation is similar to Speculative Store Bypass (Spectre v4), and a security concern arises when code implements security control that can be bypassed. This might occur if a program hosts untrusted code that can influence how other code speculates – AMD cites a web browser might deliver such an attack, similar to other Spectre-type vulnerabilities.Despite being similar to other Spectre attacks, AMD’s security analysis states that an attacker would have to effectively train the PSF of a thread with malicious code in the same thread context. This is somewhat difficult to do natively, but could be caused through elevated security accesses. That being said, PSF does not occur across separate address spaces enabled through current hardware mechanisms, such as Secure Encrypted Virtualization. The PSF data is flushed if an invalid data access occurs.For the enterprise market, AMD is stating that the security risk is mitigated through hardware-based address space isolation. Should an entity not have a way for address space isolation in their deployment, PSF can be disabled though setting either MSR 48h bit 2 or MSR 48h bit 7 to a 1. The only products that would be effected as of today are Ryzen 5000 CPUs and EPYC Milan 7003 CPUs.AMD is currently not aware of any code in the wild that could be vulnerable to this sort of attack. The security risk is rated as low, and AMD recommends that most end-user customers will not see any security risk by leaving the feature enabled, which will still be the default going forward.The full security analysis document, along with a suggested mitigation for enterprise, can be found atthis link.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16604/amd-issues-updated-speculative-spectre-security-status-predictive-store-forwarding\n",
      "Title: Intel 3rd Gen Xeon Scalable (Ice Lake SP) Review: Generationally Big, Competitively Small\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-04-06T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/16594/intel-3rd-gen-xeon-scalable-review\n",
      "Content: Section byIan CutressThe launch of Intel’s Ice Lake Xeon Scalable processors has been in the wings for a number of years. The delays to Intel’s 10nm manufacturing process have given a number of setbacks to all of Intel’s proposed 10nm product lines, especially the high performance Xeon family: trying to craft 660 mm2 of silicon on a process is difficult at the best of times. But Intel has 10nm in a place where it is economically viable to start retailing large Xeon processors, and the official launch today of Intel’s 3rdGeneration Xeon Scalable is on the back of over 200,000+ units shipped to major customers to date. The new flagship, the Xeon Platinum 8380, has 40 cores, offers PCIe 4.0, and takes advantage of the IPC gain in Intel’s Sunny Cove processor core. We’re testing it against the best in the market.Intel’s 3rdGeneration Xeon Scalable: 10nm Goes EnterpriseToday Intel is launching the full stack of processors under the 3rdGeneration Xeon Scalable Ice Lake branding, built upon its 10nm process. These processors, up to 40 cores per socket, are designed solely for single socket and dual socket systems, competing in a market with other x86 and Arm options available. With this new generation, Intel’s offering is aimed to be two-fold: first, the generational uplift compared to 2ndGen, but also the narrative around selling a solution rather than simply selling a processor.Intel’s messaging with its new Ice Lake Xeon Scalable (ICX or ICL-SP) steers away from simple single core or multicore performance, and instead is that the unique feature set, such as AVX-512, DLBoost, cryptography acceleration, and security, along with appropriate software optimizations or paired with specialist Intel family products, such as Optane DC Persistent Memory, Agilex FPGAs/SmartNICs, or 800-series Ethernet, offer better performance and better metrics for those actually buying the systems. This angle, Intel believes, puts it in a better position than its competitors that only offer a limited subset of these features, or lack the infrastructure to unite these products under a single easy-to-use brand.An Wafer of 40-core Ice Lake Xeon 10nm ProcessorsNonetheless, the launch of a new generation of products and an expanded portfolio warrants the product to actually be put under test for its raw base performance claims. This generation of Xeon Scalable, Intel’s first on 10nm, uses a newer architecture Sunny Cove core. Benefits of this core, as explained by Intel, start with an extra 20% raw performance increase, enabled through a much wider core with an improved front end and a more execution resources. Outside of the core, memory bandwidth is improved both by increasing memory channels from six to eight, but also new memory prefetch techniques and optimizations that increases bandwidth up to 100% with another +25% efficiency. The mesh interconnect between the cores also uses updated algorithms to feed IO to and from the cores, and Intel is promoting better power management through independent power management agents inside each IP block.On top of this, Intel is layering on accelerative features, stating that over the raw performance, software optimized for these accelerators will see a better-than-generational uplift. This starts with the basic core layout, especially as it pertains to SIMD commands such as SSSE, AVX, AVX2, and AVX-512: Intel is enabling better cryptography support across its ISA, enabling AES, SHA, GFNI, and other instructions to run simultaneously across all vector instruction sets. AVX-512 has improved frequencies during more complex bit operations for ICX with smarter mapping between instructions and power draw, offering an extra 10% frequency for all 256-bit instructions. On top of this is Intel’s Speed Select Technologies, such as Performance Profile, Base Frequency improvements, Turbo Frequency improvements, and Core Power assistance to ensure peak per-core performance or quality of service during a heavily utilized system depending on customer requirements. Other new features include Software Guard Extensions, enabling enclave sizes up to 512 GB per socket with select models.Ice Lake’s Sunny Cove Core: Part 2The Sunny Cove core has actually already been in the market. Intel has made a consumer variant of the core and a server variant of the core. Ice Lake Xeon has the server variant, with bigger caches and slightly different optimization points, but it’s the consumer variant that we have seen and tested in laptop form. Sunny Cove is part of Intel’s Ice Lake notebook processor portfolio, which we reviewed the performance back on August 1st2019, which 614 days ago. That length of time between enabling a core for notebooks and enabling the same core (with upgrades for servers) on enterprise is almost unheard of, but indicative of Intel’s troubles in manufacturing.Nonetheless, in our notebook testing of the Ice Lake core, we saw a raw +17-18% performance over the previous generation, however this was at the expense of 15-20% in frequency. Where the product truly excelled was in memory limited scenarios, where a new memory controller provided better-than-generational uplift. When it comes to this generation of Xeon Scalable processors with the new core, as you see in the review, in non-accelerated workloads we get very much a similar story. That being said, consumer hardware is very often TDP limited, especially laptops! With the new Ice Lake Xeon platform, Intel is boosting the peak TDP from 205 W to 270 W, which also gives additional performance advantages.The Headline Act: Intel’s Xeon Platinum 8380The head prefect of Intel’s new processor lineup is the Platinum 8380 - a full fat 40 core behemoth. If we put it side by side with the previous generation processors, there some key specifications to note.Intel Xeon Comparison: 3rd Gen vs 2nd GenPeak vs PeakXeon Platinum8380AnandTechXeon Platinum828040 / 80Cores / Threads28 / 562900 / 3400 / 3000Base / ST / MT Freq2700 / 4000 / 330050 MB + 60 MBL2 + L3 Cache28 MB + 38.5 MB270 WTDP205 WPCIe 4.0 x64PCIePCIe 3.0 x488 x DDR4-3200DRAM Support6 x DDR4-29334 TBDRAM Capacity1 TB200-seriesOptane100-series4 TB Optane+ 2 TB DRAMOptane CapacityPer Socket1 TB DDR4-2666+ 1.5 TB512 GBSGX EnclaveNone1P, 2PSocket Support1P, 2P, 4P, 8P3 x 11.2 GT/sUPI Links3 x 10.4 GT/s$8099Price (1ku)$10099*6258R, 2P Variantis only $3950Between these processors, the new flagship has a number of positives:+43% more cores (40 vs 28),nearly double the cache,+33% more PCIe lanes (64 vs 48),2x the PCIe bandwidth (PCIe 4.0 vs PCIe 3.0)4x the memory support (4 TB vs 1 TB)SGX Enclave support+7% higher socket-to-socket bandwidthSupport for DDR4-3200 Optane DCPMM 200-seriesPrice is down 20%... or up 100% if you compare to 6258RThough we should perhaps highlight some of the negatives:TDP is up +32% (270 W vs 205 W)ST Frequency is down (3400 MHz vs 4000 MHz)MT Frequency is down (3000 MHz vs 3300 MHz)If we combine the specification sheet cores and all-core (MT) frequency, Ice Lake actually has about the same efficiency here as the previous generation. Modern high-performance processors often operate well outside the peak efficiency window, however Ice Lake being at a lower frequency would usually suggest that Ice Lake is having to operate closer to the peak efficiency point to stay within a suitable socket TDP than previous generations. This is similar to what we saw in the laptop space.Features across all Ice Lake Xeon Scalable processorsWe’ll dive into the different processors over on the next page, however it is worth noting some of the key features that will apply to all of Intel’s new ICL-SP family. Across the ~40 new processors, including all the media focused parts, the network focused processors, and all the individual optimizations used, all of the processors will have the following:All Ice Lake Xeons will support eight channels of DDR4-3200 at 2DPC(new info)All Ice Lake Xeons will support 4 TB of DRAM per socketAll Ice Lake Xeons will support SGX Enclaves (size will vary)All Ice Lake Xeons will support 64x PCIe 4.0 lanesAll Ice Lake Xeons will support 2x FMAPlatinum/Gold Xeons will support 3x UPI links at 11.2 GT/s, Silver is 2x links at 10.4 GT/sPlatinum/Gold Xeons will support 200-series Optane DC Persistent MemoryIn the past, Intel has often productized some of these features at will sell the ones that are more capable at a higher cost. This segmentation is often borne from a lack of competition in the market. This time around however, Intel has seen fit to unify some of its segmentation for consistency. The key one in my mind is memory support: at the start of the Xeon Scalable family, Intel started to charge extra for high-capacity memory models. But in light of the competition now offering 4 TB/socket at no extra cost, it would appear that Intel has decided to unify the stack with one memory support option.Intel 3rdGeneration Xeon Scalable: New Socket, New MotherboardsIce Lake Xeons, now with eight memory channels rather than six, will require a new socket and new motherboards. Ice Lake comes with 4189 pins, and requires an LGA4189-4 ‘Whitley’ motherboard. This is different to the LGA4189-5 ‘Cedar Island’ in use for Cooper Lake, and the two are not interoperable, however they do share a power profile.This actually brings us onto a point about Intel’s portfolio. Technically 10nm Ice Lake is not the only member of the 3rdGen Xeon Scalable family – Intel has seen fit to bundle both 14nm Cooper Lake and 10nm Ice Lake under the same heading. Intel is separating the two by stating that Cooper Lake is focused at several specific high volume customers looking to deploy quad-socket and eight-socket systems with specific AI workloads. By comparison, Ice Lake is for the mass market, and limited to two socket systems.Ice Lake and Cooper Lake both have the ‘3’ in the processor name indicating third generation. Users can tell which ones are Cooper Lake because they end in either H or HL – Ice Lake processors (as we’ll see on the next page) never have H or HL. Most Cooper Lake processors are Platinum models anyway, with a few Xeon Gold. As we go through this review, we’ll focus solely on Ice Lake, given that this is the platform Intel is selling to the mainstream.This ReviewIn the lead up to this launch today, Intel provided us with a 2U system featuring two of the top models of Ice Lake Xeon: we have dual 40 core Xeon Platinum 8380s! At the same time, we have also spent time a dual Xeon Gold 6330 system from Supermicro, which has two 28-core processors, and acts as a good comparison to the previous generation Xeon Platinum 8280.Our review today will cover the processor stack, our benchmarks, power analysis, memory analysis, and some initial conclusions.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16594/intel-3rd-gen-xeon-scalable-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Announces Armv9 Architecture: SVE2, Security, and the Next Decade\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-03-30T18:00:00Z\n",
      "URL: https://www.anandtech.com/show/16584/arm-announces-armv9-architecture\n",
      "Content: It’s been nearly 10 years since Arm had first announced the Armv8 architecture in October 2011, and it’s been a quite eventful decade of computing as the instruction set architecture saw increased adoption through the mobile space to the server space, and now starting to become common in the consumer devices market such as laptops and upcoming desktop machines. Throughout the years, Arm has evolved the ISA with various updates and extensions to the architecture, some important, some maybe glanced over easily.Today, as part of Arm’s Vision Day event, the company is announcing the first details of the company’s new Armv9 architecture, setting the foundation for what Arm hopes to be the computing platform for the next 300 billion chips in the next decade.The big question that readers will likely be asking themselves is what exactly differentiates Armv9 to Armv8 to warrant such a large jump in the ISA nomenclature. Truthfully, from a purely ISA standpoint, v9 probably isn’t an as fundamental jump as v8 was over v7, which had introduced a completely different execution mode and instruction set with AArch64, which had larger microarchitectural ramifications over AArch32 such as extended registers, 64-bit virtual address spaces and many more improvements.Armv9 continues the usage of AArch64 as the baseline instruction set, however adds in a few very important extensions in its capabilities that warrants an increment in the architecture numbering, and probably allows Arm to also achieve a sort of software re-baselining of not only the new v9 features, but also the various v8 extensions we’ve seen released over the years.The three new main pillars of Armv9 that Arm sees as the main goals of the new architecture are security, AI, and improved vector and DSP capabilities. Security is a very big topic for v9 and we’ll go into the new details of the new extensions and features into more depth in a bit, but getting DSP and AI features out of the way first should be straightforward.Probably the biggest new feature that is promised with new Armv9 compatible CPUs that will be immediately visible to developers and users is the baselining of SVE2 as a successor to NEON.Scalable Vector Extensions, or SVE, in its first implementation wasannounced back in 2016and implemented for the first time inFujitsu’s A64FX CPU cores, now powering theworld’s #1 supercomputer Fukagu in Japan. The problem with SVE was that this first iteration of the new variable vector length SIMD instruction set was rather limited in scope, and aimed more at HPC workloads, missing many of the more versatile instructions which still were covered by NEON.SVE2 was announced back in April 2019, and looked to solve this issue by complementing the new scalable SIMD instruction set with the needed instructions to serve more varied DSP-like workloads that currently still use NEON.The benefit of SVE and SVE2 beyond addition various modern SIMD capabilities is in their variable vector size, ranging from 128b to 2048b, allowing variable 128b granularity of vectors, irrespective of what the actual hardware is running on. Purely from a view of vector processing and programming, it means that a software developer would only ever have to compile his code once, and if in the future a CPU would come out with say native 512b SIMD execution pipelines, the code would be able to already take advantage of the full width of the units. Similarly, the same code would be able to run on more conservative designs with a lower hardware execution width capability, which is important to Arm as they design CPUs from IoT, to mobile, to datacentres. It also does this all whilst remaining within the 32b encoding space of the Arm architecture, whereas alternative implementations such as on x86 have to add on new extensions and instructions depending on vector size.Machine learning is also seen as an important part of Armv9 as Arm sees more and more ML workloads to become common place in the next years. Running ML workloads on dedicated accelerators naturally will still be a requirement for anything that is performance or power efficiency critical, however there still will be vast new adoption of smaller scope ML workloads that will run on CPUs.Matrix multiplication instructionsare key here and will represent an important step in seeing larger adoption across the ecosystem as being a baseline feature of v9 CPUs.Generally, I see SVE2 as probably the most important factor that would warrant the jump to a v9 nomenclature as it’s a more definitive ISA feature that differentiates it from v8 CPUs in every-day usage, and that would warrant the software ecosystem to go and actually diverge from the existing v8 stack. That’s actually become quite a problem for Arm in the server space as the software ecosystem is still baselining software packages on v8.0, which unfortunately is missing the all-important v8.1 Large System Extensions.Having the whole software ecosystem move forward and being able to assume new v9 hardware has the capability of the new architectural extensions would help push things ahead, and probably solve some of the current situation.However v9 isn’t only about SVE2 and new instructions, it also has a very large focus on security, where we’ll be seeing some more radical changes.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16584/arm-announces-armv9-architecture\n",
      "Title: Intel Rocket Lake (14nm) Review: Core i9-11900K, Core i7-11700K, and Core i5-11600K\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-03-30T14:03:00Z\n",
      "URL: https://www.anandtech.com/show/16495/intel-rocket-lake-14nm-review-11900k-11700k-11600k\n",
      "Content: Today is the official launch of Intel’s 11thGeneration Core processor family, given the internal name ‘Rocket Lake’. Rocket Lake showcases new performance gains for Intel in the desktop space, with a raw clock-for-clock performance uplift in a number of key workloads.In order to accomplish this, Intel has retrofitted its 10nm CPU and GPU designs back to 14nm, because only 14nm can achieve the frequency required. In exchange, the new processors to get this performance run hot, cost more for Intel to produce, have two fewer cores at the high end, but customers also get PCIe 4.0 on Intel’s mainstream desktop platform for the first time.In our review today, we will be going over Intel’s new hardware, why it exists, and how it performs, focusing specifically on Intel’s new flagship, the Core i9-11900K, which has eight cores and can boost up to 5.3 GHzIntel’s Rocket Lake: Core i9, Core i7, and Core i5The new Intel 11thGen Core desktop processor family will start with Core i5, with six cores and twelve threads, through to Core i7 and Core i9, both with eight cores and sixteen threads. All processors will support DDR4-3200 natively, and offer 20 PCIe 4.0 lanes in supported motherboards – these lanes will enable graphics and storage direct from the processor, typically in an x16/x4 or x8/x8/x4 combination.Both the Core i9 and Core i7 this time around have the same core count - normally the Core i9 would offer an obvious difference, such as more cores, but for this generation the difference is more subtle: Core i9 will offer higher frequencies and Thermal Velocity Boost (TVB). The Core i9-K and i9-KF will also feature Intel’s new Adaptive Boost Technology (ABT). We’ll go over Intel’s Turbo nomenclature later in the article.Intel 11th Gen Core Rocket LakeCore i9AnandTechCoresThreadsBaseFreq1TPeaknTTurboTDP(W)IGPUHDPrice1kui9-11900K8 / 16350053004700125750$539i9-11900KF8 / 16350053004700125-$513i9-119008 / 1625005200460065750$439i9-11900F8 / 1625005200460065-$422i9-11900T8 / 1615004900370035750$439At the top of the stack is the Core i9-11900K. Intel has set the 1000-unit pricing of the Core i9-11900K at $539. Note that Intel does this 1k unit pricing for OEMs, and the final retail price is often $10-$25 higher, but in the case of the Core i9-11900K, users are currently looking at a $615 price point at Newegg. This is well above AMD’s Ryzen 7 5800X at $449 SEP (MSRP), which is also an 8-core processor, and beyond even the Ryzen 9 5900X at $549 SEP. Intel is stating that along with better gaming performance, this processor also offers next-generation integrated graphics, support for new AI instructions, and enhanced media support for the price differential.The Core i9-11900K is the highlight processor of today’s review, and it has a base frequency of 3.5 GHz, alongside a peak turbo of 5.3 GHz in Thermal Velocity Boost mode, 5.2 GHz otherwise on the favored core, or 5.1 GHz on non-favored cores. The all-core frequency is 4.8 GHz in TVB turbo mode, or 4.7 GHz otherwise, or it can ‘float’ the turbo up to 5.1 GHz when ABT is enabled, however ABT is disabled by default.The only processor not getting TVB in the Core i9 family is the i9-11900T, which is the 35 W member of the family. This processor has 35 W on the box because its base frequency is 1.5 GHz, although it will turbo up to 4.9 GHz single core and 3.7 GHz all-core. These T processors typically end up in OEM systems and mini-PCs which are more likely to strictly follow Intel’s turbo recommendations.All Core i9 processors will support DDR4-3200, and the specification is to enable a 1:1 frequency mode with the memory controller at this speed.Intel 11th Gen Core Rocket LakeCore i7AnandTechCoresThreadsBaseFreq1TPeaknTTurboTDP(W)IGPUHDPrice1kui7-11700K8 / 16360050004600125750$399i7-11700KF8 / 16360050004600125-$374i7-117008 / 1625004900440065750$323i7-11700F8 / 1625004900440065-$298i7-11700T8 / 1614004600360035750$323The Core i7 family includes the Core i7-11700K, whichwe have already reviewedwith our retail sample, and tested on the latest microcode to date. This processor offers eight cores, sixteen threads, with a single core turbo of 5.0 GHz on the favored core, 4.9 GHz otherwise, and 4.6 GHz all-core turbo. The rated TDP is 125 W, although we saw 160 W during a regular load, 225 W peaks with an AVX2 rendering load, and 292 W peak power with an AVX-512 compute load.On the topic of memory support, the Core i7 family does support DDR4-3200, however Intel’s specifications for Rocket Lake are that any non-Core i9 processor should run at a 2:1 ratio of DRAM to memory controller by default, rather than 1:1, effectively lowering memory performance. This creates some segmentation between Core i9 and the rest, as for the rest of the processors the fastest supported 1:1 memory ratio is DDR4-2933. Despite this technical specification, we can confirm in our testing of our Core i7-11700K that all the motherboards we have used so far actually default to 1:1 at DDR4-3200. It would appear that motherboard manufacturers are confident enough in their memory designs to ignore Intel’s specifications on this.On pricing, the Intel Core i7-11700K is $399, which is important in two ways.First, it is $140 cheaper than the Core i9-K, and it only loses a few hundred MHz. That leaves the Core i9 high and dry on day one. Unless there’s something special in that chip we haven’t been told about that we have to discover come retail day on March 30th, that’s a vast pricing difference for a small performance difference.Second is the comparative AMD processor, the Ryzen 7 5800X, which has 8 cores and has a $449 SEP. If both processors were found at these prices, then the comparison is a good one – the Ryzen 7 5800X in our testing scored +8% in CPU tests and +1% in gaming tests (1080p Max). The Ryzen is very much the more power-efficient processor, however the Intel has integrated graphics (an argument that disappears with KF at $374). It will be interesting to see what recommendations people come to with that pricing.Intel 11th Gen Core Rocket LakeCore i5AnandTechCoresThreadsBaseFreq1TPeaknTTurboTDP(W)IGPUHDPrice1kui5-11600K6 / 12390049004600125750$262i5-11600KF6 / 12390049004600125-$237i5-116006 / 1228004800430065750$213i5-11600T6 / 1217004100350035750$213i5-115006 / 1227004600420065750$192i5-11500T6 / 1215003900340035750$192i5-114006 / 1226004400420065730$182i5-11400F6 / 1226004400420065-$157i5-11400T6 / 1213003700330035730$182The Core i5 spreads out a lot with more offerings, from $157 for the Core i5-11400F, up to $262 for the Core i5-11600K. All these processors have six cores and twelve threads, all have the traditional Intel Turbo 2.0, and all support DDR4-3200 (2:1) or DDR4-2933 (1:1).Another difference within these parts is that the Core i5-11400 and Core i5-11400T have UHD Graphics 730, not 750, which means using a 24 EU configuration rather than the full 32 EUs.Intel’s Competition: Intel vs Intel vs AMDWith both the Core i9 and the Core i7 being eight cores and sixteen threads, the natural competitor to both would be either (a) Intel’s previous generation of processors or (b) AMD’s Ryzen 7 5800X, which is starting to come back into the market with sufficient stock that it can be purchased at its suggested retail price.Rocket Lake CompetitionAnandTechCore i710700KCore i910900KCore i711700KCore i911900KRyzen 75800XRyzen 95900XuArchCometLakeComet LakeCypressCoveCypressCoveZen 3Zen 3Cores8 C16 T10 C20 T8 C16 T8 C16 T8 C16 T12 C24 TBase Freq380037003600350038003700Turbo Freq510052005000530048004800All-Core4700490046004800~4550~4350TDP125 W125 W125 W125 W105 W105 WIGP / EUsGen 9, 24Gen 9, 24Xe-LP, 32Xe-LP, 32--L3 Cache16 MB20 MB16 MB16 MB32 MB64 MBDDR42 x 29332 x 29332 x 32002 x 32002 x 32002 x 3200PCIe3.0 x163.0 x164.0 x204.0 x204.0 x244.0 x24MSRP$387$499$399$539$449$549Retail$322$470$419$614$449$549As we saw inour Core i7-11700K review, at $399/$419, the Ryzen 7 5800X at $449 is actually a good comparison point. On high-end gaming both processor performed the same, the AMD processor was ahead an average of 8% on CPU workloads, and the AMD processor came across as a lot more efficient and easy to cool, while the Intel processor scored a big lead in AVX-512 workloads. At the time of our review, we noted that stock of AMD’s Ryzen 5000 processors would be a large part of the choice between the two processors, given that stock was low and highly volatile. Since then, as in our latest CPU Guide, stock of the AMD CPUs is coming back to normal, so then it would come down to exact pricing differences.If we focus on the Core i9-11900K in this comparison, given the small differences between itself and the Core i7, you would also have to pit it against the AMD Ryzen 7 5800X, however at its $539 tray price and $615 Newegg price, it really has to go against the 12-core Ryzen 9 5900X, where it loses out by 50% on cores but has a chance to at least draw level on single thread performance.Test Setup and #CPUOverload BenchmarksAs per our processor testing policy, we take a premium category motherboard suitable for the socket, and equip the system with a suitable amount of memory running at the manufacturer's maximum supported frequency. This is also run at JEDEC subtimings where possible. Reasons are explainedhere.Test SetupIntelRocket LakeCore i9-11900KCore i7-11700KCore i5-11600KASUS MaximusXIII Hero0610/0703**TRUECopper+ SST*ADATA4x32 GBDDR4-3200IntelComet LakeCore i9-10900KCore i7-10700KASRock Z490PG VelocitaP1.50TRUECopper+ SST*ADATA4x32 GBDDR4-2933Intel CoffeeRefreshCore i9-9900KSCore i9-9900KMSI MPG Z390Gaming Edge ACAB0TRUECopper+SST*ADATA4x32GBDDR4-2666IntelCoffee LakeCore i7-8700KMSI MPG Z390Gaming Edge ACAB0TRUECopper+SST*ADATA4x32GBDDR4-2666AMDAM4Ryzen 9 5900XRyzen 7 5800XRyzen 7 4750GGIGABYTE X570IAorus ProF31LNoctuaNHU-12SSE-AM4ADATA2x32 GBDDR4-3200GPUSapphire RX 460 2GB (CPU Tests)NVIDIA RTX 2080 Ti FE (Gaming Tests)PSUCorsair AX860iSSDCrucial MX500 2TB*TRUE Copper used with Silverstone SST-FHP141-VF 173 CFM fans. Nice and loud.**0703 was applied for stability supportWe must thank the following companies for kindly providing hardware for our multiple test beds. Some of this hardware is not in this test bed specifically, but is used in other testing.Hardware Providers for CPU and Motherboard ReviewsSapphireRX 460 NitroNVIDIARTX 2080 TiCrucial SSDsCorsair PSUsG.Skill DDR4ADATA DDR4SilverstoneCoolersNoctuaCoolersA big thanks to ADATA for the ​AD4U3200716G22-SGN modulesfor this review. They're currently the backbone of our AMD testing.Users interested in the details of our current CPU benchmark suite can refer toour #CPUOverload articlewhich covers the topics of benchmark automation as well as what our suite runs and why. We also benchmark much more data than is shown in a typical review, all of which you can see in our benchmark database. We call it ‘Bench’, and there’s also a link on the top of the website in case you need it for processor comparison in the future.Table Of ContentsRocket Lake Product ListWhy Rocket Lake Exists: Retrofitting 10nm to 14nmMotherboards and Overclocking SupportNew Turbo Features: Adaptive Boost TechnologyPower Consumption and StabilityCPU MicrobenchmarksCPU TestingGaming TestingConclusion\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16495/intel-rocket-lake-14nm-review-11900k-11700k-11600k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Xiaomi Announces Mi 11 Ultra: The Largest Smartphone Camera\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-03-29T14:30:00Z\n",
      "URL: https://www.anandtech.com/show/16582/xiaomi-announces-mi-11-ultra\n",
      "Content: It’s been a little less than 3 months since Xiaomi released their first Mi 11 series device, the baseline Mi 11which we reviewed a few weeks ago.Today, within a flurry of device announcements, Xiaomi announced the new Mi 11 Ultra as a higher-end follow-up to the Mi 11, representing the company’s newest top-of-the-line smartphone with new unprecedented camera capabilities.The new Mi 11 Ultra is defined by its camera setup, which is pretty much extraordinary in its specifications as well as design. The phone is the first device to come with Samsung LSI’s new GN2 sensor which measures in at a massive 1/1.12” optical format, featuring native 50MP of 1.4µm pixels, capable of binning down to 12.5MP 2.8µm pixels. Xiaomi also offers a 48MP ultra-wide module, as well as a 48MP 5x periscope telephoto module for far reaches. Weirdly enough, the Mi 11 also features a small 1.1” OLED display alongside the camera setup – possibly enabling more full-fledged selfie experiences using the rear cameras.Interestingly, the phone’s design and dimensions mostly match that of the Mi 11, with the camera layout, thickness and weight really being the major differences in the new Ultra model.Xiaomi Mi 11 SeriesMi 11 UltraMi 11SoCQualcomm Snapdragon 8881xCortex-X1@ 2.84GHz3xCortex-A78@ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 660 @ 840MHzDRAM12GB LPDDR5-64008/12GB LPDDR5-6400Display6.81\" AMOLED3200 x 1440120Hz Refresh480Hz TouchHDR10+ /10-bit panelSizeHeight164.3mm164.3mmWidth74.6mm74.6mmDepth8.38mm8.06mmWeight234g196gBattery Capacity5000mAh (Typical)67W Charging4600mAh (Typical)55W ChargingWireless Charging67W50WRear CamerasMain50MP GN2 1/1.12\" 1.4µm4:1 Binning to 12.5MP / 2.8µmf/1.95 w/OIS24mm eq.108MP HMX 1/1.3\" 0.8µm4:1 Binning to 27MP / 1.6µmf/1.85 w/OIS24mm eq.Telephoto5x optical telephoto48MP IMX586 1/2.0\" 0.8µm4:1 Binning to 12MP 1.6µmf/4.1120mm eq.5MP (Macro only)f/2.248mm eq.ExtraTelephoto--Ultra-Wide48MP IMX586 1/2.0\" 0.8µm4:1 Binning to 12MP 1.6µmf/2.2128° FoV13MPf/2.4123° FoVExtradToF Sensor-Front Camera20MP 0.8µm4:1 Binning 5MP 1.6µmf/2.2Storage256GBUFS 3.1128 / 256GBUFS 3.1I/OUSB-CWireless (local)802.11ax(Wifi6E),Bluetooth 5.2802.11ax(Wifi 6),Bluetooth 5.2Cellular4G + 5G NR NSA+SA Sub-6GHzSpecial FeaturesUnder-screen fingerprint sensorFull-range stereo speakersIR BlasterSecond 1.1\" 126 x 294Rear DisplayUnder-screen fingerprint sensorFull-range stereo speakersIR BlasterSplash, Water, Dust ResistanceIP68No ratingDual-SIM2x nano-SIMLaunch OSAndroid 11 w/ MIUIAndroid 11 w/ MIUILaunch Price12+256GB: 1199€8+128GB: 749€8+256GB: 799€The Mi 11 Ultra is powered by the Snapdragon 888 – much like the Mi 11. In terms of performance, we should expect the new phone to pretty much match what we’ve seen in the Mi 11, which means excellent device experience with pretty much the best of what we can expect of an Android device for this device generation. Xiaomi does advertise advanced cooling solutions, however generally phones of this form-factor are simply limited to a thermal envelope of 5W, as that is the rough external heat dissipation through convection and radiation of any device with this kind of thermal behaviour.The phone comes in a sole 12GB LPDDR5-6400 and UFS 3.1 256GB memory and storage configuration.Other novelties on the Mi 11 Ultra is the Wi-Fi upgrade from a regular Wi-Fi 6 capable chip to a newer Wi-Fi 6E implementation, and naturally it retains all the 5G connectivity features of the baseline Mi 11 model.The display of the Mi 11 Ultra appears to be the exact same as on the Mi 11. This is a 6.81” model with a QHD+ resolution of 3200 x 1440, a software based adaptive refresh rate of up to 120Hz, and advertises itself as a 10-bit capable panel, with a high touch sample rate of 480Hz for low-latency input. While the display is excellent in picture quality, we’ve seen that the Mi 11 wasn’t one of the most efficient devices on the market, characterised by higher base device power consumption and a 120Hz mode that isn’t as efficient as newer LTPO-based displays on Samsung devices. We’ll have to see if the Mi 11 Ultra suffers from similar efficiency weaknesses.The nice thing about the design of the Mi 11 Ultra is that from the front it’s pretty much identical to the Mi 11. With the same screen dimensions, the device form-factor is also almost the same, featuring an identical 164.3mm x 74.6mm height and width as the Mi 11.Where things change is in the thickness and weight of the phone. The body gains 0.32mm to a still rather thin 8.38mm, however the weight grows quite substantially from 196g to 234g.With the weight increase comes also a larger battery, up 8.6% to up to 5000mAh. Xiaomi advertises that this is the industry’s first silicon-oxygen anode Lithium battery, and is one of the key aspects which allow it to allow charging speeds of up to 67W. The new anode technology is definitely promising for improving battery charge degradation – however I’m still sceptical as to the actual absolute figures and how regular fast-charging at these rates will impact battery life-time.The top-line feature of the Mi 11 is its new triple-camera setup. Let’s start off with the oddest feature that isn’t actually a camera: a secondary small rear OLED screen, 1.1” in diagonal and a 126 x 294 resolution. It’s located within the very large camera bump to the right of the cameras. It’s quite gimmicky, but allows for easier usage of the higher-quality rear cameras for selfies and selfie videos as it can act as a view-finder.The other obvious headline feature is the usage of Samsung LSI’s newest GN2 sensor. This is currently the largest camera sensor in any mobile phone on the market, measuring in at 1/1.12” optical format – keep in mind that optical format doesn’t actually correspond to the physical dimensions of the sensor which actually falls in at 11.4 x 8.6mm (14.3mm diagonal) active area.What’s special about the sensor is also that Xiaomi is transitioning away from their 108MP resolution from the past 2 generations to 50MP. The actual native pixel size of the GN2 is 1.4µm, and the sensor has a Quad-Bayer colour filter setup, able to 4:1 bin to 12.5MP pictures with effective pixel sizes of 2.8µm. Due to the lower resolution, the actual per-pixel size is significantly larger than the 27MP 1.6µm picture shots of the Mi 11 and previous Mi 10 series – at the same time, it’s also the largest per-pixel size of any smartphone in the market.The GN2 is also the first sensor in the industry to offer a new “Dual Pixel Pro” technology – essentially this is the return to dual-pixel phase detection which we had in previous generation devices and had to be abandoned with the newer 108MP sensors. The “Pro” part of the new GN2 refers to the alternative diagonal separation of pixels: usually the traditional dual-pixel PD would allow phase-detection in on axis, while this new dual-pixel setup would also allow phase detection in the 90° angle of the DP arrangement, improving auto-focus speed.It’s also a staggered HDR sensor, much like the new HM3 sensor in the Galaxy S21 Ultra, being able to capture subsequent different shutter speed captures directly following the rolling shutter readout of the previous frame, instead of waiting for complete sensor readouts between frames.Xiaomi’s optics on the Mi 11 Ultra’s main camera should be extremely challenging given the ginormous sensor size, and thus falls in a comparatively smaller f/1.95 aperture. It’s a 24mm equivalent focal length which has been standard for Xiaomi’s recent flagships, and features OIS.Generally speaking, this main camera module right now is by far the strongest in the smartphone market, and should be easily able to outperform most other competitors – as long as Xiaomi’s processing also is able to keep up.Alongside the main sensor, Xiaomi also dons the phone with a 48MP 1/2.0” 0.8µm 5x optical periscope telephoto module with a native 120mm equivalent focal length. This setup is actually quite similar in specs to what we’ve seen in the Galaxy S20 Ultra last year, with the only difference being that Xiaomi has optics at f/4.1 while the S20U had f/3.5. How the quality of the telephoto ends up depends on the quality of the optics and how Xiaomi takes advantages of the 48MP native resolution of the sensor – and when to use the 4:1 bin mode to 12MP.Xiaomi avoids having a intermediary telephoto module between 24mm and 120mm focal lengths, here we hope that Xiaomi uses Huawei’s method of capturing images in the >2x range through the main sensor’s native 50MP mode – considering the native 1.4µm pixels this generation, this should be a quite evident solution to bridge the gap.Finally, we see a new 48MP ultra-wide module. This seems to be the same sensor as on the telephoto, meaning a 1/2.0” module with 0.8µm that bins down to 12MP 4:1 1.6µm images. It has an extremely wide 128° FoV with a 12mm equivalent focal length.It’s very clear that the Mi 11 Ultra is extremely camera centric. I consider Xiaomi’s image processing to be quite good, however the newer generation sensors of the competition still had clear advantages compared to say the baseline Mi 11. The Ultra’s new hardware is truly next-gen though, and hopefully should allow the phone to achieve new heights in photography. Of course, the proof is in the pudding is in the eating, and we’ll have to see how it performs in our upcoming future review.Alongside the new camera setup, secondary screen rear mini-display, Wi-Fi 6E, and new battery technology, the new Mi 11 Ultra adds in IP68 rating, making this the first Xiaomi phone to feature the water-resistant rating.Ultra Pricing at 1199€Being a device of the “Ultra” designation, the new Mi 11 Ultra comes at a quite higher price than its regular Mi 11 counterpart. Xiaomi doesn’t quite reveal timing of the availability, but announces the Mi 11 Ultra at 1199€ for the standard 12+256GB version. This is the same price as the 128GB variant of the S21 Ultra, so Xiaomi is definitely aiming higher than they ever did before in terms of product placement.We’ll be reviewing the Mi 11 Ultra in the near future once we get our hands on it.Related Reading:The Xiaomi Mi 11 Review: A Gorgeous Screen and DesignXiaomi Launches Mi 11 Globally: Starting at 749€Qualcomm Details The Snapdragon 888: 3rd Gen 5G & Cortex-X1 on 5nm\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16582/xiaomi-announces-mi-11-ultra\n",
      "Title: Qualcomm Announces Snapdragon 780G: New 5nm 765 Successor\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-03-25T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/16578/qualcomm-announces-snapdragon-780g-new-5nm-765-successor\n",
      "Content: Today Qualcomm is announcing the successor to last year’s quite successful Snapdragon 765 line-up, a “premium” tier that the company had debuted, featuring the same higher-end features as on the flagship Snapdragon 800 series, albeit at lower performances.The new Snapdragon 780G follows up on its predecessor with some large upgrades in terms of performance and multimedia capabilities, doubling up the number of large cores – increasing GPU performance by significant amounts, and featuring the new more performant fused AI engine with the new Hexagon 770 DSP. Furthermore, camera capture abilities have also seen great improvements with the new Spectra 570 triple-ISP.Qualcomm Snapdragon Premium SoCsSoCSnapdragon 765Snapdragon 765GSnapdragon 768GSnapdragon 780GCPU1x Cortex-A76@ 2.3GHz (non-G)@ 2.4GHz (765G)1x Cortex-A76@ 2.2GHz6x Cortex-A55@ 1.8GHz1x Cortex-A76@ 2.8GHz1x Cortex-A76@ 2.4GHz6x Cortex-A55@ 1.8GHz1xCortex-A78@ 2.4GHz3x Cortex-A78@ 2.2GHz4x Cortex-A55@ 1.9GHzGPUAdreno 620Adreno 620+15% perf over 765GAdreno 642+50% perf over 768GDSP / NPUHexagon 696HVX + Tensor5.4TOPS AI(Total CPU+GPU+HVX+Tensor)Hexagon 770Scalar+Tensor+Vector12TOPs AI(Total CPU+GPU+DSP)MemoryController2x 16-bit CH@ 2133MHz LPDDR4X / 17.0GB/sISP/CameraDual 14-bit Spectra 355 ISP1x 192MPor1x 36MP ZSLor2x 22MP with ZSLTriple 14-bit Spectra 570 ISP1x 192MPor1x 84MP ZSLor2x 64+20MP ZSLor3x 25MP ZSLEncode/Decode2160p30, 1080p120H.264 & H.26510-bit HDR pipelinesIntegrated ModemSnapdragon X52Integrated(LTE Category 24/22)DL = 1200 Mbps4x20MHz CA, 256-QAMUL = 210 Mbps2x20MHz CA, 256-QAM(5G NR Sub-6 4x4 100MHz+ mmWave 2x2 400MHz)DL = 3700 MbpsUL = 1600 MbpsSnapdragon X53 Integrated(LTE Category 24/22)DL = 1200 Mbps4x20MHz CA, 256-QAMUL = 210 Mbps2x20MHz CA, 256-QAM(5G NR Sub-6 4x4 100MHz)DL = 3300 MbpsUL = ? MbpsMfc. ProcessSamsung7nm (7LPP)Samsung5nm (5LPE)At heart, the new Snapdragon 780G is a very different SoC to its predecessor as it changes up the CPU configuration quite substantially. We’re moving from a 1+1+6 config, to a newer 1+3+4 setup, including a prime Cortex-A78 core at 2.4GHz, three Cortex-A78 cores at 2.2GHz, and four Cortex-A55 cores at 1.9GHz. Qualcomm promises CPU uplifts of up to 40% - the doubling of the large cores as well as the new microarchitecture employed should indeed offer a good boost in everyday user experience.On the GPU side, we’re seeing the use of a new Adreno 642. As usual Qualcomm doesn’t disclose much details on the design here, but they disclose a generational performance uplift of up to +50% over the Snapdragon 768G, meaning over the 765G that should grow to +72%.Based on our past benchmarks, this should end up with similar performance as the Adreno 640 of the Snapdragon 855 flagship from a few years ago – meaning the GPU is seemingly aptly named in terms of its performance.Qualcomm is employing itsnewest fused scalar+tensor+vector DSP and AI enginein the new Snapdragon 780G, meaning it should be equal in terms of its architectural design as the new unit on the Snapdragon 888, albeit at lower performance levels. Qualcomm advertises 12TOPs of AI performance across all the IP blocks of the SoC, which is over 2x over that of the predecessor.In terms of DRAM, the SoC remains a 2x16b LPDDR4X-2133 design, which seems to be crucial for cost reduction in this market segment.A very large upgrade in capabilities is found on the part of the camera ISPs. Again, much like the DSP, the new design follows up with the similar new IP architecture as employed in the Snapdragon 888, employing a new triple Spectra 570 block that is capable of operating three RGB camera sensors concurrently. 192MP captures are possible for single modules (with shutter lag), or in terms of zero shutter lag operation we can see either 1x 84MP, 64+20MP or 3x 25MP sensor configurations. In terms of video encoding, we don’t see mention of much changes compared to the predecessor so we assume that video capture abilities remain the same.What’s very interesting of the new design and probably telling of the wider market at large, is the fact that the new part no longer advertises mmWave capability on the part of its modem. The new X53 modem has seemingly chopped off this feature from its spec sheet. Generally, mmWave remains an extremely niche feature that’s currently only widely deployed in select US cities globally. Given that the SoCs target devices at lower price points, and we’ve seen some extremely cheap Snapdragon 765 phones in the past year, mmWave capabilities were probably contradictory to the market segment these phones were targeting – vendors always have the possibility to use higher-end solutions such asthe Snapdragon 870if they want to include mmWave connectivity.Finally, the new SoC is manufactured on Samsung’s 5LPE process node, which is an upgrade over the 7LPP node of last year’s Snapdragon 765. Whilethe node doesn’t seem to be as promisingwhen compared to TSMC’s 5nm node, it being employed in a SoC in this price category is definitely a positive and should show notable gains against its predecessor.Qualcomm plans to bundle the Snapdragon 780G SoC with the FastConnect 6900 Wi-Fi chips which feature Wi-Fi 6E connectivity, hopefully signalling a wider spread of adopting of the new 6GHz spectrum technology.The Snapdragon 780G is expected to see deployment in commercial devices in the second quarter of 2021.Related Reading:Qualcomm Discloses Snapdragon 888 Benchmarks: Promising PerformanceThe Snapdragon 888 vs The Exynos 2100: Cortex-X1 & 5nm - Who Does It Better?Qualcomm Announces Snapdragon 768G: Higher-bin 765 up to 2.8GHzQualcomm Goes For The Mid-Range: Snapdragon 765 and 765GSamsung Announces Exynos 1080 - 5nm Premium-Range SoC with A78 CoresMediaTek Announces Dimensity 1100 & 1200 SoCs: A78 on 6nm\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16578/qualcomm-announces-snapdragon-780g-new-5nm-765-successor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Intel’s New IDM 2.0 Strategy: $20b for Two Fabs, Meteor Lake 7nm Tiles, New Foundry Services, IBM Collaboration, Return of IDF\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-03-23T21:00:00Z\n",
      "URL: https://www.anandtech.com/show/16573/intels-new-strategy-20b-for-two-fabs-meteor-lake-7nm-tiles-new-foundry-services-ibm-collaboration-return-of-idf\n",
      "Content: The new CEO of Intel, Pat Gelsinger, has today outlined his vision for Intel over the coming years.During an online presentation entitled ‘Intel Unleashed: Engineering The Future’, Pat Gelsinger outlined five key topics upon which Intel will work towards and what it means for the company at large. At the center of this is a reaffirmed commitment for Intel to retain its own fabs, but also double down on its ability to drive the latest technologies at scale by building new manufacturing facilities inside the US.Highlights of Intel’s Announcements TodayTwo new fabs in Arizona, $20b investmentNew Intel Foundry Services, offering Intel manufacturing to customersNext generation 7nm chiplets for ‘Meteor Lake’ will finish design in Q2 2021New research collaboration with IBM in foundational semiconductor designNew Intel Innovation event in Oct 2021, Spirit of IDFWe're Keeping the Fabs: Please Stop AskingSince the start of the year, previous Intel CEO Bob Swan and new Intel CEO Pat Gelsinger have both done the equivalent of shouting from the rooftops when it comes to Intel’s market advantages. At the top of that list is Intel’s vertical integration between manufacturing and chip design, enabling Intel to control the process from top to bottom more closely than any of its chip competitors. This unique proposition labels Intel as an IDM (integrated device manufacturer) or having an IDM model (integrated device manufacturing model), and the only company that can even come close to Intel in this regard is Samsung. Intel’s plus over its competitor here is that it can apply its own designs to a scale that Intel is often attributed to.In recent months there have been comments from analysts and investors about the potential for Intel to spin out its manufacturing plants and facilities into a separate business, similar to how AMD detached its manufacturing facilities into a new company called GlobalFoundries. The benefits of this move would allow Intel to segregate the losses between the two sides of the business, and showcase the core Intel product teams in a better light by comparison to the manufacturing arm. This sort of path has significant pitfalls, most importantly that Intel’s manufacturing main customer is Intel. GlobalFoundries had this problem initially, but Intel’s foundry arm is on a much larger scale.So to that end, Pat Gelsinger today is hoping to put those rumors to rest, more than he stated on Intel’s 2020 financial call. Not only is Intel going to retain its manufacturing facilities, but it is going to embrace a new era of manufacturing, which is going to be calledIDM 2.0.IDM 2.0: Build, Expand, and ProductizeThis direction from Intel is built upon, first and foremost, the enthusiasm of Pat Gelsinger rejoining the company. Since even before he took over the role of CEO, former Intel engineering experts were coming out of retirement to work on exciting new products with Pat at the helm.IDM 2.0 will have several pieces to the puzzle.Build (7nm)Today Intel will be announcing a $20 billion (USD) investment in two new manufacturing facilities (fabs) in Arizona, set to come online for production in 2024. Gelsinger will point out today that Intel is ready to break ground, increasing the number of factories on Intel’s Ocotillo campus (Chandler, AZ) from four to six.These new fabs will be on leading edge process node technologies, with Intel extending cooperation with the State of Arizona, as well as the current administration’s target of improving semiconductor manufacturing inside the country. All parties involved seem raring to go, and Intel will be seeking to outfit its production facilities with the equipment necessary to enable leading edge manufacturing, including using Extreme Ultra Violet (EUV) technology.Intel's Ocotillo campus, Fab 42, Chandler AZIt is noteworthy that the machines that enable EUV manufacturing are only supplied by a single company, ASML, and demand for these machines is at a record high, with a waiting list of over a year. Intel believes that its technology, with its increasing use of EUV to simplify manufacturing and enabling higher performance and higher yielding products, will be fully aligned and there will be enough EUV to go around by the time these new fabs are up and running.The two new fabs are expected to bring 3000+ high wage jobs direct with Intel, 3000+ construction jobs for construction during the project, and up to 15000 long-term ecosystem support jobs for the area. Planning for construction activities are expected to start immediately. That being said, TSMC has also indicated plans to build a factory in Arizona as well, likely in the Phoenix area, with Samsung also considering a site there (or Austin, TX). There have been questions as to the sustainability of supporting that many semiconductor manufacturing plants in a single city. Intel states that it already recycles 9 million gallons (US gal.) of water each day, and the company purchases green power for its facilities, as well as on-site alternative energy projects.As part of this side of the announcement, Pat Gelsinger will be stating today that the 7nm manufacturing node from Intel is now running on schedule, with a solid footing. The first product enabled with 7nm will be Ponte Vecchio, the upcoming high-performance compute accelerator for the Aurora supercomputer, however end-users might be more interested in Meteor Lake, a client CPU compute tile for a volume 2023 product. Intel will announce today that the compute tile / chiplet will finish tape-in (design IP verification) by Q2 2021, and will leverage Intel’s advanced packaging techniques. After design manufacturing, tape-out (whole chip design verification) usually takes 4-6+ months, and then the designs are sent to the fabs for initial production and test runs. Given that Intel is talking about Meteor Lake as a chiplet design, starting with this compute tile, then no doubt an IO related chipset will be announced at a later time. Intel has a number of packaging technologies it could deploy here, such as EMIB or Foveros, depending on the cost intercept of the intended market.Edit: Gelsinger confirmed that Meteor Lake will have Foveros technology.Expand (TSMC)Alongside building new manufacturing facilities, Intel today will reaffirm its roadmap to use a mixture of internal and external process node manufacturing depending on the product capabilities. Intel already makes heavy use of external partners, such as TSMC, and already spends enough to account for 6-7% of TSMC's annual revenue. But today’s announcement will double down on ensuring that Intel is ready to use the right process at the right time for the right products.This includes developing its leading edge products on external foundry offerings. As Intel moves more into a chiplet ecosystem (Intel is calling it ‘tiles’), the company is prepared to manufacture its high-performance computing chiplets on external foundries. This means in both client and the datacenter, and likely means we will be seeing the latest x86 cores enabled beyond Intel. Exact announcements will follow.Gelsinger’s presentation today will discuss that using external partners like TSMC, Samsung, GlobalFoundriers, and UMC, will allow the company to optimize roadmaps for cost, performance, schedule, and supply. This does somewhat go against the grain of the IDM 2.0 messaging, where Intel can control its own supply chain and expand production as required, however Intel expects it can find a happy medium.Productize (IFS)The two new manufacturing plants / fabs also filter under this Expand section, however this area is more focused on how Intel will keep those fabs fully occupied. Other semiconductor manufacturers in the market, such as TSMC, Samsung, GlobalFoundries, SMIC etc, they all have what are called foundry services which allow customers to build silicon using their manufacturing technology. With today’s announcement, the company is ready to enable its external Intel Foundry Services (IFS) to new customers. IFS will be a standalone company, with unique access to Intel's current and future offerings.Intel has made silicon for others before, so this isn’t new. However, that project came at a time where Intel’s 10nm faltered, and the company lost a number of high-profile contracts with partners as a result. One of the issues is that Intel at the time used so many customized software tools in its silicon design process that it limited its customers’ access to these tools to build processors. This made the whole process very complicated.The new Intel Foundry Services look will be a lot different. Intel today will have partnership announcements with Cadence and Synopsys to enable industry standard design tools (EDA tools) and workflows such that customers can use industry standard process development kits (PDKs) to build their silicon designs. This is part of the job that some of Intel’s hires have recently been enabling, such as Renduchintala, Keller, and Koduri. Intel is committing to embracing the entire EDA ecosystem to ease new customers into the use of Intel’s foundry tools.There will be somewhat of a black cloud over Intel on how its external foundry offerings have failed in the past, however Gelsinger and the company are hoping that commitments to industry standards will help on that path to rebuilding trust and reputation.As part of Intel’s Foundry Services, the company is announcing that it will work with customers to build SoCs with x86, Arm, and RISC-V cores, as well as leveraging Intel’s IP portfolio of core design and packaging technologies. What will be key here is the extent of exactly how Intel will offer its x86 designs – it could offer them in a licensing style similar to Arm, allowing customers to go build their own SoCs, or it could be only in a custom design services model, where you tell Intel what you want and they design/manufacture it for you. More details about how this will work is expected to come through the year.The drive into Foundry Services is an obvious choice for Intel. Demand for semiconductor manufacturing is at an all-time high, and discussions about keeping manufacturing inside the US has been a key talking point for over a year. Intel has stated that it has keen enthusiastic support from the industry for IFS, but it will also disclose later in the year how it will expand its manufacturing capabilities in other parts of the world, such as Europe.Collaboration with IBM, New Intel Events for EngineersAlongside the core IDM 2.0 path, Intel also has announcements relating to the future of its R&D roadmap, as well as its outreach to engineers and commercial partners.The collaboration with IBM on process node development and development of next-generation logic is the wildcard of today’s announcements. The two companies are set to work together on foundational technologies to move the needle on both semiconductor performance and semiconductor efficiency. The collaboration will scale through to the ecosystem, with a significant nod towards key US government initiatives.Intel’s announcement today on this is light on details. The teams from both Oregon and New York would seem to initially start separate with collaboration-at-a-distance, although the nature of the wording of the announcement would seem to suggest that the two will come together with a semi-unified team at a later date. Intel is keeping details on the topics for research at a very high-level, but both companies have a range of expertise on fundamental silicon design as well as complex manufacturing. This might also be a nod to IBM getting access to Intel’s leading edge technologies for its POWER and z product offerings.Also on the cards is Intel’s new series of events for engineers and commercial partners. Broadly marketed as Intel’s *on series, Gelsinger aims to rekindle the spirit behind Intel’s previous popular events such as Intel’s Developer Forum (IDF) with a new range of Intel Vision (commercial) and Intel Innovation (engineering) outreach.The first of these events will be an Intel Innovation Event, coming in October later this year in San Francisco. If we are travelling to events again, no doubt we will be there to report on the highlights of Intel’s strategy at that point of time.Pat Gelsinger’s Intel Legacy: Part OneOver the many interviews that Gelsinger has given over the years, his love of Intel is a persistent thread. Having finished his degree at the early age of 18, he was hired straight out of college and spent over 30 years at the company, reaching the title of Chief Technology Officer. Gelsinger then spent 12 years at EMC (now Dell EMC) and VMWare, and has come back as CEO. The hiring of an engineer at the top of the company has, in Intel’s own words, reinvigorated the enthusiasm of its engineering base.It’s no secret that while Intel has been rewarded with record revenues these past five years, the state of play of Intel’s manufacturing roadmaps have stalled. Intel’s future has been in flux, affecting employees and followers of the company alike, leading to many column inches about how the company moves forward. The announcement of Pat Gelsinger to the role of CEO, plus his remarks during Intel’s FY2020 call, seems to have lit a fire inside Intel. Today’s announcements are the first stage in Pat Gelsinger building his CEO legacy at the company.Intel’s path with the announcements today is very much towards a more flexible Intel. In the past where the company was rigid with its designs, rigid with its manufacturing, it is clear that Gelsinger wants to be more open and accepting of industry standard manufacturing whilst at the same time re-offering its facilities to external customers. The mantra of Intel staying on Intel manufacturing also seems to dissolve slightly, with Gelsinger not afraid to mention the name of other foundry offerings that Intel is prepared to use.Our discussions with employees at Intel who know Pat Gelsinger have repeatedly said that he likes to ‘geek out’ over the details of new technology, and will happily sit for hours talking about both the direction of the industry as well as how Intel is positioned alongside its competition. Pat, if you’re reading this, we’re ready to geek out whenever you are.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16573/intels-new-strategy-20b-for-two-fabs-meteor-lake-7nm-tiles-new-foundry-services-ibm-collaboration-return-of-idf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: OnePlus Announces OnePlus 9R, OnePlus 9 & OnePlus 9 Pro\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-03-23T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/16569/oneplus-announces-oneplus-9-oneplus-9-pro\n",
      "Content: Today OnePlus is launching their new 2021 line-up of flagship devices, the new OnePlus 9 series. This year, the company is refining its formula that it hademployed in 2020 following the successful OnePlus 8 series, doubling down on aspects that make the company’s devices no longer value alternatives to the competition, but outright amongst the best devices you can get on the market.OnePlus is launching three devices today, the regular new OnePlus 9, the OnePlus 9 Pro, and a more budget oriented refresh in the form of the OnePlus 9R. The OnePlus 9 Pro follows in the footsteps of the 8 Pro last year in that it’s a no-compromise device, and this year OnePlus is again equipping the phone with the best possible display hardware that’s available on the market. Also, a change in the usual model line-up is the fact that both the OnePlus 9 and OnePlus 9 Pro feature the same two new-generation camera modules and sensors, with OnePlus promising great advances in photography and picture quality in collaboration with Hasselblad – an area in the last few generations which was admittedly a weak-point for the company in the last few generations.OnePlus 9 SeriesOnePlus 9ROnePlus 9OnePlus 9 ProSoCSnapdragon 8701x Cortex A77@3.2GHz3x Cortex A77@ 2.42GHz4x Cortex A55@ 1.80GHzAdreno 650 @ 670MHzQualcomm Snapdragon 8881xCortex-X1@ 2.84GHz 1x1024KB pL23xCortex-A78@ 2.42GHz 3x512KB pL24x Cortex-A55 @ 1.80GHz 4x128KB pL24MB sL3Adreno 660 @ 840MHzDisplay6.55-inchFHD+ 2400 x 1080 (20:9)120Hz Refresh Rate6.7-inchQHD+ 3216 x 1440 (20.1:9)120Hz Refresh RateLTPO HW VRRSAMOLEDHDR10+Dimensions161.0 x 74.1 x 8.4 mm189g160.0 x 74.2 x 8.7 mm192g163.2 x 73.6 x 8.7 mm197gRAM8/12GB LPDDR4X8/12GBLPDDR5NANDStorage128/256 GB UFS 3.1128/256 GB UFS 3.1Battery4500mAh (17.41Wh) typ.65W Warp-Charge (Proprietary)45W USB-PD-15 Qi Wireless30W Wireless Fast Charging(only via proprietary charger)5W Qi WirelessFront Camera16MP Sony IMX471f/2.4Primary Rear Camera48MP 0.8µm 1/2\" IMX586(12MP 1.6µm 2x2 binning)f/1.7 w/ OIS48MP 1.12µm 1/1.4\" IMX689(12MP 2.24µm 2x2 binning)f/1.8 w/ OIS23mm equivalent48MP 1.12µm 1/1.4\" IMX789(12MP 2.24µm 2x2 binning)f/1.8 w/ OIS23mm equivalentSecondaryRear CameraUltra-Wide-Angle16MP IMX481123° FoVUltra-Wide-Angle50MP 1.0µm 1/1.56\" IMX766Rectilinear Free-Form Opticsf/2.214mm equivalentTertiaryRear Camera5MP Macro-3.3x Telephoto8MP OV08a10w/ OISExtraCamera2MP Monochrome2MP Monochrome2MP Monochrome4G / 5GModemX55 (Discrete)(LTE Category 24/22)DL = 2500 Mbps7x20MHz CA, 1024-QAMUL = 316 Mbps3x20MHz CA, 256-QAMX60 integrated(LTE Category 24/22)DL = 2500 Mbps7x20MHz CA, 1024-QAMUL = 316 Mbps3x20MHz CA, 256-QAM(5G NR Sub-6 + mmWave)DL = 7500 MbpsUL = 3000 MbpsSIM SizeNanoSIM + NanoSIMWireless802.11a/b/g/n/ac/axBT 5.2 LE, NFC, GPS/Glonass/Galileo/BDSConnectivityUSB Type-Cno 3.5mm jackSpecial FeaturesOn-screen fingerprint sensor,Stereo SpeakersOn-screen fingerprint sensor,Stereo SpeakersLaunch OSAndroid 11 w/ Oxygen OSLaunch Prices8+128GB:₹39 99912+256GB:₹43,9998+128GB:$729 / £629 / 699€12+256GB:£729 / 799€8+128GB:$1069 / £829 / 899€12+256GB:£929 / 999€Starting off with the new OnePlus 9 and 9 Pro, both devices now feature the new Snapdragon 888 SoC from Qualcomm. We’veextensively reviewed the chip in our coverage of the S21 Ultra, and whilst generationally it might not be as great as we’ve been used to in the last few years, it’s an improvement and will likely have no issues in giving you excellent device experiences in these new smartphone models.The lower-end model OnePlus 9R uses Qualcomm’s new higher-binned Snapdragon 865 variant called the Snapdragon 870 – essentially a faster variant of the 865+ with main CPU clocks of up to 3.2GHz. It’s to be noted that the actual performance differences between this chip and the newer Snapdragon 888 in the 9 and 9 Pro should be rather small, so it’s definitely a valid way to create a more budget flagship device this year.OnePlus keeps the DRAM and memory configurations similar this year with either 8 or 12GB of main memory, alongside 128 or 256GB storage options. The storage has been updated to a faster UFS 3.1 module.The OnePlus 9 Pro: All the bells and whistlesWe start off with the new OnePlus 9 Pro which is the star of today’s announcement. Following up the 8 Pro last year, OnePlus again integrates everything but the kitchen sink into the new high-tier flagship smartphone this year – however doesn’t quite go as far as to venture into the “Ultra” category of devices that have emerged with Samsung’s bigger flagship megalodons, and being followed by some other vendors.At 163.2 x 73.6mm footprint, the OnePlus 9 Pro remains relatively mid-size by today’s standards, and is actually narrower than the 8 Pro by 0.8mm. Generally, I find this to be an important characteristic of the phone as the competition’s top-end devices keep getting bigger and heavier.The phone’s design appears to follow closely the ergonomics of the regular OnePlus 8 last year, featuring rounded front display glass and back cover glass – however with the curvatures on both sides being more symmetrical to each other, compared to the more prominent front display curvature of the 8 Pro design. This is actually quite welcome and a big plus in my view as I had found the regular OnePlus 8 to be an extremely ergonomic device.The star feature of the OnePlus 8 Pro is the new display – it’s still a 6.7” 3216 x 1440 resolution and 120Hz refresh rate display like the 8 Pro, however this year OnePlus has moved on to a newer LTPO panel technology and promises hardware adaptive refresh rates, much like employed inthe Note20 Ultraor the newS21 Ultra. As we’ve seen on those devices, the hardware LFD/VRR is a key component to advancing battery life of the device when under 120Hz mode into usable every-day experiences. OnePlus advertises new 8192 brightness levels, indicating that the new DDIC is 10-bit capable. It’s not clear if the emitter on this display features the same new high-efficiency characteristics as on the S21 Ultra panel – but even if it doesn’t, the OnePlus 9 Pro should have among one of the best screens of any phone in 2021.The 9 Pro comes in a glossy silver, matte green, or sandstone-like matte black finishes.The OnePlus 9: Same primary cameras, more budget screenIn comparison to the 9 Pro, the new regular OnePlus 9 features a more cut-down display. In its dimensions it’s smaller at 6.55”, but it’s not curved, and thus doesn’t affect much the device footprint which lands in at 74.2mm width, actually larger than the 9 Pro.It’s a 2400 x 1080 resolution screen with 120Hz refresh rate, and is of a more regular panel technology as we’ve seen in past years. The display would have to rely on more coarse software based refresh rate switching to achieve higher power efficiency at 120Hz, we currently don’t have any more details on how this is implemented on this phone.From the back of the phone, the OnePlus 9 is extremely similar to the OnePlus 9 Pro, only differentiated by the camera setup differences. The front is more obvious as it’s a 2.5D display, being more flat for much of its surface, bar the extreme edges.What’s quite unusual and not immediately visible from pictures is the fact that the mid-frame of the phone is made of a fibre-glass composite material, rather than metal, and a metallisation finish. It’s an interesting choice that we haven’t seen before in the industry – usually it’s the back cover which gets a material change, not the actual frame of the phone.New Cameras: Partnership with Hasselblad?One of the big new features of the OnePlus 9 and 9 Pro are their completely new camera setups. It differs quite considerably from the more exotic attempts we’ve seen in the market, and opts to go for a simpler, but extremely solid camera setup.In terms of main sensor of the 9 Pro, we’re seeing a new IMX789 sensor landing in at 48MP. The sensor has 1.12µm pixels and a Quad-Bayer colour filter setup, regularly binning down to 12MP 2.24µm effective pixels. OnePlus here also advertises it’s a native 12-bit ADC capable unit, which should increase the dynamic range that the module is able to capture.The ultra-wide sensor on the new phones is also very interesting. The sensor is a 50MP IMX766, the same sensor OPPO had used in the main camera sensors of the Find X3 Pro, featuring 1.0µm pixels that bins down to 12.5MP shots with 2µm effective pixel sizes.What is actually most interesting about the Ultra-Wide module here isn’t particularly the sensor, but the optics. OnePlus is employing newer advanced free-form lens optics which are able to correct the distortion of ultra-wide captures into rectilinear captures, meaning that throughout the frame, straight edges remain straight. This is actually a big deal when it comes to image quality of the sensor as correcting distortions optically is infinitely higher quality than correcting them in post-processing, meaning we should be expecting much sharper images out of the OnePlus 9 and 9 Pro’s UWA’s module. Huawei had been the first to employ this technology, but only used it in the super-high end Mate 40 Pro+, which is scantily available.Exclusive to the OnePlus 9 Pro, we also see a 3.3x telephoto module in the form of a 8MP OV08a10 sensor. How the quality of this sensor pans out compared to the main module’s native high-res modes remains to be seen. Generally, it’s interesting that OnePlus here doesn’t go much more aggressive with the telephoto, and the company resists of using some of the more exotic periscope or more aggressive sensor or optics setups compared to other competitors. I find this a rather sensible approach, and I do like if that means that the size and weight of the phone as well as the camera bump remains reasonable.While the ultra-wide sensor between both phones is the same, the regular 8 uses lasty year’s IMX686 sensor module. The main specifications are the same in terms of 48MP 1.12µm – but the older sensor lacks the newer 12-bit capture capability and likely has the worse dynamic range compared to the 8 Pro.Finally, the wildcard in the whole camera setup is on the part of the software. OnePlus has been marketing their collaboration with Hasselblad on the camera system, and they promise much better colour calibration this generation. We’ve seen these kinds of collaborations before in the market, most notably maybe Huawei with their partnership with Leica. This can vary from hardware co-design on the part of the optics through to actual image processing input from the partnering company. Exactly how the Hasselblad partnership will help OnePlus isn’t clear beyond their promises of more accurate colour reproductions, but at least I hope this results in better tone-mapping, gamma curves and realistic colour reproduction for the new phones, as it always had been the greatest area of weakness in past OnePlus camera implementations.OnePlus 9R - Budget Refresh, Limited MarketsAlongside the new 9 and 9 Pro, there’s also a 9R. This is seemingly a more budget oriented device. Alongside the Snapdragon 870 SoC, the phone features the same display as the OnePlus 9, but differs considerably in the camera department.With a 48MP IMX586 main camera sensor and a 16MP ultra-wide, the camera setup here seems to be in line with the OnePlus 8 and 8T of last year, and adds in a 5MP macro sensor alongside the enigmatic 2MP monochrome unit. Generally, I’m pessimistic about the camera quality here, particularly if OnePlus is also just inheriting the camera processing from last generation – there’s no Hasselblad partnership on this model.In terms of availability, we won’t be seeing the 9R in western markets, with the phone being reserved only for Asian markets such as India, starting at ₹39,999.Availabiltiy & PricingIn terms of pricing for the OnePlus 9 and 9 Pro, things are a bit interesting as the company is employing quite different pricing strategies depending on the market.In terms of pricing for the OnePlus 9 and 9 Pro, things are a bit interesting as the company is employing quite different pricing strategies depending on the market.In the US, the OnePlus 9 starts at $729 which is extremely attractive and very competitive to most alternatives on the market. The 9 Pro however starts in at $1069, which is quite steep, particularly considering we’ve seen massive discounts from the likes of Samsung, with an S21 Ultra actually coming in cheaper than the 9 Pro at the time of writing. What’s to be noted here is that OnePlus seemingly isn’t offering the baseline 8+128GB model of the 9 Pro in the US.In Europe, the pricing is also very interesting on the baseline OnePlus 9, starting at 699€ - notably undercutting the competition in this range. What’s much more interesting is the 899€ price point of the 9 Pro in the 8+128GB model. The 12+256GB model is also 999€ - actually less than the expected 1:1 conversion from USD to EUR that we typically see.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16569/oneplus-announces-oneplus-9-oneplus-9-pro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qualcomm Completes Acquisition of NUVIA: Immediate focus on Laptops (Updated)\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-03-16T15:45:00Z\n",
      "URL: https://www.anandtech.com/show/16553/qualcomm-completes-acquisition-of-nuvia\n",
      "Content: Today Qualcomm is announcing that the company has completed the acquisition of NUVIA, a start-up company consisting of industry veterans who were behind the creation of Apple’s high-performance CPU cores, and who were aiming to disrupt the server market with a new high-performance core called “Phoenix”.The acquisition had been announced only several weeks ago in mid-January, so the whole process has been extremely speedy in terms of timeline.“Qualcomm Incorporated (NASDAQ: QCOM) today announced that its subsidiary, Qualcomm Technologies, Inc., has completed its acquisition of the world-class CPU and technology design company, NUVIA for $1.4 billion before working capital and other adjustments.”Today Qualcomm even went as far as put out a concrete roadmap for new SoCs using the newly acquired IP from Nuvia:“The first Qualcomm Snapdragon platforms to feature Qualcomm’s new internally designed CPUs are expected to sample in the second half of 2022 and will be designed for high performance ultraportable laptops. “Sampling in late 2022 would require a tape-out in early 2022, and a design-in essentially as soon as possible following the acquisition today. The whole process seems extremely fast and aggressive in terms of timing, pointing out that Qualcomm is putting a lot of emphasis on the project.Qualcomm had shown a lot of positive reaction to Apple M1, I quoteour interview with Alex Katouzianfrom back in December in terms of their reaction to the competitor design:“[…] the laptops these days are really moving towards mobile. The camera is super important. The audio is super important. The battery life is super important. Not having a fan is super important. Portability, thinness, connectivity, always-on always-connected, all those traits of mobile are moving to the PC.And people say, imitation is the best form of flattery. Look at look what happened with the [Apple] M1. Their product pitch is almost a duplicate of what we've been saying for the past two or three years.”NUVIA’s prompt acquisition and immediate disclosure of plans to tackle the high-performance ultraportable laptop market could be seen as Qualcomm’s direct response to the new Apple M1 powered laptops and to compete with their high-performance CPU cores.Article Update:We had the opportunity to have a call with Qualcomm’s Keith Kressin, SVP and GM, Edge Cloud and Computing, answering several questions as for company’s current plans for the NUVIA team. Qualcomm views the acquisition as an important strategic addition to the company’s design capabilities, filling a gap in IP design where the company for several years now had been relying on external IP such as Arm’s Cortex cores. Keith made important note of this ability to have total in-house design control over every IP block in an SoC, allowing the company better flexibility to respond to market demands and creating competitive products.The immediate goals for the NUVIA team will be implementing custom CPU cores into laptop-class Snapdragon SoCs running Windows, and enable the company to offer higher performance CPUs than would have been otherwise possible. When asked about plans for other product stacks and the possibility of using both in-house CPUs as well as continuing to use Arm Cortex CPU IP for lower segments, it was stated that Qualcomm will continue to evaluate every metric and choose the best fitting design that makes the most sense for that product category.We asked the team if Qualcomm would continue to invest into NUVIA’s original plans to enter the server and enterprise market, with a response that this wasn’t the main goal or motivation of the acquisition, that Qualcomm however would very much keep that as an open option for the future, and let the NUVIA team explore those possibilities. Keith here acknowledged that it’s a tough market to crack, and that Qualcomm had made no definitive decisions yet in terms of long-term planning.Related Reading:Qualcomm to Acquire NUVIA: A CPU Magnitude ShiftNUVIA Completes Series B Funding Round: $240MNUVIA Phoenix Targets +40-50% ST Performance Over Zen 2 for Only 33% the PowerNUVIA: New Server CPU Startup Going After Intel and AMD\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16553/qualcomm-completes-acquisition-of-nuvia\n",
      "Title: AMD 3rd Gen EPYC Milan Review: A Peak vs Per Core Performance Balance\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-03-15T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/16529/amd-epyc-milan-review\n",
      "Content: Disclaimer June 25th:The benchmark figures in this review have beensuperseded by our second follow-up Milan review article, where we observe improved performance figures on a production platform compared to AMD’s reference system in this piece.Section by Ian CutressThe arrival of AMD’s 3rdGeneration EPYC processor family, using the new Zen 3 core, has been hotly anticipated. The promise of a new processor core microarchitecture, updates to the connectivity and new security options while still retaining platform compatibility are a good measure of an enterprise platform update, but the One True Metric is platform performance. Seeing Zen 3 score ultimate per-core performance leadership in the consumer market back in November rose expectations for a similar slam-dunk in the enterprise market, and today we get to see those results.AMD EPYC 7003: 64 Cores of MilanThe headline numbers that AMD is promoting with the new generation of hardware is an increase in raw performance throughput of +19%, due to enhancements with the new core design. On top of this, AMD has new security features, optimizations for different memory configurations, and updated performance with the Infinity Fabric and connectivity.3rd Gen EPYCAnyone looking for the shorthand specifications on the new EPYC 7003 series, known by its codename Milan, will see great familiarity with the previous generation, however this time around AMD is targeting several different design points.Milan processors will offer up to 64 cores and 128 threads, using AMD’s latest Zen 3 cores. The processor is designed with eight chiplets of eight cores each, similar to Rome, but this time all eight cores in the chiplet are connected, enabling an effective double L3 cache design for a lower overall cache latency structure. All processors will have 128 lanes of PCIe 4.0, eight channels of memory, with most models supporting dual processor connectivity, and new options for channel memory optimization are available. All Milan processors should be drop-in compatible with Rome series platforms with a firmware update.AMD EPYC: Generation on GenerationAnandTechEPYC7001EPYC7002EPYC7003CodenameNaplesRomeMilanMicroarchitectureZenZen 2Zen 3Core Manufacturing14nm7nm7nmMax Cores/Threads32 / 6464 / 12864 / 128Core Complex4C + 8MB4C + 16MB8C + 32MBMemory Support8 x DDR4-26668 x DDR4-32008 x DDR4-3200Memory Capacity2 TB4 TB4 TBPCIe3.0 x1284.0 x1284.0 x128SecuritySMESEVSMESEVSMESEVSNPPeak Power180 W240 W*280 W*Rome introduced 280 W for special HPC mid-cycleOne of the highlights here is that the new generation of processors will offer 280 W models to all customers – previous generations had only 240 W models for all and then 280 W for specific HPC customers, however this time around all customers can enable those high performance parts with the new core design.This is exemplified if we do direct top-of-stack processor comparisons:2P Top of Stack GA OfferingsAnandTechEPYC7001EPYC7002EPYC7003IntelXeonProcessor7601774277636258RuArchZenZen 2Zen 3CascadeCores32646428TDP180 W240 W280 W205 WBase Freq2200225024502700Turbo Freq3200340035004000L3 Cache64 MB256 MB256 MB37.5 MBPCIe3.0 x1284.0 x1284.0 x1283.0 x48DDR48 x 26668 x 32008 x 32006 x 2933DRAM Cap2 TB4 TB4 TB1 TBPrice$4200$6950$7890$3950The new top processor for AMD is the EPYC 7763, a 64-core processor at 280 W TDP offering 2.45 GHz base frequency and 3.50 GHz boost frequency. AMD claims that this processor offers +106% performance in industry benchmarks compared to Intel’s best 2P 28-core processor, the Gold 6258R, and +17% over its previous generation 280 W version the 7H12.Peak Performance vs Per Core PerformanceOne of AMD’s angles with the new Milan generation is going to be targeted performance metrics, with the company not simply going after ‘peak’ numbers, but also taking a wider view for customers that need high per-core performance as well, especially for software that is invariably per-core performance limited or licensed. With that in mind, AMD’s F-series of ‘fast’ processors is now being crystallized in the stack.AMD EPYC 7003 F-SeriesProcessorsCoresThreadsBaseFreqTurboFreqL3(MB)TDP(W)PriceF-SeriesEPYC 75F332 / 6429504000256( 8 x 32 )280 W$4860EPYC 74F324 / 4832004000240 W$2900EPYC 73F316 / 3235004000240 W$3521EPYC 72F38 / 1637004100180 W$2468These processors have the peak single threaded values of anything else in AMD’s offering, along with the full 256 MB of L3 cache, and in our results get the best scores on a per-thread basis than anything else we’ve tested for enterprise across x86 and Arm – more details in the review. The F-series processors will come at a slight premium over the others.AMD EPYC: The Tour of ItalyThe first generation of EPYC was launched in June 2017. At that time, AMD was essentially a phoenix: rising from the ashes of its former Opteron business, and with a promise to return to high-performance compute with a new processor design philosophy.At the time, the traditional enterprise customer base were not initially convinced – AMD’s last foray into the enterprise space with a new generation of paradigm-shifting processor core, while it had successes, fell flat as AMD had to stop itself from going bankrupt. Opteron customers were left with no updates in sight at the time, and to the willingness to jump on an unknown platform from a company that had stung so many in the past was not a positive prospect for many.At the time, AMD put out a three year roadmap, detailing its next generations and the path the company would take to overcoming the 99% market share behemoth in performance and offerings. These were seen as lofty goals, and many sat back willing to watch others take the gamble.1st Gen EPYC LaunchAs the first generation Naples was launched, it offered impressive some performance numbers. It didn’t quite compete in all areas, and as with any new platform, there were some teething issues to begin. AMD kept the initial cycle to a few of its key OEM partners, before slowly broadening out the ecosystem. Naples was the first platform to offer extensive PCIe 3.0 and lots of memory support, and the platform initially targeted those storage or PCIe heavy deployments.2nd Gen EPYC LaunchThe second generation Rome, launched in August 2019 (+26 months) created a lot more fanfare. AMD’s newest Zen 2 core was competitive in the consumer space, and there were a number of key design changes in the SoC layout (such as moving to a NUMA flat design) that encouraged a number of skeptics to start to evaluate the platform. Such was the interest that AMD even told us that they had to be selective with which OEM platforms they were going to assist with before the official launch. Rome’s performance was good, and it scored a few high-profile supercomputer wins, but more importantly perhaps it showcased that AMD was able to execute on that roadmap back in June 2017.That flat SoC architecture, along with the updated Zen 2 processor core (which actually borrowed elements from Zen 3) and PCIe 4.0, allowed AMD to start to compete on performance as well as simply IO, and AMD’s OEM partners have consistently been advertising Rome processors as compute platforms, often replacing two Intel 28-core processors for one AMD 64-core processor that also has higher memory support and more PCIe offerings. This also allows for compute density, and AMD was in a place where it could help drive software optimizations for its platform as well, extracting performance, but also moving to parity on the edge cases that its competitors were very optimized for. All the major hyperscalers also evaluated and deployed AMD-based offerings for their customers, as well as internally. AMD’s sticker of approval was pretty much there.3rd Gen EPYC CPUAnd so today AMD is continuing that tour of Italy with a trip to Milan, some +19 months after Rome. The underlying SoC layout is the same as Rome, but we have higher performance on the table, with additional security and more configuration options. The hyperscalers have already been getting the final hardware for six months for their deployments, and AMD is now in a position to help enable more OEM platforms at launch. Milan is drop-in compatible with Rome, which certainly helps, but with Milan covering more optimization points, AMD believes it is in a better position to target more of the market with high performance processors, and high per-core performance processors, than ever before.AMD sees the launch of Milan as that third step in the roadmap that was shown back in June 2017, and validation on its ability to execute reliably for its customers but also offer above industry standard performance gains for its customers.The next stop on the tour of Italy is Genoa, set to use AMD’s upcoming Zen 4 microarchitecture. AMD has also said that Zen 5 is in the pipeline.CompetitionAMD is launching this new generation of Milan processors approximately 19 months after the launch of Rome. In that time we have seen the launch of both Amazon Graviton2 and Ampere Altra, built on Arm’s Neoverse N1 family of cores.Milan Top-of-Stack CompetitionAnandTechEPYC7003AmazonGraviton2AmpereAltraIntelXeonPlatformMilanGraviton2QuickSilverCascadeProcessor7763Graviton2Q80-336258RuArchZen 3N1N1CascadeCores64648028TDP280 W?250 W205 WBase Freq2450250033002700Turbo Freq3500250033004000L3 Cache256 MB32 MB32 MB37.5 MBPCIe4.0 x128?4.0 x1283.0 x48DDR48 x 32008 x 32008 x 32006 x 2933DRAM Cap4 TB?4 TB1 TBPrice$7890N/A$4050$3950From Intel, the company has divided its efforts between big socket and little socket configurations. For big sockets (4+) there is Cooper Lake, a Skylake derivative for select customers only. For smaller socket configurations (1-2), Intel is set to launch its 10nm Ice Lake portfolio at some point this year, but as yet it still remains silent on exact dates. To that end, all we have to compare Milan to is Intel’s Cascade Lake Xeon Scalable platform, which was the same platform we compared Rome to.Interesting times for sure.This ReviewFor this review, AMD gave us remote access to several identical servers with different processor configurations. We focused our efforts on the top-of-the-stack EPYC 7763, a 280 W 64-core processor, the EPYC 7713, a 225 W 64-core processor, and the EPYC 7F53, a 280 W 32-core processor designed as the halo Milan processor for per-core performance.On the next page we will go through AMD’s Milan processor stack, and its comparison to Rome as well as the comparison to current Intel offerings. We then go through our test systems, discussions about our SoC structure testing (cache, core-to-core, bandwidth), processor power, and then into our full benchmarks.This Page, The OverviewMilan Processor OfferingsTest Bed Setups, Compiler OptionsTopology, Memory Subsystem and LatencyProcessor Power: Core vs IOSPEC: Multi-Thread PerformanceSPEC: Single-Thread PerformanceSPEC: Per Core Performance Win for 75F3SPECjbb MultiJVM: Java PerformanceCompilation and Compute BenchmarksConclusions and End RemarksThese pages can be accessed by clicking the links, or by using the drop down menu below.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16529/amd-epyc-milan-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Xiaomi Mi 11 Review: A Gorgeous Screen and Design\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-03-10T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16447/the-xiaomi-mi-11-review\n",
      "Content: Xiaomi had first announced the Mi 11 inback in the last few days of 2020, with availability of the domestic Chinese version of the phone taking place in the early weeks of January. Later in February, the companyfollowed up with the global launch of the device, divulging the pricing strategy in markets such as Europe.The Mi 11 is a very interesting device – it was actually the very first Snapdragon 888 device worldwide, and to this date in many countries still is the only such available smartphone – at least until other vendors catch up with their model releases. In the past year or so, we’ve seen Xiaomi having gained a lot of market share in global countries, mostly picking up customers from Huawei – particularly in European markets where Xiaomi has been seeing lots of new success with carriers.What characterises the Mi 11 from other devices – other than the new Snapdragon 888 SoC, is Xiaomi’s new design cantered around a new 6.81” AMOLED 3200 x 1440 screen with up to 120Hz refresh rate. It’s not Xiaomi’s first QHD device as we’ve seen some other implementations in the past, but those were all on LCD panels. The move to QHD and 120Hz this generation is a major leap for the Mi 11 series, particularly because Xiaomi is still pricing the device starting at only 749€ - much less than other current generation devices featuring the same specifications such as the S21 Ultra.Xiaomi Mi SeriesMi 10Mi 11(Reviewed)SoCQualcomm Snapdragon 8651x Cortex-A77 @ 2.84GHz3x Cortex-A77 @ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 650 @ 587MHzQualcomm Snapdragon 8881xCortex-X1@ 2.84GHz3xCortex-A78@ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 660 @ 840MHzDRAM8GB LPDDR5-55008/12GB LPDDR5-6400Display6.67\" AMOLED2340 x 1080 (19.5:9)90Hz Refresh6.81\" AMOLED3200 x 1440120Hz Refresh480Hz TouchHDR10+ /10-bit panelSizeHeight162.58mm164.3mmWidth74.80mm74.6mmDepth8.96mm8.06mmWeight208g196gBattery Capacity4780mAh (Typical)30W Charging4600mAh (Typical)55W ChargingWireless Charging30W50WRear CamerasMain108MP 1/1.3\" 0.8µm4:1 Binning to 27MP / 1.6µmf/1.69 w/ OIS108MP Modulef/1.85 w/OISTelephoto-5MP (Macro only)50mm eq.f/2.2ExtraTelephoto--Ultra-Wide13MP 1.12µmf/2.4117° FoV13MPf/2.4123° FoVExtra2MP Depth Camera2MP Macro Camera-Front Camera20MP 0.8µmf/2.3f/2.2Storage128 / 256GBUFS 3.0128 / 256GBUFS 3.1I/OUSB-CWireless (local)802.11ax(Wifi 6),Bluetooth 5.1Cellular4G + 5G NR NSA+SA Sub-6GHzSpecial FeaturesUnder-screen fingerprint sensorFull-range stereo speakersSplash, Water, Dust ResistanceNo ratingDual-SIM2x nano-SIMLaunch OSAndroid 10 w/ MIUIAndroid 11 w/ MIUILaunch Price8+128GB: 799€8+256GB: 899€8+128GB: 749€8+256GB: 799€As mentioned, the new phone is powered by Qualcomm’s newest generation Snapdragon 888 SoC. We’ve covered the new chip extensively inour chipset-centric article of the Galaxy S21 Ultrawhere we pitted the new Snapdragon against the latest Exynos. In general, although the new chip does bring performance advantages this generation, it has to compromise in power efficiency as the new process node had seen some regressions compared to last year.Xiaomi offers the Mi 11 in 8GB LPDDR5-6400 configurations in the global markets – the 12GB variant only exists in China. Storage comes in the form of either 128GB or 256 of UFS 3.1. Xiaomi is one of the vendors who do not offer expandable storage, but at least the 256GB version of the phone only costs an extra 50€ this year, half the cost of last year’s configuration up-sell.The new display is a key feature of the phone and probably the main characteristic of the device. After a few years of 1080p OLED flagships, I’m very happy to see Xiaomi finally jump over to QHD, particularly given the phones are growing in size and the PPI of 1080p really isn’t sufficient at these dimensions. Xiaomi of course had to include 120Hz this generation as well as it’s a must-have feature to incorporate in 2021. Surprisingly, Xiaomi advertises that this is a 10-bit panel, which should offer better brightness and colour graduations, however comes at a cost power efficiency.The screen lacks variable refresh rate – neither hardware, nor software based,so it’s more in line with the screen technology generation we’ve seen in the Galaxy S20 and OnePlus 8 Pro series devices.Edit March 12th:The Mi 11 does have a coarse software-based refresh rate switching mechanism, however it does not function below 110 nits screen brightness (around 70% on the brightness slider).The design of the Xiaomi Mi 11 is quite attractive, and the company goes for an almost completely rounded aesthetic, with curved front and back glass. The back glass is a matte chemically etched finish similar to that of the Mi 10 Pro – it’s still quite smooth, but it’s much better than glossy materials.What’s maybe a bit weird about the design is the corners of the screen and the frame, the metal frame actually covers most of the phone’s thickness in the corners, unlike the sides where the front display curves around. It’s a design that reminds me of the Huawei P40 Pro – it’s a bit odd at first glance, but I think the companies are either doing this on purpose for better drop resistance, or it’s a manufacturing side-effect of the new build.The side frames on both sides are extremely minimal and thin, which give the phone excellent ergonomics and in-hand feel even though it’s a larger device at 74.6mm width.Xiaomi also managed to shave off some thickness this generation, going from 8.96mm to 8.06mm, a difference that’s immediately noticeable between generations. The weight of the phone has also been reduced from 208g to 196g, which is also very welcome in my book. The reduced form-factor cost 180mAh from its predecessor, and the new battery lands in at 4600mAh which is still plenty respectable.On the right side of the phone, we see a simple textured power button as well as the volume rocker – the metal side frame here slightly extends towards the back glass cover which reminds me of Samsung’s recent aesthetic.The main camera is the same as last year’s 108MP Xiaomi shooters – a Samsung HMX sensor that natively captures 108MP shots, but in general photography 2x2 bins things down to 27MP. Since there’s no telephoto module on the phone, the Mi 11 uses the raw resolution of this module to get tighter framed shots. The optics were changed this generation to a smaller f/1.85 aperture, which in my view might be more reasonable as the Mi 10 suffered in terms of its optics. There’s still OIS present here, which is good news. The Ultra-wide module is a 13MP f/2.4 module with a very wide 123° FoV, great for indoor capture, architecture, or landscapes.I like how Xiaomi engineered the camera island to accommodate for the thick Z-height of the main camera module: it’s a 3-stepped design in terms of its thickness, with each “island” only being as thick as the minimum the cameras require. While the main camera module thickness is still thick, it’s not as evident as on other phones with similar giant sensors.The top and bottom sides of the phone feature a relatively flat edge, and are characterised by dual-speaker setups the company employs. There’s a hardman/kardon logo on the top side, and generally the speakers get extremely loud and are among the best sounding on any device right now.Generally, I really liked the design of the Mi 11, and I feel the company has managed to create their sleekest and most ergonomic design to date. It feels very premium and is very clearly a flagship device, which comes at a surprise given its 749€ starting point.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16447/the-xiaomi-mi-11-review\n",
      "Title: Intel Core i7-11700K Review: Blasting Off with Rocket Lake\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-03-05T21:30:00Z\n",
      "URL: https://www.anandtech.com/show/16535/intel-core-i7-11700k-review-blasting-off-with-rocket-lake\n",
      "Content: The march on performance with desktop platforms has unique challenges compared to other platforms. Peak single thread throughput is often considered the Holy Grail, with a fast follow up of good multi-core and all-core performance given the nature of how desktop platforms are used with background processes and multiple concurrent applications. In order to bring its best single core performance to the desktop market, Intel had to redesign its 10nm product on 14nm, which combines the high throughput of the design with the high frequency of 14nm. These redesigned Cypress Cove cores form the basis of Intel’s new 11thGen Desktop Processor Family, Rocket Lake. Today we are reviewing the Core i7-11700K, an eight-core processor with hyperthreading able to boost up to 5.0 GHz.NoticeThe official launch date for these processors, and full reviews, is March 30th. We are currently under NDA with Intel for the information that has been provided by Intel, and will publish that information in due course. However, as noted in a number of press outlets, some units have already been sold at retail before that sales date. Units obtained by that method are not under NDA by definition, and we obtained the Core i7-11700K for this review at retail, and as such we are not under NDA for any information we have obtained through using this processor.Before publishing this review, we gave Intel advance notice to respond to us having a full review ahead of the formal release. Our email seemingly generated some excitement inside (and to our surprise, outside) Intel, but wereceived a response from Intel stating that they had no comment to offer.Update 1: This review was originally posted on March 5thusing 0x2C microcode, and has been updated on March 14thwith data from 0x34 microcode. The difference between the two is about +1.8% on CPU tests, +3% on Gaming tests, including performance regression in some areas. This review will showcase both sets of results. Details of the update can be foundhere.Rocket Lake We Know AboutCore i9-11900K and Core i7-11700KBack at the start of the year, during CES,Intel disclosed product informationabout its lead halo product on the Rocket Lake platform, the Core i9-11900K. This includes some microarchitecture details, as well as core count, frequency, memory, graphics, and features relating to IO and the chipset. With our review here today, we can add the 11700K to that data with what we can probe from the processor.AnandTechCorei9-11900KCorei7-11700KSoCRocket LakeRocket LakeMicroarchitectureCypress CoveCypress CoveCores / Threads8 / 168 / 16TDP125 W125 WBase Frequency?3600 MHzTurbo 2.0 (1-2 C)?4900 MHzTurbo 3.0 (1-2 C)?5000 MHzThermal Velocity Boost5300 MHz-All Core Turbo4800 MHz4600 MHzDDR42 x DDR4-32002 x DDR4-3200GPU + EUsXe-LP, 32 EUsXe-LP, 32 EUsPCIe4.0 x16 + 4.0 x44.0x16 + 4.0 x4AVX-512YesYesPrice?We paidequivalent $469The differences between the two Rocket Lake processors, based on available information, are slim. The main difference is that the Core i9 is known to have Intel’s Thermal Velocity Boost technology, however the Core i7 does not – this means the peak frequency is only 5.0 GHz, not 5.3 GHz. The all core frequency is only 200 MHz different.The new generation Rocket Lake is the combination of two different backported technologies. Intel took theSunny Cove corefrom10nm Ice Lake, and re-built it on 14nm, calling it now Cypress Cove. Intel also took the Xe graphics from 10nm Tiger Lake and re-built those on 14nm, but these are still called Xe graphics.We can see that the new design is an amalgam of new technologies, by comparing Rocket Lake to Comet Lake, Ice Lake, and Tiger Lake:Microarchitecture ComparisonAnandTechCometLakeRocketLakeIceLakeTigerLakeRyzen5000Form FactorDesktopDesktopLaptopLaptopDesktopMax Cores1084416TDP125 W125 W28 W35 W105 WuArchCometCypressSunnyWillowZen 3IGPGen 9Xe-LPGen 11Xe-IGP Cores24326496-L1-D32 KB /c48 KB /c48 KB /c48 KB/c32 KB/cL2 Cache256 KB /c512 KB /c512 KB/c1280KB /c512 KB/cL3 Cache20 MB16 MB8 MB12 MB64 MBPCIe3.0 x164.0 x203.0 x84.0 x44.0 x24DDR42 x 29332 x 32002 x 32002 x 32002 x 3200LPDDR4X--4 x 37334 x 4266-There are obviously some differences between the notebook and desktop parts, most noticeably that the new platform at the high-end has only eight cores, two fewer than Comet Lake.This is because Intel found 8 cores to be the best balance of die area, power consumption, performance, and cost. Several times I’ve seen Intel spokespeople say the reason for 8 cores being ‘the most we could fit’, although that’s categorically false. More cores can be added, but overall they would run at a lower frequency for the same power, the interconnect might not scale, or the die size/yield would raise the price too much. The phrase ‘the most we could fit’, by all technical understanding, is a steaming pile. It needs additional qualifiers, or to simply say 'the best fit given die area, yield, and cost'.Additional improvements over Comet Lake include AVX512 units, support for 20 PCIe 4.0 lanes, and faster memory. With the new chipsets, Intel has already disclosed that the Rocket Lake platform will have native USB 3.2 Gen 2x2 (20 Gbps), and with the Z590 motherboards, a double bandwidth link from CPU to the chipset, moving from DMI x4 to DMI x8, effectively a PCIe 3.0 x8 link.Rocket Lake on 14nm: The Best of a Bad SituationThe delays around the viability of Intel’s 10nm manufacturing have been well documented. To date, the company has launched several products on its 10nm process for notebooks, such as Cannon Lake, Ice Lake,Jasper Lake, Elkhart Lake, and Tiger Lake. There have been other non-consumer products, such asAgilex FPGAsand Snow Ridge 5G SoCs, and Intel has confirmed that its 10nm server products ‘Ice Lake Xeon Scalable’, are currently in volume production for a late Q1 launch.The one product line missing from that list is the desktop and enthusiast segments that typically use socketed processors paired with discrete graphics. Intel has always committed to launching desktop processors on its 10nm process, however we are yet to see the results of their efforts. The issues Intel is having with 10nm have never been fully elaborated on, with Intel instead opting to promote some of the improvements made, such as its new SuperFin technology, which is in Tiger Lake and the next-generation server platform beyond Ice Lake Xeon Scalable (for those keeping track, that would be Sapphire Rapids). The 10nm improvements so far has enabled Intel to launch notebook processors and server processors, both of which have lower power-per-core than a typical desktop offering.As 10nm has not been able to meet the standards required for desktop-level performance, rather than leave a potential 3 year gap in the desktop product family, Intel has been in a holding pattern releasing slightly upgraded versions of Skylake on slightly improved variants of 14nm. The first two members of the Skylake family, Skylake and Kaby Lake were released as expected. While waiting, we saw Intel release Coffee Lake, Coffee Lake Refresh, and Comet Lake. Each of these afforded minor updates in frequency, or core count, or power, but very little in the way of fundamental microarchitectural improvement. The goal all along was to move to 10nm with the same architecture as the mobile Ice Lake processors, but that wasn’t feasible due to manufacturing limitations limiting how well the processors scaled to desktop level power.Skylake, Core 6thGen in August 2015Kaby Lake, Core 7thGen in January 2017Coffee Lake, Core 8thGen in October 2017Coffee Lake Refresh, Core 9thGen in October 2018Comet Lake, Core 10thGen in April 2020Rocket Lake, Core 11thGen in March 2021With previous generations, Intel traditionally had either upgraded the process node technology, or updated the microarchitecture – a process that Intel called Tick-Tock. Originally Intel was set to perform a normal ‘Tick’ after Kaby Lake, and have Cannon Lake with the same effective Skylake microarchitecture move to 10nm. Cannon Lake ending up only as a laptop processor with no working graphics in a small number of notebooks in China as it was a hot mess (as shown in ourreview). As a result, Intel refocused its 10nm for notebook processors hoping that advances would also be applicable to desktop, but the company had to release minor upgrades on desktop from Coffee Lake onwards to keep the product line going.This meant that at some level Intel knew that it would have to combine both a new architecture and a new process node jump into one product cycle. At some point however, Intel realized that the intercept point with having a new microarchitecture and the jump for the desktop to 10nm was very blurry, and somewhat intangible, and at a time when its main competitor was starting to make noise about a new product that could reach parity in single core performance. In order to keep these important product lines going, drastic measures would have to be taken.After many meetings with many biscuits, we presume, the decision was made that Intel would take the core microarchitecture design from 10nm Ice Lake, which couldn’t reach high enough frequencies under desktop power, and repackage that design for the more dependable 14nm node which could reach the required absolute performance numbers.This is known as a ‘backport’.Sunny Cove becomes Cypress CoveThe new Core 11thGen processor which we are looking at today has the codename Rocket Lake. That’s the name for the whole processor, which consists of cores, graphics, interconnect, and other different accelerators and IP blocks, each of which also have their own codenames, just for the sake of making it easier for the engineers to understand what parts are in use. We use these codenames a lot, and the one to focus on here is the CPU core.Intel’s10nm Ice Lake notebook processor familyuses Sunny Cove cores in the design. It is these cores that have been backported to 14nm for use in the Rocket Lake processors, and because it is on a different process node and there are some minor design changes, Intel calls them Cypress Cove cores.The reason behind this is because taking a design for one manufacturing process and designing it for a second is no easy task, especially if it’s a regressive step – transistors are bigger, which means logic blocks are bigger, and all the work done with respect to signaling and data paths in the silicon has to be redone. Even with a rework, signal integrity needs to be upgraded for longer distances, or additional path delays and buffers need to be implemented. Any which way you cut it, a 10nm core is bigger when designed for 14nm, consumes more power, and has the potential to be fundamentally slower at execution level.Intel’s official disclosures to date on the new Cypress Cove cores and Rocket Lake stem from a general briefing back in October, as well as a more product oriented announcement at CES in January. Intel is promoting that the new Cypress Cove core offers ‘up to a +19%’ instruction per clock (IPC) generational improvement over the cores used in Comet Lake, which are higher frequency variants of Skylake from 2015. However, the underlying microarchitecture is promoted as being identical to Ice Lake for mobile processors, such as caches and execution, and overall the new Rocket Lake SoC has a number of other generational improvements new to Intel’s desktop processors.In This Review, and LimitationsAs mentioned at the outset, this is a review prior to the official review embargo for these processors. We are able to post this outside of the NDA as we were able to obtain the hardware at retail. There is still a lot of information that has not been disclosed, the sort of thing that normally accompanies a new processor launch, and whatever Intel has told is still part under NDA – details of which are also under the same NDA. So we won’t be able to go into those just yet, but we can start to fire some benchmark data at you. In this review we’re focusing mainly on the generational 8-core offerings across a number of products and generations.8-Core CPU ComparisonAnandTechCorei9-9900KSCorei7-10700KCorei7-11700KRyzen 75800XRyzen 74750GuArchCoffeeRefreshCometLakeCypressCoveZen 3Zen 2+ VegaCores8 C / 16 T8 C / 16 T8 C / 16 T8 C / 16 T8 C / 16 TBase Freq40003800360038003600Turbo Freq50005100500048004400All-Core500047004600~4550~4150TDP127 W125 W125 W105 W65 WIGP / EUsGen 9, 24Gen 9, 24Xe-LP, 32-Vega, 8L3 Cache16 MB16 MB16 MB32 MB8 MBDDR42 x 26662 x 29332 x 32002 x 32002 x 3200PCIe3.0 x163.0 x164.0 x204.0 x243.0 x8MSRP$513 box$387 box?$449 SEP~$345We paid 394€for our processor pre-tax, which comes to $469. We suspect this is well above Intel's recommended retail price, given that this was sold before the official sales date and demand for high performance processors is very high.Test Setup and #CPUOverload BenchmarksAs per our processor testing policy, we take a premium category motherboard suitable for the socket, and equip the system with a suitable amount of memory running at the manufacturer's maximum supported frequency. This is also run at JEDEC subtimings where possible. Reasons are explainedhere.Test SetupIntelRocket LakeCore i7-11700KMB1: Microcode 0x2CMB2: Microcode 0x34TRUECopper+ SST*ADATA4x32 GBDDR4-3200IntelComet LakeCore i7-10700K--TRUECopper+ SST*ADATA4x32 GBDDR4-2933Intel CoffeeRefreshCore i9-9900KSMSI MPG Z390Gaming Edge ACAB0TRUECopper+SST*ADATA4x32GBDDR4-2666AMDAM4Ryzen 7 5800XRyzen 7 4750GGIGABYTE X570IAorus ProF31LNoctuaNHU-12SSE-AM4ADATA2x32 GBDDR4-3200GPUSapphire RX 460 2GB (CPU Tests)NVIDIA RTX 2080 Ti FE (Gaming Tests)PSUCorsair AX860iSSDCrucial MX500 2TB*TRUE Copper used with Silverstone SST-FHP141-VF 173 CFM fans. Nice and loud.**Other CPUs in graphs tested in same systems for CPU familyWhile we can't disclose the motherboard used due to NDA reasons, it has already been announced by the manufacturer. Meanwhile, the BIOS used is likely not the final variant that will be used for Rocket Lake's retail launch later this month, and further BIOSes may contain potential minor adjustments to performance or turbo responses.As an addendum to this review a week after our original numbers, we obtained a second motherboard that offered a newer microcode version from Intel. The original motherboard still offered the same microcode at that time. For more details, pleasesee this link.We must thank the following companies for kindly providing hardware for our multiple test beds. Some of this hardware is not in this test bed specifically, but is used in other testing.Hardware Providers for CPU and Motherboard ReviewsSapphireRX 460 NitroNVIDIARTX 2080 TiCrucial SSDsCorsair PSUsG.Skill DDR4ADATA DDR4SilverstoneCoolersNoctuaCoolersA big thanks to ADATA for the ​AD4U3200716G22-SGN modulesfor this review. They're currently the backbone of our AMD testing.Users interested in the details of our current CPU benchmark suite can refer toour #CPUOverload articlewhich covers the topics of benchmark automation as well as what our suite runs and why. We also benchmark much more data than is shown in a typical review, all of which you can see in our benchmark database. We call it ‘Bench’, and there’s also a link on the top of the website in case you need it for processor comparison in the future.Table Of ContentsIntel Core i7-11700K Review: Blasting Off with Rocket LakePower Consumption: Hot Hot HOTCPU Tests: MicrobenchmarksCPU Tests: Office and ScienceCPU Tests: Office and ScienceCPU Tests: EncodingCPU Tests: SPECGaming TestsConclusion: The War of AttritionNote: If you've reached this far, great! This is not the end of the review. We have over a dozen pages with data and benchmark results, as well as a conclusion. They are all in the drop down box below.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16535/intel-core-i7-11700k-review-blasting-off-with-rocket-lake\n",
      "Title: The Intel Moonshot Division: An Interview with Dr. Richard Uhlig of Intel Labs\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-03-02T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/16515/the-intel-moonshot-division-an-interview-with-dr-richard-uhlig-of-intel-labs\n",
      "Content: Some analysts consider Intel to be a processor company with manufacturing facilities – others consider it to be a manufacturing company that just happens to make processors. In the grand scheme of things, Intel is a hybrid of product, manufacturing, expertise, investment, and perhaps most importantly, research. Intel has a lot of research and development on its books, most of it aimed at current product cycles in the 12-36 month time span, but beyond that, as with most big engineering companies, there’s a team of people dedicated to finding the next big thing over 10-20+ years. This is usually called the Moonshot Division in most companies, but here we find it called Intel Labs, and leading this team of path-finding gurus is Dr. Richard Uhlig.I’ve had a number of divisions of Intel in my periphery for a while, such as Intel’s Interconnect teams, Intel Capital (the investment fund), and Intel Labs. Labs is where Intel talks about its Quantum Computing efforts, its foray into neuromorphic computing, silicon photonics, some of which become interesting side announcements at events like CES and Computex. Beyond that, Intel Labs also has segments for federated learning, academic outreach and collaboration, government/DARPA project assistance, security, audio/visual, and many others that are likely kept out of the eye from persistent journalists.Recently Intel Labs was put under the head of Raja Koduri’s division, and part of the momentum behind Intel’s messaging of late has been to focus on the future of the company, which means more outreach from departments like Intel Labs, and the opportunity to talk to some of the key people inside. Leading up to this interview, I had briefings from Intel’s neuromorphic and integrated photonics teams as part of a new annual ‘Intel Labs Day’ event for the community around Intel’s research offerings.Dr. Richard Uhlig’s official title is ‘Senior Fellow and Vice President in the Technology, Systems Architecture and Client Group and the Director of Intel Labs’. Dr. Uhlig joined Intel in 1996 and led the definition of multiple generations of virtualization architecture for Intel processors and platforms, known collectively as Intel Virtualization Technology (Intel VT), and has worked on projects in this line leading up to Software Guard Extensions (SGX) and beyond. Uhlig was the director of Systems and Software Research in Intel Labs from 2013 to 2018, where he led research efforts in virtualization, cloud-computing systems, software-defined networking, big-data analytics, machine learning and artificial intelligence.This interview was performed in late 2020 - references are made to the Intel Labs event, which was in December 2020.What is Intel Labs, How Does It OperateIan Cutress: When someone hears the words ‘Intel Labs’, one might conjure up something akin to a chemistry laboratory with scientists in white coats and safety glasses. However as far as I can tell, Intel Labs is more similar to Alphabet's X division (previously Google's X), an entity that exists purely to find the next big innovations. How close is that to the truth, or how would you put Intel Labs in your words?Rich Uhlig:You got it right. We are meant to explore the future for Intel, and to look at the sort of the disruptive technologies that could change the way that the business works in the future. So we’re not so much about incremental advancements, we try to look for the moonshot ideas - the things that move the needle for the company by exploring new areas. Our scope is everything from the circuits up - we look at circuit innovation and microarchitecture. Within the architecture we look at system software, OS and virtual machine monitors, we look at programming systems, we look at emerging workloads and applications, and we look at how people use systems. We take that full stack view, and we have people that can contribute in all of those areas as we seek out possible disruptive innovation for the company.IC: Because the goal of Intel Labs is to focuses on those future generations of compute to solve the world's problems, such as new computational and networking models and paradigms, users might think that you’re also involved in next-generation manufacturing methods, but you are not. Is that a holistic split at the vision level, or just the dynamics of Intel?RU:That’s right – Intel has our own process development department, what we call internally as ‘TD’ or ‘Technology Development’, and there is a [branch of Intel] that supports that called Components Research.Organizationally it's separate from what we look at, although of course we collaborate closely with them because there are often times of opportunity at the intersection between process and circuits as well as at the microarchitecture/architecture level and what circuits we build.IC: For the sort of stuff Intel Labs does, building on the leading edge process node isn’t necessarily always a requirement in that long term vision?RU:We do silicon prototypes and we use Intel’s fabrication facilities for that. But much of what we do doesn't even involve silicon prototyping at all - it may involve some new software innovation, it may we may be putting together systems with other methods or ingredients.IC: Intel Labs almost sounds as if it’s a separate entity to Intel, albeit with access to manufacturing and such. I understand that Intel Labs now fits under Raja Koduri’s side of the organization – how much autonomy does Intel Labs get (and is that the right amount)?RU:In our history we've always had a good deal of autonomy and that that's by design. [This is] because our purpose is to explore disruptive technologies. We are funded at the corporate level, and in a manner that allows us to select the research bets that we think could pay off, or to take risks that other parts of the company wouldn't do. The recent change where we became part of Raja’s organization - it helps us in that it creates new linkages into the product teams and the engineering organizations that Raja runs. We still have our autonomy to explore innovation, but we also have new pathways to transfer expertise and knowledge within the organization. And I would say that we/Intel Labs has always had to fit somewhere inside the company, and this this most recent move I think has been a really positive one.IC: Can you give us a sense of scale of how big Intel Labs is – budgets, employees, offices? It’s my understanding its more than just Silicon Valley.RU:We are round about 700 researchers, largely PhDs in those domains that I talked about at the beginning, and we cover everything up and down the stack. We're a worldwide organization as you noted, and we have labs in on the West Coast in Oregon and California. But we're also present in India, in China, in Germany, in Israel, in Mexico. That worldwide footprint is important to how we do our work, because we don't just do research inside the company - we engage academia, and we are spread out as this allows us to be working closely and directly with researchers at leading universities across the planet. This also allows us to engage different government agencies as well, and to understand the market specifics of each of those geographies. It's important to our whole methodology.The Focus of Intel Labs ResearchIC: As part of the Intel Labs event (Dec 2020), the company is [going to give/has given] us some insight into five key areas: Integrated Photonics, Neuromorphic Computing, Quantum Computing, Security for Federated Learning, and Machine Programming. That’s quite a mouthful! I’m guessing that each of these areas isn’t simply 20% of Intel Labs! Is there a topic that you want to talk to the world about that isn’t on this list (and can you give us a teaser)?RU:In the time that we had for the Intel Labs event we had to be selective so we're picking a few highlight areas, but it's certainly not the full scope of what we do. A big chunk of our investment is in new compute models: neuromorphic and quantum computing would be examples of that. But we also do core research in accelerators for different kinds of specialization, as you know there's been a lot of focus in the industry towards improving the energy efficiency of AI algorithms, like deep neural networks and things like that. So we do research in ways to improve those kinds of workloads. We [also] do work in storage and memory technologies, we do work in novel sensing technologies, we look at connectivity technologies. In addition to silicon photonics, which is a connectivity technology, we have substantial investment in wireless, in mmWave communications, and in supporting 5G and beyond. We also have a thrust in ways to more efficiently program systems or design systems - we have a strategic CAD lab that does work in those areas, as well as just a general focus on trust, security, privacy research and such.IC: When you're talking about 5G, such as mmWave, we know that Intel sold, obviously the modem business, to Apple over the last 12-18 months. So when you say Intel is working on mmWave, how does that fit into the context of the industry?RU:Modems are the endpoint, the thing that goes into devices, but Intel still has a huge bet on building out 5G infrastructure. [For that] you need research and advanced technologies to succeed with that kind of a strategy, and that's really where our thrust is. Also, not only mmWave, but we're also looking at all the things that go into building radio access networks, the network infrastructure core, [and so on]. The big transition that's happening in the industry is that we're going from purpose built networking gear to things that are based on more general purpose hardware, much like the transition that happened in cloud data centers, but it's a different kind of workload and it has to be optimized for in a different way. But we do a lot of work in that area, applying technologies that we've developed in the Labs and including things like virtualization, and network function virtualization would be an example of how you know we're contributing to that opportunity for Intel.IC: Intel has announced Snow Ridge, its base station 5G platform – did Intel Labs have a hand in that?RU:We did the research in the Labs in a bunch of different areas, including network function virtualization and optimizing for baseband processing and things like that, which are key technology ingredients for the Snow Ridge platform. Labs did contribute to that through the product team, so that's an example of how we interface with the product teams to bring things to market, although Labs didn’t directly deliver that platform [as a product].IC: What proportion of Intel Labs is Software versus Hardware?RU:Almost everything we do involve some kind of software, even the hardware parts! So I would venture to guess something like two thirds of software to one third hardware. It depends on how you want to define it - we often at times don't think of ourselves specific software people or hardware people, but rather as systems people. We really believe in that multi-disciplinary approach.IC: How many of the projects at Intel Labs are necessarily non-public at any given time?RU:Much of what we do is public, and in fact we publish a lot of our work. When we're in a pre-competitive phase of the research, we view that that's important because we need to be at the top of our game. We need to make sure that the research is relevant and competitive, and stands up to the best research in the world. So we test ourselves [in] that way, by publishing in the top conferences and venues, and we'll typically do that quite freely at early stages of a research project. What can happen is, once we decide that something is an idea that we want to productize, it can go through a phase where we have to go silent for a while until we're ready at the other end, prior to a product or capability launch, that we can be public again. Often at times it has to do just with the phase of the project that determines when we can speak about it externally.New Ideas at Intel LabsIC: I've heard the term ‘Technology Strategic Long Range Planning’ in relation to Intel Labs as an event where new ideas are discussed. Is this how new seed projects inside Intel Labs are planted, and perhaps where budgets and developments are discussed?RU:It is one of the ways [in which that happens]. We call it, affectionately, ‘TSLRP’ (tea slurp): the Technology Strategic Long Range Planning. You can think of it as a place to sort of crowdsource the best ideas and technology ideas in the company. It's something that Intel Labs administers and organizes, but it's actually open to all the technologists within the company. We run it as a yearly process, and it is also something that runs throughout the year, and we invite our technologists to make proposals about something that they think is important that the senior executives in the company should be paying attention to. Then we can run it through a development process where we really start to kick/bounce the ideas around, test them and sort of challenge the proposers in a way that gets it in a form that can be presented to the leadership. Often at times what comes out of these sorts of presentations is a new investment or new direction that the company may take. That's been true for a lot of the things that have come out of the Labs or that Intel has decided to pursue.IC: Would you say Intel Labs gets enough funding from Intel? With the right project, the right people, and the right goal, would you think there would be billions poured into it? Or at that point would it perhaps graduate out into a separate division?RU:We have the right level of funding to do our mission, which is to explore possible investments that the larger Intel could make once you want to scale that idea. We have sufficient funding for that exploration and we have financials to take things to a certain degree of maturity, so we can have confidence that a given technology makes sense. At that point we have mechanisms to transfer it to the larger execution machine of the company, and then additional resources and funding goes into it at that point. That is basically how we scale the things is through partnership with the rest of the company when we get to that stage.IC: It’s worth mentioning that the Neuromorphic Computing division inside Intel Labs came through an acquisition. Can you talk us through how that process came about before the M&A team stepped in?RU:The talent behind our neuromorphic efforts are experts in asynchronous design methodology, and that was from our Fulcrum acquisition. They were a networking switch company, and asynchronous design was used in that switch. But the leaders behind our neuromorphic team are taking that that same design methodology and applying it to our neuromorphic computing programme which is a completely different application of that computational design. It [is] a very interesting body of work, and we're looking at [various] different objectives with that work.Successes From Intel LabsIC: I’ve heard that one vector of Intel Labs you like to promote is the ‘program graduates’. Are there any that we’ve heard of, and are there any particular highlights?RU:Something that would probably be familiar to many would be USB and Thunderbolt! We've done a lot of work in IO technologies. So as you may know with Thunderbolt, we started it as a vision inside the Labs towards converging all the different IO connector types that we had on the PC platform years ago, and just get them all onto a single connector and tunneled the protocols over the same link. That was a combination of architectural innovation as well as circuit and signaling technology that we bought together to make that that case.Something that I personally worked on before I led Intel Labs, before my current mission of leading Intel Labs, is virtualization technology. I spent a good 15 years, and it goes all the way back to the late 90s, when I started working on the very earliest proposals around Virtualization and what we might do to our processors and platforms to make them more easily virtualized. We’ve delivered multiple generations of Intel Virtualization Technology, VT.Silicon Photonics as well, that started in the Labs more than a decade ago, just doing the basic physics behind the different ingredients behind building a Silicon Photonics solution – they hybrid laser, the silicon modulators, the waveguides, all of these things and packaging them together. That worked for many years in the Labs, and that created a brand new business unit that Intel is now delivering those Silicon Photonics solutions to market.We've done a lot of work in Trusted Execution environments, building a place in the platform where you can run code in a secure way and in a testable way so that you know what the surrounding environment for that code is. Those were extensions to VT in the first instantiations of Trusted Execution environments, but we also did the work around Software Guard Extensions (SGX), which was an architecture that came out of the Lab. Those would be some of the highlights off the top of my head!IC: Are there any projects that had amazing potential to start but led to a dead-end?RU:We had a big thrust in wearables and, really energy efficient endpoint devices. We were working on things like zero-net energy in computing devices that we thought would have a lot of promise, the idea being that you could harvest energy from the environment and then just be able to run that device continuously without charging. Those technologies actually were quite interesting from a prototyping point of view, and I think it was demonstrated that it was a success, but it was harder to figure out what the business behind that was. As a result we sort of moved away from that, in part the company itself also moved away from that direction. But that be an example of where it didn't pan out.IC: How much of your role is involved with locking down IP under the Intel heading, or ensuring that collaboration with academia and industry runs smoothly?RU:That's a great question. One of our important missions is to engage academia, and we have to do so on terms that that are agreeable to them. Often in that sort of pre-competitive phase of research we have a funding model where we say that this is an open collaborative [approach], which just means that we're not expecting any IP rights or patents from the research that we're funding. In fact, [we tell them that] we want [them] to publish and to do open source releases, in order to get the technology out there on the academic side. The benefit that we get is that we're close to the work as it's happening, and we have to pay attention so that we can we can [identify] those key technologies. [Then] at some point we begin the process of productization, and when that happens when we do that transition, that's when we’ll start looking at different IP models, or we may be filing patents or even just keeping trade secrets on the further development that we do once we take it more into an internal research development process. But that's kind of how we manage that tension. We realized we have to take different approaches based on the collaborations and collaborators that are happening at any point in time.IC: One of Intel Labs’ biggest wide-appeal public win was enabling the late Stephen Hawking to communicate and even access the internet. How has that technology evolved and where does it sit today?RU:That was work led by Lama Nachman and her team, and it's a great example of the multidisciplinary approach that we took. Many others had tried to work on technologies for Stephen Hawking, and he rejected those because they just didn't fit with the way that he worked and his expectations around that. It was trying to enable a brain-computer interface for him. What this team did is they really spent time with him to understand how he worked and what he needed. Based on that feedback they developed [and iterated on] the solution. So that's just a note on the methodology. But to answer your question, that's an example of technology that we contributed into open source as an assistive computing technology that can be used by other disabled individuals and [it can be] adapted for them.Intel Labs Research Today: Silicon PhotonicsIC: Silicon Photonics has been a slowly growing success story, being able to generate and process light in silicon, particularly as it comes to networking. We’ve seen Intel discuss now millions of units sold, speeds up to 400 Gbps and Silicon Photonics networking engines. As a successful product with a roadmap, why is it still under the Intel Labs umbrella, rather than say, Networking Solutions? Is there untapped potential?RU:We have a whole silicon photonics product division that delivers those products that you mentioned, and those are not in the Intel Labs umbrella. But because there are future generations of improvement possible, we have a parallel track where we continue to do research in the area. To explain that further, the products we offer today are still discrete devices, they sit outside of a CPU or GPU or an FPGA - they're not integrated links into the compute engines. That's important because when you think about the end to end path, the data follows from one compute engine to another, it still has to follow an electrical link, even if it's a short one, even if just a few inches, before you get to the optical device and the transceiver. Just that little few inches is where a lot of power still goes. In fact, if you study the power budget for a high end Compute Engine, increasingly just to feed the beast more and more of the data is going to IO power.So what we are exploring with our research is ways to truly integrate the photonics, the silicon photonics, into the package. There's a bunch of innovation that's required to make that possible. The first is that you have to find a solution for modulation. The modulators that go into these discrete devices are [typically] large devices, and we've figured out how to build micro ring modulators that are much smaller in dimensions. Because they're smaller, we can array them around the shoreline of the package, and we can run them at different wavelengths so now we can get much more bandwidth over the optical links - that's what we call integrated photonics. It's something that we think will overcome that IO power wall and something that we're really excited about.IC: So there’s the Silicon Photonics product team, and then you've got Integrated Photonics which is the Intel Labs side.RU:Yeah, we're exploring the ingredient technologies to do the integrated photonics and then once we prove it out with research prototypes will we will partner with our silicon photonics product division to bring it to market at some point in the future. To be clear, there are no plans for that [productization] today, but our methodology is that we closely collaborate with those the product teams.Intel Labs Research Today: NeuromorphicIC: On the Neuromorphic computing side, we are also starting to see product come to market. The Loihi chip built on 14nm, with 128000 neurons, scales up to Pohoiki Springs with 768 chips and 100 million neurons, all for 300 watts – Intel’s slide deck says this is the equivalent to a hamster! Intel recently promoted an agreement with Sandia National Laboratories, starting with a 50 million neuron machine, scaling up to a billion neurons, or 11 hamsters worth of neurons, as needed when research progresses.IC: Can neuromorphic computing simply be scaled in this way? Much like interconnect performance is the ultimate scale-out limiter for traditional compute, where is Neuromorphic computing headed?RU:We're exploring two broad applications application areas. The first is small configurations, maybe just a single low Loihi chip in an energy constrained environment where you want may want to do some on-the-fly learning close to the sensor with the data. You might want to, for example, build a neuromorphic vision sensor. The other fork is to look at these bigger configurations where we're clustering together lots of Loihi chips. There you might be trying to solve a different problem like a constraint satisfaction problem or similarity search across a large dataset. Those are the kinds of things that we would like to solve with [large amounts of Loihi]. Incidentally, we have a neuromorphic research community, or IRC, that that is that we collaborate with as an example of where we work with academic researchers to enable them with these platforms to look at different areas.But to answer your question: what are the limiters to building the larger configurations? It's not so much the interconnect, it’s a matter of fabric design, and we can figure that out. Probably the biggest issue right now is that if you look inside a Loihi chip, it's the logic that helps you build the neuron model and run it efficiently as an event processing engine. But [there’s] a lot of SRAM, and the SRAM can be low power, but it's also expensive. So as you get [to] really large clusters of networked together SRAM, it's an expensive system. We have to figure out that memory cost problem in order to really be able to justify these larger Loihi configurations.IC: So the cost is more dollars than die area, so stacking technologies are too expensive?RU:It is expensive, and however you slice it it’s going to be costly on a cost-per-bit perspective. We have to overcome that in some way.IC: You mentioned that for smaller applications, a vision engine processing model is applicable. So to put that into perspective, does that mean Loihi could be used for, say, autonomous driving?RU:It might be a complement to the other kinds of sensors that we have in autonomous vehicles - we've got regular RGB cameras that are capturing visual input, but LIDAR [is also] useful in autonomous vehicles, [which] could be another sensor type. The basic argument for having more than one is redundancy and resiliency against possible failure, or [avoiding] misperception of things that just makes the system safer overall. So the short answer is yes.IC: One of the things with neuromorphic computing, because it's likened so much to brain processing, is the ability to detect smells. But what I want to ask you is what the weirdest thing you've seen partners and developers do with neuromorphic hardware that couldn't necessarily easily be done with conventional computing?RU:Well, that's one of them! I think that's a favorite, teaching a computer how to smell - so you already took my favorite example!But I think it's quite interesting how the results that we're getting around problems like similarity search. If you imagine you've got a massive database of visual information and you want to find similar images, so things that look like a couch or [have] certain dimension or whatever, being able to do that in a very energy efficient way is kind of interesting. [It] can also be done with classical methods, but that that's a good one [for Neuromorphic]. Using it in control systems for like a robotic arm controller, those are interesting applications. We really are still at that exploratory stage to understand what are the best ways that you could do stuff - sometimes for control systems you can you can solve them with classical methods but it's just really energy consuming, and the methods for training the system make it less applicable in dynamically changing environments. We're trying to explore ways that neuromorphic might be able to tackle those problems.IC: One of the examples you’ve mentioned is kind of like an image tag search - something that's typical machine learning might do. If we take YouTube, when it's looking for copyrighted audio and clips, is neuromorphic still applicable to that scale?RU:One straightforward application for neuromorphic is that we were looking at artificial neural networks, like a DNN or CNN, and that would be trained with a large dataset. Once it's been trained, we're transferring it over into a spiking neural network (or SNN) which is what Loihi does, and then seeing that once trained we can run the inference part of the task more efficiently.That's a straightforward application, but one of the things that we're trying to explore from a research point of view with Loihi is how can it learn with less data? How can it adapt more quickly, without having to go back to the extensive training process where you run a large labelled data set against the network.IC: Brains take years to train using a spiking neural net method - can Intel afford years to train a neuromorphic spiking neural network?RU:That's one of the big unanswered questions in AI. Biological brains experience the environment and they learn continuously, and it can take years. But even then, in the early stages they can do remarkable things - a child can see a real cat, and then see a cartoon of a cat, and generalize from those two examples with very little training. So there's something that happens in natural biological brains that we aren't able to quite replicate. That's one of the things that we're trying to explore - I should be really clear, we've not yet solved this yet, but that's one of the one of the interesting questions we're trying to understand.IC: The Loihi chip is still a 2016 design – are there future hardware development plans here, or is the work today primarily focused on software?RU:We are doing another design, and you'll be hearing more about that in the future. But we haven't stopped on the hardware side - we've learned a lot from the current design, and [we’re] trying to incorporate [what we’ve learned] into another one. At the same time I would say that we really are trying to focus on what is it good for what are the applications that make the most sense and that's why we have this methodology of getting working Loihi systems out in the hands of researchers in the field. I think that's a really important aspect of the work - it is more of that workload, [more] exploration software development.Intel Labs Research Today: Quantum ComputingIC: On the quantum computing side of Intel Labs, the focus has primarily been on developing Spin Qubit technology, rather than other sorts of Qubits. Is this simply a function of Intel’s manufacturing expertise, or does it appear that spin qubits are where the future of Quantum Computing is going?RU:When we started our quantum programme, we decided to look at both quantum dot spin qubits as well as transmon superconducting qubits - we had a bet on both. We decided to focus on spin qubits after the first few years of investigation because we were trying to look forward to what has to happen in order to build a truly practical quantum system. You have to be able to scale to really large numbers of qubits - not just hundreds or thousands, but probably millions of qubits. These also [have to be] fault tolerant – we have quantum error correcting codes that require more physical qubits than you have logical qubits. So if you're going to get to millions, you can't have qubits that are big - it's almost like you can't have vacuum tube computing systems as you're going to be limited in how much you can do. So that was one thing that we figured out, but it's not just the selection of the qubit – [spin qubits] align to our core competences, and we're able to build these devices in small dimensions and at scale, so it does align to the core competence that the company has.But getting qubits to scale is going to require other solutions. We [also] have to figure out how to how to control the qubits, and that's where Horse Ridge comes in - being able to control these qubits [requires] running at very low temperature, [which means] you have to have the control electronics run at very low temperature as well. If you can't do that, then you've got lots of bulky coax cables coming from a room temperature [environment] into the dilution fridge - that's not going to scale. You can't have millions of cubits controlled in that way. These are the kinds of things that drive our decisions - what do we have to do, what problems you have to solve, so that we can get to a million qubits at some point in the future.IC: When you look at those dilution chambers, they all look impressive when you take the covers off with all the thin tubes, and it looks like a chandelier. So to that extent, would you say that quantum computing has the biggest R&D budget in Intel labs?RU:Actually, it doesn't! We're able to do a lot with a relatively modest investment I would say. It's definitely not the bulk of our investment at all - those fridges do cost a lot, but it's not where the bulk of the money goes!IC: Intel's current press materials state that the commercial phase for quantum computing sits at 2025 and ‘beyond’. Sitting where we are today, moving into 2021, is that still the goal for the commercialization process? Is Intel still looking at 2025, or should we be thinking another five years out beyond that?RU:We always talk about this as a 10 year journey. I think it's a bit early to be talking about productization at this stage - even picking a date, there's still some fundamental science and engineering problems that have to be solved before we would we would pick a date. We ask ourselves questions around what time to start engaging the ecosystem of application developers. That's important in the same way that we have with neuromorphic - we go outside with working hardware to understand what this might be good for - we have to get to that point with quantum as well. We, at some level, already do that - we already have collaborations with TU Delft (Dutch University) and QuTech. That's the way that we're collaborating with universities and partners - I think we're still a ways away from productization.IC: You say that to get to that point, Intel has to start speaking about having millions of cubits. Intel's third generation Tangle Lake quantum processor is currently at 49 qubits. So does that mean we should wait and expect two, three, four, or five more generations before we get hit that inflection point where perhaps current commercialization is more of a reality?RU:So Tangle Lake was an example of the transmon superconducting qubit type, which as I explained earlier we began to deprioritize. We're looking to scale our spin qubit designs, and we're on a path to increase the numbers there. But really you've got to get quality qubits before you think about scaling to larger numbers. [We have to] solve these other problems around controlling [the qubits], and I think we've made really great progress on that with Horse Ridge.IC: A report in the past couple of years from Google claimed that they had achieved Quantum Supremacy. Do you believe it was achieved?RU:By the definition of Quantum Supremacy, which is to pick a problem that you can’t solve with classical methods but that’s computationally complex and build a system that can solve it, they’ve achieved that. Notice that the problem doesn’t have to be something that’s useful! It doesn’t have to be a problem that people need to solve, but it was a milestone in the field, and certainly it was a good thing to have achieved. The way we think about it is about when do we get to a point where we get to a practical solution that we’re solving - something that people would care about, and something that you can’t solve in other ways with classical methods more economically. We’re still far from that, I don’t think we’ve reached that era of quantum practicality.Intel Labs Growth and OutreachIC: Should any engineers be reading and get excited by any of the topics, how would they go about either aligning their research to something Intel Labs could help accelerate, or get involved with Intel Labs directly?RU:For those who are still in grad school we have internships programs in a broad range of areas – you can check out our website and reach out to the researchers in different areas. We engage academia and universities in the various areas that we’re interested in, and it could very well be that your university has a program with Intel. We often at times set up research centers in different areas, and we have a number of those kinds of programmes. That’s the most natural way to get plugged into the work that happens in the Labs. We work directly with our research collaborators, we publish papers together with them, and so even during your studies you can be working with Intel Labs folks. Based on that experience, then it can develop into joining the Labs in the future. I think that through that internship programme, and through our academic funding, that’s the most natural path for [people in their] early career. For those outside of school already, then it’s a matter of reaching out to the researchers in your area, as well as finding out information from our website.One of the things that we do is, as an outgrowth of our academic investment, is that a lot of startups come out of academia, and we do have with Intel Capital programs where we look at startup that are at that seed stage. We often know them because we funded their research, but we will help them through those early stages of the company. Often at times we look at not just funding opportunities but also at the technologies that might help the startup to succeed. We do have programs like that present some opportunities.IC: On the Intel Labs, what is the future of Intel Labs’ public outreach? Is Intel Labs day going to become an annual occurrence?RU:I expect it will. We want to put a lot more energy into talking about our work outside the company. We did go through a period where we did less of that, but in our history we did more of it, but now we expect to do more of this! We were going to have a physical Labs event at the beginning of the year, but we will certainly be talking a lot more about our work going into the future.Many thanks to Dr. Richard Uhlig and his team for their time.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article already exists: https://www.anandtech.com/show/16515/the-intel-moonshot-division-an-interview-with-dr-richard-uhlig-of-intel-labs\n",
      "Title: The Samsung Galaxy S21 Ultra & S21 Review: The Near Perfect and The Different\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-02-22T17:00:00Z\n",
      "URL: https://www.anandtech.com/show/16488/the-samsung-galaxy-s21-ultra-s21-review\n",
      "Content: After many years of ever-increasing flagship device prices, this year Samsung is taking a quite different approach with the new Galaxy S21 series – not only stopping the price increases, but actually reducing this year’s flagship line-up prices compared to the 2020 predecessors.This year, Samsung is also more notably differentiating the specifications of the devices – there’s still a base model, a larger “+” model, and the super-sized “Ultra” model, however only the Ultra model has managed to come out rather unscathed, with the two other traditional models finding themselves with technical compromises that we haven’t seen in past years, such as lower resolution, last-gen panels, different build materials and designs, on top of the usual different camera configuration.Today we’re reviewing the Galaxy S21 Ultra in both Exynos and Snapdragon SoC flavours, as well the baseline Galaxy S21 – contrasting two very different devices in Samsung’s new series, coming in at two very different price points.Samsung Galaxy S21 SeriesGalaxy S21Galaxy S21+Galaxy S21 UltraSoCQualcomm Snapdragon 8881xCortex-X1@ 2.84GHz 1x1024KB pL23xCortex-A78@ 2.42GHz 3x512KB pL24x Cortex-A55 @ 1.80GHz 4x128KB pL24MB sL3Samsung LSI Exynos 21001xCortex-X1@2.9GHz1x512KB pL23xCortex-A78@ 2.8GHz 3x512KB pL24x Cortex A55 @ 2.2GHz 4x64KB pL24MB sL3Display6.2-inch2400 x 1080 (20:9)48-120Hz1300nits peak6.7-inch2400 x 1080 (20:9)48-120Hz1300nits peak6.8-inch3200 x 1440 (20:9)10-120Hz1500nits peakSAMOLEDHDR10+120Hz Refresh RateDimensions151.7 x 71.2 x 7.9mm171g (mmWave),169g (sub6)161.5 x 75.6 x 7.8mm202g (mmWave),200g (sub6)165.1 x 75.6 x 8.9mm229g (mmWave),227g (sub6)RAM8GB8GB12 / 16GBNANDStorage128, 256GB128, 256 GB128, 256, 512 GBBattery4000mAh(15.4Wh) typ.3880mAh (15.01Wh) rated4800mAh(18.57Wh) typ.4660mAh (18.03Wh) rated5000mAh(19.25Wh) typ.4855mAh (18.87Wh) ratedFront Camera10MP4K video recordingF/2.2, 80-degree40MP4K video recordingF/2.2, 80-degreePrimary Rear Camera79° Wide Angle12MP 1.8µm Dual Pixel PDAF79° Wide Angle108MP 0.8µm DP-PDAF3x3 Pixel Binning to 12MP8K24 Video Recordingfixed f/1.8 opticsOIS, auto HDR, LED flash4K60, 1080p240, 720p960 high-speed recordingSecondaryRear Camera76° Wide Angle(Cropping / digital zooming telephoto)64MP0.8µmf/2.0 optics, OIS8K24 Video Recording12° Telephoto(10x optical)10MPf/4.9 prism optics, OISTertiaryRear Camera-3x optical10MPf/2.4QuartenaryRear Camera120° Ultra-Wide Angle12MP 1.4µm f/2.2ExtraCamera-Time of Flight (ToF) 3D Sensor4G / 5GModemSnapdragon 5GTBCExynos 5GTBCSIM SizeNanoSIM + eSIMWireless802.11a/b/g/n/ac/ax2x2 MU-MIMO,BT 5.1 LE, NFC, GPS/Glonass/Galileo/BDS+ WiFi 6EConnectivityUSB Type-Cno 3.5mm headsetSpecial FeaturesUnder-screen ultrasonic fingerprint sensor(Qualcomm QC 2.0, Adaptive Fast Charging, USB 3.0 PD PPS),reverse wireless charging (WPC & PMA),Ultra WidebandIP68 water resistanceLaunch OSAndroid 11 with Samsung OneUI 3.1Launch Prices128GB 5G:$799/€849/£769256GB 5G:$849/€899/£819128GB 5G:$999/€1049/£949256GB 5G:$1049/€1099/£999128GB 5G:$1199/€1249/£1149256GB 5G:$1249/€1299/£1199512GB 5G:$1379/€1429/£1329Starting off with the core hardware components of the new S21 series, the new flagships are amongst the first devices in the market powered by Qualcomm’s new Snapdragon 888 SoC as well as Samsung LSI’s new Exynos 2100. The two new silicon chips this year are more similar to each other than in previous years – both featuring almost identical CPU setups and both being manufactured on the same process node, only leaving more significant differences on the part of the GPUs and other multimedia design aspects.The Snapdragon 888 vs The Exynos 2100:Cortex-X1 & 5nm - Who Does It Better?Ahead of our full device review today, we spun off a dedicated SoC-centric two weeks ago which goes into more technical details of how this year’s new silicon chips perform as the baseline computing platforms for 2021 devices, so I would recommend readers mostly interested in those technical details to read that piece ahead of continuing with the other aspects of the new S21 series today.As a short summary, I’d say that this year’s SoCs are to be viewed as smaller incremental improvements over last generation iterations – at least on the Snapdragon side of things which applies to north American variants of the new S21 series. Other global customers of the S21 series which will be seeing the Exynos variants being deployed in their devices, while not having fully caught up to the Snapdragon’s performance and power efficiency, will however see extremely large generational improvements compared to what we’ve seen in the Exynos-powered S20 series, so this year’s differences between the two SoC types will be smaller.In terms of DRAM and storage configurations, the S21 and S21+ come with 8GB of LPDDR5 in either 128GB or 256GB storage configurations. The larger S21 Ultra features generally 12GB of RAM alongside its 128 and 256GB configurations, however also sees a 16GB top-of-the-line version with 512GB storage.Samsung this year has dropped the microSD slot for storage expansion, marking the S21 series as the second time Samsung has removed this feature after the S6 series a few years ago. Back then, baseline storage capacities landed in at 32GB up to a maximum of 128GB, and Samsung had reintroduced the microSD slot in the S7 series which only went up to 64GB of internal capacity.I’m not too sure what Samsung’s reasonings were with the removal of the slot on the S21 series this year – but I do have to admit I haven’t used a microSD in a few years now following increases of the baseline storage capacities of phones. A further consideration is that the industry has utterly failed to transition away from slow microSD cards onto newer standards.Samsung’s own UFS Card standardannounced back in 2016 has seen zero adoption in the mobile market – I’m not actually aware of SoCs who actually sport a second UFS controller to actually enable these cards. I’m also not aware of any phone supporting the UHS-II microSD interface standard, so again quite standards failure here in the broader industry.The silver lining here is that Samsung does employ 128GB as the base storage, and for the first time ever, the 256GB storage option upgrade this year only costs $/€/£50 – a much cheaper upgrade not only compared to past years, but also cheaper than the up-sell many contemporary competitors are offering today, and for the first time an upgrade that I would consider of actual good value and which I wouldn’t hesitate recommending.In terms of other feature discrepancies between the S21 Ultra and its cheaper siblings, one of note is that the new model is the only one in the line-up which features a new Broadcom WiFi 6E compatible BCM4389 chipset, whereas the S21 and S21+ features the same WiFi 6 module from last year. This is the first time Samsung has actually differentiated the models within a flagship series in this manner – so although WiFi 6E isn’t widespread yet and most users likely are still lacking a proper compatible router, it does mean the Ultra is theoretically more future-proof in this regard.For US users particularly, one other feature removal of the S21 series is the lack of MST payment options. This should be largely irrelevant in most of the rest of the world, but Samsung is removing an important and unique differentiation factor for the S-series in America – if you’ve been relying on it for smartphone payments, it’s something to keep in mind as it’s now gone.The Galaxy S21 Ultra – The True FlagshipAs we’ve noted, Samsung’s differentiation between the various S21 models is greater than ever before – making the new S21 Ultra the only real (almost) no-compromise device in the series this year. In many regards, the new Ultra is actually the new pinnacle of smartphone flagship technologies, with Samsung going all-out in almost every aspect.Starting off with the biggest ticket item on the spec list, the new S21 Ultra’s display is the most technically impressive piece of technology in the new phone. Still at QHD 3200 x 1440 resolution, Samsung now also actually allows native software rendering at the full 120Hz refresh rate.Furthermore, this is now as Samsung calls it an “Adaptive Refresh Rate” display, which in the S21 Ultra implementation not only means coarse software-based refresh rate switching between 60 and 120Hz modes, but also fine-grained transparent hardware-based LFD (low frequency drive) refresh rate switching down to 10Hz. Although with a few quirks, this is currently the most cutting-edge high refresh rate display implementation in any mobile phone in the market right now, essentially solving almost all battery draw concerns of the much-praised 120Hz HFR mode of modern flagships. It really seems to be an outstanding display in every regard.Investigating The Galaxy S21 Ultra New OLED Emitter:Huge Efficiency ImprovementsOn top of the new high-refresh rate technologies, the S21 Ultra is also the first phone OLED display to employ a new generation emitter material, allowing it to go significantly brighter, or to be much more power efficient with its luminosity compared to other displays in the market. It really seems to be an outstanding display in every regard.In terms of design, the front face of the new Galaxy S21 Ultra is nearly indistinguishable from that of the S20 Ultra – it’s still a large glass panel with gently curved sides with minimal bezels in every direction, as well as a central front-camera hole-punch design which doesn’t look to have changed from last year’s model.The new phone is actually 1.8mm shorter and 0.4mm narrower this year, and Samsung does say the display is now 6.8” instead of 6.9”, however for other design reasons we’ll go into in a bit, this actually isn’t immediately noticeable.The big new fresh design element of the new S21 series has to be the new camera layout as well as the “camera island”. Unlike the hodgepodge camera design of the S20 Ultra which looked like a last-minute attempt to cram in as many cameras as possible into something manufacturable, the new S21 Ultra camera layout feels extremely well though-out in terms of its looks, and how it integrates into the rest of the phone body. The whole camera “island” has a metal cover protecting it, rather than a larger glass piece over all the cameras, with the new design featuring individual recessed glass pieces over each camera module.The new design partly solves one issue with these new huge camera bumps – the edges of the camera island, by fully integrating itself into the corner of the phone and seamlessly melting with the actual side-frame of the device. This does mean however that the side-frame in that corner of the phone is hilariously thick, however I still much prefer this kind of design over previous attempts – it much more embraces the cameras rather than trying to accommodate for them.S21 Ultra (left) vs S20 Ultra (right)One aspect that I didn’t see being talked too much about the new S21 Ultra is its general ergonomics and the design of the side-frame and back panel. Unlike the S20 Ultra, the new phone actually has now a thicker side frame on the sides of the phone, whereas the S20 Ultra had more of the glass back panel flowing further towards the sides. I had expected the S21 Ultra to feel smaller than the S20 Ultra, however because of this new frame design this actually isn’t the case, as the back curvature this year is much less pronounced.S21 Ultra (top) vs S20 Ultra (bottom)The top and bottom frame is also less rounded than the S20 Ultra, and in general I’d say this makes the phone feel thicker than before, even though both generations are measuring at the same 8.9mm body thickness.One last thing I wanted to comment on the S21 frames is the fact that they feel more grippy and have more friction to them. Although both are glossy finishes, the S21 (on purpose or not) has more friction to it, and also does for this reason more easily collect dirt and fingerprints.The Galaxy S21 - The Cheaper Not-Quite FlagshipThe new Galaxy S21 is this year’s weird new little flagship for Samsung. This year we opted to buy the smaller S21 rather than the more usual “+” model as we wanted to give the new device a try-out, as it does have some unique design elements.Again, much like on the Ultra, the new S21’s key differentiating feature is its design. Samsung has opted to go with a very different screen panel and general screen design for this year’s S21 and S21+ - going lower resolution as well as going for fully flat glass.In terms of the panel itself, I immediately questioned Samsung’s choice of going with a FHD-class display, after essentially 5 years and generations of 1440p panels on the S-series. I am able to say that I did notice the change quite immediately, even though we’re talking about the smaller S21 - the larger S21+’s display will make the resolution difference likely even more of a point of contention.The panels on the S21 and S21+ don’t have the fancy new features the S21 Ultra has. It’s still the same emitter generation as on the S20 series, meaning it has the same brightness and efficiency, and it lacks the more advanced LFD high-refresh rate power management, although the “Adaptive Refresh Rate” mode this year does actually work on a software granularity and has that going for itself versus the S20’s series constant 120Hz mode.In general, the display design here isn’t bad – it’s still a great display, and I do feel it has generational advantages such as better viewing angles and a seemingly better lamination, giving it a more popping sticker effect. The flat display and the lower resolution do however feel very conventional, and not-quite flagship like.The phones this year are cheaper than their predecessors, but I feel like Samsung here is compromising on one of the most defining aspects of the Galaxy S series – having uncompromising best-in-class displays, no matter the model you chose. Other than the software adaptive refresh rate mode, I don’t feel like the S21 display is any better than the S20’s, and in terms of build quality, subjectively feels worse and cheaper. You might argue or not if that makes sense on the S21, but the S21+ is still a $1000/€1049 phone, competing with the likes of the Xiaomi Mi 11 which now has a better screen design and specifications at only €749. The whole design decision gives me very mixed feelings, and I’m still not too sure on the general conclusion of the matter.The back panel of the S21 is made of plastic, which had been a contention point ever since Samsung reintroduced the material in their flagship series with the Note20. After having experienced the device first-hand, I can say that it’s absolutely not an issue. In fact, you might very well be fooled about what the material actually is, were it not for the temperature behaviour – glass still noticeably feels cooler than plastic.In terms of texture, the matte finish works extremely well, but it is quite different to that of the glass S21 Ultra and S21+ - it has much more friction to it and just isn’t as smooth, but that’s about it when it comes to the differences. I haven’t had the phone long enough in daily usage to talk about one contention point of the new plastic material: long-term durability and scratch resistance. I still remember past plastic Galaxy phones suffering from scratches and abrasions due to use – we’ll have to see how the S21 fares after several months or a year.The camera island on the S21 is brilliant in its design. Much like on the S21 Ultra, this is an aluminium cover that protects individual recessed glass elements for the three main camera modules. Samsung harmonised the camera design between the S21 and S21+ by dropping the ToF sensor from the latter in comparison to the S20+ - which is fine by me as frankly I never really used it even though it was my daily driver for the past year.Streamlining the camera design around the three main modules and pushing them towards the corner works extremely well when it comes to aesthetics, and Samsung really hit it out of the park here with the S21 as it achieves in my opinion an attractive and very unique look, especially on this violet-gold variant of the phone.Probably the only gripe I have here is that Samsung should probably swap the ultra-wide-angle module away from the corner from the phone with one of the other modules – I’ve caught myself with my fingers in the camera frame a few times.Much like on the S21 Ultra, the frame of the phone isn’t quite as round as on the S20 series. This is particularly important for the S21+ which this year has grown from 73.7mm width to 75.6mm, and also increasing its weight from 187g to 202g – it’s no longer the medium sized model to the Ultra, but actually essentially the same footprint although it is 1.1mm thinner. This increase in width along with the flat display means the S21+ really can’t be compared to the S20+ in terms of form-factors, especially when the S21 Ultra has the same footprint now.Overall, then new S21 series this year are quite brave divergences for Samsung. The new camera design and layout in my opinion are fantastic. The S21 Ultra also looks great, although it’s a massive device by nature. The S21 and S21+’s new screen designs still give me mixed feelings, and although the new lower price points seem to be working well for Samsung, I do feel it a pity to see the series compromise in its defining features.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16488/the-samsung-galaxy-s21-ultra-s21-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NVIDIA Nerfs Ethereum Hash Rate & Launches CMP Dedicated Mining Hardware\n",
      "Author: Ryan Smith\n",
      "Date Published: 2021-02-18T14:45:00Z\n",
      "URL: https://www.anandtech.com/show/16493/nvidia-launches-cmp-dedicated-mining-hardware\n",
      "Content: One of the critical points during this period of high demand for graphics cards is that a portion of them are being purchased by professional users looking to mine cryptocurrencies. The recent launch of new cards coupled with record highs in the cryptocurrency market has led to a rebirth of the mining community, who as of recently could earn ~$15/day per RTX 3090 graphics card. These professional miners buy graphics card by the pallet load, sometimes bypassing retailers and going direct to distributors, as they can guarantee a complete shipment sale in one go. The knock on effect is fewer cards available for gamers looking to build new systems, leading to empty shelves and causing prices to spike for the handful of cards that ever make it to retailers.In order to at least offer a fig leaf to gamers, in the past certain graphics board partners started producing mining-only graphics cards. These had no graphical outputs, making them almost impossible for gaming use cases, but it filtered off some of the mining market into buying those rather than taking stock away from shelves for gamers. This was a poor band-aid, and now NVIDIA has gone one step further to separate mining from gaming.NVIDIA’s announcement today is two-fold: firstly addressing the upcoming launch of the RTX 3060 graphics on February 25th, and secondly announcing a new range of dedicated mining hardware.RTX 3060: Halving the Mining RateOne of the key drivers as to why the new graphics cards are being sold is because they are so good at doing the mining operations for various cryptocurrencies (namely for Ethereum and other derived coins) and earning the users a semblance of return on their purchase. Mining requires hardware and software, and it’s the software side that NVIDIA is tackling with this first announcement.For the upcoming RTX 3060, the software drivers for this graphics card will automatically limit cryptocoin hashing rates to half – making how much they can earn specifically halved. The software drivers will do this by detecting the math coming through the pipeline and restricting access to the hardware for those operations. At this point we’re not sure if it’s a cut in frequency that the drivers will cause or simply limiting the operations to half of the hardware, but either way NVIDIA is hoping this will detract professional miners from buying these cards if the return on them is halved.Update: NVIDIA has also confirmed that performance restrictions will be going in for their Linux drivers as well as their Windows drivers. The inclusion of Linux drivers is incredibly important, as most dedicated miners are thought to be using Linux rather than Windows.No plans are being announced for cards currently in the market, perhaps because the drivers for those cards already allow a full-rate compute solution, and those can simply keep older drivers installed.NVIDIA CMP: Dedicated Mining Silicon for EthereumIn the same way that ‘crypto’ cards without video outputs were pushing into the market for balance, NVIDIA is going a step further and removing the video outputs from the silicon entirely. There are other potential optimizations that could be made for power and performance, but at this point NVIDIA is simply stating as graphics-less silicon. This could be a mix of customized new silicon, or simply silicon already manufactured that had defects in the video output pipeline.The new NVIDIA CMP HX dedicated mining cards will come in four variants up to 320 W, and from authorized partners including ASUS, Colorful, EVGA, Gigabyte, MSI, Palit, and PC Partner. These cards (along with drivers) are also set to be designed such that more of these cards can be enabled in a single system.NVIDIA CMP HX Mining HardwareAnandTech30HX40HX50HX90HXEth Hash Rate*26 MH/s36 MH/s45 MH/s86 MH/sRated Power125 W185 W250 W320 WReference Connectors8-pin8-pin2 x 8-pin2 x 8-pinMemory Size6 GB8 GB10 GB10 GBAvailabilityQ1Q1Q2Q2*NVIDIA Measured to DAG and Epoch 394What’s interesting here is that these stats aren’t that great. Here is a breakdown of what NVIDIA’s cards do today, and you can see why:NVIDIA Hardware Hash RatesAnandTechHash RatePowerEfficiencyMH/s/WRTX 3090121 MH/s290 W0.42RTX 308098 MH/s224 W0.4490HX86 MH/s320 W0.27RTX 307062 MH/s117 W0.53RTX 3060 Ti60 MH/s120 W0.50RTX 2080 Ti49 MH/s240 W0.2050HX45 MH/s250 W0.1840HX36 MH/s185 W0.1930HX26 MH/s125 W0.21HX Data from NVIDIARTX Data from MinerstatThe only way these new CMP HX mining add-in cards make financial sense is if they are really cheap, around $600 for the 90HX, otherwise the retail gaming GPUs seem to be a lot more efficient.NVIDIA isn’t giving any more details on when these mining add-in cards will be made available, aside from Q1 for the slower ones and Q2 for the faster ones. No word on pricing, nor on distribution methods – there’s a chance here that these cards will only be sold by distributors direct to professional mining outlets. By the pallet. Note that this doesn’t stop the high demand for power supplies. That market is also feeling the effects.Analysis: Will This Work?NVIDIA’s actions come as Ethereum mining has essentially broken the retail market for GPUs for the last few months – and quite possibly will keep it broken for months to come. And while selling every last GPU they can make is hardly a bad thing for NVIDIA in the short term, in the long term a broken market risks hurting NVIDIA’s brand and consumer customer base, never mind the threat of all of those mining cards boomeranging back once the bubble pops (again). All of which raises a very important question: will this work?For better or worse, nothing NVIDIA is doing today will fundamentally change the market forces at work. So long as Ethereum is running over $1000 or so, miners can make a tidy profit using video cards for mining – and thus miners value the hardware more than gamers. NVIDIA can nudge things in one direction or another, but even NVIDIA isn’t going to be able to beat the laws of supply and demand. As a result, the problem at hand won’ttrulygo away until either mining stops being profitable, either by Ethereum’s price coming down or the market being flooded with cards (and thereby spiking the difficulty level).In the interim, the best NVIDIA can do is to try to keep miners from snatching up consumer video cards, which is what today’s announcement focuses on. There’s every reason to believe that miners will end up with the bulk of GPUs – this is market forces at work; miners pay more – but if NVIDIA can at least ensure that the fastest video card in stock at Newegg is better than a GT 1030, then that’s a big improvement over where things stand today.Making consumer GeForce cards less palatable to miners is certainly going to be the most important step of this, though it’s unfortunately also the hardest to execute on. Detecting Ethereum is easy enough, but because the block is being done at the driver level, it’s also extremely vulnerable to being patched out. Miners only need to hack one driver, and then every RTX 3060 card from here-on out can be used for mining by using that driver. NVIDIA is essentially implementing driver-based DRM, which historically has not worked out all that well over the long-run.A few more details on NVIDIA's Ethereum rate limiter, straight from NV PR:https://t.co/XGnXKCaIG6— Ryan Smith (@RyanSmithAT)February 19, 2021Which isn’t to say that the driver throttling approach won’t work. But there is a very real chance it’s not going to work for very long, especially with miners so financially motivated to work-around it. Complicating matters, NVIDIA has been shipping mobile RTX 3060 hardware and drivers since late January as part of the RTX 30 series for laptops, so driver hackers already have a starting point for “clean” GA106 code.As an aside, this is also why NVIDIA can’t do anything about the existing RTX video cards already on the market. Even if NVIDIA puts Ethereum throttling code into future drivers, miners can just use the existing drivers. In other words, NVIDIA can’t put the genie back into the bottle; they can only try to keep any more genies from getting out.Which is why the second aspect of NVIDIA’s strategy – introducing a line of dedicated mining cards – is more likely to have a lasting impact. Even though it probably won’t help with either the supply of consumer video cards nor the high demand for them, it at least will help NVIDIA manage how many cards are offered to consumers versus miners.I won’t try to ascribe too much to NVIDIA’s specific motivations here, but at the end of the day NVIDIA has built an empire on very rigidly defined market segmentation, so siphoning off miners into their own category plays to NVIDIA’s strengths. Along with more specific control over product allocations, mining-only cards allows NVIDIA to price those cards as the market will bear, all without disrupting the consumer market. And, perhaps most importantly NVIDIA, mining-only cards won’t boomerang back into the used video card market once the bubble does pop. The resulting crypto-hangover from the last time that happened significantly hurt NVIDIA (and AMD), so they’re going to be eager to avoid it.But will miners buy mining-only cards? That’s a more nebulous question. The ability to offload used cards into the arms of gamers is a significant part of the financial calculus for miners, because it means their hardware investment doesn’t become worthless overnight. Consequently, mining-only cards are not nearly as desirable. On the other hand, given how hard it is to getanyvideo cards right now, even a mining-only card is better than nothing if you want to start mining for profit today. Currently, miners are taking virtually everything they can get, and certainly NVIDIA seems to be counting on that to continue.Ultimately, introducing mining-only cards is likely to be the more successful half of NVIDIA’s announcement today. It’s clear that the mining market isn’t going away any time soon, and until it does, it’s in NVIDIA’s own interests to try to control it so that it doesn’t continue to wreak havoc on the consumer video card market.As for consumers looking to get a video card for gaming purposes, today’s announcement is probably a bit of a wash. It certainly doesn’t hurt that NVIDIA is trying to get better control over the market and drive miners away from consumer video cards; it’s just not likely to stop them entirely given the profits at hand. Put another way, I don’t realistically expect that the RTX 3060 will be any more available than NVIDIA’s other RTX cards; but I will give NVIDIA some credit for trying with today’s announcement. If nothing else, today is a first step towards a long-term solution for what may end up being a long-term problem.Related ReadingNVIDIA’s GeForce RTX 3060 Gets a Release Date: February 25thNVIDIA Reveals GeForce RTX 3060: Launching Late February For $329The NVIDIA GeForce CES 2021 Live Blog: Game OnLaunching This Week: NVIDIA’s GeForce RTX 3060 Ti, A Smaller Bite of Ampere For $400Launching This Week: NVIDIA’s GeForce RTX 3070; 1440p Gaming For $499NVIDIA Announces the GeForce RTX 30 Series: Ampere For Gaming, Starting With RTX 3080 & RTX 3090\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16493/nvidia-launches-cmp-dedicated-mining-hardware\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Xiaomi Launches Mi 11 Globally: Starting at 749€\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-02-08T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16477/xiaomi-launches-mi-11-globally-starting-at-749\n",
      "Content: Today Xiaomi is launching their new Mi 11 flagship for the global market, following theirdomestic launch of the phone in China almost 6 weeks ago.The Mi 11 is an interesting device as it really balances out its features as a affordable flagship device. The European prices for the new phone start at 749€, featuring the new Snapdragon 888 SoC, and what seems to be a top-of-the line 1440p 120Hz OLED display, all while featuring a high-end 108MP camera module, though the phone compromises on its other cameras.Xiaomi Mi SeriesMi 10Mi 11SoCQualcomm Snapdragon 8651x Cortex-A77 @ 2.84GHz3x Cortex-A77 @ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 650 @ 587MHzQualcomm Snapdragon 8881xCortex-X1@ 2.84GHz3xCortex-A78@ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 660 @ 840MHzDRAM8GB LPDDR5-55008/12GB LPDDR5-6400Display6.67\" AMOLED2340 x 1080 (19.5:9)90Hz Refresh6.81\" AMOLED3200 x 1440120Hz Refresh480Hz TouchHDR10+ /10-bit panelSizeHeight162.58mm164.3mmWidth74.80mm74.6mmDepth8.96mm8.06mmWeight208g196gBattery Capacity4780mAh (Typical)30W Charging4600mAh (Typical)55W ChargingWireless Charging30W50WRear CamerasMain108MP 1/1.3\" 0.8µm4:1 Binning to 27MP / 1.6µmf/1.69 w/ OIS108MP Modulef/1.85 w/OISTelephoto-5MP50mm eq.f/2.2ExtraTelephoto--Ultra-Wide13MP 1.12µmf/2.4117° FoV13MPf/2.4123° FoVExtra2MP Depth Camera2MP Macro Camera-Front Camera20MP 0.8µmf/2.3f/2.2Storage128 / 256GBUFS 3.0128 / 256GBI/OUSB-CWireless (local)802.11ax(Wifi 6),Bluetooth 5.1Cellular4G + 5G NR NSA+SA Sub-6GHzSpecial FeaturesUnder-screen fingerprint sensorFull-range stereo speakersSplash, Water, Dust ResistanceNo ratingDual-SIM2x nano-SIMLaunch OSAndroid 10 w/ MIUIAndroid 11 w/ MIUILaunch Price8+128GB: 799€8+256GB: 899€8+128GB: 749€8+256GB: 799€The new Mi 11 is powered by the Snapdragon 888 whichwe had a deeper dive into earlier today. The new chip’s CPU setup is definitely more powerful, as well as its GPU, however it can also be quite power hungry. I’m expecting Xiaomi’s implementation to be more aggressive than Samsung’s, allowing for higher power dissipation levels under sustained workloads – we’ll confirm this soon enough as we prepare for a review of the phone.The design of the Mi 11 looks to be quite attractive, and it’s definitely a sleeker phone than the Mi 10, as Xiaomi was able to thin the phone by 0.9mm and also make it 12g lighter – doesn’t sound like much, but such differences can make for a substantial change in feel.The phone features a 6.8” 3200 x 1440 OLED display that’s able to reach 120Hz refresh rates, as well as a staggering 480Hz touch input sample rate. Xiaomi advertises extremely high brightness levels; however, the company hadn’t commented on the OLED generation of the panel so as of right now we’re not sure if this is using one of Samsung Display’s newer emitter panels or if it even has VRR/LFD capabilities – but as of right now I’m assuming that it doesn’t.On the camera side, the Mi 11 features their signature 108MP sensor, binning down to 27MP in regular photos. This generation Xiaomi has opted for different optics, going from f/1.69 to a new f/1.85 system – which in my view is absolutely the right choice as the previous generation did suffer from optical underperformance on the part of the main camera. The module here also has OIS.Alongside the main sensor, we see a 13MP ultra-wide with 123°V FoV and f/2.4 optics, as well as a 5MP “telemacro” module with 2x magnification. I’m curious to see what this last module does, but I think most people should treat the phone as a dual-camera phone, which is absolutely just fine as well.749€ - A great priceThe big news today was the reveal of the global/European price of the new Mi 11: 749€ for the 8+128GB variant. This is actually a pretty great price, considering it’s competing against the likes of 999€ Galaxy S21+. The Xiaomi has some drawbacks in the camera department, though we’ll have to see how they compete against each other, but also has notable advantages such as a higher quality display, and the superior Snapdragon 888 in most global markets, which outperforms the new Exynos 2100.In that regard, Xiaomi’s pricing seems to be spot-on, and of great value. We’ll be review the Mi 11 in the coming future.Related Reading:Xiaomi Announces the Mi 11: First Snapdragon 888 DeviceXiaomi Globally Launches Mi 10, Mi 10 Pro; Snapdragon 865 & 108MP CamerasThe Xiaomi Mi 10 Pro Review - A Solid Overall ValueXiaomi Announces Mi 10T & Mi 10T Pro: More Budget, But With 144HzXiaomi Launches Mi Note 10 Lite, Redmi Note 9 & Note 9 Pro Globally\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16477/xiaomi-launches-mi-11-globally-starting-at-749\n",
      "Title: The Snapdragon 888 vs The Exynos 2100: Cortex-X1 & 5nm - Who Does It Better?\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-02-08T08:00:00Z\n",
      "URL: https://www.anandtech.com/show/16463/snapdragon-888-vs-exynos-2100-galaxy-s21-ultra\n",
      "Content: The new Galaxy S21 series of devices have been out commercially for a week now, and we’ve managed to get our hands on two Galaxy S21 Ultras – one with Qualcomm’s new Snapdragon 888 SoC, and one with Samsung’s new Exynos 2100 SoC. Both chipsets this year are more similar than ever, both now sporting similar CPU configurations, and both being produced on a new Samsung 5nm (5LPE) process node.Ahead of our full device review of the Galaxy S21 Ultra (and the smaller Galaxy S21), today we’re focusing on the first test results of the new generation of SoCs, putting them through their paces, and pitting them against each other in the new 2021 competitive landscape.The Snapdragon 888Qualcomm Details The Snapdragon 888:3rd Gen 5G & Cortex-X1 on 5nmQualcomm Snapdragon Flagship SoCs 2020-2021SoCSnapdragon 865Snapdragon 888CPU1x Cortex-A77@2.84GHz 1x512KB pL23x Cortex-A77@2.42GHz 3x256KB pL24x Cortex-A55@ 1.80GHz 4x128KB pL24MB sL31xCortex-X1@ 2.84GHz 1x1024KB pL23xCortex-A78@ 2.42GHz 3x512KB pL24x Cortex-A55@ 1.80GHz 4x128KB pL24MB sL3GPUAdreno 650 @ 587 MHzAdreno 660@ 840MHzDSP / NPUHexagon 69815 TOPS AI(Total CPU+GPU+HVX+Tensor)Hexagon 78026 TOPS AI(Total CPU+GPU+HVX+Tensor)MemoryController4x 16-bit CH@ 2133MHz LPDDR4X / 33.4GB/sor@ 2750MHz LPDDR5 / 44.0GB/s3MB system level cache4x 16-bit CH@3200MHz LPDDR5/ 51.2GB/s3MB system level cacheISP/CameraDual 14-bit Spectra 480 ISP1x 200MPor64MP with ZSLor2x 25MP with ZSL4K video & 64MP burst captureTriple 14-bit Spectra 580 ISP1x 200MP or 84MP with ZSLor64+25MP with ZSLor3x 28MP with ZSL4K video & 64MP burst captureEncode/Decode8K30 / 4K120 10-bit H.265Dolby Vision, HDR10+, HDR10, HLG720p960 infinite recording8K30 / 4K120 10-bit H.265Dolby Vision, HDR10+, HDR10, HLG720p960 infinite recordingIntegrated Modemnone(Paired withexternal X55only)(LTE Category 24/22)DL = 2500 Mbps7x20MHz CA, 1024-QAMUL = 316 Mbps3x20MHz CA, 256-QAM(5G NR Sub-6 + mmWave)DL = 7000 MbpsUL = 3000 MbpsX60 integrated(LTE Category 24/22)DL = 2500 Mbps7x20MHz CA, 1024-QAMUL = 316 Mbps3x20MHz CA, 256-QAM(5G NR Sub-6 + mmWave)DL = 7500 MbpsUL = 3000 MbpsMfc. ProcessTSMC7nm (N7P)Samsung5nm (5LPE)Starting off with the new Snapdragon 888 SoC, Qualcomm’s new flagship model makes iterative steps this generation, with the biggest changes of the new design actually being in the form of the new Hexagon 780 accelerator, which fuses together traditional scalar and vector DSP operations with tensor execution engines within one single IP block.Of course, we’re also seeing upgrades elsewhere in the architecture, with the Snapdragon 888 being among the first SoCs to use Arm’s new Cortex-X1 CPU IP, promising large performance gains relative to last generation Cortex-A77 cores. The single X1 cores in the Snapdragon 888 clocks in at 2.84GHz – the same as the previous generation Snapdragon 865’s prime Cortex-A77 cores, and less than the 3.1GHz and 3.2GHz Snapdragon 865+ andrecently announced Snapdragon 870 SoCs.Alongside the X1, we find three Cortex-A78 cores at 2.42GHz, again the same clocks as the previous generation 865 SoCs, but this time around with double the L2 caches at 512KB.The Cortex-A55 little cores remain identical this generation, clocking in at 1.8GHz.Although we had been expecting 8MB L3 cache flagship SoCs this year, it does look like Qualcomm opted to remain at 4MB for this generation – but at least the company dons the X1 core with the maximum 1MB L2 cache configuration.On the GPU side of things, Qualcomm’s new Adreno 660 GPU now clocks in up to a peak 840MHz – a whopping 43% higher frequency than the Snapdragon 865 GPU. The company’s performance claims here are also astonishing, promising a +35% boost in performance. We’ll have to see how this all ends up in terms of power consumption and long-term performance in the later dedicated GPU section.What’s quite different for the Snapdragon 888 this year is that Qualcomm has moved from a TSMC N7P process node to Samsung’s new 5LPE node – the generally wildcard in this whole situation as we haven’t had any prior experience with this new 5nm node.The Exynos 2100Samsung Announces Exynos 2100 SoC:A New Restart on 5nm with X1 CoresSamsung Exynos SoCs SpecificationsSoCExynos 990Exynos 2100CPU2xExynos M5@ 2.73GHz 2MB sL23MB sL32x Cortex-A76@ 2.50GHz 2x256KB pL24x Cortex-A55@ 2.00GHz 4x64KB pL21MB sL31xCortex-X1@ 2.91GHz 1x512KB pL23xCortex-A78@ 2.81GHz 3x512KB pL24x Cortex-A55@ 2.20GHz 4x64KB pL24MB sL3GPUMali G77MP11 @ 800 MHzMaliG78MP14@ 854 MHzMemoryController4x 16-bit CH@ 2750MHz LPDDR5 / 44.0GB/s2MB System Cache4x 16-bit CH@3200MHz LPDDR5/ 51.2GB/s6MB System CacheISPSingle: 108MPDual: 24.8MP+24.8MPSingle: 200MPDual: 32MP+32MP(Up to quad simultaneous camera)NPUDual NPU + DSP + CPU + GPU15 TOPsTriple NPU + DSP + CPU + GPU26 TOPsMedia8K30 & 4K120 encode & decodeH.265/HEVC, H.264, VP98K30 & 4K120 encode &8K60 decodeH.265/HEVC, H.264, VP9AV1 DecodeModemExynos ModemExternal(LTE Category 24/22)DL = 3000 Mbps8x20MHz CA, 1024-QAMUL = 422 Mbps?x20MHz CA, 256-QAM(5G NR Sub-6)DL = 5100 Mbps(5G NR mmWave)DL = 7350 MbpsExynos ModemIntegrated(LTE Category 24/18)DL = 3000 Mbps8x20MHz CA, 1024-QAMUL = 422 Mbps4x20MHz CA, 256-QAM(5G NR Sub-6)DL = 5100 MbpsUL = 1920Mbps(5G NR mmWave)DL = 7350 MbpsUL = 3670 MbpsMfc. ProcessSamsung7nm (7LPP)Samsung5nm (5LPE)On the Samsung LSI side of things, we find the brand-new Exynos 2100. Unlike the Snapdragon 888’s more incremental changes in terms of SoC design, the new Exynos is a rather large departure for Samsung’s SoC division as this is the first flagship design in many years that no longer uses Samsung’s own in-house CPU microarchitecture, but rather reverts back to using Arm Cortex cores, which in this case is also the new Cortex-X1 and Cortex-A78 cores.From a high-level, the CPU configuration of the Exynos 2100 looks nigh identical to that of the Snapdragon 888, as both are 1+3+4 designs with X1, A78 and A55 cores. The differences are in the details:The X1 cores on the Exynos 2100 clock slightly higher at up to 2.91GHz, while the Cortex-A78 clock in significantly higher than the Snapdragon as they reach 2.81GHz. The Cortex-A55 cores are also quite aggressive in terms of frequency as they now reach 2.20GHz – so overall across the board higher clocks than the Snapdragon variant.Where the Exynos isn’t as aggressive though is in its cache configurations. Most importantly, the X1 cores here only feature 512KB of L2 cache, which is a bit weird given the all-out-performance philosophy of the new CPU. The Cortex-A78s also see the usage of 512KB L2 caches, while the little A55 cores feature 64KB L2’s – less than the Snapdragon counterparts.Much like the Snapdragon, the L3 cache also falls in at 4MB rather than the 8MB we would have hoped for this generation, however Samsung does surprise us with the usage of an estimated 6-8MB system level cache, up from the 2MB design in the Exynos 990.On the GPU side of things, we see a Mali-G78MP14 at up to 854MHz. That’s 27% more cores and 6.7% higher frequency, and the company is also boasting massive performance gains as it touts a 40% generational improvement.Let them fightIn today piece, we’ll be mostly focusing around CPU and GPU performance, as an especially interesting comparison will be to see how the two designs do against each other, given that they both now use Arm’s newest Cortex-X1 cores and both are sporting the same manufacturing node.The GPU comparisons will also be interesting – and maybe quite controversial, as the results won’t be what many people will have been expecting.While we would have liked to showcase AI performance of the two SoCs – unfortunately the software situation on the Galaxy S21’s right now means that neither SoC are fully taking advantage of their new accelerators, so that’s a topic to revisit in a few months’ time once the proper frameworks have been updated by Samsung.Table Of ContentsThe Snapdragon 888 & Exynos 21005nm / 5LPE: What Do We Know?Memory Subsystem & Latency: Quite DifferentSPEC - Single Threaded Performance & PowerMixed-Usage Power & Preliminary Battery LifeGPU Performance & Power: Very, Very HotConclusion & End Remarks\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16463/snapdragon-888-vs-exynos-2100-galaxy-s21-ultra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Redragon Devarajas K556 RGB Mechanical Keyboard Review: Jack Of Most Trades\n",
      "Author: E. Fylladitakis\n",
      "Date Published: 2021-02-04T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/16466/the-redragon-devarajas-k556-rgb-mechanical-keyboard-review\n",
      "Content: The market of mechanical keyboards has been on the rise for nearly a decade, with an ever-growing number of products and the sales volumes to match. The high margins triggered the diversification of many known companies to include mechanical keyboards into their products portfolio, as well as the founding of many new companies, be it either as daughter companies of established manufacturers or as entirely new startups. Most of these new companies failed to reach a global presence, mainly due to the lack of resources and production capabilities.Established manufacturers, however, took advantage of the growing market and diversified outside of their region’s borders. One of these companies was Redragon, a Chinese manufacturer of PC gaming peripherals. Although the brand was unknown outside of Asia until recently, Redragon has been around since 1996. They also are one of the largest gaming peripherals manufacturers on the planet, with over 1.000 employees.In today’s review, we are taking a close look at one of Redragon’s most popular mechanical keyboards: the Devarajas K556 RGB gaming keyboard. Designed by the company to be a jack-of-all-trades gaming keyboard, the K556 boasts top performance, full programmability, RGB lighting, and a very competitive retail price. With its name roughly translating as “god king”, we definitely hope that the product will live up to it.Packaging and BundleWe received the Devarajas K556 RGB in a relatively sturdy cardboard box with a carrying handle. The artwork on the box is based on a colorful rendering of the keyboard itself. Inside the box, the keyboard is protected only by a nylon bag, with the walls of the box being virtually the only layer of shipping protection.Inside the box, we found a user’s manual, a vinyl sticker, a plastic keycap puller, and a metallic switch puller. One can, in theory, replace the switches of the keyboard without dismantling it, either for mixing-&-matching different switches or for replacing a bad switch.The Redragon Devarajas K556 RGB Mechanical KeyboardThe core design of the Devarajas K556 is similar to that of many other minimalistic mechanical keyboards, with the exception of Redragon adding a few features to aesthetically enhance the keyboard. It has a metallic top and side plates, with chamfered and polished edges, and a plastic bottom frame. The top plate acts as a support for the mechanical switches, with the keys “floating” over it. Redragon’s logo can be seen on a metallic badge right above the arrow keys.We received the US layout version of the Devarajas K556 RGB. The company fully adhered to the 104 key ANSI layout, with the sole overall deviation being the replacement of the right OS key with an Fn key that allows for additional keystroke functions. Most are simple multimedia functions, but there is a rudimentary macro recording feature available, allowing for the on-the-fly programming of basic keystroke macros. It has a 6.25× Spacebar and seven 1.25× bottom row keys.Redragon is using a futuristic font on the keycaps, which have both the primary and the secondary characters printed towards their top edge. They also painted the sides of the keycaps glossy black, making them partially reflective.The bottom side is very simple and plain. There are two large anti-skid pads near the bottom of the keyboard and two smaller pads surrounding the tilt adjustment feet surround. The feet are small considering the weight of the keyboard and will easily fold if the keyboard is forced backwards. Moreover, when the keyboard’s feet are open, the front anti-skid pads barely make any contact with the surface, meaning that the front metal part of the keyboard is what actually touches the desk – which is not healthy for desks made of softer materials, such as real wood. A sticker with the keyboard’s basic data and serial number is present at the center.Beneath the keycaps of the Devarajas K556 RGB, we find Redragon's dust-proof Brown switches, which are made by OUTEMU. OUTEMU is a Chinese manufacturer that effectively copies Cherry’s products, meaning that the OUTEMU brown switches are an almost direct copy of Cherry’s tactile MX Brown switch. OUTEMU is also using clear plastic for the switch housing, much like what Cherry does with their RGB switch variants. Redragon also copies Cherry’s cross-type supports for the larger keys, even though most keyboards that come with OUTEMU switches stick with the classic bar supports.The RGB lighting of the Devarajas K556 RGB is crisp and well applied. The basic colors are bright and clear, with minimal backlight bleeding around the keys. Redragon’s glossy keycaps, however, reflect some of the LED lights and it is not a pleasant visual effect, especially when seen from side angles and not from the user’s point of view.After taking the keyboard apart, we are left with a green PCB and its metal support plate. The assembly job is clean and we can see Redragon applying clear lacquer over the vital parts of the assembly, probably to enhance their resilience against moisture and acids.The heart of the Devarajas K556 RGB keyboard is branded as an eVision VS11K09A-1 – which, however, actually is a keyboard model number from another Chinese OEM. The actual manufacturer of the MCU is Sonix and the MCU itselfprobablyis the SN32F248B. This MCU is very popular amongst Asian designers and manufacturers. It features a 48 MHz 32-bit ARM Cortex-M0 CPU, 8KB of RAM, and 64KB of Flash ROM, which generally tend to be enough for a gaming keyboard.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16466/the-redragon-devarajas-k556-rgb-mechanical-keyboard-review\n",
      "Title: AMD Ryzen 9 5980HS Cezanne Review: Ryzen 5000 Mobile Tested\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-01-26T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/16446/amd-ryzen-9-5980hs-cezanne-review-ryzen-5000-mobile-tested\n",
      "Content: Since AMD’s introduction of its newest Zen 3 core for desktop users, the implementation for notebooks and mobile users has been eagerly awaited. In a single generation, on the same manufacturing process, AMD extracted +19% more performance per clock (we verified), so for any system that is power limited, extra performance is often very well received. AMD announced its new Ryzen 5000 Mobile processor family at the start of the year, with processors from 15W to 45W+ in the pipeline, and the first mobile systems coming to market in February. AMD sent us a peak example of Ryzen 5000 Mobile for today’s review, the 35W Ryzen 9 5980HS, as contained in the ASUS ROG Flow X13.Ryzen 5000 Mobile: Eight Zen 3 cores and Vega 8 GraphicsFor those that didn’t catchthe original announcement a couple of weeks ago, here is a recap of the Ryzen 5000 Mobile family as well as the key points from the announcement.The Ryzen 5000 Mobile processor series is an upgrade over last year’s Ryzen 4000 Mobile processor series. AMD co-designed both of these processor families to re-use key parts of the chip design, enabling a fast time-to-market and quicker upgrade cycle for AMD’s notebook manufacturing partners (OEMs), like Dell, HP, Lenovo, and others. The major difference between the two processors that most users will encounter is that the new hardware uses eight of AMD’s latest Zen 3 CPU cores, which is an upgrade over the eight Zen 2 cores from last year. The highlight is the +19% raw performance uplift when comparing the two at the same frequency.Under the hood, there are a few more key changes that enthusiasts will be interested in. The new 8-core Zen 3 design shares a combined 16 MB L3 cache, which enables any of the eight cores to access the full cache, reducing latency to main memory (from 4 MB to 16 MB) compared to the previous design which had two clusters of four Zen 2 cores, each with 4 MB of cache.The new processor is 180 mm2 in size, compared to 156 mm2 of last generation, but still fits into the same socket. It contains 10.7 billion transistors, which is up from 9.8 billion. This means an effective decrease in transistor density, although we know that Zen 3 cores are slightly larger than Zen 2 cores, and some additional security measures have been added (more on this on the next page).AMD CEO Dr. Lisa Su Showcasing Ryzen 5000 Mobile at CESUsers may be upset that the new processor range only features Vega 8 graphics, the same as last year’s design, however part of the silicon re-use comes in here enabling AMD to come to market in a timely manner. The Vega 8 design in the previous generation already had a big boost in efficiency and frequency, and this time around we get another +350 MHz on the high-end. Users who want to see RDNA in a mobile processor may have to wait longer. AMD’s re-use strategy may lend itself to changing CPU one generation, GPU the next – we will have to wait and see.There are other SoC changes, which we will get to later in this review.The Ryzen 5000 Mobile family is split into two broad product ranges, but both ranges use the same underlying silicon. At the top is the traditional 45 W H-series processors, aimed at productivity notebook designs. For this generation, AMD is pairing the traditional 45 W parts with a range of 35 W ‘HS’ models, optimized for more efficient designs – this will be AMD’s second generation of 35 W ‘HS’ class processors. AMD is also introducing a new range of ‘HX’ processors at 45 W and above which will allow AMD’s partners to co-design high-performance and/or overclockable AMD notebook solutions.AMD Ryzen 5000 Mobile: H-SeriesAnandTechCoresThreadsBaseFreqBoostFreqGPUCoresGPUFreqTDPZenRyzen 9 5980HX8C / 16T330048008210045W+Zen3Ryzen 9 5980HS8C / 16T300048008210035WZen3Ryzen 9 5900HX8C / 16T330046008210045W+Zen3Ryzen 9 5900HS8C / 16T300046008210035WZen3Ryzen 7 5800H8C / 16T320044008200045WZen3Ryzen 7 5800HS8C / 16T280044008200035WZen3Ryzen 5 5600H6C / 12T330042007180045WZen3Ryzen 5 5600HS6C / 12T300042007180035WZen3When HS was introduced last year with Ryzen 4000 Mobile, it was an AMD-OEM partnership-only co-designed product requiring AMD approval in order to have access to them. This year however, they seem to be part of the full stack, indicating perhaps that demand for these HS designs was higher than expected.The new HX models are here to enable high-end gaming, workstation, and desktop-replacement systems, as well as enabling vendors to supply overclockable laptops into the market with sufficient cooling provided. Overclockable laptops isn’t a new concept (Intel has been doing it a while), but it seems that AMD’s partners have requested higher power parts in order to enable this market on AMD. The official TDP for these processors is 45+ W, showcasing that partners can adjust the sustained TDP values north of 45 W if required, likely up to 65 W as needed. In the past, if OEMs wanted to go down this route, they would need to build a portable chassis capable of supporting a desktop processor.There is some slight deviation from the regular H-series, in that there is no Ryzen 9 standard ‘H’ processor. The Ryzen 7 5800H will sit at the top of that particular market, but the way these numbering systems work means that the Ryzen 7 still has the full eight cores and fast integrated graphics. In that instance, Ryzen 9, with only HS and HX models, are arguably for more ‘specialist’ focused designs.AMD is advertising the Ryzen 9 5980HS as the best processor for portable gaming performance, while the Ryzen 9 5980HX is ‘the best mobile processor for gaming’. As part of the launch day materials, AMD showcases the Ryzen 9 5980HS as scoring 600 pts in Cinebench R20, which would put it at the same level of performance as AMD’s desktop-class Zen 3 processors. We didn’t quite score 600 in this review with the R9 5980HS (we scored 578).The traditional 15 W processors, used for ultra-thin and light portable notebooks, form part of the Ryzen 5000 Mobile U-series. AMD is enabling a number of parts with updated Zen 3 cores, but also introducing several processors based on the older Zen 2 design, albeit with updates.AMD Ryzen 5000 Mobile: U-SeriesAnandTechCoresThreadsBaseFreqBoostFreqGPUCoresGPUFreqTDPZenZen3Ryzen 7 5800U8C / 16T190044008200015WZen3Ryzen 5 5600U6C / 12T230042007180015WZen3Ryzen 3 5400U4C / 8T260040006160015WZen3Zen2Ryzen 7 5700U8C / 16T180043008190015WZen2Ryzen 5 5500U6C / 12T210040007180015WZen2Ryzen 3 5300U4C / 8T260038006150015WZen2The simple way to identify this is with the digit after the number 5. Even digits (5800, 5600, 5400) are based on Zen 3, whereas odd digits (5700, 5500, 5300) are the updated versions of Zen 2. A lot of users will consider these latter processors identical to the previous generation, however we have learned that there are a number of key differences which we will cover in a separate article.Nonetheless, AMD is promoting the top Ryzen 7 5800U as the company’s most efficient mobile processor to date. Based on a number of enhancements to the silicon design, AMD is claiming a +2hr battery life from a simple processor swap from Ryzen 4000 to Ryzen 5000, even if everything else in the chassis is the same. Nonetheless, AMD is stating that it has worked with controller companies, power delivery suppliers, and notebook designers in order to ensure that those OEMs that want to build systems with more than 20+ hours battery life have the tools to do so. Other OEMs however, particularly for low cost designs or perhaps education models, can freely change the processor from old to new with only a firmware update, as both Ryzen 4000 and Ryzen 5000 are pin compatible.Overall AMD is claiming 150+ designs with Ryzen 5000 Mobile so far, a significant step up from the 100 designs on Ryzen 4000 Mobile. These are set to include high-end gaming designs with the latest premium graphics cards, a market that AMD has had difficulty breaking into so far.AMD Generation Code NamesAnandTechBrandCoreGraphicsProcessNodeMobile ProcessorsCezanneRyzen 5000 Mobile8 x Zen 3Vega 8TSMC N7LucienneRyzen 5000 Mobile8 x Zen 2Vega 8TSMC N7RenoirRyzen 4000 Mobile8 x Zen 2Vega 8TSMC N7PicassoRyzen 3000 Mobile4 x Zen+Vega 11GF 12nmRaven RidgeRyzen 2000 Mobile4 x ZenVega 11GF 14nmDaliAthlon 30002 x ZenVega 3GF 14nmPollock?2 x ZenVega 3GF 14nmDesktop ProcessorsVermeerRyzen 500016 x Zen 3-TSMC N7MatisseRyzen 300016 x Zen 2-TSMC N7Pinnacle RidgeRyzen 20008 x Zen+-GF 12nmSummit RidgeRyzen 10008 x Zen-GF 14nmHEDT ProcessorsGenesis Peak'4th Gen'Zen 3-?Castle PeakThreadripper 300064 x Zen 2-TSMC N7ColfaxThreadripper 200032 x Zen+-GF 12nmWhitehavenThreadripper 100016 x Zen-GF 14nmServer ProcessorsGenoa'4th Gen'Zen 4-?MilanEPYC 700364 x Zen 3-TSMC N7RomeEPYC 700264 x Zen 2-TSMC N7NaplesEPYC 700132 x Zen-GF 14nmHere is a handy table of processor codenames we might use at various parts of these review. These refer to AMD’s internal codenames for the silicon designs, and act as an easier way to talk about the hardware without constantly referring to the branding (especially if certain silicon is used in multiple product ranges).Testing AMD’s Claims: The NotebookFor this review, AMD supplied the Ryzen 9 5980HS inside theASUS ROG Flow X13 laptop. It is one of AMD’s key design wins, with a 35 W-grade processor in a sleek design aimed for mobility. As a reviewer who in a normal year spends a lot of time travelling, the specifications on the box make a lot of sense to my regular workflow.The system features a 13.4-inch 360º hinged display, which as an IPS touchscreen with a 3840x2400 resolution (16:10, finally) running at 120 Hz with adaptive sync, Pantone color certified, and coated in Corning Gorilla Glass. The display is rated for 116% sRGB, 86% Adobe, and 85% DCI-P3.Under the hood is that AMD Ryzen 9 5980HS processor, with eight Zen 3 cores and sixteen threads, with a 3.0 GHz base frequency and a 4.8 GHz single core turbo frequency, rated at 35 W. ASUS says that they buy the best versions of the 5980HS for the Flow X13 to ensure the best performance and battery life. This processor has Vega 8 graphics, however ASUS has paired it with a GTX 1650 4 GB discrete graphics processor, enabling CUDA acceleration as well as higher performance gaming when needed.Our unit comes with 32 GB of LPDDR4X-4267 memory, as well as a Western Digital SN350 1TB PCIe 3.0 x4 NVMe storage drive. Both of these would appear to be the standard install for the Flow X13.ASUS claims the 62 Wh battery is good for 18 hours of use, and the Flow X13 is one of a handful of devices that supports 100 W USB Type-C power delivery. ASUS claims the bundled charger can charge the unit from 0% to 60% in around 39 minutes.Other features include a back-lit keyboard with consistently sized arrow keys, a full-sized HDMI output as well as a USB 3.2 Gen 2 (10 Gbps) Type-A port, a USB 3.2 Gen 2 (10 Gbps) Type-C ports, a 3.5 mm jack, and a custom PCIe 3.0 x8 output connector for use withASUS’ XG Mobile external graphics dock. This custom graphics dock can come with a custom designed RTX 3070/3080, and along with graphics power also provides the system with four more USB Type-A ports, HDMI/DP outputs, and Ethernet. With this dock installed, technically the system would have three graphics cards.All of this comes in at 2.87 lbs / 1.30 Kg, all for under 16mm thick. This is often a key category for both AMD and Intel when it comes to mobility combined with productivity. ASUS has not announced pricing of the ROG Flow X13 yet – the other model in the range is based on the Ryzen 9 5900 HS, but is otherwise identical.This review is going to be mostly about the processor rather than the Flow X13, due to time constraints (our sample arrived only a few days ago). However, it is worth noting that as with most notebooks, the ROG Flow X13 comes with multiple power and performance modes.In fact, there are two: Silent and Performance. In each mode there are different values for idle temperature, in order to keep the any audible noise lower, and then different values for power/thermals for turbo and sustained power draw.These two differ primarily in the sustained power draw and thermal limits:ASUS ROG Flow X13 Power OptionsAnandTechIdleTemp*PowerInstantTurboPowerTurboTempTurboTimeSustainedSilent70ºC65 W42 W85ºC6 sec15 W @ 68ºCPerformance65ºC65 W42 W85ºC120 sec35 W @ 75ºC*The idle temperature here is so high, as you'll see later in the review, because AMD's high-frequency levers are very aggressive such that our sensor monitoring tools are activating high frequency modes, despite the small load.Testing AMD’s Claims: The Ryzen 9 5980HSSimilar to the launch of Ryzen 4000 Mobile, the unit AMD has supplied us is their top of the line but most efficient H-series processor. For the last generation it was the Ryzen 9 4900HS found in the ASUS ROG Zephyrus G14. The Zephyrus G14 is slightly bigger than the ROG Flow X14 we have today, but the GPU is also better on the G14 (2060 vs 1650). Both processors are rated at 35 W, and both showcase some of the best design AMD wants to lead with at the start of a generation.The main competition for these processors is Intel’s Tiger Lake. A couple of weeks ago Intel announced its new line of H35 processors, whereby they boost the 15 W U-series processors up to 35 W for additional performance. We have no word on when those units will be in the market (we are told soon), however we have managed to securean MSI Prestige 14 Evowhich contains Intel’s best U-series processor (Core i7-1185G7) and allows for sustained performance at 35 W.Comparison PointsAnandTechAMDR9 5980HSAMDR9 4900HSIntel Corei7-1185G7Intel Corei7-1185G7DeviceASUS ROG Flow X13ASUS ROG Zephyrus G14Intel Reference DesignMSI Presige 14 EvoCPUR9 5980HSR9 4900HSi7-1185G7i7-1185G7DRAM32 GBLPDDR4-426716 GBDDR4-320016 GBLPDDR4-426716 GBLPDDR4-4267IGPVega 8Vega 8Iris Xe 64Iris Xe 64dGPUGTX 1650RTX 2060--StorageWD SN3501TBPCIe 3.0 x4Intel 660p1TBPCIe 3.0 x4Samsung1TBPCIe 3.0 x4Phison E16512 GBPCIe 3.0 x4Alongside these numbers we also have historical data fromIntel’s Tiger Lake reference platformwhich ran in 15 W and 28 W modes.But first, let us discuss the new features in Ryzen 5000 Mobile.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article already exists: https://www.anandtech.com/show/16446/amd-ryzen-9-5980hs-cezanne-review-ryzen-5000-mobile-tested\n",
      "Title: Intel Core i7-10700 vs Core i7-10700K Review: Is 65W Comet Lake an Option?\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-01-21T15:30:00Z\n",
      "URL: https://www.anandtech.com/show/16343/intel-core-i710700-vs-core-i710700k-review-is-65w-comet-lake-an-option\n",
      "Content: Over the years, Intel’s consumer processor lineup has featured its usual array of overclocking ‘K’ models, and more recently the ‘F’ series that come without integrated graphics. The bulk of the lineup however are still the versions without a suffix, the ‘nones’, like the Core i7-10700 in this review. These processors sit in the middle of the road, almost always having a 65 W TDP compared to the 91-125 W overclockable models, but also having integrated graphics, unlike the F family. What makes it interesting is when we pair one of these 65 W parts against its 125 W overclocking counterpart, and if the extra base and turbo frequency boost is actually worth the money in an era where motherboards don't seem to care about power?Intel’s Core i7-10700 at 65 W: Is It Really 65 W?The understanding of the way that Intel references its TDP (thermal design point) values has gone through a mini-revolution in the last few years. We have had an almost-decade of quad-core processors at around 90 W and 65 W, and most of them would never reached these numbers even under turbo modes - for example, the Core i5-6600K was rated at 91 W, but peak power draw was only 83 W. This has been the norm for a while, until recently when Intel had to start boosting the core count. As we have slowly gone up in core count, from 4 to 6 to 8 and now 10, these numbers have seemed almost arbitrary for a while.The reason comes down to what TDP really is. In the past, we used to assume that it was the peak power consumption of the processor was its TDP rating – after all, a ‘thermal design point’ of a processor was almost worthless if you didn’t account for the peak power dissipation. What makes Intel’s situation different (or confusing, depending on how you want to call it) is that the company defines its TDP in the context of a 'base' frequency. The TDP will be the maximum power under a sustained workload for which the base frequency is the minimum frequency guarantee. Intel defines a sustained workload one in which the 'turbo budget' has expired, and the processor will achieve its best frequency above base frequency (but not turbo modes) .The point about ‘not turbo’is the key element here. Intel’s TDP ratings are only in effect for the base frequency, not the turbo frequency. If a PC is built with a maximum power dissipation in mind, allowing a processor to turbo above that power might have catastrophic consequences for the thermal performance of that system. The other angle is that Intel never quotes the turbo power levels (also known as Power Level 2, or PL2) alongside the other specifications, although they are technically in the specification documents when they get released.On top of all this, motherboard manufacturers also get a say in how a processor performs. Because turbo power is only an optional suggestion from Intel, technically Intel will accept any value for the ceiling of the turbo power, and accept turbo under any circumstances if the motherboard manufacturer wants it. Motherboard manufacturers overengineer their motherboards to support longer turbo times (or overclocking), and so they will often ignore these Intel recommended values for PL2, allowing the processor to turbo harder for longer, and in a lot of cases of premium motherboards, indefinitely.So why does all this matter with respect for this review? Well my key comparison in this review is our new processor, the Core i7-10700, up against its overclocking counterpart, the Core i7-10700K. Aside from the suffix difference, the K variant has a TDP almost twice as high, and this manifests almost entirely in the base frequency difference.Intel SKU vs SKU(an homage to Spy vs Spy)Intel Corei7-10700KAnandTechIntel Corei7-107008 C / 16 TCores / Threads8 C / 16 T3.8 GHzBase Frequency2.9 GHz5.1 GHzPeak Turbo (1-2C)4.8 GHz4.7 GHzAll-Core Turbo4.6 GHz2 x DDR4-2933Up to 128 GBDRAM Support2 x DDR4-2933Up to 128 GB125 WTDP / PL165 WIntel UHD 630Integrated GraphicsIntel UHD 630$374Price (1ku)$323Even though the TDP is 125 W vs 65 W, the peak turbo frequency difference is only +300 MHz, and the all-core turbo difference is only +100 MHz. In contrast, the base frequency difference is +900 MHz, and that is ultimately what the user is paying for. But this base frequency only matters if the motherboard bothers to put a cap on turbo budgets.The base frequency is more of a minimum guaranteed frequency, than an absolute 'this is what you will get' value under a sustained workload. Intel likes to state that the base frequency is the guarantee, however if a processor can achieve a higher frequency while power limited, it will - if it can achieve that power value with 200 MHz above base frequency, it will run at the higher frequency. If this sounds familiar, this is how all AMD Ryzen processors work, however Intel only implements it when turbo is no longer available. This ends up being very processor dependent.For the turbo, as mentioned, Intel has recommendations for power levels and turbo time in its documentation, however OEMs and motherboard manufacturers are free to routinely ignore it. This is no more obvious than when comparing these two processors. What does this mean for end-users? Well, graphs like this.First time I saw these numbers, it shocked me. Why is this cheaper, and supposedly less powerful version of this silicon running at a higher turbo power in a standard off-the-shelf Intel Z490 motherboard?Welcome to our review. There’s going to be a lot of discussion on the page where we talk about power, frequency, and the quality of the silicon. Also when it comes to benchmarking, because if we were to take an extreme view of everything, then benchmarking is pointless and I'm out of a job.The MarketThe Core i7-10700 and Core i7-10700K are both members of Intel’s 10thGeneration ‘Comet Lake’ Core i7 family. This means they are based on Intel’s latest 14nm process variant (14+++, we think, Intel stopped telling us outright), but are essentially power and frequency optimized versions of Intel’s 6thGeneration Skylake Core, except we get eight cores rather than four.Intel 10th Gen Comet LakeCore i9 and Core i7AnandTechCoresBaseFreqTB22CTB2nTTB32CTVB2CTVBnTTDP(W)IGPMSRP1kuCore i9i9-10900K10C/20T3.75.14.85.25.34.9125630$488i9-10900KF10C/20T3.75.14.85.25.34.9125-$472i9-1090010C/20T2.85.04.55.15.24.665630$439i9-10900F10C/20T2.85.04.55.15.24.665-$422i9-10900T10C/20T1.94.53.74.6--35630$439i9-10850K10C/20T3.65.04.75.15.24.8125630$453Core i7i7-10700K8C/16T3.85.04.75.1--125630$374i7-10700KF8C/16T3.85.04.75.1--125-$349i7-107008C/16T2.94.74.64.8--65630$323i7-10700F8C/16T2.94.74.64.8--65-$298i7-10700T8C/16T2.04.43.74.5--35630$325T = Low PowerF = No Integrated GraphicsK = OverclockableTB2/TB3 = Intel Turbo Boost 2 (any core in CPU), TB3 (specific core in CPU)TVB = Thermal Velocity Boost (Spec = 70ºC); routinely ignored by motherboard vendorsBoth CPUs are rated to run dual channel memory at DDR4-2933 speeds, and have 16 PCIe 3.0 lanes with support for Intel 400-series chipsets. These are socket LGA1200 processors, and are incompatible with other LGA115x motherboards.Aside from the power and frequency differences, the other one is the price: $335 MSRP for the Core i7-10700, and $387 MSRP for the Core i7-10700K. This is a +$52 difference, which is designed to enable better frequencies and overclocking on the K processor. The non-K processor may be shipped with Intel’s 65 W PCG-2015C thermal solution, depending on location, although the first thing you would want to do is to buy something/anything else to cool the processor with given that it'll peak at 215W in enthusiast systems.On the competing side from AMD, the nearest solution is the Ryzen 5 5600X, a 65W version of Zen 3 with two fewer cores but higher IPC, with an MSRP of $300. This does come with a reasonably good default cooler. Our full review of the Ryzen 5 5600X can be foundhere.This ReviewThe goal of this review was initially just to benchmark the Core i7-10700 and see where it fits into the market. As our testing results came into focus, it was clear that we had an interesting comparison on our hands against the Core i7-10700K, which we have also tested. In this review the focus will be on the difference between the two, focusing primarily on where the i7-10700 lands compared to the competition, and perhaps some of the complexities involved.Test SetupAs per our processor testing policy, we take a premium category motherboard suitable for the socket, and equip the system with a suitable amount of memory running at the manufacturer's maximum supported frequency. This is also typically run at JEDEC subtimings where possible. It is noted that some users are not keen on this policy, stating that sometimes the maximum supported frequency is quite low, or faster memory is available at a similar price, or that the JEDEC speeds can be prohibitive for performance. While these comments make sense, ultimately very few users apply memory profiles (either XMP or other) as they require interaction with the BIOS, and most users will fall back on JEDEC supported speeds - this includes home users as well as industry who might want to shave off a cent or two from the cost or stay within the margins set by the manufacturer.Test SetupIntel LGA1200Core i9-10900KCore i9-10850KCore i7-10700KCore i7-10700ASRock Z490PG VelocitaBIOSP1.50TRUECopper+ SST*Corsair DomRGB4x8 GBDDR4-2933AMD AM4Ryzen 9 5900XRyzen 7 5800XRyzen 5 5600XMSI MEGX570 Godlike1.B3T13NoctuaNHU-12SSE-AM4ADATA2x32 GBDDR4-3200GPUSapphire RX 460 2GB (CPU Tests)NVIDIA RTX 2080 Ti FE (Gaming Tests)PSUCorsair AX860iCorsair AX1200iSilverstone SST-ST1000-PSSDCrucial MX500 2TB*TRUE Copper used with Silverstone SST-FHP141-VF 173 CFM fans. Nice and loud.Many thanks to...We must thank the following companies for kindly providing hardware for our multiple test beds. Some of this hardware is not in this test bed specifically, but is used in other testing.Hardware Providers for CPU and Motherboard ReviewsSapphireRX 460 NitroNVIDIARTX 2080 TiCrucial SSDsCorsair PSUsG.Skill DDR4ADATA DDR4SilverstoneCoolersNoctuaCoolersA big thanks to ADATA for the ​AD4U3200716G22-SGN modulesfor this review. They're currently the backbone of our AMD testing.Users interested in the details of our current CPU benchmark suite can refer toour #CPUOverload articlewhich covers the topics of benchmark automation as well as what our suite runs and why. We also benchmark much more data than is shown in a typical review, all of which you can see in our benchmark database. We call it ‘Bench’, and there’s also a link on the top of the website in case you need it for processor comparison in the future.If anyone is wondering why I've written the SKU of the processor on it with a sharpie, as per our lead image, it's because when you're shuffling through a box of them in low light, what is printed on the headspreader can be difficult to read if the light isn't right. With a perminent marker, it makes it much easier to read at-a-glance.Read on for our full review.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16343/intel-core-i710700-vs-core-i710700k-review-is-65w-comet-lake-an-option\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: MediaTek Announces Dimensity 1100 & 1200 SoCs: A78 on 6nm\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-01-20T11:30:00Z\n",
      "URL: https://www.anandtech.com/show/16436/mediatek-announces-dimensity-1100-1200-socs-a78-on-6nm\n",
      "Content: Today MediaTek announced two new top-end SoCs in the form of the new Dimensity 1100 and Dimensity 1200. The two new designs are a follow-up to last year’s Dimensity 1000 SoC which marked the company’s return to the high-end in 2020, with a relatively solid SoC design.The two new chipsets upgrade the Dimensity 1000 in terms of CPU configuration and camera ability, as well as coming in a new 6nm process node. The new chips however don’t upgrade every aspect of their designs, as things such as the GPU configuration and the modem capabilities seem to be identical to that of the Dimensity 1000.MediaTek SoCsSoCDimensity 1000(+)Dimensity 1100Dimensity 1200CPU4x Cortex A77 @ 2.6GHz4x Cortex A55 @ 2.0GHz4x Cortex A78 @ 2.6GHz4x Cortex A55 @ 2.0GHz1x Cortex A78 @ 3.0GHz3x Cortex A78 @ 2.6GHz4x Cortex A55 @ 2.0GHzGPUMali-G77MP9 @ ? MHzMali-G77MP9 @ ? MHzMali-G77MP9 @ ? MHzAPU / NPU / AI Proc. / Neural IP\"3rd gen APU\"2 \"big\" + 3 \"small\" + 1 \"tiny\"4.5TOPs total perf\"3rd gen APU\"2 \"big\" + 3 \"small\" + 1 \"tiny\"\"3rd gen APU\"2 \"big\" + 3 \"small\" + 1 \"tiny\"+12.5% perfMemory4x 16b LPDDR4x4x 16b LPDDR4x4x 16b LPDDR4xISP/Camera80MPor32MP + 16MP108MPor32MP + 16MP200MPor32MP + 16MPEncode/Decode2160p60H.264 & HEVC& AV1(Decode)Integrated Modem5G Sub-6DL = 4700Mbps200MHz 2CA, 256-QAM,4x4 MIMOUL = 2500Mbps200MHz 2CA, 256-QAM,2x2 MIMOLTE Category 19 DLConnectivityWiFi 6 (802.11ax)+ Bluetooth 5.1+ Dual Band GNSSWiFi 6 (802.11ax)+ Bluetooth 5.2+ Dual Band GNSSMfc. Process7nm (N7)6nm (N6)Starting off where we see the biggest changes, the two new SoCs differ the most from its predecessor on the CPU side of things. The Dimensity 1100 is quite straightforward in that it replaces the Cortex-A77 cores with newer Cortex-A78 cores. The configuration still remains at 4 cores running at 2.6GHz (as the higher-bin Dimensity 1000+), with four Cortex-A55 cores at 2.0GHz as the little cores.The Dimensity 1200 changes the configuration to a 1+3+4 setup, with one performance Cortex-A78 core clocking up to 3.0GHz, and MediaTek stating that it sports double the L2 cache compared to the other cores. This would mean it has a 512KB configuration while the other 3 cores feature 256KB L2’s. The 1+3 big cores setup is also accompanied by 4x Cortex-A55 cores at 2GHz.The GPU side of things is a bit weird, as oddly enough it features the exact same setup as the Dimensity 1000, and we again see a Mali-G77MP9 configuration on both new chipsets. That’s very odd given the fact that other competitor chipsets are deploying new Mali-G78 designs.DRAM capability remains at LPDDR4X at 2133MHz – the lack of LPDDR5 isn’t too surprising here given the performance gains aren’t too great and these chipset designs are to be used in more cost-effective devices.NPU and AI capabilities aren’t exactly clear on the Dimensity 1100 and it seems to be identical to that of the Dimensity 1000 – the Dimensity 1200 advertises a performance boost of 12.5% which could just be a slight clock frequency upgrade compared to its predecessor.On the camera capabilities, the new chips don’t change their multi-camera configurations, still being at 32+16MP, however both increase the single-camera advertised capability up to respectively 108MP and 200MP for the Dimensity 1100 and Dimensity 1200.On the 5G modem side, we’re seeing no apparent changes in the specifications supported by the new design, it still being a sub-6GHz only implementation.The new chip is manufactured on a new 6nm process node at TSMC – a design-rule compatible shrink compared to the 7nm node that was used in the Dimensity 1000.Both Dimensity 1100 are Dimensity 1200 are rather odd SoC designs in that they are rather small upgrades compared to their predecessor – in fact looking at them one would think they are just minor refreshes on the ground-up redesign of their predecessor. It’s also very odd to see the very small feature disparity between the 1100 and 1200, even though these should be different chip designs and tape-outs given their CPU differences. The fact that MediaTek hasn’t upgraded the NPU/APU or GPUs in any substantial fashion also point out these would be rather small design upgrades to the previous generation. That’s not really a negative in itself, but it begs the question what MediaTek’s market plan for the new SoCs are.MediaTek quotes vendors such as Xiaomi, Vivo, OPPO and realme as expressing support for the new SoCs, with devices using both SoCs being expected to be available at the end of Q1 and beginning of Q2 this year.Related Reading:MediaTek Announces Dimensity 1000+ SoCMediaTek Announces Dimensity 1000 SoC: Back To The High-End With 5GOPPO's Reno3 5G vs Reno3 Pro vs Reno3 Pro 5G: Why Don't We See More MediaTek Dimensity 1000 Phones?Samsung Announces Exynos 1080 - 5nm Premium-Range SoC with A78 CoresArm's New Cortex-A78 and Cortex-X1 Microarchitectures: An Efficiency and Performance Divergence\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16436/mediatek-announces-dimensity-1100-1200-socs-a78-on-6nm\n",
      "Title: Samsung Announces Galaxy S21, S21+ & S21 Ultra: Cheaper Baseline, Higher High-End\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-01-14T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/16414/samsung-announces-galaxy-s21-s21-s21-ultra\n",
      "Content: Today, Samsung is taking the virtual stage to release the new Galaxy S21 series of devices, including the Galaxy S21, S21+ and S21 Ultra. In a time where smartphones become more and more expensive, Samsung is attempting a different approach this year, and instead of increasing device prices, actually reduces them – at least for the baseline S21 and S21+ models. The move is an interesting one, however very much dilutes the Galaxy S series as a flagship line-up of devices, with Samsung doing some feature compromises that wouldn’t had been thinkable in past iterations of the series.Meanwhile, the Galaxy S21 Ultra continues to be Samsung’s focus point in delivering true no-compromises flagship experiences and features, with this year’s iteration aiming for more capabilities than ever before, including a new quad-camera setup, the only device in the line-up now featuring a 1440p screen at 120Hz, and for the first time now also offering S-Pen support as an external accessory. Naturally, this continues to come at a very high(er) end cost point, creating a larger discrepancy within the line-up than ever before.Samsung Galaxy S21 SeriesGalaxy S21Galaxy S21+Galaxy S21 UltraSoCQualcomm Snapdragon 8881xCortex-X1@ 2.84GHz 1x1024KB pL23xCortex-A78@ 2.42GHz 3x512KB pL24x Cortex-A55@ 1.80GHz 4x128KB pL24MB sL3Samsung LSI Exynos 21001xCortex-X1@2.9GHz? pL23xCortex-A78@ 2.8GHz ? pL24x Cortex A55 @ 2.2GHz ? pL2?MB sL3Display6.2-inch2400 x 1080 (20:9)48-120Hz1300nits peak6.7-inch2400 x 1080 (20:9)48-120Hz1300nits peak6.8-inch3200 x 1440 (20:9)10-120Hz1500nits peakSAMOLEDHDR10+120Hz Refresh RateDimensions151.7 x71.2 x 7.9mm171g (mmWave),169g (sub6)161.5 x75.6 x 7.8mm202g (mmWave),200g (sub6)165.1 x75.6 x 8.9mm229g (mmWave),227g (sub6)RAM8GB8GB12 / 16GBNANDStorage128, 256GB128, 256 GB128, 256, 512 GBBattery4000mAh(15.4Wh) typ.3880mAh (15.01Wh) rated4800mAh(18.57Wh) typ.4660mAh (18.03Wh) rated5000mAh(19.25Wh) typ.4855mAh (18.87Wh) ratedFront Camera10MP4K video recordingF/2.2, 80-degree40MP4K video recordingF/2.2, 80-degreePrimary Rear Camera79° Wide Angle12MP 1.8µm Dual Pixel PDAF79° Wide Angle108MP 0.8µm DP-PDAF3x3 Pixel Binning to 12MP8K24 Video Recordingfixed f/1.8 opticsOIS, auto HDR, LED flash4K60, 1080p240, 720p960 high-speed recordingSecondaryRear Camera76° Wide Angle(Cropping / digital zooming telephoto)64MP0.8µmf/2.0 optics, OIS8K24 Video Recording12° Telephoto(10x optical)10MPf/4.9 prism optics, OISTertiaryRear Camera-3x optical10MPf/2.4QuartenaryRear Camera120° Ultra-Wide Angle12MP 1.4µm f/2.2ExtraCamera-Time of Flight (ToF) 3D Sensor4G / 5GModemSnapdragon 5GTBCExynos 5GTBCSIM SizeNanoSIM + eSIMWireless802.11a/b/g/n/ac/ax2x2 MU-MIMO,BT 5.1 LE, NFC, GPS/Glonass/Galileo/BDS+ WiFi 6EConnectivityUSB Type-Cno 3.5mm headsetSpecial FeaturesUnder-screen ultrasonic fingerprint sensor(Qualcomm QC 2.0, Adaptive Fast Charging, USB 3.0 PD PPS),reverse wireless charging (WPC & PMA),Ultra WidebandIP68 water resistanceLaunch OSAndroid 11 with Samsung OneUI 3.1Launch Prices128GB 5G:$799 / €849 / £769256GB 5G:$849 / €899 /£819128GB 5G:$999 / €1049 /£949256GB 5G:$1049 / €1099/£999128GB 5G:$1199 / €1249 /£1149256GB 5G:$1249 / €1299 /£1199512GB 5G:$1379 / €1429/£1329Starting off with the heart of the new phones, that’s one area where the new S21 series don’t differ – the trio of devices are either powered by Qualcomm’s new Snapdragon 888, or Samsung LSI’s Exynos 2100. Both chipsets are characterised by being among the first Samsung 5nm manufactured SoCs in the market, and both feature Arm’s newest Cortex-X1 performance core at either 2.84GHz or 2.9GHz. The Exynos chip has slightly higher frequencies on its middle and little cores, and should give it a small multi-threaded performance advantage. Both promise large GPU performance upgrades over the S20 SoC generations, and both chips are now integrated with an internal 5G modem instead of an external discrete one.Unlike past years, the SoC situation on the Galaxy S21 series this year should be significantly more balanced as both the Snapdragon 888 and Exynos 2100 share a lot more commonalities, featuring the same CPU IP, being manufactured on the same process node, and also quoting similar performance figures when it comes to things like NPU and GPU performance, which is good news for users out there which in the past might have seen inferior device experiences with particularly the Exynos variants of Galaxy phones.In terms of DRAM, the S21 and S21+ come with 8GB. This is actually a downgrade compared to the S20 series as the 5G variants there had been configured with 12GB. The S21 Ultra remains an option of either 12 or 16GB variants, depending on the storage size, with the 16GB option only available for the 512GB variant.The S21 and S21+ come in either 128 or 256GB storage, while the S21 Ultra also has a 512GB option. What’s controversial about this generation’s devices is the removal of the microSD slot. While it’s true that microSD’s have become quite outdated and the industry has failed to transition to a newer standard, Samsung here is simply removing the option without either giving an alternative or increasing the base storage of the phones.Galaxy S21 Ultra - The new \"True\" FlagshipStarting off with the Galaxy S21 Ultra, Samsung here is equipping the new mega-flagship device with its best display ever, basing off on the work of the brand new panel technology that was introduced inthe Galaxy Note20 Ultra which uses a new hybrid oxide backplane. The new display is at the expected 3200 x 1440 resolution, however unlike last year’s devices, Samsung has now finally enabled the full 120Hz operation at this native software rendering resolution as well – joining in the rare few devices capable of this, such as the OnePlus 8 Pro. The usage of the new panel technology and VRR should allow the S21 Ultra to offer this resolution and refresh rate without too many compromises in battery life. Last year we saw that the Note20 Ultra’s screen VRR feature depended on ambient brightness, with it not working in dim situations – it’s to be seen if the S21 Ultra has a similar limitation or if Samsung’s engineers have been able to optimise this limitation away.One big feature addition of the S21 Ultra is the addition of a Wacom digitiser in the screen, a feature that to date had always been reserved for the Note series. Unlike the Note series, the S-Pen isn’t integrated into the phone, but is rather an external accessory.Design-wise, the front of the Galaxy S21 Ultra looks to be quite similar to that of the S20 Ultra, however Samsung has actually slightly reduced the footprint of the phone as the screen is now 6.8” instead of 6.9”.The new Galaxy S21 Ultra is actually slightly smaller than the S20 Ultra, being 0.4mm narrower and 1.8mm shorter, although it’s now also 0.1mm thicker. It’s still a large phone, and actually it’s now 7 or 9g heavier than its predecessor, weighing up to 229g in the mmWave variant.On the back of the phone, we’re seeing a more significant redesign of the devices, particularly around the cameras. The Galaxy S20 Ultra’s camera bump really wasn’t one of pretty aesthetics, with more of a feel towards functionality – making it feel out of place and stand out in context of the rest of the phone design. The Note20 Ultra had slightly improved upon this with a somewhat rearranged design, however the new S21 series and the new S21 Ultra really showcase a large design shift in the camera bump.The “bump” is actually now a flowing with the frame of the phone (though it’s still a separate piece), starting off at the edges and corner of the phone, instead it being a standalone design element raised within the back glass panel. With this design, Samsung gets rid of two sharper edges on the part of the camera bump. Instead of a larger glass cover over the camera assembly, the individual cameras seem to have their own smaller glass covers, recessed against the metal frame bump.It’s to be noted that this camera frame looks to be flat, in contrast to past generation devices which have a very small metal lip upon which the phone rests on. I don’t know how this will play out in terms of scratches – usually the lip itself would get damaged whereas here it could be that the whole surface would be more prone to scratches.New Quad-Camera SetupOn the camera side of the S21 Ultra, we’re now seeing a move towards a quad-camera setup, with Samsung changing up the telephoto camera formula yet again.First of all, the new main camera seems to be of similar specifications as on the S20 Ultra and Note20 Ultra: a 108MP sensor within an f/1.8 optics system with OIS. It’s not clear if this is the same generation sensor as last year of if Samsung has been able to upgrade it.Samsung here still promises 8K video recording and even now adds 12-bit RAW video recording capabilities – however if this still uses a sensor crop mode, it’ll still result in a not very optimal resulting focal-length compared to the native optics of the module.On the ultra-wide, we’re seeing the same 12MP f/1.4 f/2.2 120° module as on the Galaxy S20 series – which we really can’t complain about as it was a great module and amongst the best performers of the year.Where Samsung has made larger changes now is in the telephoto department. I really was not a big fan of the S20 Ultra’s telephoto module here which had offered a 4x optical magnification. On the S21 Ultra, Samsung actually increased this magnification ability to 10x through a redesigned prism optics module. The longer focal length comes at a cost of a decrease in aperture from f/3.5 to f/4.9, as well as a move from a 48MP sensor to a newer smaller 10MP unit. This means that although the S21 Ultra will have greater optical telephoto capabilities, it’ll come at a cost of light capture ability.Unlike the S20 Ultra and the Note20 Ultra however, Samsung now makes up for the compromises in the new prism telephoto module by adding an additional secondary 3x optical module with f/2.4 aperture and a 10MP sensor. Essentially, this solves the great gap in optical magnification between the main module and the new further reaching telephoto module, and also hopefully solves the quality gap in this common focal range that I consider to be more important for every-day photography.Galaxy S21 & S21+ - Cheaper, but Ugly CompromisesWhile the Galaxy S21 Ultra looks to be a great device, checking off every single feature you’d expect in a 2021 flagship phone, the new Galaxy S21 and S21+ are more controversial devices, really questioning Samsung’s product line-up as a whole.First of all, the biggest feature disparity of the new S21 “baseline” models is that they are not getting the same top-of-the-line displays as the new S21 Ultra. In fact, the new displays are seemingly somewhat of a downgrade compared to even last year’s Galaxy S20 series. Samsung here has decided to downgrade the panels to 2400 x 1080 resolution, still maintaining a 120Hz refresh rate.I find this choice actually extremely negative, and I see it as Samsung shooting themselves in the foot in terms of their device line-up and essentially discarding a very important aspect of what a Galaxy S-series device is. For the past decade, no matter what generation Galaxy S-phone you look at, the one thing that was always the key differentiator compared to other device models and competitor models is that the S-line phones always had no-compromise displays. Samsung used to actually take pride in this fact, and consumers could always count that each generation would be the able to achieve the best of what’s technically possible in a phone display.Starting last year with the Note20, and unfortunately continued this year with the S21 and S21+, this doesn’t seem to be the case anymore, and essentially Samsung is diluting a key aspect of what their flagship devices were always known for.I understand that Samsung is trying to price the devices lower, but it’s just not possible that an S21+ comes with a 1080p screen when for example a cheaper new Xiaomi Mi 11 is able to feature a 1440p 120Hz display.Other design differences to last year’s models is the fact that the displays now are “flatter”, with almost no side-curvature at all. This is actually mostly a positive for a majority of users, as after several years of side curvature experiments, it seems that most people prefer functionality. Personally, I liked the subdued curvature of the S20 series phones – I’ll have to see how the S21 feel in hand before arrive to a proper opinion on the new devices.The new S21 and S21+ phones are wider than their predecessors, with the S21 growing by 2.1mm in width, and the S21+ being 1.9mm wider. This actually has large implications on the usability and the general form-factor of the phones. The new S21+ actually has yet again gotten heavier this generation, weighing in at 200/202g. The extra weight is somewhat explained by the larger battery capacity which has grown from 4500 to 4800mAh, but it’s still a quite large departure compared to past years – the Galaxy S10+ only weighed 175g.On the back of the phones, the new camera setup looks quite nice and follows the same design elements we described earlier for the S21 Ultra. Although at first glance it looks like the camera bump flows with the frame here as well, it’s actually constructed differently to the S21 Ultra, and there’s an extra in-between element between the frame and the camera cover element. Undoubtedly this would be part of a cost-cutting measure to reduce the manufacturing cost of the piece.In regards to the camera modules themselves, it’s relatively straightforward: They’re the same as on the Galaxy S20 series. This includes a main 12MP camera sensor with large 1.8µm pixels with f/1.8 aperture and OIS, a secondary wide-angle module with a 64MP camera sensor which serves both as the 8K capture module as well as a “telephoto” module that is able to crop-magnify quite well thanks to its massive native resolution, as well as the same 12MP f/2.4 120° ultra-wide-angle module as on the S20 series and the S21 Ultra.I’m actually quite happy with this camera setup and I think it was one of the best and most well-balanced configurations in 2020, so reusing it on the S21 series is a general positive in my view.One change in the new design is the move of the flash to the phone back panel, as well as the removal of the ToF sensor on the S21+ (The S20 never had it).Too Many Compromises?While the S21 Ultra seems like a proper follow-up to last year’s models, the new S21 and S21+ feel like they’re doing a lot of compromises for the price drop that they are offering.Particularly the Note20’s “glasstic” back was a point of contention alongside the 1080p display panel – and Samsung here is doing the very same choices for the S21, partly also to the S21+. Furthermore, there’s other feature compromises this generation such as the removal of the microSD slot, and also long-term component feature discrepancies such as the fact that new WiFi 6E connectivity is solely reserved for the Galaxy S21 Ultra.While we’ll have to see how the S21 series are being received,I’ve made a poll today on Twitterregarding the Note20, and how users perceived that device, whether it was a good compromise, or not fulfilling users’ expectations:Unfortunately, even many months after the release of the Note20, the general response to the device is massively one-side and very much negative, which pretty much lines up with my initial impressions of the phone.Samsung’s choice to apply the same design and feature compromises to the S21 series is I feel like a massive mistake, and Samsung crippling its own device line-up. I don’t know what they’re attempting here – maybe they’re trying to redirect users towards the more complete and expensive Ultra models.If that is indeed what they’re trying to do, I feel like they’re going about it the wrong way. The S21 and S21+ look like fine devices, but still too expensive for what they are, especially since there’s now very major feature overlap with the cheaper phones, even within Samsung’s own series, such as the Galaxy S20 FE. Besides the SoC, I actually feel like the S21 and S21+ are actual downgrades this year compared to the S20 and S20+.Apple’s line-up of iPhone 12 mini, iPhone 12, iPhone 12 Pro, and iPhone 12 Pro Max makes more sense to me to cater to the wider range of users. For the S21 series, I feel like the S21 Ultra should have been called the “S21 Ultra+” and we’re missing a smaller, slightly cheaper, maybe without the 10x telephoto, but otherwise feature identical S21 Ultra device.$200 Cheaper Sounds Good ThoughThere are no such thing as bad devices, only bad prices. In this regard, Samsung’s S21 and S21+ come in at $200 cheaper compared to their predecessors. In that regard, it might actually be a winner combination, with the Galaxy S21 for example starting at $799. In the US, that’s actually only $100 more than a Pixel 5 while offering a vastly superior device in every regard.The S21+ also goes down from $1199 to $999, so it’s a also a relatively good compromise if you can live with the FHD+ screen.Meanwhile, the S21 Ultra is also starting $200 cheaper, at $1199 compared to the initial launch price of $1399 of the S20 Ultra. Given that the S21 Ultra hasn’t seen any feature compromises, you could argue that we’re seeing much better value generation-on-generation compared to the S21 and S21+.Samsung is also posting some reasonable prices for the upgrade from the 128GB to the 256GB models, with the upgrade costing only $/€/£50 – a much better upgrade path than past generations where the jump usually cost 100 more of your bills.These are generally sensible prices, and I think the base S21 and the S21 Ultra seem like they’ll be the most popular devices among the line-up. The S21+ however feels like it’s a bit off the mark, and we might see better options from the competition, particularly now that the + variant is losing its size and weight sweet-spot.In the US, the new Galaxy S21 series will be available for shipping January 27thwith preorders starting today.Related Reading:The Samsung Galaxy S20+, S20 Ultra Exynos & Snapdragon Review: Megalomania DevicesSamsung Announces The Galaxy S20, S20+ and S20 Ultra: 120Hz, 5G, Huge Batteries, Crazy Cameras and $$$Samsung Announces Exynos 2100 SoC: A New Restart on 5nm with X1 CoresQualcomm Details The Snapdragon 888: 3rd Gen 5G & Cortex-X1 on 5nm\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16414/samsung-announces-galaxy-s21-s21-s21-ultra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qualcomm to Acquire NUVIA: A CPU Magnitude Shift\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-01-13T13:30:00Z\n",
      "URL: https://www.anandtech.com/show/16416/qualcomm-to-acquire-nuvia-a-cpu-magnitude-shift\n",
      "Content: Today Qualcomm has announced they will be acquiring NUVIA for $1.4bn – acquiring the start-up company consisting of industry veterans which originally were behind the creation of Apple’s high-performance CPU cores. The transaction has important ramifications for Qualcomm’s future in high-performance computing both in mobile, as well as laptop segment, with a possible re-entry into the server market.NUVIA was originally founded in February 2019 andcoming out of stealth-mode in November of that year. The start-up was founded by industry veterans Gerard Williams III, John Bruno and Manu Gulati, having extensive industry experience at Google, Apple, Arm, Broadcom and AMD.Gerard Williams III in particular was the chief architect for over a decade at Apple, having been the lead architect on all of Apple’s CPU designs up to the Lightning core in the A13 – with the newer Apple A14 and Apple M1 Firestorm cores possibly also having been in the pipeline under his direction.NUVIA had been able to recruit a lot of top industry talent from various CPU design teams across the industry, and had planned to enter the high-performance computing and enterprise market with a new server SoC with a new CPU core dubbed “Phoenix”.NUVIA particularly had made aggressive claims about how their design would be able to significantly outperform the competition both in raw performance and power efficiency once it came to market – usually such claims are always to be taken with scepticism, however due to the members of the design team and talent having proven themselves in the form of Apple’s very successful CPU microarchitectures, there’s a lot more weight and credibility to them compared to other start-ups.As a new entity in the industry, the company always had an uphill battle against the established giants, so even though they could have had talent and the technology, it’s not a guarantee that they would have been successful in business. I admit that the during the initial company announcement back in 2019 I did think to myself that it would have been possible that the team is looking to get acquired by another big player, which ended up happening today.Qualcomm’s Gain and PossibilitiesQualcomm’s purchase of the whole company for 1.4bn USD can very much be seen as an endorsement to NUVIA’s talent and claims, and could mark an important shift in the industry, vastly expanding the possibilities of the combined entities compared to as if they were separate entities.From Qualcomm’s perspective, it’s a bit of a bitter-sweet deal that follows the company’s failed Centriq business which back in 2018 had taken critical blows and cancellations as the company had to cut costs andlay off significant amount of people amongst their data-center unit.At the time, Qualcomm was still maintaining a custom CPU microarchitecture team for server SoCs, having a few years earlier abandoned their efforts at custom CPUs for mobile, given Arm’s more power efficient and better PPA (Performance, Power, Area) advantages of licensable Cortex cores. Eventually the design teams fizzled out with the years, leaving Qualcomm no longer having the capability to design custom CPU microarchitectures, on top of them also never being all that competitive.Qualcomm now acquiring NUVIA gives them the possibility to take advantage of the start-up’s early work in the server space, possibly reinvigorating the company’s ambitions in the server space, and giving them a second shot at the market. It’s to be noted however that in today’s press release about the acquisition there had been no mention of server or enterprise plans.Furthermore, the move also has larger repercussions in the consumer space, with Qualcomm claiming that NUVIA CPU designs are expected to be deployed in flagship mobile SoCs and next generation laptops, as well as other industrial applications such as digital cockpits and ADAS.In essence, Qualcomm is looking to leverage NUVIA’s CPUs to replace Arm’s current Cortex CPU IP and gain a competitive advantage in terms of performance. This is an important point of the transaction as it means that Qualcomm has confidence that NUVIA’s CPU designs and roadmap would be competitive or exceed that of Arm’s offerings, and put forth the money and investment towards those goals.There’s also two more aspects in Qualcomm’s consideration for the purchase: With Nvidia’splans to acquire Arm Holdings announced last September, this would give Qualcomm an important level of independence and safety in regards to their future product roadmaps – just in case Nvidia would make substantial changes to the CPU IP licensing model.Secondly, Apple’s recent move to ditch x86 in favour of their own Arm-based Apple Silicon SoCs, starting with the new Apple M1 and planning to make a complete product transition in the coming 2 years has greatly pushed the Arm ecosystem forward. While Qualcomm to date has released laptop-specific Snapdragon designs, they still rely on Arm’s Cortex CPU IP and currently cannot compete with Apple’s silicon. In essence, Qualcomm could be viewing this as a large long-term bet at an attempt to establish themselves as the de facto Arm silicon supplier in this market segment, and alternative to Apple Silicon products. NUVIA in the past had commented that this would have been a possible long-term goal beyond their server space focus, however the acquisition by Qualcomm now vastly accelerate any such plans.Press release:SAN DIEGO, Jan. 13, 2021 /PRNewswire/ -- Qualcomm Incorporated (NASDAQ: QCOM) today announced that its subsidiary, Qualcomm Technologies, Inc., has entered into a definitive agreement to acquire NUVIA for approximately $1.4 billion before working capital and other adjustments. The transaction is subject to customary closing conditions, including regulatory approval under the Hart-Scott-Rodino Antitrust Improvements Act of 1976, as amended.NUVIA comprises a proven world-class CPU and technology design team, with industry-leading expertise in high performance processors, Systems on a Chip (SoC) and power management for compute-intensive devices and applications. The addition of NUVIA CPUs to Qualcomm Technologies' already leading mobile graphics processing unit (GPU), AI engine, DSP and dedicated multimedia accelerators will further extend the leadership of Qualcomm Snapdragon platforms, and positions Snapdragon as the preferred platform for the future of connected computing....NUVIA CPUs are expected to be integrated across Qualcomm Technologies' broad portfolio of products, powering flagship smartphones, next-generation laptops, and digital cockpits, as well as Advanced Driver Assistance Systems, extended reality and infrastructure networking solutions....As part of the transaction, NUVIA founders Gerard Williams III, Manu Gulati and John Bruno, and their employees will be joining Qualcomm.\"CPU performance leadership will be critical in defining and delivering on the next era of computing innovation,\" said Gerard Williams CEO of NUVIA. \"The combination of NUVIA and Qualcomm will bring the industry's best engineering talent, technology and resources together to create a new class of high-performance computing platforms that set the bar for our industry. We couldn't be more excited for the opportunities ahead.\"In essence, the acquisition of NUVIA greatly increases Qualcomm’s future prospective in the mobile and consumer laptop market, with possible long-term positive repercussions for the company’s product’s competitiveness. We’re looking forward to see how this plays out over the next coming years.Related Reading:NUVIA: New Server CPU Startup Going After Intel and AMDNUVIA Phoenix Targets +40-50% ST Performance Over Zen 2 for Only 33% the PowerNUVIA Completes Series B Funding Round: $240MQualcomm’s Server Team Loses VP of Technology, Centriq Future Unknown\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16416/qualcomm-to-acquire-nuvia-a-cpu-magnitude-shift\n",
      "Title: AMD CEO Dr. Lisa Su: Interview on 2021 Demand, Supply, Tariffs, Xilinx, and EPYC\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-01-12T19:40:00Z\n",
      "URL: https://www.anandtech.com/show/16409/amd-ceo-dr-lisa-su-interview-on-2021-demand-supply-tariffs-xilinx-and-epyc\n",
      "Content: Following the keynote press conference, AMD invited a number of key press partners for some Q&A time with Dr. Lisa Su. On the table, we were told, was any topic relating to AMD. Given that the company launched a number of products just as the previous year ended, and supply issues are tight for end-users, there were opportunities to quiz the CEO on production demand against supply, AMD’s product cadence, and expectations for 2021.Topics on the front of mind were AMD’s announcements that had just come through the wire – the new Ryzen 5000 Mobile family, featuring an updated processor core, as well as target markets for gaming ultraportables as well as the best gaming notebooks AMD has ever been in. There was also a brief preview of AMD’s interplay in the enterprise market with next-generation Milan processors, and a tie in with the Mercedes AMG Formula 1 team, given that AMD provides a technical partnership.This roundtable was a little different than most. Despite the whole discussion being on the record, AMD requested that we (or any of the press) did not post a full transcript of the call. Traditionally we post these transcripts, mildly edited for clarity and sometimes arranged for consistency – we fought our corner with AMD, and ultimately agreed to disagree. As a result, this Q&A will be more a ‘quote and explain’ situation.Lisa Su: ‘AMD’s focus is about high performance and significant generational improvements; x86 is a strong ecosystem and we continue to invest heavily. Custom designs [like Arm] are in the market, and are purpose built for use. If anything, it is a validation about how much the demand for compute is growing. We see a larger opportunity for customized solutions for specific workloads, and AMD has a strong semi-custom division to meet those opportunities’.My initial question to Lisa was about the emergence of Arm. It’s a question I have asked Lisa before, as well as AMD CTO Mark Papermaster, however this position of the market now is very different. Amazon’s Graviton2 is now being deployed widely over at AWS (read our review), and Ampere’s new 80-core Altra processor (read our review) sits at the top of the stack for single socket HPC performance. Our argument is that these Arm based models are encroaching on the traditional x86 market, and given Arm’s year-on-year expected performance gains from its roadmap ranging from 30% to 50%, if x86 is still focusing purely on high-performance compute, it might stand to be overtaken. Our goal with a question like this is to extract an idea of how AMD (and Intel,I asked a similar thing to CEO Bob Swan) plans to approach the competition, either by building above it, or using a two pronged approach between x86 and Arm. These would be the two high-level approaches we might expect.Lisa’s answer was not particularly committal either way, instead citing the fact that as the industry is growing, it gives more opportunities for specialized products. If anything, according to Lisa, it validates the desire for compute and the agility to build for and target specific workloads. Not to say that we disagree with Lisa here, but the development of the Arm server ecosystem is for broad workloads, not simply specific markets. Lisa states that AMD at any time has its semi-custom division to take on these opportunities, although when asked if AMD has any semi-custom enterprise partners, Lisa cited the strong work the company is doing with the console market and spoke more to potential than a specific entity. No doubt if a company asked AMD to build a Neoverse chip, with some AMD secret sauce, they would.In a similar vein, I approached the topic about Apple and its M1 chip. M1 is a clear building block for the company to move forward into desktops, workstations, and potentially the enterprise market. This has the potential to affect the relationship between Apple and AMD, especially if Apple decides to also branch out its graphics integration for its own solutions.Lisa Su: ‘The M1 is more about how much processing and innovation there is in the market. This is an opportunity to innovate more, both in hardware and software, and it goes beyond the ISA. From our standpoint, there is still innovation in the PC space – we have lots of choices and people can use the same processors in a lot of different environments. We expect to see more specialization as we go forward over the next couple of years, and it enables more differentiation. But Apple continues to work with us as their graphics partner, and we work with them.’A large part of the Q&A discussion was centered around supply and demand, especially given the notable lack of hardware on the shelves for end-users and enthusiasts. At a time when AMD has built out a number of products in a very compressed time frame, including consoles for two partners, it stands to reason that if users cannot buy the newest ‘leading performance hardware’, there are going to be complaints.Lisa Su: ‘This is the result of a demand focused environment, rather than manufacturing issues. There is tightness in the supply chain due to demand, and that invariably puts pressure on our consumer, PC, and gaming product lines. As it relates to our semiconductor production, we’re putting in additional capacity to meet this unexpected demand. It will take time to catch up, but that’s what we’re seeing.’When asked if this high demand environment and capacity ceiling would potentially cap AMD at 22% market share, Lisa said that AMD doesn’t believe that it will. Beyond actually building the silicon, Lisa also noted that there are substrate shortages, simply due to the increased demand, and that the ecosystem is working to also build additional capacity (including AMD investments), but that will take time and continue through 2021.The question is always if and when this level of supply will improve to meet this increase in demand, and if AMD is still producing as many CPUs and GPUs as it can, where they might be going.Lisa Su: ‘We are shipping lots of parts, and volumes in all segments are increasing, and that will happen through 2021. There will be tightness in the first half of the year, but alongside consumers we also ship to OEM partners. There is some real-time prioritization between end-user and OEM, but we understand that consumers want more and it’s very high on our priority list to meet this high demand.’One element Lisa did speak to is how production level decisions happen in advance. AMD launched a lot of products in Q4 2020, including Ryzen 5000, Radeon RX 6000 GPUs, two games consoles, and also started shipping its next generation EPYC Milan processors. On top of all that, AMD had to build its new Ryzen 5000 Mobile processors to meet notebook demand in Q1 2021.Lisa Su: ‘One of the things that was important with Cezanne (Ryzen 5000 Mobile) is that we were shipping for production in early 2021. With the OEM cycle of those products, enabling them to get the first batches of hardware in Q4 was vital for launch through the first quarter and the first half of 2021. This is how AMD matches its product cadence, and the choice on hardware manufacture is one of timing. It’s nothing fundamental, it’s not design limited, it’s all about making the right bets (sometimes months in advance) and enabling market estimation of demand. We had to enable millions of console APUs as well, and there is higher demand than we thought here as well. Working with OEM partners like Sony and Microsoft means enabling different strategies as well.’The topic of pricing was also mentioned, not only due to low supply for end-users but also the expiration of US tariffs on electronics. Combined with this, the role of miners has also seen a sharp demand for some graphics solutions, making the pricing situation a lot worse. Lisa spoke to AMD’s strategy here for keeping prices lower than usual, but explained it’s not an immediately solvable solution.Lisa Su: ‘We knew about the expiration of some tariff policies, and in advance worked towards a more flexible supply chain as it relates to AMD. We are committed to keeping GPU pricing as close to our suggested retail pricing as much as possible, because it’s the only way to be fair to the users. Normally when we have GPU launches, our own branded cards are available initially but then fade away for our partners to pick up. This time around we’re not phasing out our RX 6000 series, enabling us to sell direct to customers as low as possible. We’re encouraging partners to do the same. Not only tariffs, but the COVID environment has increased shipping and freight costs, which are hard to avoid. As we get into a more normal environment, this should improve. This also matters for our planned graphics updates through the first half of the year, as we have a lot of product coming to market.’The topic came up that perhaps current 8-core on mobile, 16-core on desktop, and 64-core on enterprise were fundamental functional limits for these markets, in light of this round of products having the same core counts as the previous generations. Lisa commented that‘There will be more core counts in the future – I would not say those are the limits! It will come as we scale the rest of the system.’To that point, AMD’s design philosophy was a question on table, given that these design teams have a slow flow of personnel in and out of them. Lisa explained that:‘Mark, Mike, and the teams have done a phenomenal job. We are as good as we are with the product today, but with our ambitious roadmaps, we are focusing on Zen 4 and Zen 5 to be extremely competitive. Also on GPUs, David Wang and the team focus on our long term roadmaps, and we pick the right mix of risk to get innovation, performance, and predictability. Bets are made, and we track the progress. We’re happy with RDNA2 on performance per watt, and overall performance, and we have a lot of focus on RDNA3. On elements such as AI specific integration, we are making investments. CDNA launched in November, and you will see us adding more AI capability to our CPUs and GPUs.’On the topic of heterogeneous CPUs, pairing a high-performance core with a high-efficiency core, Lisa’s response was fairly telling, in that ‘our Zen 3 solution already scales very well from entry to enterprise, with the right mix of power, performance, and die area – to enable a heterogeneous design we have to find the right balance of cores, and we would have to see a significant value addition.’Pivoting to more business operations, and the topic of AMD’s acquisition of Xilinx came into conversation. The dealannounced last year for $35 billionin an all-stock acquisition has had its share of encouragement and detractors, questioning if the merger is the right time, the right fit, and if it makes sense to remove competition from the market.Lisa Su: ‘We’re excited about closing the Xilinx deal; it’s the next big step in the story for AMD. There’s going to be a lot of technology going forward, and AMD has made good progress in recent years – we have deep customer partnerships, and we’re building trust to be behind the most important platforms. We want the largest enterprises to trust AMD as their supplier, and the Xilinx acquisition will help this. Xilinx is the right next step – we have the desire to have a much bigger footprint, and Xilinx will help AMD execute on its base business. Xilinx CEO Victor Peng will join us as part of that strategy, and we expect a seamless transition saying laser focused on execution. This is what leaders have to do – we have to expand and scale. AMD can do both’.The last topic was on AMD’s business value, especially as it relates to how AMD integrates with its OEM partners and business customers. Lisa cited that growing the commercial and enterprise businesses were key targets for the company in 2021, especially as these are areas where AMD sees an opportunity for sizeable sustainable growth. Specifically Lisa was asked about how AMD’s strategy is going to develop based on recent improvements made from the competition.Lisa Su: ‘For our Ryzen 5000 Mobile parts, we’ve stated that we will have 150+ design wins in 2021, which is 50% more than what we saw with Ryzen 4000 Mobile. Those numbers include our commercial system deployments, and we’re working hard enabling our security solutions in our Ryzen Pro platform – we already have a number of designs from top OEMs coming later this year. There will be new initiatives around Ryzen Pro as well. On the enterprise side, we are broadening our focus on EPYC, which means to say that we will have business solutions targeted to different vertical markets. This enables a faster time-to-market for those customers to deploy our solutions. We also started shipping Milan to OEM partners in Q4 last year as a lead up to this. More of this will come as we formally launch Milan in Q1.’Round UpIn Q4 last year, AMD launched Ryzen 5000 for desktops, Radeon RX 6000 discrete graphics, was the silicon partner for the two major consoles that have sold in their millions, was shipping out EPYC Milan processors to customers, and was starting to build Ryzen 5000 Mobile processors for its OEMs to be ready for a Q1 launch.In the first half of this year AMD is expecting to launch its 3rdGen EPYC, the next generation of Radeon mobile graphics, more discrete graphics options, and we will see the deployment of Ryzen 5000 Mobile laptops and notebooks into the ecosystem.If we combine Q4 with Q1 (and Q2), this is all at a time when AMD is experiencing higher-than-expected demand for its product line, and it appears it needs to order more from TSMC (as well as develop its substrate supply chain) in order to enable this. An order from TSMC might take a few months to come through, and so even with orders in place, AMD expects a tight supply through the first half of the year, but doesn’t see this as a theoretical ceiling on its market share numbers.For other matters, the company still seems ambivalent about Arm’s potential progress, at least publically to the tech press. As solutions built on Arm come to market, as well as AMD’s own, you can be sure to read about here at AnandTech. But on that, AMD is looking to build out its enterprise offering to more focused verticals, which is certainly of benefit to both AMD and its customers. Some of these may be based in CDNA, AMD’s new compute accelerator architecture, and AMD is looking to increase AI support in future products. The Xilinx acquisition is still a critical business outlay to be considerate of, and is expected to complete by the end of the year.For the longer roadmap, Lisa is clear that the company is working hard on Zen 4, Zen 5, and RDNA3. Future processors seem set to have higher core counts too.We would like to thank Lisa and her team for their time.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16409/amd-ceo-dr-lisa-su-interview-on-2021-demand-supply-tariffs-xilinx-and-epyc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Samsung Announces Exynos 2100 SoC: A New Restart on 5nm with X1 Cores\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2021-01-12T15:30:00Z\n",
      "URL: https://www.anandtech.com/show/16316/samsung-announces-exynos-2100\n",
      "Content: Today, Samsung LSI took the virtual stage to announce the new Exynos 2100 flagship SoC. The new chip is quite special for Samsung’s chip division as it marks a departure from past iteration designs – being the first SoC not using Samsung’s own in-house performance CPU microarchitecture, but instead relying on Arm’s Cortex cores such as the new X1.It’s a big change for Samsung not just technologically, but also in terms of general tone and messaging – with a much more upbeat enthusiasm from the company and also the first ever actually presented launch event for a Samsung mobile SoC, more closely mimicking what we would see from Qualcomm.The new SoC promises some very large technical upgrades – the aforementioned new Cortex-X1 performance CPU, a very large GPU performance boost, but also very large gains in things like AI performance, a re-integrated leading 5G modem, camera support for up to 200MP sensors, AV1 video decoding, all on a new 5nm process node.Samsung Exynos SoCs SpecificationsSoCExynos 990Exynos 2100CPU2xExynos M5@ 2.73GHz2MB sL23MB sL3+2x Cortex-A76 @ 2.50GHz2x 256KB pL24x Cortex-A55 @ 2.00GHz4x 64KB pL21MB sL31x Cortex-X1 @ 2.9GHz3x Cortex-A78 @ 2.8GHz4x Cortex A55 @ 2.2GHz?MB sL3GPUMali G77MP11 @ 800 MHzMaliG78MP14@ ? MHzMemoryControllerLPDDR5 @ 2750MHz2MB System CacheLPDDR5 @ ?MHz?MB System CacheISPSingle: 108MPDual: 24.8MP+24.8MPSingle: 200MPDual: 32MP+32MP(Up to quad simultaneous camera)NPUDual NPU + DSP>10 TOPsTriple NPU + DSP26 TOPsMedia8K30 & 4K120 encode & decodeH.265/HEVC, H.264, VP98K30 & 4K120 encode &8K60 decodeH.265/HEVC, H.264, VP9AV1 DecodeModemExynos ModemExternal(LTE Category 24/22)DL = 3000 Mbps8x20MHz CA, 1024-QAMUL = 422 Mbps?x20MHz CA, 256-QAM(5G NR Sub-6)DL = 5100 Mbps(5G NR mmWave)DL = 7350 MbpsExynos ModemIntegrated(LTE Category24/18)DL =3000 Mbps8x20MHz CA, 1024-QAMUL = 422 Mbps4x20MHz CA, 256-QAM(5G NR Sub-6)DL = 5100 MbpsUL = 1920Mbps(5G NR mmWave)DL = 7350 MbpsUL = 3670 MbpsMfc. ProcessSamsung7nm LPPSamsung5nm LPEStarting off with the big-ticket item today, it surrounds Samsung’s usage of the new Arm Cortex-X1 CPU cores. This follows a 5 year journey that had started off with theExynos 8890back in 2016 and the Galaxy S7 series of devices- with Samsung using their own designed CPU microarchitecture dubbed the M1 through to the M5 CPU inlast year’s Exynos 990used in the Galaxy S20 series.Unfortunately, Samsung’s own designs were never really successful, and actually brought the opposite of what the company had hoped for – instead of positive differentiation in the SoC market the usage of custom cores was actually more of a negative, bringing with them reduced performance and quite worse power efficiency compared to the Arm Cortex counterparts. While Samsung had been patient with the SARC CPU design team, too many failures in a row, particularly big blunders such asthe Exynos 9810and theExynos 990resulted in thetermination of the project, with Samsung now choosing to simply just use Arm Cortex cores.In last year’sCortex-X1 announcement from Arm, Samsung was actually one of the leading partners which publicly acknowledged their use of the new high-performance CPU IP, and today’s announcement of the Exynos 2100 unveils the fruits of this collaboration.The new Exynos 2100 follows a CPU configuration that had been first introduced by Qualcomm in the Snapdragon 855 in that it’s a 1+3+4 CPU design, featuring one new high-performance Cortex-X1 core clocking up to 2.9GHz, three Cortex-A78 cores up to 2.8GHz, and four Cortex-A55 cores at up to 2.2GHz.What’s interesting here about this setup is the clock frequencies, and how they contrast toQualcomm’s new Snapdragon 888which was released a few weeks ago and is starting to ship in thefirst new flagship devices of 2021.Particularly on the middle and on the efficiency cores, the Exynos chip shows a 400MHz advantage in clock frequencies over the Snapdragon chip, which should result in a notable advantage when it comes to multi-threaded CPU workloads.On the part of the X1 cores at 2.9GHz – well it’s a shame we again just missed the mark on the symbolic 3GHz mark this generation, but it’s still 60MHz higher than the Snapdragon counterpart so it should result in a small performance boost, although I doubt it’ll be noticeable.Samsung actually did have some performance figures shown during the presentation, including the expected 33% multi-threaded performance boost thanks to the now much higher performance middle cores.I was actually delighted to also see Samsung post a separate 19% single-threaded performance figure – however this number is a bit odd to me as the jump from a 2.7GHz Exynos M5 to a 2.9GHz Cortex-X1 should be larger than that. I don’t know what Samsung uses in terms of coming to these figures, but I’m optimistic that things could end up higher than just 19%.As part of the launch event presentation Samsung also claimed to have included “better cache memory” – this could be a reference to a possible 8MB L3 cache in the CPU cluster, but could also be related to a better system-cache structure. They had also made mention of a more efficient voltage control implementation – what exactly this means in practice is open to interpretation but one area Samsung could differentiate itself from the Snapdragon 888 is to give the Cortex-X1 cores their own dedicated voltage rail. We’ll have to see how the actual SoC behaves as Samsung doesn’t go into such technical depth for their chip launches.A 40% GPU Performance Uplift?!On the GPU side, we’ve seen the move from a Mali-G77MP11 at up to 800MHz to a new Mali-G78MP14 at an undisclosed frequency. That’s generally an expected implementation boost in terms of number of cores – 27% more cores to be exact.What’s interesting here is the fact that Samsung is proclaiming a massive 40% better graphics performance figure. This actually really gives me pause and I question as to how Samsung has achieved such a figure.The Mali-G78 was meant to bea rather small generational upgrade with only 10% better perf/W improvements. The process node of the Exynos 2100 is advertised as bringing a 20% power efficiency improvement. In theory and on paper, the new process and GPU IP should results in a 32% better perf/W improvement but it’s very rare that such theory-crafting actually results in accurate estimates for actual silicon designs, particularly on the GPU side.So while 40% performance improvement sounds fantastic, it’ll all depend on how much power and how efficient the new Exynos 2100 is – and how well it can sustain those performance figures in prolonged gaming sessions.A More Powerful NPUAnother big upgrade is found on the part of the new generation NPU. Samsung has evolved their IP from 2-core design to a new 3-core design, and promises new architectural changes such as pruning and weight compression to improve the power efficiency of the NPU by 2x.Samsung today proclaimed AI performance figure of 26 TOPS, but this figure seems to be an aggregate figure across all the SoC computational blocks, including CPU, GPU, NPU and DSPs, comparing itself to a 15 TOPS figure of the Exynos 990. The predecessor chipset was actually announced with a 10 TOPS figure last year, so there’s a bit of marketing revisionism going on here, probably wanting to match the figures that Qualcomm is putting out for their SoCs. It’s unfortunate that both companies are resorting to such blurring of the actual IP block performances.More Powerful ISP - Up to 4 Simultaneous SensorsIn terms of multi-media features, Samsung says to have made large improvements to its camera ISP and DSP’s. The new chip supports single-sensors up to 200MP, while also connecting up to a total of 6 camera sensors, and simultaneously running 4 of them in parallel.At least in this regard, the new chip flexes higher capabilities than the Snapdragon 888 which only supports up to 3 simultaneous cameras.Edit: Qualcomm had reached out to clarify that it supports 5 simultaneous cameras in the form of 3 RGB sensors + 2 monochrome sensors - it's not clear if the Exynos also has such a distinction between it's 4 concurrent sensors.Samsung has updated its MFC block (Multi-format Codec) video encoder and decoder to support up to 8K60 video decoding, although encoding has remained at 8K30/4K120. What’s interesting here is that Samsung does now support AV1 decoding, marking it the second chipset in the mobile market after MediaTek’s Dimensity 1000 to support the new feature.5G modem integratedFinally, the new platform is again a new monolithic design, reintegrating the 5G modem on the same silicon die, promising some gains in power efficiency and allowing for better PCB space usage. The actual modem itself doesn’t seem to have seen notable changes in features compared to the discrete Exynos 5123 modem – however that’s probably because it already was fully featured in terms of supporting all new technologies such as 5G sub-6GHz and mmWave connectivity.In the announcement video, Samsung made an emphasis that mmWave deployments are growing in 2021, and that we’ll be seeing more Exynos powered devices using mmWave. This is going to be interesting as we haven’t yet seen Samsung’s own mmWave antennas in the market, although the companyhad let out in 2019that they had multiple generations of such designs.5nm Process NodeFinally, Samsung reveals that the new Exynos 2100 is manufactured on Samsung’s 5nm process node, which brings either 10% boost in clock frequencies, or a 20% drop in power at the same frequency. Last year we had suspected that Samsung’s 7nm process node wasanywhere between 20 and 25% less power efficientthan TSMC’s latest N7P node, so essentially we’re expecting the new process to just catch up in terms of power efficiency, with newer TSMC 5nm SoCs suchas the Apple A14and the Kirin 9000 to still have a notable process node lead.Positive Messaging, Good OutlookToday’s launch event was extremely positive in terms of tone and general enthusiasm on the part of Samsung, proclaiming catchphrases such as “Exynos is back” and during the presentation acknowledging that the previous generation SoCs had issues. I think that’s a major change in messaging from the company, and at least on paper the new Exynos 2100 looks to be extremely competitive. I do hope this actual also translates onto the actual products, with the upcoming new Galaxy S21 series in a few weeks’ time sporting the new Exynos chipset for the first time.Related Reading:Samsung Announces Exynos 1080 - 5nm Premium-Range SoC with A78 CoresISCA 2020: Evolution of the Samsung Exynos CPU MicroarchitectureSamsung Confirms Custom CPU Development CancellationThe Samsung Galaxy S20+, S20 Ultra Exynos & Snapdragon Review: Megalomania DevicesSamsung Announces Exynos 990: 7nm EUV, M5, G77, LPDDR5 Flagship SoC alongside Exynos 5123 5G Modem\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16316/samsung-announces-exynos-2100\n",
      "Title: An Interview with Intel CEO Bob Swan: Roundtable Q&A on Fabs and Future\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-01-11T21:31:00Z\n",
      "URL: https://www.anandtech.com/show/16360/an-interview-with-intel-ceo-bob-swan-roundtable-qa-on-fabs-and-future\n",
      "Content: Intel has a very complex year ahead. On the back of what should be strong revenue year for 2020 as well as a widened scope of markets in which it participates, the key fundamentals at Intel such as manufacturing give those that follow the company care for concern. Throughout its existence, Intel’s key market leadership feature has been its manufacturing excellence, so now in 2021, after several high-profile manufacturing delays, what does Intel need to accomplish this year in order to get back on track? I was joined by several other journalists at a rare roundtable with Intel CEO Bob Swan, to ask about Intel’s future.The Very Rare OpportunityAccess to Intel’s C-Level executives for anything other than financial analysis is a Very Rare Thing Indeed. In a world where Intel’s competitors give the prominent tech press access to their most senior people, Intel was separated from the crowd. So cue my surprise when Intel offered us a seat at a roundtable with CEO Bob Swan, ahead of the company’s announcements at the annual CES trade show and just ahead of the 2020 year-end financial report. Now was a chance to ask Intel's top brass some exciting questions on the record.Note that this is a roundtable, and not a one-on-one interview. In a one-on-one interview, especially if you have never met before, there is opportunity to build a rapport, start with some easier questions and lead into some tough questions. There is the ability to set a level of expectation, make the person being interviewed comfortable, and potentially increase what level of detail the interviewee wants to go into that particular day, and questions can be re-worded on the fly as needed to get as much information as possible. In a roundtable like this one, it’s quite different. The time available is divided between all the press in the room (CEOs often have hard stops, so it has to be complete on time). When there are six journalists and only 30 minutes to get everything in, it gives each member of the press at best one question each, maybe one-point-five, sometimes zero-point-five. This means as an interviewer you have to select which question matters most to your audience, choose one that they will *actually* answer (very important not to ask something you already know theywon’tanswer), and dive right in. And so we did.End-to-end, this round table was a 30 minute session, however only the latter 15 minutes ended up being Q&A time.Bob Swan’s Prepared RemarksHappy New Year! Thanks for joining us today. Right as I was joining the call someone was saying it was good to have 2020 behind us – I couldn’t agree more! At the same time the start of the year hasn’t been that great just in terms of stuff we have to deal with, but it’s a pleasure to be here.To start I want to kick off with a little bit about strategically what we’ve been up to which I know all of you were at Architecture Day – Pat (Patrick Moorhead), I know you were at Analyst Day. But I want to start about strategically where we’re going, the progress we’re making, and then a little foreshadowing into what we expect you see on the 11th(the CES press event).So first, I’ll start with the market dynamics in terms of how we see it. It is, as you say, a good time to be in semis (semiconductors), right? The digitization of everything seems to be accelerating computing, and computing is everywhere. It’s no longer just our PC or our server, but everything seems to need high-performance compute and the PC is essential once again. So they dynamics of the market are extremely favorable.The second thing I would say is [that we need] to capitalize on these data-centric transitions. It requires, as you know, massive transformation for your clients, and obviously for us, but it’s a massive transformation so that we are seen adapting and developing the new technologies that are going to enable those transformations, like 5G, like AI, like autonomous edge.And then the third thing is, you know in some ways fortunately but in some ways unfortunately, we’re not the only ones that have noted this massive opportunity in front of us. You know, competition is intense, and they never sleep. That means we have got to be on top of our game, which we have every intention to be, [especially] as we close out last year and enter this year. So we’re excited about the competitive landscape – sometimes I wish it wouldn’t be as intense, but I think it makes us stronger along the way.Intel showcasing networking, storage, Optane, compute, 5GSo the market overview is relatively attractive. Strategically what we’ve been talking about for the last couple of years in terms of our role in this market is just to be the trusted performance leader, [one] that takes this insatiable appetite for data and makes that data relevant and actionable by analyzing it, storing it, and moving it faster. [This means that] whether it’s a business or a consumer going through their own digital transformation, our technologies are there to make the data that everybody's getting after increasingly relevant. And the third thing I would say is that we know that for us, to achieve our dreams, that requires us to transform ourselves on three fundamental dimensions.One, as we characterize it, and Pat I know I read every word you write (only Pat was mentioned by name), but from the CPU to the XPU. As the industry evolves, as workloads evolve, having a variety of architectures, enhancing our core CPU, but also adding additional architectures is increasingly important.Secondly, you know, from Silicon to Platform. It’s not just the hardware, but how do we couple the hardware with software and with other technologies to build platforms that can delight our customers.And then the third area for us have been a transformation from what we’ve characterized as the traditional IDM (Integrated Device Manufacturer) to a more modern IDM. This is not to get rid of the IDM, as we think it is a unique advantage of this company. But we know the industry has evolved quite a bit, we know that leveraging design disaggregation and packaging technologies that the IDM of the future is going to be a little bit different than the IDM of the past. That’s the third leg of our kind of transformation.Then to kind of bring it all together: [it’s] our intensity and focus on both execution and [the] tightly coupled re-energizing the culture of the company. So we’ve moving in a much more nimble way as we see opportunities to grow and [to] innovate, as opposed to protect what we built in the past. So that’s a little bit about what we’ve been up to.I also want to start with the kind of momentum entering the year. I think you guys, a year ago I flagged you know three fundamental areas that I thought were critical for us to improve execution. One was capacity, to ensure that we have the capacity to meet the demands of our customer base. Second was, once and for all, the ramp of 10nm. Third was to increase the rate of innovation of these multiple/multiplicity of architectures.So as we enter 2021, you know, we built some really good momentum. Obviously we have a lot more to do, but yeah we’ve added over $20 billion in revenue over the last five years, and we’ve been chasing it, and chasing it is disruptive. So we entered the year (2021) having essentially doubled our capacity over the course of the last couple of years, and we’re going into 2021 you know with a lot more capacity in place, both for 14nm but also for 10nm.The second area is 10nm itself. It was a year ago that we finally launched it, and during the course of the year going to second generation SuperFin, the biggest internode enhancement in transistor density ever for us. [It] was a big introduction during the course of the year and coupling that with our packaging technology is kind of bringing them to life. [It] was a really important aspect of execution on 10nm.10nm SuperFinThen third is the products, and just the rate of innovation. During the course of the year, whether it was beginning the year with LakeField, or 5G SOCs, FPGAs, discrete graphics, oneAPI – the amount of products that we launched during the course of the year was really good momentum. And I would say that maybe most importantly, the Tiger Lake launch during the course of the year, that t ramped on the new node faster than we anticipated. It grew better, the adoption [of Tiger Lake], the designs by our customers was more than we expected. So we enter the year with that momentum kind of behind us, and the opportunity really to scale as we ended the year.The last thing I [want] to say is about the execution as we exit the year. We launched [the] Ice Lake server product, you know, qualified it at the end of the year, started production of the third generation of Xeon Scalable processors, and will ramp it in the first quarter. So as you know, that was an important launch to kind of complete the portfolio of products that we have, from LakeField at the beginning of the year, all the way to server at the end of the year. So that’s, you know, a little bit about strategically where we’re headed, [and] the momentum we have entering the year.Just quickly on the things you’re going to hear about at the [press] conference on the 11th. [To] start with, I mentioned the Ice Lake Xeon Scalable. Next year the client business is going to, you know, show more advances and industry firsts for virtually every type of experience in every segment of the PC market. During the year we’ll have four families of processors, from entry to premium, a really good lineup of products during the course of the year. In that lineup we’ll include some real desktop innovation coming in. Greg (Bryant) or Chris (Walker) will showcase two technologies coming to market in 2021, including the 11thGen Core desktop processor Rocket Lake, as you know, and our next generation processors, Alder Lake, which from our vantage point represents a significant breakthrough in the x86 architecture. So we’re really excited about the innovation on the desktop. So between server, client, mobile, and desktop, we have an exciting lineup of things as we enter the year.CES 2020, Tiger Lake (10nm SuperFin) WaferThe second thing we will talk about is Silicon to Platform – we will also enhance our platform offering in the year. We will launch our first ever 11thGen Core Chromebooks, based on the Evo platform, and we’re going to debut the Intel Evo vPro platform as well, which is going to feature the highest performance business PC platform with the most comprehensive hardware based security.So [we’re] very excited about going beyond the silicon, the platforms, not just for Chromebooks but also you’re going to hear from Annan how Mobileye is expanding its offering as well. That talk is about expanding autonomous vehicle efforts globally, with four new testing locations [in] Detroit, Tokyo, Shanghai and Paris, in the first part of 2021. [This is] basically demonstrating the ability to have that kind of rapid global deployment by leveraging the proprietary crowdsource mapping technology called RAM, or Road Experience Management. So [we have] really exciting announcements coming from the Mobileye team on the 11thas well.Just to wrap up: the demand for compute is pervasive. It’s not just the PC or the server, it’s the network, at the edge, in the car, in the retail store, in the industrial environment, and that incredible demand for compute, on billions and billions of devices, is a rich opportunity for us to expand the role that we play in our customer success. We feel like we have good momentum [from] 2020 in entering the year, and we’re excited to demonstrate a bunch of new innovation.Questions and AnswersPatrick Moorhead, Moor Insights and Analysis: Can you give the latest and greatest on 10nm?Bob Swan:There are several components to this. Firstly, we have three fabs ramping [up production] now. We’re building capacity for this year and the next couple of years, and both front-end and back-end yields have been progressing as we ramp. We ramped even faster in Q4 2020. With this, as yields increase, unit costs continue to come down, and gross margins get better. But don’t forget it’s not just about the process technology, but also the products we build – I mean, we ramped everything during 2020. At the end of 2020, I think in the Q3 financial call, I said that volume on 10nm was 30% higher than we expected compared to the beginning of the year. We have felt great about our design wins on 10nm, and look to continue ramping 10nm across all three fabs as we exit our financial year 2020, into 2021.Intel Roadmap, as per mid-2019Paul Alcorn, Tom’s Hardware: One of the key benefits of Intel is that it manages its own manufacturing supply chain, however recently Intel has discussed using external suppliers for key components. We're moving into an era, due to COVID and other political relations, where the scope for a sustained third-party supply might leave Intel on the back foot should it decide to outsource its market leading products. How does Intel move forward in a future where it blends its product lines with a mix of third-party derivatives, the supply and economics of which will be further removed from Intel's usual control?Bob Swan:Access to supply has been a critical part of Intel’s product management. A year ago, as we collectively looked into 2020, we expected that the PC market might be grow or contract by about 1%. It felt quite flat, and the demand profile wasn't great. Then as COVID hit, most of the industry thought that the first half of the year was going to fall off in terms of demand. But then in the second half of the year, demand grew like crazy and the mix of Intel’s products changed considerably. As a result, the whole ecosystem scrambled to gain pace, and at Intel we had to react and schedule for that.For Intel, this is our advantage as an IDM (Integrated Device Manufacturer). While an IDM is still dependent on its ecosystem, it helps being the allocator rather than the allocatee - it means a great deal. If we think about design disaggregation, where we might leverage a third party foundry process, in any discussions that we are trying, even if we don't make those agreements happen, we always have to look at how we preserve the advantages of being a [vertically integrated] IDM. This means the ability to leverage Intel's size and scale, especially to get preferential treatment when allocation decisions are being made. We've got practice in using third party resources for our commodities, and as we plan to expand and use more third party foundries, maintaining and securing the advantages to Intel’s IDM strategy is going to be critical.Paul Alcorn: Is there scope for Intel to consider sub-licensing leading process node technology from other foundries in order to use in its own manufacturing facilities?Bob Swan:Possibly. I think the strategically for Intel, the ecosystem has evolved over the last 10 years. If there are ways or opportunities to leverage other industry advancements, then it is going to be very front-and-center for Intel to capitalize on industry innovations. Intel is coming to the realization that we don't have to do all the innovations ourselves. So in that thought we might outsource, we might move to using third party IP, opening up our own manufacturing and become a foundry. Is there a scenario in which Intel might use another’s process technology in our own manufacturing? That's certainly possible. The industry is evolving and we need to leverage that innovation, within our walls or elsewhere, and take advantage.Mark Hachman, PCWorld: A question on the competition. If I am a PC vendor, I can buy from Intel, I can buy from AMD, or I can go find an Arm chip (or roll my own). With that in mind, what is the competitive advantage of Intel over the competition?Bob Swan:Competition makes us stronger and helps us to move faster. It’s all about the rate of innovation, and then developing a broad based product offering. This means providing a range of products from intro to premium, and for products to business, to the consumer, for notebooks, for desktops, and offering a predictable annual cadence of leadership. It comes down to Intel’s IDM advantage – at Intel it is the knowledge to the customer base is that we are not going to contain your growth. At Intel we need to have more capacity than needed, and Intel needs to be the allocator and have the inventory to meet the spikes in customer demand. Also on the knowledge – we work with engineers at these companies and co-optimize their products. We're part of their design team, not just the order taker.Dave Altavilla, HotHardware: Is there an update on the progress for Intel’s higher-end XeGPUs? How is it going, and how do you see it competing in the gaming or the datacenter?Bob Swan:This is something that we've had a couple of goes with, as you know, when we started over two years ago. Rather than starting from scratch like we did with Larrabee, it was a case of how do we start but this time with integrated graphics. The process is about driving from integrated graphics to discrete graphics, and leveraging the same design as much as possible (to enable scale up). You saw the progress with Xein 2020, with the launch of DG1, which we are very excited about. We've not given a date for DG2, but clearly we have strong intentions in this CPU to XPU space where there are multiple architectures needed. Intel is going to invest to expand GPU to entry up the stack over time. We will have real innovation in the next two yearsDr. Ian Cutress, AnandTech: Given Intel's position in market, its outlook for 2021, and where the company currently stands, what matters more: its financial side or its technical side?Bob Swan:Technical. But they’re not decoupled – one powers the other and vice versa. Our rate of innovation, our rate of products, and our rate of investment helps one to drive the other which feeds back in.Raja Koduri, Intel SVP and Chief ArchitectDr. Ian Cutress: Intel has had a recent exodus of prominent key technical personal in 2020, such as Renduchintala and Keller. Where does Intel see the future of its technical expertise coming from?Bob Swan:Don’t forget that Intel has 70,000 engineers! Intel’s background is technical. Our model for the longest time was to hire engineers right out of their college campuses, and grow them within the system*. As the industry has moved on, when we think about new architectures, and the ecosystem still has to be growing from within but also we are bringing in outsiders.As you know, Jim left for personal reasons. When Murthy left, he was a wonderful executive, but in so many ways it was ‘how to get five engineers in the room when we’re making big decisions, not just one?’ I think it’s the realization that we can get more diverse points of view, more technologists in the room, making big time decisions. As you know we’ve done substantial promotions within the company to fill those rolls, such as Ann Kelleher on technology development, Keyvan Esfarjani in manufacturing, Raja Koduri on company-wide architecture and software, and Josh Walden fills in the Jim Keller type position as we look to fill that role on a permanent basis. We also have Dr. Randhir Thakur from Applied Materials on our supply chain. Having 5 senior, what I’d characterize as brilliant, engineers in that room gives us more diverse thought on the technology and the future. Then there's also the core hardware and software engineers that work for each of them, and I'm convinced we have the best talent, by a long shot, in the world. Our expectations are to continue to have the best talent, and we’ll get it the same way as we have recently, which is from the campuses and the competition.*Ian: I often address the engineers that have been there 20+ years as Intel ‘lifers’Final Words from Bob SwanAs a company, it has been a tough couple of years [especially on the technology side]. But coming out of 2020, it will be the 5th best year in a row for revenue. We understand it's all about innovation and execution, and we're excited about what we're going to do for customers in 2021.Analysis of Bob’s RemarksDespite the short time to ask questions, there are several points to highlight.1. Keeping the FabsFirst, it sounds like Intel is almost certainly going to keep its fabs. Bob repeatedly stated that Intel’s key advantage is as a vertically integrated company, and being able to have the resources to scale out production in order to meet demand from customers, as well as deal with spikes in that demand. Obviously the question as to whether Intel will have a sufficiently competitive manufacturing technology to remain in such high demand still remains on the table for this isolated explanation, but our interpretation of this shows that keeping the fabs internal allows Intel to remain agile for its customers supply needs, something that may not be possible when using third party resources.2. Outsourcing only with preferential treatmentTo that, and secondly, the comment Bob has made about only using third party resources such that Intel has preferential treatment and maintains an IDM-like advantage (i.e., similar benefits to that of being vertically integrated). This has significant implications as it boils down to which third party resources Intel might consider using. Intel has a significant financial advantage over a number of its direct competitors in the traditional markets it plays, but for third party foundry, the competitors are different. Apple is TSMC’s lead partner on 5nm and won’t be outbid, whereas AMD and Qualcomm take up most of the 7nm volume now that Huawei is no longer a customer. TSMC has already stated that it isn’t prepared to build out additional operations just for Intel, especially if they would be temporally limited until Intel reaches parity on its own manufacturing. If Intel were to outbid for capacity against AMD or Qualcomm, then it puts severe question marks on the gross margin, something that Bob Swan is very keen to monitor and keep in line with Intel’s historical metrics. The other option would be Samsung, where Intel would have to bid against Samsung itself and NVIDIA for capacity.3. Intel could sub-license process node techThird, that Intel could consider licensing another fab technology and integrate it into its own. To put some perspective on this, all of these fabs (between Intel, TSMC and Samsung), use mostly the same machines to build transistors on wafers. There are only a few companies that make the hardware, and all three have major investment in all the players. The differences in the technology is going to be how those steps are built up, and machine-to-machine ratio, as well as EDA tools in play. But much in the same way GlobalFoundries licensed Samsung’s 14nm for its own use, Intel could do the same, at least in a facility or two. If Intel were to do this, for example on its 7nm by licensing Samsung’s 5nm, it could re-channel research and development teams that were working on its own 7nm over to Intel’s 5nm process, where we expect the company to debut nanoribbon transistors. Dropping its own 7nm and licensing someone else’s technology might be an initial cost, time delay, and impact gross margins for a year or two, but focusing on its own 5nm might be what Intel needs to restart its manufacturing arm. The fact that Bob didn’t immediately swat it away means that it remains a valid option, especially as Intel appears to be committed to continuing to build out its fab technology (as per the first point).4. Technical matters more to Intel than FinancialFourth, Bob Swan when asked if technical or financial matters more to Intel, he stated the technical side matters more. I should note that he stated that without hesitation. A question continually asked among a number of analysts is if having a non-technical CEO is the right fit for Intel, especially at a time when Intel’s future seems to be predicated on its technical delays than anything else. But the fact that Bob recognizes that Intel’s technical side is where the company needs to focus gives me more hope. It all comes down to whether Intel can have the right technical people in the right positions being able to make decisions that don’t require months of debate. Which also leads me on to the final part…5. Not enough voices at the topFifth, the comments about Renduchintala took me by surprise. The nature of Murthy’s sudden departure from Intel in July 2020 led to a number of hushed private comments as to what exactly was happening inside the four walls. Suggestions have been raised that he was asked to leave, that he was forced to leave, or that he left as his position became untenable with others. The fact that Bob has stated that Murthy’s departure has enabled the company to add more technical voices to the future of Intel indicates that Renduchintala’s id was perhaps inflexible to commentary from similar level peers. Renduchintala’s hiring was widely promoted in 2015, not only for being a rare circumstance of Intel hiring from outside the company to such a senior position, but also the $8.1m signing on bonus with $17m in additional incentives (culminated in a $27m compensation package in 2019). It was also as a nod to the potential future of Intel, with whispers that Murthy was on the track to the CEO position and could help fix Intel’s process node delays. So the fact that Renduchintala left with little fanfare, along with Bob’s comments today, might imply the split was less than amicable when it came to deciding Intel’s future, at least to the point on the future of the company and how each party wanted to be involved. It sounds like Murthy wanted to go his way and his way only, but Intel’s management wanted more voices in the room to debate those calls.6. Silent Launch of Ice Lake XeonPerhaps this last point is a bit of a big question mark. When a company starts talking about announcements, launches, and availability, the exact position of the product can get a bit murky, because the terms of somewhat used interchangeably. Normally an official ‘launch’ is code for ‘we’ve started selling it and generating revenue’, which is why this is all a bit odd. Intel’s Ice Lake Xeon Scalable product has not had an official launch date and information event, however Bob in this Q&A stated that this 10nm silicon we’ve been waiting for has already been launched. To add confusion to the mix, he also mentions that production is ‘ramping’, which usually means that Intel is enabling high volume production for wider availability. However, the Intel press release today only states that the product is ‘entering production’. It can’t be both – either it is in production already and Intel is selling it, or they’ve started to make it. Normally with these Xeon products, the hyperscalers and big customers act as Intel’s testing ground, and so these customers will have been using the engineering sample hardware for several months already, sometimes in deployed servers. A few years ago Intel made a hubbub about its first 10nm product Cannon Lake (which the company seems to want people to forget about) being shipped for revenue at the end of 2017, however it wasn’t until Q3 2018 where Intel put it in their own NUCs for consumer use. Between the launch, the ramp, the production, and revenue, Intel’s messaging here is unclear. However, it is safe to assume that the big customers already have some form of it up and running.When Intel first announced 10nm was shipping for revenue. Click through for videoFinal Words from AnandTechWhichever way you slice it this was a very interesting call about Intel and the company’s future. This was done before Intel briefed us on its CES announcements, so there might be some cross over as that happens. But I’m glad Intel was able to snare its CEO for some time with the technical press, even if the Q&A time ended up being 15 minutes for 5/6 journalists. We enjoy these chances to have honest discussions about how the head of a company perceives the business they are in, as well as the vision of the company as we move forward. It was a very interesting first talk with Bob, and hopefully we’ll get the opportunity for many more.Related ReadingInterview with Intel CEO Bob SwanIntel at CES 2021: OverviewIntel at CES 2021: Press Event Live BlogIntel Ice Lake Xeon in ProductionIntel 11thGen Desktop Rocket Lake Core i9-11900K ‘Preview’Intel 11thGen Tiger Lake-U Boosted to 35WIntel 11thGen Tiger Lake-H with 8 coresIntel 11thGen Tiger Lake Goes vProIntel’s Next Gen Tremont Atom in Jasper Lake for Q1\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16360/an-interview-with-intel-ceo-bob-swan-roundtable-qa-on-fabs-and-future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Jim Keller Becomes CTO at Tenstorrent: \"The Most Promising Architecture Out There\"\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-01-06T02:50:00Z\n",
      "URL: https://www.anandtech.com/show/16354/jim-keller-becomes-cto-at-tenstorrent-the-most-promising-architecture-out-there\n",
      "Content: It is high praise when someone like Jim Keller says that your company ‘has made impressive progress, and has the most promising architecture out there’. That praise means twice as much if Keller actually joins the company. Today Tenstorrent is announcing that Jim Keller, compute architect extraordinaire, has joined the company as its Chief Technology Officer, President, and joins the company board.To our regular audience, Jim Keller is a known expert in all things computer architecture. His history starts at DEC, designing Alpha processors, before moving to a first stint at AMD for two years working to launch K7 and K8. Keller spent four years as Chief Architect at SiByte/Broadcom designing MIPS for network interfaces, four years at P.A. Semi, four years at Apple (A4+A5), then back to AMD for three years two years as Corporate VP and Chief Cores Architect in charge of the new generation of CPU architectures, K12 and Zen. This was then followed with two years at Tesla as VP of Autopilot Hardware Engineering creating the Full Self-Driving chip, then two years as Intel’s Senior VP of the Silicon Engineering Group, before leaving in June 2020. Since his departure from Intel, a number of key industry analysts (and ourselves) have been guessing where Jim would land. He briefly appearedin the audience of Elon Musk’s Neuralink presentation in August 2020, alongside Lex Fridman.Jim Keller: Work ExperienceAnandTechCompanyTitleImportantProduct1980s1998DECArchitectAlpha19981999AMDLead ArchitectK7, K819992000SiByteChief ArchitectMIPS Networking20002004BroadcomChief ArchitectMIPS Networking20042008P.A. SemiVP EngineeringLow Power Mobile20082012AppleVP EngineeringA4 / A5 Mobile8/20129/2015AMDCorp VP andChief Cores ArchitectSkybridge / K12(+ Zen)1/20164/2018TeslaVP AutopilotHardware EngineeringFully Self-Driving(FSD) Chip4/20186/2020IntelSenior VPSilicon Engineering?2021TenstorrentPresident and CTOTBDToday Tenstorrent reached out to inform us that Jim Keller has taken the position of President and Chief Technology Officer of the company, as well as being a member of its Board of Directors. Jim's role, based on his previous expertise, would appear to be in the design of future products for the company as well as building on the team at Tenstorrent to succeed in that goal.CEO Ljubisa Bajic confirmed Jim’s appointment as President and CTO of the company, stating that:Tenstorrent was founded on the belief that the ongoing shift towards ML-centric software necessitates a corresponding transformation in computational capabilities. There is nobody more capable of executing this vision than Jim Keller, a leader who is equally great at designing computers, cultures, and organizations. I am thrilled to be working with Jim and beyond excited about the possibilities our partnership unlocks.Tenstorrent is a pure-play fab-less AI chip design and software company, which means that they create and design silicon for machine learning, then use a foundry to make the hardware, then work with partners to create solutions (as in, chips + system + software + optimizations for that customer). For those that know this space, this makes the company sound like any of the other 50 companies out in the market that seem to be doing the same thing. The typical split with pure-play fabless AI chip design companies is whether they are focused on training or inference: Tenstorrent does both, and is already in the process of finalizing its third generation processor.Founded in 2016, Tenstorrent has around 70 employees between Toronto and Austin. The critical members of the company all have backgrounds in silicon design: the CEO led power and performance architecture at AMD as well as system architecture for Tegra at NVIDIA, the head of system software spent 16 years across AMD and Altera, and there’s expertise from neural network accelerator design from Intel, GPU systems engineering at AMD, Arm CPU verification leads, IO virtualization expertise at AMD, Intel’s former neural network compiler team lead, as well as AMD’s former security and network development lead. It sounds like Jim will fit right in, as well as have a few former colleagues working alongside him.Tenstorrent’s current generation product is Grayskull, a ~620mm2processor built on GF’s 12nm that was initially designed as an inference accelerator and host. It contains 120 custom cores in a 2D bidirectional mesh, and offers 368 TeraOPs of 8-bit compute for only 65 W. Each of the 120 custom cores has a packet management engine for data control, a packet compute engine that contains Tenstorrent’s custom TENSIX cores, and five RISC cores for non-standard operations, such as conditionals. The chip focuses on sparse tensor operations by optimizing matrix operations into compressed packets, enabling pipeline parallelization of the compute steps both through the graph compiler and the packet manager. This also enables dynamic graph execution, and compared to some other AI chip models, allows both compute and data transfer asynchronously, rather than specific compute/transfer time domains.Grayskull is currently shipping to Tenstorrent’s customers, all of which are still undisclosed.The next generation chip, known as Wormhole, is more focused on training than acceleration, and also bundles in a 16x100G Ethernet port switch. The move from training to acceleration necessitates a faster memory interface, and so there are six channels of GDDR6, rather than 8 channels of LPDDR4. This might seem low compared to other AI chips discussing HBM integration, however Tenstorrent’s plan here seems to be more aligned for more mid-range cost structure, but also offering machine learning compute at a better rate of efficiency than those chips pushing the bleeding edge of frequency and process node (part of this will be in yields as well).So where exactly does Keller fit in if the current generation is already selling, and the next generation is almost ready to go? In speaking to the CEO, I confirmed that Keller ‘will be building new and interesting stuff with us’. This seems to suggest that the vision with Keller’s involvement is going to be on 2022/2023 hardware in mind, following Tenstorrent’s overriding Software 2.0 strategy that the hardware, compiler, and run-time offer a full-stack approach to sparse (and dense) AI matrix calculations. In Jim’s own words:Software 2.0 is the largest opportunity for computing innovation in a long time. Victory requires a comprehensive re-thinking of compute and low level software. Tenstorrent has made impressive progress, and with the most promising architecture out there, we are poised to become a next gen computing giant.Jim Keller officially started last Wednesday, and the official wire announcement is set for 1/6, but we've been allowed to share in advance. Our request for an interview with Jim has been noted and filed, potentially for a few months down the line as the company has some more details on its platform and roadmap (I’ve also asked for an up-to-date headshot of Jim!). For those interested, I interviewed Jim back in July 2018, just after he started at Intel –you can read that interview here.Related ReadingHot Chips 32 (2020) Schedule Announced: TenstorrentJim Keller Resigns from Intel, Effective ImmediatelyAn AnandTech Exclusive: The Jim Keller InterviewCPU Design Guru Jim Keller Joins Intel; Completes CPU Grand TourJim Keller Leaves AMDAMD Announces Project SkyBridge: Pin-Compatible ARM and x86 SoCs in 2015, Android SupportApple A4/A5 Designer & K8 Lead Architect, Jim Keller, Returns to AMD\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16354/jim-keller-becomes-cto-at-tenstorrent-the-most-promising-architecture-out-there\n",
      "Title: Intel Core i9-10850K Review: The Real Intel Flagship\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2021-01-04T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/16341/intel-core-i9-10850k-review-the-real-intel-flagship\n",
      "Content: When a company like Intel creates a CPU design, the process of manufacturing brings about variation on the quality of the product. Some cores will only reach a certain frequency, while others have surprisingly good voltage characteristics. Two goals of processor design are minimizing this variance, but also shifting the peak higher, all while controlling how much of the silicon is actually useable. This is part of the magic of ‘binning’, the process of filtering the silicon into different ‘bins’ for applicability to a given product. It is through this process that the Core i9-10850K exists, albeit reluctantly.Intel’s Core i9-10850K: Doing The Heavy LiftingThe Core i9-10850K is the entry member to Intel’s Core i9 lineup, with 10 unlocked cores and hyperthreading, and can turbo up to 5.2 GHz under Thermal Velocity Boost. The added bonus is that it's more widely available in the market than the Core i9-10900K.Intel 10th Gen Comet LakeCore i9 and Core i7AnandTechCoresBaseFreqTB22CTB2nTTB32CTVB2CTVBnTTDP(W)IGPMSRP1kuCore i9i9-10900K10C/20T3.75.14.85.25.34.9125630$488i9-10900KF10C/20T3.75.14.85.25.34.9125-$472i9-1090010C/20T2.85.04.55.15.24.665630$439i9-10900F10C/20T2.85.04.55.15.24.665-$422i9-10900T10C/20T1.94.53.74.6--35630$439i9-10850K10C/20T3.65.04.75.15.24.8125630$453Core i7i7-10700K8C/16T3.85.04.75.1--125630$374i7-10700KF8C/16T3.85.04.75.1--125-$349i7-107008C/16T2.94.74.64.8--65630$323i7-10700F8C/16T2.94.74.64.8--65-$298i7-10700T8C/16T2.04.43.74.5--35630$325As with the other 10thGen Intel Core i9 processors, this one supports two channels of DDR4-2933, uses the LGA1200 socket on Intel 400-series motherboards, and has sixteen lanes of PCIe 3.0 for add-in hardware. Intel likes to point out it has another 24 PCIe 3.0 lanes through the chipset, however this is limited by the DMI/PCIe 3.0 x4 uplink to the processor.With the Core i9-10850K, users are essentially getting the Core i9-10900K top tier model, but at 100 MHz lower across the board. The peak turbo is 5.2 GHz rather than 5.3 GHz, the base frequency is 3.6 GHz rather than 3.7 GHz, and both processors are set at a 125 W TDP. Saving 100 MHz also saves $35 from the bulk pricing of the processor, but because of the lack of availability in the 10900K, we’ve seen the difference between the two vary from $50 to $200 in recent months. This all feeds into the main story of what is going on here.Despite being part of Intel 10thGeneration Core family, the Core i9-10850K was released after the launch of the more public and vocal members, such as the 10900K or 10700K. The email about the 10850K dropped in our inbox on July 27th, two months after the official launch of the rest of the family, and while Intel didn’t have samples ready for that launch today (to be honest, it took us by surprise), requests were lodged and our arrived a few weeks later.While not completely surreptitious, Intel pushed this processor onto the market without so much fanfare.The Secret Art of BinningBinning is a fancy word for quality management and filtering – when the silicon is manufactured, some of it is better quality than others, and by testing the quality each product can be filtered into where it is best suited, filtered into ‘bins’. The nature of binning is not new in the industry by any stretch of the imagination, as depending on the manufacturing process quality can vary wildly, and binning enables a semiconductor company to make the most out of the fixed price wafer costs. If a given wafer provides 50 processors, only 10 meet the ideal quality level but another 35 meet a lower quality level, then the yield is 45 out of 50, rather than just 10, enabling less waste and arguably better value for the end customer. Normally when a company talks about yield, they are talking about this 45 out of 50 number.The main element to how this binning manifests is in two forms of variability: the variability in the process and the variability in the design. Manufacturing is vastly complex, however the methods and order of tasks in the lithography process, as well as the speed of production, can affect this variability (it comes down to a lot of R&D). Design variability is somewhat different, as it requires engineers to build mechanisms into a processor design to minimize quality variability, and this might come at the expense of power or die area.For a unique company like Intel, technically they can manipulate both of these sources of variability, but for others who rely on foundry manufacturing, it’s a one sided affair and the companies that pay the most to TSMC (like Apple) get key details on the other side of the equation.The end goal is to minimize variability (make each processor off the line reach the same target), but also to move that variability peak nearer a more desirable goal, such as performance, or power. The whole process is a conveyor belt of 10000 levers and switches, where each one can affect the performance of a dozen others, and so finding the best configuration in a sea of options can be very difficult.Example shmoo plots of Samsung's 5nm Test SoC, showing simple pass/fail at voltage and frequencySimple pass/fail metrics are often graphed in a shmoo plot, like the one above. Beyond a pass/fail metric, companies like Intel have to also determine what percentage of the processors on a given wafer or batch meet those requirements. Because a graph of variability in the quality of silicon can be so varied, where the processor company defines its product binning targets is very important. A company like Intel needs to decide how many processors of each type it will need, what its customers will need, how that will change over time, and what it can do to maximize sales $ per square millimeter of silicon. In a situation where customers might want a cheaper product, Intel could take higher quality silicon and label it as a lower quality product, so it doesn’t sit on a lot of unsold processors. But contrary, if customers are demanding a higher quality product that manufacturing can’t deliver, then it can become an issue. It also matters when it comes to marketing as well.A shmoo plot of Intel's Centrino CPU, with the letters potentially indicatingwhat % of processors pass/fail at those points.A semiconductor company like Intel can do themselves a favor by choosing binning targets and metrics that are not as aggressive. But it’s really the high-end halo products that matter when it comes to promoting the best of the best. Intel has a history of very aggressive binning, to the point where its quality requirements for the top product are so strict that only a handful of processors will ever meet that level for every million produced.Over the past two decades Intel has made super not-so-secret ‘Everest’ or ‘BlackOps’ processor models. These are technically off-roadmap processors not for general sale, because of the very strict quality requirements. These units are selected because of the super-high frequency possible, usually at a completely disregard for power or cooling requirements. One of the first good examples of this was the special-order-only dual core Xeon X5698 rated for 4.4 GHz in Q1 2011, based on Intel’s 32nm Westmere platform, and was built solely for high-frequency stock market traders who needed the lowest latency whatever the cost. The concept of a microsecond for these traders can be millions of dollars, so throwing $20k+ each for the fastest processor available is chump change (that includes OEM markup). These were 1000 MHz faster than any of Intel’s regularly binned processors for the open market.Sandy Bridge also had an Everest model, whereas for Ivy Bridge it was known as BlackOps, offering 6 cores at 4.6 GHz all-core and a massive 250 W TDP. These were again destined for Wall Street, but came with no product identifier, and as far as we can tell, no warranty except for dead-on-arrival (DOA). These were such at the edge of Intel’s manufacturing capabilities that if you wanted one, you had to accept that it might not work beyond a couple of months. Again, for these traders, we’re talking fractions of a percent of cost, so that was almost of zero concern.The only reason we know about these is because over time some have filtered into the hands of enthusiasts and collectors. More recently, Intel’s latest high-frequency trading processor was a bit of a doozy. We overheard (and confirmed) at an event that Intel was planning to launch an auction-only OEM-only high-performance processor where it couldn’t guarantee stock nor would it offer any warranty.Special auction-only CPUThis was a 14-core Core i9-9990XE processor, with all cores running at 5.0 GHz all the time, built for Intel’s high-end desktop platform (quad channel memory, 48 PCIe lanes). We actually gotone into reviewthrough our industry contacts, in a special single socket server unit and custom cooling. The unit had a peak power loading somewhere north of 600 W. It went well above Intel’s 28-core high-end Xeon products. Single threaded performance was crazy.Unfortunately the Core i9-9990XE didn’t do so well commercially. The idea was that the OEMs that ended up ‘winning’ these CPUs at Intel’s auctions would build systems for high-frequency traders and sell them on that way. Most of the companies that ‘won’ the processors at Intel’s auctions ended up selling them as CPU-only models instead, after it couldn’t get sufficient interest. Intel has since stopped making these CPUs available, except for the one company that is still selling to HFT.Since then, Intel has launched its consumer grade 8-core Core i9-9900KS, which also had all eight cores at 5.0 GHz, albeit with only dual channel memory and 16 PCIe lanes. This was still a very limited edition product, and was only on sale for four months before being discontinued. Intel also gave away 400 special binned versions of its 10900K as part of an influencer giveaway, centered around its new thermoelectric/liquid all-in-one cooler designed to bring temperatures below ambient during idle operation. These are not a special numbered version you can buy, however.When Intel (or anyone else) designs a processor, there are a number of tools at the company’s disposal in order to help the efficacy and variability of the manufacturing to help guide its products into given quality bins. Should Intel focus on making a very specific HFT processor, then there are 1000 design decisions that would be made which would make no sense for a consumer product, and so you can see the complexity of trying to extract the best performance out of the silicon if the bulk of the sales for that design are going to be for a more mainstream product. For an example of a processor built for extreme tasks, then I point towards IBM’s z-series hardware, which starts at 12 cores running 5.2 GHz, and four processors in a server share a 960 MB L4 cache chip. It’s a crazy (but fun) rabbit hole of processor design to go down.Binning: Core i9-10850KSo why spend so much talking about special 1-in-a-million processors? The point is that when Intel decides where to draw the line on silicon quality, where it draws the line for its commercial flagship is very important. It needs to draw the line at a place where it has a competitive product, but can also produce enough to satisfy the needs of the market. There’s no point drawing that line for consumer flagship at the 1-in-a-million level if you can only sell 10 a month. If reviewers lead with that 1-in-a-million review on launch day but no-one can buy it, ever, then as a brand you’ve misdirected your customer base. At least with a BlackOps processor most users understand they will never see one, but for the top Core i9 product, it has to be widely available - it needs to be somewhere in that 10000-in-a-million level at least (one might imagine).Intel's 10-core Flagship Comet Lake Family: 10900K, 10850K, W-1290PWhich brings me to the sole reason why the Core i9-10850K exists and why it was launched two months after the main product line, after the Core i9-10900K. Simply put, Intel drew that quality line for its top bin processor too aggressive to the point where the company could not meet the demand. We’ve never had a x50K processor in previous generations because that drawn line was so aggressive, we’ve only ever had the line drawn further away (such as Devil’s Canyon on Haswell). This is very much the first instance in recent memory I’ve seen a vendor have to introduce a processor with more lax requirements because it couldn’t build enough of the top-end chip.In the first six months since the launch of the Core i9-10900K, stock has been scarce is popular markets, and non-existent in secondary markets, with a very slow trickle through. That level has somewhat normalized now, but users who could not purchase the 10900K had two options: (1) wait until they could get one, or (2) buy something else, perhaps from a competitor.In order to bridge that gap, the Core i9-10850K came out onto the market, where the binning line was ever so slightly more lax compared to the 10900K, and more silicon could achieve the targets. We’ve seen in the SKU list that it appears to be a 100 MHz drop across the board, but as we’ll see in this review, the voltage also seems to be higher, so it’s burning more power to get a lower frequency.The Core i9-10850K has been widely available since launch. In our monthly CPU Guides, I’ve seen sufficient quantities of 10850K units with prices hovering around the Intel expected pricing (sometimes cheaper with OEM-only non-packaged 1-year seller-only versions), especially at a time where stock of the 10900K is so limited that sellers were charging +$200 premiums for the stock they did have. At that price, users were looking at 16-core AMD models, especially for a high-performance market where multi-core performance is still a key metric.If Intel had drawn the line at the 10850K level in the first instance, the concept of binning wouldn’t be a discussion at this time, and there would have been sufficient stock on the shelves since launch. The question at the time was all about the message that Intel wanted to project with its top-tier 10-core overclockable model, and the 5.3 GHz mark provided that message, even if quantity of available processors was limited. There’s no point having a BlackOps style processor as the flagship if no-one can buy it. This is why the title of this review is that the Core i9-10850K is the true flagship – it’s the one that has been actively available to purchase.This ReviewIntel provided a Core i9-10850K retail processor for review, and as a result we’ve put it through our benchmark suite. Over the next few pages we will cover CPU performance, gaming performance, and some of our microbenchmarks for power and frequency ramping response. The Core i9-10850K was launched before the Ryzen 5000 processors, however as both are available today, both sets of numbers are included.Test SetupAs per our processor testing policy, we take a premium category motherboard suitable for the socket, and equip the system with a suitable amount of memory running at the manufacturer's maximum supported frequency. This is also typically run at JEDEC subtimings where possible. It is noted that some users are not keen on this policy, stating that sometimes the maximum supported frequency is quite low, or faster memory is available at a similar price, or that the JEDEC speeds can be prohibitive for performance. While these comments make sense, ultimately very few users apply memory profiles (either XMP or other) as they require interaction with the BIOS, and most users will fall back on JEDEC supported speeds - this includes home users as well as industry who might want to shave off a cent or two from the cost or stay within the margins set by the manufacturer.Test SetupIntel LGA1200Core i9-10900KCore i9-10850KCore i7-10700KASRock Z490PG VelocitaBIOSP1.50TRUECopperCorsair DomRGB4x8 GBDDR4-2933AMD AM4Ryzen 9 5900XRyzen 7 5800XRyzen 5 5600XMSI MEGX570 Godlike1.B3T13NoctuaNHU-12SSE-AM4ADATA2x32 GBDDR4-3200GPUSapphire RX 460 2GB (CPU Tests)NVIDIA RTX 2080 Ti FE (Gaming Tests)PSUCorsair AX860iCorsair AX1200iSilverstone SST-ST1000-PSSDCrucial MX500 2TBAdditional Cooling provided by SST-FHP141-VF 173 CFM fansMany thanks to...We must thank the following companies for kindly providing hardware for our multiple test beds. Some of this hardware is not in this test bed specifically, but is used in other testing.Hardware Providers for CPU and Motherboard ReviewsSapphireRX 460 NitroNVIDIARTX 2080 TiCrucial SSDsCorsair PSUsG.Skill DDR4ADATA DDR4SilverstoneCoolersNoctuaCoolersA big thanks to ADATA for the ​AD4U3200716G22-SGN modulesfor this review. They're currently the backbone of our AMD testing.Users interested in the details of our current CPU benchmark suite can refer toour #CPUOverload articlewhich covers the topics of benchmark automation as well as what our suite runs and why. We also benchmark much more data than is shown in a typical review, all of which you can see in our benchmark database. We call it ‘Bench’, and there’s also a link on the top of the website in case you need it for processor comparison in the future.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article already exists: https://www.anandtech.com/show/16341/intel-core-i9-10850k-review-the-real-intel-flagship\n",
      "Title: Xiaomi Announces the Mi 11: First Snapdragon 888 Device\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-12-28T14:30:00Z\n",
      "URL: https://www.anandtech.com/show/16339/xiaomi-announces-mi11-first-snapdragon-888-device\n",
      "Content: Today Xiaomi held the Chinese launch announcement of the new Mi 11 – the company’s new mainstream flagship phone for 2021. The new phone features a new super-high-end OLED screen that ticks off every checkmark feature that you’d expect from a 2021 design, including 1440p resolution and 120Hz refresh rates, features the new Snapdragon 888 SoC at its heart – as well as comes in a new thinner, lighter, and more stylish industrial design compared to its predecessors.Xiaomi Mi SeriesMi 10Mi 11SoCQualcomm Snapdragon 8651x Cortex-A77 @ 2.84GHz3x Cortex-A77 @ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 650 @ 587MHzQualcomm Snapdragon 8881xCortex-X1@ 2.84GHz3xCortex-A78@ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 660 @ 747MHzDRAM8GB LPDDR5-55008/12GB LPDDR5-6400Display6.67\" AMOLED2340 x 1080 (19.5:9)90Hz Refresh6.81\" AMOLED3200 x 1440120Hz Refresh480Hz TouchHDR10+ /10-bit panelSizeHeight162.58mm164.3mmWidth74.80mm74.6mmDepth8.96mm8.06mmWeight208g196gBattery Capacity4780mAh (Typical)30W Charging4600mAh (Typical)55W ChargingWireless Charging30W50WRear CamerasMain108MP 1/1.3\" 0.8µm4:1 Binning to 27MP / 1.6µmf/1.69 w/ OIS108MP Modulef/1.85 w/OISTelephoto-5MP50mm eq.f/2.2ExtraTelephoto--Ultra-Wide13MP 1.12µmf/2.4117° FoV13MPf/2.4123° FoVExtra2MP Depth Camera2MP Macro Camera-Front Camera20MP 0.8µmf/2.3f/2.2Storage128 / 256GBUFS 3.0128 / 256GBI/OUSB-CWireless (local)802.11ax(Wifi 6),Bluetooth 5.1Cellular4G + 5G NR NSA+SA Sub-6GHzSpecial FeaturesUnder-screen fingerprint sensorFull-range stereo speakersSplash, Water, Dust ResistanceNo ratingDual-SIM2x nano-SIMLaunch OSAndroid 10 w/ MIUIAndroid 11 w/ MIUILaunch Price8+128GB: 799€8+256GB: 899€?At the heart of the new Mi 11 lies the new Qualcomm Snapdragon 888 SoC. The new 5nm chip is the first to use Arm’s new Cortex-X1 cores, with Qualcomm also claiminglarge GPU performance boosts this generation. What’s more different from the previous Snapdragon 865 flagship design is that the new S888 reverts back to a monolithic SoC design that integrates the 5G modem back into the SoC – in turn this means that there’s more PCB component space in available inside the phone and vendors can optimise their designs better.The new Mi 11 also features either 8GB or 12GB of the new LPDDR5 memory at its full 3200MHz speed (6400MT/s), and comes in either 128GB or 256GB storage variants, although we’re missing details on the specifications of the modules used here.In terms of design, the new Mi 11 is quite a departure from the Mi 10, being a much sleeker device than its predecessor. Xiaomi has been able to reduce the bezels of the device on all sides, still using a curved front and back glass design that fits well into your palms. The new design is actually 0.9mm thinner than its predecessor, now coming in at 8.06mm, and also shaves 12g off its weight at 196g – still relatively heavy, but not as much as its predecessor.The most exciting feature of the Mi 11 has to be the new OLED screen. It’s grown a bit compared to its predecessor, filling in the space in place of the bezels, going from 6.67” to 6.81”, but the most important aspect of the phone is the fact that this is now a QHD class resolution at 3200 x 1440 – a first from Xiaomi OLED screen and a resolution we haven’t seen used by the company since back in the LCD days.Furthermore, this is a bleeding-edge display, featuring a 120Hz refresh rate, and is advertised as being a new generation 10-bit panel that also features up to 8192 brightness levels – pointing out to a new generation DDIC. The panel also is advertised as using a new next-generation emitter material, and Xiaomi claims it goes up to 1500 nits brightness, which is a notch above what we’ve seen from 2020 phones.Lastly, like a cherry on top of the cake, the panel features a staggering 480Hz touch input rate, which is the highest we’ve heard of in the mobile industry, and hopefully results in outstanding touch input and scrolling fluidity.Other features of the new design includes a redesigned camera setup, which transitions from a completely vertical camera arrangement to a new rounded square element with three camera modules.Instead of having a large camera bump edge and a single glass element protecting the whole camera arrangement, Xiaomi uses a three-step glass element, each thicker than the other, protecting the three cameras modules. It looks quite interesting and gives off a much more organic feel compared to other rectangular or more symmetric camera arrangements out there.Unfortunately for today’s announcement the Mi 11’s camera specifications were quite sparse. What we do know is that the main camera is again a 108MP module, similar to that of its predecessor, but we don’t know if it’s the exact same sensor, or a newer iteration. What we do know is that the optics are definitely different as the aperture now lands in at f/1.89 instead of f/1.69 – an actual positive change in my view as the optics of the Mi 10 was one of its main weaknesses.Alongside the main camera there’s a 13MP ultra-wide angle with a 123° FoV and F/2.4 aperture, as well as a 5MP 2x optical 50mm equivalent telephoto module with f/2.2. That latter module sounds a bit lacklustre – maybe we’ll see an eventual Mi 11 Pro with a stronger telephoto module.Today’s announcement covers the Chinese variant of the phone, with the global and European variants certainly to follow in the next few weeks. Pre-orders for the Chinese models start today with delivering going out the 1stof January, with the Mi 11 with an 8+128GB variant coming in at ￥3999.00, or USD $611. Western prices are certainly set to be different, but that’s already a quite promising start for the new flagship.Related Reading:Xiaomi Globally Launches Mi 10, Mi 10 Pro; Snapdragon 865 & 108MP CamerasThe Xiaomi Mi 10 Pro Review - A Solid Overall ValueXiaomi Announces Mi 10T & Mi 10T Pro: More Budget, But With 144HzXiaomi Launches Mi Note 10 Lite, Redmi Note 9 & Note 9 Pro Globally\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16339/xiaomi-announces-mi11-first-snapdragon-888-device\n",
      "Title: AnandTech Year In Review 2020: Flagship Mobile\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-12-28T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16328/anandtech-year-in-review-2020-flagship-mobile\n",
      "Content: We’re a few days away from completing the 2020 calendar year, and it’s been a quite a hectic year for everybody. In times of troubles, the smartphone industry had been under a two-prong attack from both an economic stand-point as well as the from a product maturity standpoint – trying hard to innovate with new features to convince users to upgrade their previous generation devices. This year, we’ve seen several new industry trends make breakthrough advances in terms of technology in smartphones, beyond the obvious elephant in the room, by which 2020 will be remembered by: Big camera sensors, 120Hz displays, several large SoC moves, 5G, and several other vendor product choices.Big Sensors go MainstreamAlthough a subjective opinion, what I view as one of the most important developments in 2020 mobile devices has been the widespread adoption of larger camera sensors. While in years past, Huawei had been the notable exception to the rule in terms of adopting larger camera sensors in their smartphones, this year, we’ve seen almost everybody make the transition to larger formats – either 1/1.78” sensors or even the new humongous 1/1.33” beasts in some flagships.Galaxy S20 Ultra & Galaxy S20+Bigger sensors most of the time means that vendors are able to use bigger pixels – which in turn results in better dynamic range capability and better low light captures. This year we’ve seen many flagships at least adopt a 1/1.78” sensor which is notably larger than previous generation 1/2.3” or 1/2.5” units – resulting in 12MP 1.8µm pitch units like on the new S20 series and some other vendors such as Sony.Other vendors have opted to go even bigger – ranging from 1/1.3” to 1/1.4” sensors. The most notable and talked about has been Samsung’s new 108MP units in the S20 Ultra and Note20 Ultra series. Although the high-pixel count sensors deployed on these devices have questionable benefits in their native resolution, the ability to bin 3x3 pixels down to an 12MP image means we’re looking at effective light gathering capabilities similar to that of a 2.4µm pixel pitch sensor, something well beyond other current sensors on the mobile market.We’ve seen variations of these huge sensors – Xiaomi also uses a 108MP native unit but bins down by 2x2 to a 27MP capture mode in general use-cases, while OnePlus had opted for a slightly smaller 1/1.4” sensor in the OnePlus 8 Pro with a 48MP native resolution, binning down to 12MP.The big caveat with these big sensors is their optics systems – many times the plastic lenses can’t keep up with the resolution of the sensors, partly negating some of their characteristics, making much of the 108MP contenders relatively gimmicky in real-world scenarios.iPhone 11 Pro vs Galaxy S20 UltraThe other big issue with larger sensor sizes is that they require a larger z-height of the camera module, which means larger camera bumps. And oh boy, we did get some really funky looking camera bumps in this year’s devices, such as the massive design element on the S20 and Note Ultras.Related Reading:Xiaomi’s Mi Note 10 Family: World’s First Smartphones with 108 MP Penta Camera ArraySamsung Unveils ISOCELL Bright HMX 108 MP Sensor for SmartphonesThe Samsung Galaxy S20+, S20 Ultra Exynos & Snapdragon Review: Megalomania DevicesThe OnePlus 8, OnePlus 8 Pro Review: Becoming The FlagshipXiaomi Globally Launches Mi 10, Mi 10 Pro; Snapdragon 865 & 108MP CamerasThe Xiaomi Mi 10 Pro Review - A Solid Overall ValueSamsung Announces New 50MP Dual-Pixel and Quad-Bayer ISOCELL Sensor120Hz, An Almost MustAnother big development this year has been the mainstream adoption of high refresh rate display panels. While in 2019 90 or 120Hz displays were exotic or reserved for special gaming-oriented phones, in 2020 it could be said that it was a checkmark feature for almost every vendor, essentially eliminating 60Hz from almost all new product releases bar the mid-range or low-end.Though 120 and 90Hz displays this year certainly have augmented the fluidity and user experience of flagship devices, only the Note20 Ultra was the one device which was able to adopt this with relatively few drawbacks, thanks to itsnew polycrystalline and oxide panel technology. For the rest of the market, 120Hz came with a relatively larger battery impact, meaning the new fluidity came with a device longevity compromise.For 2021, I see more vendors adopt the new 120Hz feature in a more optimised manner that improves upon the battery consumption, maturing the technology to a point where it becomes a no-brainer to have it enabled by default.Related Reading:The Samsung Galaxy S20+, S20 Ultra Exynos & Snapdragon Review: Megalomania DevicesThe OnePlus 8, OnePlus 8 Pro Review: Becoming The FlagshipThe ASUS ROG Phone III Review: A 144Hz 6000mAh Beast With CaveatsSamsung's Note20 Ultra Variable Refresh Rate Display ExplainedExynos Core Death (& Revival?)Continuing with our takes on key hardware component changes in 2020, one large move has been Samsung’s abandonment of their custom CPU cores in their Exynos chipsets. Struggling with power efficiency and performance competitiveness for several years now, Samsung LSI finally cut the cord on their custom CPU core design project and team, opting to instead fully rely on Arm’s future Cortex CPU IP for next-generation Exynos SoCs.We had been covering Samsung’s cores in detail for several years now, and it’s unfortunate to say that the project’s cancellation is likely the best-case scenario for future Exynos SoCs, dropping one of the chip’s most negative aspects – and in turn likely result in significantly better designs in 2021, hopefully eliminating some of the performance and power efficiency differences between Snapdragon and Exynos Galaxy phones.Related Reading:Samsung Announces Exynos 990: 7nm EUV, M5, G77, LPDDR5 Flagship SoC alongside Exynos 5123 5G ModemThe Samsung Galaxy S20+, S20 Ultra Exynos & Snapdragon Review: Megalomania DevicesISCA 2020: Evolution of the Samsung Exynos CPU MicroarchitectureSamsung Announces Exynos 1080 - 5nm Premium-Range SoC with A78 CoresSamsung Teases CES Announcement For Next Exynos SoCMediaTek Dimensity Resurgence2020 marked MediaTek’s re-entry into the high-end SoC market, introducing the newDimensity 1000SoC. Although the new design was off to an extremely slow start with very few design wins in the first half of the year, the company’s efforts in the mid-range Dimensity series has results in a large amount of design wins, andaccording to Counterpoint Research, allowed the company to actually position itself as the #1 SoC vendors in Q3 2020, taking market share from the likes of Qualcomm.We had reviewed theDimensity 1000 inside of the OPPO Reno3 5Gand found it to be an excellent SoC in terms of performance and efficiency – what’s really lacking though is a more prestigious high-volume design win that has more visibility. Hopefully MediaTek manages to achieve such a design in 2021.Related Reading:MediaTek Announces Dimensity 1000 SoC: Back To The High-End With 5GCES 2020: MediaTek Announces New Dimensity 800 Mid-Range 5G SoCMediaTek Announces Dimensity 1000+ SoCMediaTek Announces Dimensity 820 Mid-Range SoC With More PerformanceOPPO's Reno3 5G vs Reno3 Pro vs Reno3 Pro 5G: Why Don't We See More MediaTek Dimensity 1000 Phones?5G for MostWhile 5G has been a premium “the new thing” feature in some 2019 devices, it can be said that it’s really in 2020 where the new cellular communication standard has really been adopted as a mainstream technology in new smartphone products. Almost every new Android flagship device has had 5G as a selling point in 2020, and with Apple’s release of 5G-only iPhone 12 series, we can pretty much say we’ve made a turning point between 4G and 5G.Although cellular networks in many countries are still in deployment and it’ll probably be a few years before 5G becomes ubiquitous for everybody, buying a 4G device today is no longer the best long-term investment unless you’re really looking at low-end devices. 5G also has trickled down to the premium and mid-range thanks to the likes of SoCs such as the Snapdragon 765 – with 2021 projected as being the year where we’ll also see the $250 device market also adopt 5G connectivity.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16328/anandtech-year-in-review-2020-flagship-mobile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Ampere Altra Review: 2x 80 Cores Arm Server Performance Monster\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-12-18T11:00:00Z\n",
      "URL: https://www.anandtech.com/show/16315/the-ampere-altra-review\n",
      "Content: As we’re wrapping up 2020, one last large review item for the year is Ampere’s long promised new Altra Arm server processor. This year has indeed been the year where Arm servers have had a breakthrough;Arm’s new Neoverse-N1 CPU corehad been the IP designer’s first true dedicated server core, promising focused performance and efficiency for the datacentre.Earlier in the year we had the chance to test out the first Neoverse-N1 siliconin the form of Amazon’s Graviton2inside of AWS EC2 cloud compute offering. The Graviton2 seemed like a very impressive design, but was rather conservative in its goals, and it’s also a piece of hardware that the general public cannot access outside of Amazon’s own cloud services.Ampere Computing, founded in 2017 by former Intel president Renée James, built uponinitial IP and design talent of AppliedMicro’s X-Gene CPUs, and withArm Holdings becoming an investor in 2019, is at this moment in time the sole “true” merchant silicon vendor designing and offering up Neoverse-N1 server designs.To date, the company has had a few products out in the form of the eMAG chips, but withrather disappointing performance figures- understandable given that those were essentially legacy products based on the old X-Gene microarchitecture.Ampere’s new Altra product line, on the other hand is the culmination of several years of work and close collaboration with Arm – and the company first “true” product which can be viewed as Ampere pedigree.Today, with hardware in hand, we’re finally taking a look at the very first publicly available high-performance Neoverse based Arm server hardware, designed for nothing less than maximum achievable performance, aiming to battle the best designs from Intel and AMD.Mount Jade Server with Altra QuicksilverAmpere has supplied us with the company’s server reference design, dubbed “Mount Jade”, a 2-socket 2U rack unit sever. The server came supplied with two Altra Q80-33 processors, Ampere’s top-of-the-line SKU with each featuring 80 cores running at up to 3.3GHz, with TDP reaching up to 250W per socket.The server was designed with close collaboration with Wiwynn for this dual socket, and with GIGABYTE for the single socket variant, as previously hinted bythe two company’s announcements of leading hyperscale deployments of the Altra platforms.The Ampere-branded Mount Jade DVT reference motherboard comes in a typical server blue colour scheme and features 2 sockets with up to 16 DIMM slots per socket, reaching up to 4TB DRAM capacity per socket, although our review unit came equipped with 256GB per socket across 8 DIMMs to fully populate the chip’s 8-channel memory controllers.This is also our first look at Ampere’s first-generation socket design. The company doesn’t really market any particular name to the socket, but it’s a massive LGA4926 socket with a pin-count in excess of any other commercial server socket from AMD or Intel. The holding mechanism is somewhat similar to that of AMD’s SP3 system, with a holding mechanism tensioned by a 5-point screw system.The chip itself is absolutely humongous and amongst the current publicly available processors is the biggest in the industry, out-sizing AMD’s SP3 form-factor packaging, coming in at around 77 x 66.8mm – about the same length but considerably wider than AMD’s counterparts.Although it’s a massive chip with a huge IHS, the Mount Jade server surprised me with its cooling solution as the included 250W type cooler only made contact with about 1/4ththe surface area of the heat spreader.Ampere here doesn’t have a recessed “lip” around the IHS for the mounting bracket to hold onto the chip like on AMD or Intel systems, so the actual IHS surface is actually recessed in relation to the bracket which means you cannot have a flat surface cooler design across the whole of the chip surface.Instead, the included 250W design cooler uses a huge vapour chamber design with a “pedestal” to make contact with the chip. Ampere explains that they’ve experimented with different designs and found that a smaller area pedestal actually worked better for heat dissipation – siphoning heat off from the actual chip die which is notably smaller than the IHS and chip package.The cooler design is quite complex, with vertical fin stacks dissipating heat directly off the vapour chamber, with additional large horizontal fins dissipating heat from 6 U-shaped heat pipes that draw heat from the vapour chamber. It’s definitely a more complex and high-end design than what we’re used to in server coolers.Although the Mount Jade server is definitely a very interesting piece of hardware, our focus today lies around the actual new Altra processors themselves, so let’s dive into the new Q80-33 80-core chip next.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16315/the-ampere-altra-review\n",
      "Title: Testing The World’s Best APUs: Desktop AMD Ryzen 4750G, 4650G and 4350G\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-12-16T15:30:00Z\n",
      "URL: https://www.anandtech.com/show/16308/testing-the-worlds-best-apus-desktop-amd-ryzen-4750g-4650g-and-4350g\n",
      "Content: There are two very important levels of graphics performance in modern systems to consider – one is if the graphics system is sufficient for seamless use, and the second is such that it meets a substantial standard for gaming. On one side we use integrated graphics, which take advantage of a unified processor to simplify the system, and on the other we look to a range of options, such as smartphones, consoles, and discrete graphics options. Somewhere in there we have a middle ground – can an integrated option have enough thermal headroom and graphics power to worthwhile for gaming? This is the pitch of AMD’s Ryzen 4000 based APUs, which combine Zen 2 CPU cores with fast Vega 8 graphics. With a 65W headroom, it should surpass anything that mobile processors have to offer, but is it enough to replace the low-end discrete graphics market?When a CPU meets GPUAMD is the company of the accelerated processing unit, or APU. The company introduced the term in 2011 when it started combining its x86 CPU cores and some form of graphics accelerator into the same piece of silicon. This combined processor, built for the laptop and desktop market , was designed to remove the need for a completely separate graphics card in a system, simplifying the design and bringing down overall cost for anyone that simply needed a graphics output for simple tasks. At the time, these solutions were very much for the low-end market.Combining a CPU and a GPU on the same piece of silicon has a variety of tradeoffs involved. The key benefit is reducing that bill of materials, but there are also advantages in the latency of communication between the CPU cores and the GPU acceleration as the data does not need to go off the chip. There can also be benefits in power control, with a system being able to manipulate how much power goes to each in a simpler way.But there are a number of downsides. The total power consumption of the system now gets condensed into one thing, rather than split across two. This makes the one APU a central hotspot for cooling support. Also, adding in graphics will make the single CPU die size larger, making it more difficult to yield compared to two separate pieces of silicon. It can also be complex if both CPU and GPU have to be made on the same manufacturing process, depending on the initial design for those architectures. There is also the memory problem – graphics loves memory bandwidth, and CPU memory controllers are slow by comparison; while a GPU might love 300 GB/s from some GDDR memory, a CPU with two channels of DDR4-3200 will only have 51.2 GB/s. Also, that memory bank needs to be shared between CPU and GPU, making it all the more complex.For ultra-mobile laptops, the tradeoff in having a single combined APU is worth it, as it also means there can be a bigger battery and reducing the number of items inside the shell helps with aesthetics and thermals. Also, ultra-mobile laptop users are not often demanding super graphics performance for 4K gaming, and so something that provides ‘enough’ performance, at a suitably low power, is often preferred.The higher the performance that a combined CPU+GPU piece of silicon, this arguably also reduces the market for graphics by taking away options at the low-end. If a simple APU can perform graphics duties of a $100 graphics card, then there is arguably no need for $100 graphics cards any more. We can compare what each GPU vendor has launched in the last few years for the ‘entry gaming market’ to confirm that the market below $100 is now for APUs and simple ‘must-have-a-screen’ cards for the pre-built market:Entry Gaming MarketAnandTechYearModelMSRPAMD GPUs300 Series2015R7 360 (2 GB)$109400 Series2016RX 460 (2 GB)$109500 Series2017RX 550 (2 GB)$79RX Vega Series2017RX Vega 56 (8 GB)$3995000 Series2019RX 5500XT (4 GB)$1696000 Series2020RX 6800 (16 GB)$579NVIDIA GPUs700 Series2014GTX 750 (1 GB)$119900 Series2015GTX 950 (2 GB)$1591000 Series2016GTX 1050 (2 GB)$1091600 Series2019GTX 1650 (4 GB)$1492000 Series2019RTX 2060 (6 GB)$3003000 Series2020RTX 3060 Ti (8 GB)$399Perhaps surprisingly over the last couple of years, despite at one point AMD promoting its RX 480 card as a possible $200 gaming card, both companies are veering heavily towards the high-end gaming market, leaving the budget range for OEMs, and arguably also the mid-range as well. Both AMD and NVIDIA with the latest releases start at a relatively hefty $399 MSRP, which is a world away from the $200 suggested low-end price for the AMD RX 480 at launch. Part of this is driven by new gaming features like Ray Tracing, the fact that leading edge graphics tend to launch at the high-end first, as that is where the biggest return on investment is, and with the rise of high resolution gaming, 8 GB of video memory seems to be the new minimum, if not more, which drives up the total cost.So if APUs are there to bridge the gap, then we’re at a bit of a quandary. Intel has leading edge integrated graphics solutions with its latest Xe-LP Tiger Lake processors, however these are for mobile use only. In that market segment, a good performing chip has a better financial return than the same silicon used in a desktop socketable processor, and with Intel looking to drive mobile volume it is putting all that silicon for mobile use right now.This means that the only company taking socketed desktop graphics seriously right now is AMD, who is starting to use its mobile-first Renoir silicon for desktop processors. This involves moving the TDP from 15W/45W up to 65W, and putting it in an AM4 socket package, similar to what AMD has done with its previous APU silicon. But now we get onto a specific issue with AMD’s Ryzen 4000 desktop APUs.Ryzen 4000 Desktop APUs: Not for General SaleThat’s correct – the Ryzen 4000 desktop APUs from AMD are not available at retail. While AMD announced twelve different model numbers for the latest generation, varying in core count, graphics count, and power, the company has decided not to create special retail packaging and offer them for general consumption.What AMD has done here is enable these products for two specific markets. Companies like HP, Dell and Lenovo can order these processors from AMD and put them into pre-built systems for consumers like you and I, or they can order the Ryzen PRO versions and build commercial systems with extra management features for corporate management.By enabling these processors only in pre-built and commercial systems, this allows AMD to have a tighter control on its stock of processors. These companies purchase processors on the scale of tens of thousands, so if a big OEM like HP wants to create a series of pre-built computers, they can put the order in with AMD and AMD will give HP a delivery date. If a product is sold on the open market, then AMD has to work with distribution channels dealing with a scale of tens of units, rather than thousands, making the operation more complex with stock potentially either sitting idle, or not being available if they cannot manufacture enough.By keeping this hardware as OEM only, AMD can adjust its silicon between desktop and mobile as required with much tighter controls. This is important for a company if the same product in one market (e.g. this silicon in mobile) is worth more than the other, as it focuses the silicon in the mobile market while also meeting contractual demands on the desktop side. Reports of AMD needing more 7nm wafers from TSMC could also play into this, as AMD would rather use those wafers for higher margin products.So given all this, why test these processors at all? Well the truth is end-users can actually buy them. But it is not as easy as putting an order in at Amazon.AMD calls its retail product line as PIBs, or ‘product in box’. These parts have a consumer warranty attached, fancy packaging to draw you in, and usually a cooler depending on the product. The other type, which it sells to HP and Dell, is more for business-to-business (B2B) sales, and these processors are called ‘tray’ or ‘OEM’ products. Here AMD just sells the CPU with a basic B2B warranty, no packaging, no cooler. If you are an OEM like Dell, you don’t want to be opening 10000 packages to build 10000 systems, so these processors just come in a tray and that is that.Retailers that sell CPUs to general consumers will almost certainly carry PIBs. But some retailers, especially those that also make their own pre-built systems, will sell the tray versions as well.These are sold as CPU only, in a protective case, without a cooler, and often only a limited warranty solely with the retailer (usually 1 year). Stock of these OEM processors is often very transient day-to-day, and some of the bigger retailers will often include third-party sales of these processors as well. It should be noted that direct-to-consumer sales of OEM-style processors tends to be more prevalent in Eastern Europe and Russia than in North America, from personal experience.Ultimately this is how we sourced these APUs for this review.How We Acquired the 65 W Ryzen 4000 (Pro) APUsAMD was not sampling Ryzen 4000 APUs for review, and so we had to scour the internet for a system builder that was also selling the individual hardware. The other alternative was to buy three distinct pre-built systems, but we found a UK retailer that was prepared to sell the processors on their own direct to consumers. Actually we had to fudge it a little bit. Time for a story.I found a retailer that listed all three processors as ‘awaiting stock’, and all three had dates about a week apart from each other. I could not pre-order them, but I could add them to my basket. I had to wait for stock to arrive before putting in an order. As the first one was enabled on the website, I put in the order for the Ryzen 5 Pro 4650G, and it arrived next day. As soon as I made the order, I put the next one in my basket. One down, two to go, and the other two were expected to arrive over the next two weeks. I kept checking the website daily to ensure that the ETA was consistent – I even emailed the company to confirm the dates. When the second processor was expected to go into stock, I loaded up my basket to see the Ryzen 3 Pro 4350G was no longer there.I moved on over to the product page, where it was listed as in stock, but the add-to-basket button had been disabled. I was somewhat confused as to what was going on – perhaps AMD had asked them to stop selling the hardware direct to consumers, and to only use it for pre-built systems? I have no idea as to the real reason, but what comes next was an interesting element of trickery.I went through the website source code to see how items were added to the basket, and noticed that each ‘add-to-basket’ button had an ID related to the stock item. I found the stock item for the Ryzen 3, and adjusted the add-to-basket button of the Threadripper 3990X to point to the Ryzen 3. After a few tries where it didn’t seem to work, it finally did! I had a Ryzen 3 Pro 4350G in my basket. I put in the order, no issues there, and off it went. It arrived next day, and the stock count listed on the website went down by one. The add-to-basket button was still disabled, and I wondered if the retailer had just suspected that I had one in my basket all along and just went along with it.So a week later the Ryzen 7 Pro 4750G was expected to be in stock. Again, I was checking it daily to see the ETA slowly count down. The day when the stock was supposed to arrive, the whole product page had vanished. All the product pages for the Ryzen 4000 APUs had vanished. What in the world was going on?I decided to put my previous plan into action a second time – could I modify the add-to-basket product ID to point to the Ryzen 7 Pro 4750G to get it in the basket? Then here was a second problem – I didn’t know the ID for the processor. The basket ID for each product was different to the URL ID, so I had to do some guess work based on the previous two IDs that I had used for the Ryzen 5 and Ryzen 3. It wasn’t as straight forward as the products being sequential, and as mentioned before, trying to get the button to work properly was a bit hit-and-miss.It took about 10 minutes, and I added a wide variety of processors to my basket, but I did finally get the 4750G in there. It was listed as in stock, for next day delivery. I clicked purchase, handed over my details, and it arrived the next day. There was no questioning from the retailer as to how I put in an order. Clearly a sale is a sale, right?Now I’m not expecting users to go out and have to work out how their retailer’s website works in order to buy these APUs. The hardware has been out long enough now that there are a number of third-party sellers on leading etailers offering these APUs at a variety of prices. These sellers seem to be focused in the Hong Kong region, which means warranty might be an issue, and shipping import taxes might be a part of bringing it into your country. Some of the sellers have dodgy ratings too. But they are out there, in larger numbers than before.The AMD Desktop Ryzen 4000 OfferingsAs mentioned, AMD launched twelve desktop Ryzen 4000 processors in the family. These were split into six for Ryzen PRO and six not-for-Pro, and in each of those six, three were for 65W and three were for 35W. In each set of three was a Ryzen 7, a Ryzen 5, and a Ryzen 3. AMD is covering all the bases with these parts.AMD Ryzen 4000G Series APUsAnandTechCore /ThreadBaseFreqTurboFreqGPUCUsGPUFreqPCIe*TDPRyzen 4000GRyzen 7 4700G8 / 16360044008210016+4+465 WRyzen 7 4700GE8 / 16310043008200016+4+435 WRyzen 5 4600G6 / 12370042007190016+4+465 WRyzen 5 4600GE6 / 12330042007190016+4+435 WRyzen 3 4300G4 / 8380040006170016+4+465 WRyzen 3 4300GE4 / 8350040006170016+4+435 WRyzen Pro 4000GRyzen 7 Pro 4750G8 / 16360044008210016+4+465 WRyzen 7 Pro 4750GE8 / 16310043008200016+4+435 WRyzen 5 Pro 4650G6 / 12370042007190016+4+465 WRyzen 5 Pro 4650GE6 / 12330042007190016+4+435 WRyzen 3 Pro 4350G4 / 8380040006170016+4+465 WRyzen 3 Pro 4350GE4 / 8350040006170016+4+435 W*PCIe lanes on the SoC are listed in GFX+Chipset+StorageThe top of the line is the Ryzen 7 4700G, with eight Zen 2 cores, sixteen threads, and Vega 8 graphics. This processor has a base frequency of 3.6 GHz, a turbo frequency of 4.4 GHz, and a peak graphics frequency of 2100 MHz. This is a substantial graphics frequency jump over the previous generation halo desktop APU, which ran Vega 11 graphics only at 1450 MHz. AMD puts this down to both the advantages of 7nm, but also physical design optimizations of the Vega graphics, providing a better gen-on-gen improvement than expected, which also enables a smaller graphics package which is better fed by the Zen 2 cores.At the lower end is the Ryzen 3 4300G, with four cores and eight threads, with a base of 3.8 GHz and a turbo of 4.0 GHz, which should mean that performance is very consistent. This part has six compute units for graphics, running at 1700 MHz.Every 4000G processor at 65 W has a GE counterpart at 35 W, which for the most part reduces the base frequency and TDP only. The exception is the Ryzen 7, where 100 MHz is lost on turbo and 100 MHz is lost on graphics. All the Ryzen non-Pro hardware has a Pro version equivalent.All of the processors support DDR4-3200 memory, and have 16x PCIe 3.0 lanes for graphics, 4x PCIe 3.0 lanes for storage, and 4x PCIe 3.0 lanes to connect to the chipset. These are PCIe 3.0 connections primarily on the basis of power – this is the same silicon that goes into 15 W mobile processors, and the power draw of PCIe 4.0 would have been too high, so AMD only enabled these processors with a PCIe 3.0 controller.For this review, we sourced all three of the Ryzen Pro 65 W processors.Desktop Discrete Graphics vs Integrated GraphicsDue to the difficulty in obtaining these processors, I would assume that anyone obtaining them will be using the integrated graphics in order to get the most out of their purchase. These processors still have 16x PCIe 3.0 lanes for graphics, which means we could stick in a discrete GPU if we wanted. As part of this review, we will test both, if only to see where a Renoir APU would fit if it had access to a full-blown directly connected discrete graphics card.It is worth noting that AMD has made a big fuss recently with its Zen 3 Ryzen 5000 CPUs, stating that having 32 MB of L3 cache available for each core as being a big improvement to discrete graphics. This is double that of the Zen 2-based Ryzen 4000 CPUs, which enable each core to have access to 16 MB of L3 cache. These Renoir APUs are hamstrung using the same dimension: each Zen 2 CPU core only has access to 4 MB of L3 cache. By contrast, the Renoir APUs are monolithic; the CPUs rely on a chiplet design, which adds latency. This was an AMD design choice, so it will be interesting to see how this works out for performance.The benchmark results are over the next few pages.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16308/testing-the-worlds-best-apus-desktop-amd-ryzen-4750g-4650g-and-4350g\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Investigating Performance of Multi-Threading on Zen 3 and AMD Ryzen 5000\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-12-03T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/16261/investigating-performance-of-multithreading-on-zen-3-and-amd-ryzen-5000\n",
      "Content: One of the stories around AMD’s initial generations of Zen processors was the effect of Simultaneous Multi-Threading (SMT) on performance. By running with this mode enabled, as is default in most situations, users saw significant performance rises in situations that could take advantage. The reasons for this performance increase rely on two competing factors: first, why is the core designed to be so underutilized by one thread, or second, the construction of an efficient SMT strategy in order to increase performance. In this review, we take a look at AMD’s latest Zen 3 architecture to observe the benefits of SMT.What is Simultaneous Multi-Threading (SMT)?We often consider each CPU core as being able to process one stream of serial instructions for whatever program is being run. Simultaneous Multi-Threading, or SMT, enables a processor to run two concurrent streams of instructions on the same processor core, sharing resources and optimizing potential downtime on one set of instructions by having a secondary set to come in and take advantage of the underutilization. Two of the limiting factors in most computing models are either compute or memory latency, and SMT is designed to interleave sets of instructions to optimize compute throughput while hiding memory latency.An old slide from Intel, which has its own marketing term for SMT: Hyper-ThreadingWhen SMT is enabled, depending on the processor, it will allow two, four, or eight threads to run on that core (we have seen some esoteric compute-in-memory solutions with 24 threads per core). Instructions from any thread are rearranged to be processed in the same cycle and keep utilization of the core resources high. Because multiple threads are used, this is known as extracting thread-level parallelism (TLP) from a workload, whereas a single thread with instructions that can run concurrently is instruction-level parallelism (ILP).Is SMT A Good Thing?It depends on who you ask.SMT2 (two threads per core) involves creating core structures sufficient to hold and manage two instruction streams, as well as managing how those core structures share resources. For example, if one particular buffer in your core design is meant to handle up to 64 instructions in a queue, if the average is lower than that (such as 40), then the buffer is underutilized, and an SMT design will enable the buffer is fed on average to the top. That buffer might be increased to 96 instructions in the design to account for this, ensuring that if both instruction streams are running at an ‘average’, then both will have sufficient headroom. This means two threads worth of use, for only 1.5 times the buffer size. If all else works out, then it is double the performance for less than double the core design in design area. But in ST mode, where most of that 96-wide buffer is less than 40% filled, because the whole buffer has to be powered on all the time, it might be wasting power.But, if a core design benefits from SMT, then perhaps the core hasn’t been designed optimally for a single thread of performance in the first place. If enabling SMT gives a user exact double performance and perfect scaling across the board, as if there were two cores, then perhaps there is a direct issue with how the core is designed, from execution units to buffers to cache hierarchy. It has been known for users to complain that they only get a 5-10% gain in performance with SMT enabled, stating it doesn't work properly - this could just be because the core is designed better for ST. Similarly, stating that a +70% performance gain means that SMT is working well could be more of a signal to an unbalanced core design that wastes power.This is the dichotomy of Simultaneous Multi-Threading. If it works well, then a user gets extra performance. But if it works too well, perhaps this is indicative of a core not suited to a particular workload. The answer to the question ‘Is SMT a good thing?’ is more complicated than it appears at first glance.We can split up the systems that use SMT:High-performance x86 from IntelHigh-performance x86 from AMDHigh-performance POWER/z from IBMSome High-Performance Arm-based designsHigh-Performance Compute-In-Memory DesignsHigh-Performance AI HardwareComparing to those that do not:High-efficiency x86 from IntelAll smartphone-class Arm processorsSuccessful High-Performance Arm-based designsHighly focused HPC workloads on x86 with compute bottlenecks(Note that Intel calls its SMT implementation ‘HyperThreading’, which is a marketing term specifically for Intel).At this point, we've only been discussing SMT where we have two threads per core, known as SMT2. Some of the more esoteric hardware designs go beyond two threads-per-core based SMT, and use up to eight. You will see this stylized in documentation as SMT8, compared to SMT2 or SMT4. This is how IBM approaches some of its designs.Some compute-in-memory applications go as far as SMT24!!There is a clear trend between SMT-enabled systems and no-SMT systems, and that seems to be the marker of high-performance. The one exception to that is the recent Apple M1 processor and the Firestorm cores.It should be noted that for systems that do support SMT, it can be disabled to force it down to one thread per core, to run in SMT1 mode. This has a few major benefits:It enables each thread to have access to a full core worth of resources. In some workload situations, having two threads on the same core will mean sharing of resources, and cause additional unintended latency, which may be important for latency critical workloads where deterministic (the same) performance is required. It also reduces the number of threads competing for L3 capacity, should that be a limiting factor. Also should any software be required to probe every other workflow for data, for a 16-core processor like the 5950X that means only reaching out to 15 other threads rather than 31 other threads, reducing potential crosstalk limited by core-to-core connectivity.The other aspect is power. With a single thread on a core and no other thread to jump in if resources are underutilized, when there is a delay caused by pulling something from main memory, then the power of the core would be lower, providing budget for other cores to ramp up in frequency. This is a bit of a double-edged sword if the core is still at a high voltage while waiting for data in an SMT disabled mode. SMT in this way can help improve performance per Watt, assuming that enabling SMT doesn’t cause competition for resources and arguably longer stalls waiting for data.Mission critical enterprise workloads that require deterministic performance, and some HPC codes that require large amounts of memory per thread often disable SMT on their deployed systems. Consumer workloads are often not as critical (at least in terms of scale and $$$), and so the topic isn’t often covered in detail.Most modern processors, when in SMT-enabled mode, if they are running a single instruction stream, will operate as if in SMT-off mode and have full access to resources. Some software takes advantage of this, spawning only one thread for each physical core on the system. Because core structures can be dynamically partitioned (adjusts resources for each thread while threads are in progress) or statically shared (adjusts before a workload starts), situations where the two threads on a core are creating their own bottleneck would benefit having only a single thread per core active. Knowing how a workload uses a core can help when designing software designed to make use of multiple cores.Here is an example of a Zen3 core, showing all the structures. One of the progress points with every new generation of hardware is to reduce the number of statically allocated structures within a core, as dynamic structures often give the best flexibility and peak performance. In the case of Zen3, only three structures are still statically partitioned: the store queue, the retire queue, and the micro-op queue. This is the same as Zen2.SMT on AMD Zen3 and Ryzen 5000So much like AMD’s previous Zen-based processors, the Ryzen 5000 series that uses Zen3 cores also have an SMT2 design. By default this is enabled in every consumer BIOS, however users can choose to disable it through the firmware options.For this article, we have run our AMD Ryzen 5950X processor, a 16-core high-performance Zen3 processor, in both SMT Off and SMT On modes through our test suite and through some industry standard benchmarks. The goals of these tests are to ascertain the answers to the following questions:Is there a single-thread benefit to disabling SMT?How much performance increase does enabling SMT provide?Is there a change in performance per watt in enabling SMT?Does having SMT enabled result in a higher workload latency?**more important for enterprise/database/AI workloadsThe best argument for enabling SMT would be a No-Lots-Yes-No result. Conversely the best argument against SMT would be a Yes-None-No-Yes. But because the core structures were built with having SMT enabled in mind, the answers are rarely that clear.Test SystemFor our test suite, due to obtaining new 32 GB DDR4-3200 memory modules for Ryzen testing, we re-ran our standard test suite on the Ryzen 9 5950X with SMT On and SMT Off. As per our usual testing methodology, we test memory at official rated JEDEC specifications for each processor at hand.Test SetupAMD AM4Ryzen 9 5950XMSI X570Godlike1.B3T13AGESA 1100NoctuaNH-U12SADATA4x32 GBDDR4-3200GPUSapphire RX 460 2GB (CPU Tests)NVIDIA RTX 2080 TiPSUOCZ 1250W GoldSSDCrucial MX500 2TBOSWindows 10 x64 1909Spectre and Meltdown PatchedVRM Supplimented with Silversone SST-FHP141-VF 173 CFM fansAlso many thanks to the companies that have donated hardware for our test systems, including the following:Hardware Providers for CPU and Motherboard ReviewsSapphireRX 460 NitroNVIDIARTX 2080 TiCrucial SSDsCorsair PSUsG.Skill DDR4ADATADDR4-3200 32GBSilverstoneAncillariesNoctuaCoolers\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16261/investigating-performance-of-multithreading-on-zen-3-and-amd-ryzen-5000\n",
      "Title: Qualcomm Tech Summit 2020: Interview with Alex Katouzian\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-12-02T19:00:00Z\n",
      "URL: https://www.anandtech.com/show/16273/interview-with-alex-katouzian\n",
      "Content: Within today’s Qualcomm Tech Summit 2020, we’ve seen the announcement of the new Snapdragon 888 which we’vedetailed extensively in our dedicated coverage article.As part of the show, we’ve had the opportunity to interview Alex Katouzian, Qualcomm’s SVP and GM of the mobile, compute and infrastructure business – including Handsets, XR, Compute, Edge/AI Cloud, 5G/4G businesses.Similar to last year’sinterview, we were able to talk to Alex about this year’s new announcements, the 5G ecosystem, and the new technologies which enable these new generation products.Alex KatouzianQualcommAndrei FrumusanuAnandTechThe Snapdragon 888 - What's happened to dateAndrei F. : Let's first start with an introduction for the readers out there, which are going to be reading this during the launch event.Can you describe in a few brief sentences, what the new Snapdragon 888 is all about?Alex K. : So as you know, the whole Tech Summit is about showcasing our technology. And what happens, is this premium tier technology, including the modem, and the system solution, start to dictate what we do not only in all the other tiers of our products and for mobile, but actually a lot of the adjacent businesses that we're going after as well, they use all of this technology. In fact, I can't see any business that we have, whether it being IoT, wearables, automotive, or even cloud edge, mobile broadband PCs, or XR - all of them use almost the exact same technology across the board.And so this is the this premium tier announcement, like every year, which really dictates what we do across multiple businesses, and across multiple tiers of our products. Those features and those capabilities start to waterfall all the way down into into lower and lower tiers. I would say, somewhere in the neighborhood of three to six months, you see a lot of these features water falling down. So we're very, very excited about this.I think 5G now is very well established, there's like 40 countries, a hundred plus operators with 5G around. It is the case where, we introduced the new chipset and the end-unit solutions before all the networks were ready to go.It's a really good thing to have, because now many of the service providers, so many of the app developers, so many of the ISVs, they have the vehicles to start to develop stuff from the ground up for 5G.I think we have we're expecting somewhere in the neighborhood of, about 180 million to maybe 220 million 5G phones coming through in 2020, and then, about 450 to maybe 550 million units of 5G devices in 2021.I think I think China is going full speed ahead. Big OEMs like Samsung, Apple, LG, Motorola, Vivo, Oppo and Xiaomi. I don't know what's going to happen to to to Huawei, it's still up in the air. But I don't know if you saw the news - there was a recent sale of their sub-brand called Honor. So, Honor is now like a company owned by the Shenzen government. And so they're gonna start to get into the market, they want to get active in 5G very fast. So I think we have super amounts of momentum.Getting back to your question on the Snapdragon 888. It is really probably the the most advanced part we've done, with the most advanced technologies across the board.Including 5G, we have a third generation 5G modem going inside that. We have multiple different carrier aggregation modes between mmWave and Sub-6 between TDD and FDD.We have, if I look at the pillars, if I look at AI, we have a 26 TOPS AI engine that we're putting in, this is the sixth generation engine. We have a tensor processor really designed from the ground up for for mobile AI capabilities embedded in there.We have our graphics core, that is latest graphics core where we're seeing huge improvement from the 865 mobile processor that came out last year. Our elite gaming is advancing in a big way. We have multiple different partners that we're working with now.AF: So because this is your third generation 5G product, and you could pretty much say that 2020 has been the year where it's gone mainstream, have you gained any additional learnings this year?It has been such a wide deployment across so many companies, so many countries. What have you actually improved this year, which might be new, compared to the previous two generations?AK: So absolutely, I think we've done a hell of a lot more testing with the infrastructure vendors. You know, we've been able to optimize power on multiple different use cases. We've been able to, like I said, work with so many service providers try to figure out how to have their services be built with 5G in mind versus and agnostic approach.The handsets have gotten more slick. Power dissipation has grown and battery life has gotten longer. You know, there were some reports, if you remember, in the beginning, like the phones would heat up - none of that stuff is an issue anymore, in our opinion.AF: So on that point, I remember early this year when the first 5G devices came out, especially in the US, those with the mmWave, that there was some mixed reaction. I mean, the speeds were fantastic. But also many people said that the devices heating up, that battery life is not as good. Is that something solved in this generation?Absolutely solved. And let me tell you what the problem was. They would take the phones, and they would run full speed mmWave wave for two hours straight on something. Of course, the phone would heat up, you know, even even running a game without mmWave, if you're going for like two hours straight, full blast, the phone heats up.So I think they put it through some extreme types of testing. But now, we have optimizations across the networks that have been put up, we have multiple different networks that we're working with. We have developed different methods by which, we can reduce not only the die area of the modem itself, but then methods to try to figure out how to optimize all the channel optimizations and all the data optimizations that we're going through. All of those things are big learnings for us over the past two years,AF: For networks where you have both mmWave wave and sub-6. How do you work to balance out the traffic between the two the two sides of the network? Is that something which you have to work closely with the carrier, or is that something which you on your site can control more?AK: Well, both. So so for example, the carrier lays out their networks, to try to figure out how to best serve the most amount of traffic. When you go into dense urban areas, that's where mmWave kicks in the most, because they want to have the maximum amount of bandwidth available to most amount of people. And so, based on their layouts, and based on their maps, we can figure out with the infrastructure vendor and the carrier how to switch over from one to the other.AF: Okay, but what I mean is for example, if you're browsing your Twitter, maybe you don't need the bandwidth from mmWave, do you have some smart mechanisms to direct the traffic over to Sub-6 or LTE in those use cases?AK: Not particularly from Qualcomm, but we do work with the carriers to try to accommodate what they want to do. And if there's, if there's any technical issues with the infrastructure, we work with the vendors there to figure that out.AF: But the chipsets themselves would be able to support such a thing, right?AK: Correct. Yeah. Anytime we get a signal from the carrier to switch to something else we we accommodate right away.Manufacturing Technologies and Moore's LawAF: So you mentioned the 888 represents the pinnacle of the newest technologies. One big aspect of that is the new process node, the new chip is your first flagship SoC on an EUV node. I know last generation, you had the 765 on EUV, but this one is like the big prestigious design.In the past, we talked about process nodes, and how Moore's Law is slowing down. How do you see, in the broad sense of terms, how the technology is evolving? Is it getting harder, is getting more expensive? What's your view?AK: Very good question. It is slowing down. And by when I say slowing down, we are also slowing down in moving into advanced nodes. And the reason for it is exactly what you described; Moore's Law is not as it was before, you're not getting the shrinks as you did before, you do get some power dissipation advantages, and you get some shrink advantages, but not as big returns as is adding functionality every year. And the functionality gets added every year.So here's the cool part. When you have a process technology, and it's leading edge, what happens is when you try to squeeze the maximum amount of performance out of it, and you try to maximize your power dissipation, your yields in the beginning are not very good. And so what happens is, you're on a learning curve on these yields. It looks like a bell shaped curve. So, as the bell shaped curve tightens up, you can increase performance and decrease power.But every process, nowadays, we, and fabrication facilities like TSMC, and Samsung and others, we can squeeze those curves down over like a two to three year period.So a process, when it first comes out, it's great, because it gives you some advantages, but it's not at its maximum capability. So over two or three years, they introduce best known methods, better transistor capabilitiees, they're just slightly better transistor capabilities, but you're not actually shrinking the transistor.So our designs start to flow with that. So maybe you'll see one or two generations at least of a similar process. So like 5 nanometer is not exactly the same as 7 nanometre, but it reuses a lot of the technology learning curves of 7 into 5. So not only is the transistor getting slightly smaller, but we're squeezing more and more performance out of it. And so, as the migration from process to process starts to slow down, we also learn how to get better performance and better power out of the same, or very similar process.AF: At which point do you see that the return on investment wouldn't be enough to just rely on process improvements, and when do you go to look into more exotic solutions, things like new packaging technology, or disaggregated designs like chiplets? Is that something which is on the radar for the mobile? Or do you still try to keep everything together in a monolithic design because of power efficiency?AK: Very good question. Look, I think we have a few years. And the reason for it is, let me give you an analogy, you know, how you put up solar panels. Say you have 100% efficiency, or you're close to whatever you're spending. You could add more, but you get diminishing returns. On the process technology, I think we have a few years left until we say that we'll no longer going to integrate an SoC, with application processor and modem and a bunch of other things.Obviously, the die area starts to grow. But the shrinking of the transistor can't keep up as much as the functionality is growing. But, like I said, the efficiency of reusing a process or a tweaked process is also yielding us quite a bit of capability. So I think we have a few years before we have to think about separating the die in such a way that it makes sense for us to partition.The other thing you have to take into consideration is, you're talking about a heterogeneous system on a chip, which means parts of parts of the die depend on other parts of the die to operate. So it's not as simple as saying, I'm going to cut graphics out and put it someplace else, or I'm just going to cut CPU and put it someplace else. We are in the process of looking at that - I would say in the next three or four years, our architecture and partitioning will change. But I think we have a few years before that happens.AF: This generation, you're using the Cortex-X1 which is basically like a new category within Arm's CPU IP offering. From your side, in terms of a product segmentation and marketing aspect, does that allow you to go higher up in product range?We obviously might not see it go into lower-range Snapdragon. How do you see that differentiation with new CPU technology?AK: I think that core does help, and especially on the premium tier. Let me also explain; the premium tiers life doesn't end in the first year. So, people use that, people look to use those premium tier parts in the following year for designs that are inbetween high tier and premium tier.The feedback from our customers in our ecosystem has been consistent, there are four or five technologies that they hone in on: CPU, GPU, camera, AI, modem. But, top of that list it's always, always CPU and GPU. And so, the performance allows you to extend the life of those designs, and it helps you set a precedent. So it absolutely helps us, and it also helps us for the length of the life of the device.Snapdragon on Windows and Apple's M1AF: Moving on from mobile to the laptop side of Snapdragon, such as last year's 8CX.It's been a year now, and pick-up and adoption hasn't been that great. We haven't seen that many designs, the ones that are available aren't that popular. What's your view of that segment, and is Qualcomm as motivated as you were one year ago in terms of engaging that ecosystem?AK: For sure one hundred percent behind it. Let me let me also explain - the laptops these days are really moving towards mobile. The camera is super important. The audio is super important. The battery life is super important. Not having a fan is super important. Portability, thinness, connectivity, always-on always-connected, all those traits of mobile are moving to the PC.And people say, imitation is the best form of flattery. Look at look what happened with the [Apple] M1. Their product pitch is almost a duplicate of what we've been saying for the past two or three years.It's almost exactly the same; like I have an SoC, and you don't have to have anything on the outside, everything's integrated, multimedia capability, AI capability, camera capability, you know, great battery life, the whole thing.And now, I think the momentum of having so many different apps, and so many different capabilities running on an Arm-based solution that has multiple traits associated with it, and low power capability, and being able to do video conferencing and have great audio clarity.I'm talking to you from a Samsung Book S (with 8CX processor), I've been using it for a year and a half, one of the best laptops ever had. You're correct in saying that the consumer hasn't yet caught up. Enterprises are catching up. Our channel push has been stronger than ever, carriers are starting to realize that this is a good thing to have.And then here's something else that really is going to catch. We're going to have small cells based on 5G going inside enterprises, we're going to have access points that have 5G backhaul going inside of enterprises. Having the capability of that 5G PC with the small cells to go into an enterprise is a very, very attractive formula for all of the carriers. So that channel is going to start to open up, Microsoft is 100% behind us as a partner to try to get all of the apps, all of the 64 bit emulations up and running and resolve all these issues. And on top of that, there's Apple who's now in the market and everyone wants to react to it.AF: And so do you think that Apple's announcement now has pushed an ecosystem forward to be more motivated? There's kind of a few things happening at the same time right now, such as Microsoft bringing 64 bit emulation to Windows, and that's going to open up a huge swath of software compatibility. Do these different things coming in at the same, do you think that it's going to pick up the ecosystem for Windows on Snapdragon?AK: Yes, huge momentum, huge momentum. And the important thing is we're continuing to put together roadmaps of devices that are going to be competitive in the market, they're going to be higher performance, they're going to be better and better every year. And you will see our upcoming SoCs in the future.AF: So you're saying every year, but you're still on a different cadence than on the mobile SoC side, because the 8CX Gen2, the refresh, it's the same chip, right? So you're on a different cadence - when would we see something way higher performance to enable more use cases?AK: I can't tell you exact dates, but I can tell you, it's coming.And, and I'll say this, too. We are 100% dedicated to this market. I think Microsoft is 100% dedicated to us, to make sure that this is going to happen. And I think that, you have a $2 trillion company coming into the market and saying, this is the way to go.We're only $170 billion, but you know, it helps to have a $2 trillion company saying, \"Yeah, this is the right thing to do\". And guess what, we're inundated with calls to make sure that this is going to happen. So we're 100% behind this stuff.AF: Speaking of Microsoft, when do you see you might be designing SoCs with the new security IP from Microsoft,Pluton?AK: We're definitely working with them on that. I can't predict, but you know, our working relationship with them is super tight. So we're working with them on that, but I can't predict to you when that's going to happen.AF: On the x86-64 emulation side, is there some collaboration you have with them in terms of optimizing the performance for your chips?AK: 100%. And you saw SQ1 and SQ2, we work with them super tightly. Even though everything is based on Qualcomm technology, our optimizations with them is second to none. Our working relationship with them, and trying to figure out how we fit our hardware layers in their stack, and how the APIs are optimized that way, to present a full system solution to a consumer. And I think it's gonna get even better from now.AF: So, from your view, what's the biggest growth market? On one side, we have 5G. On the other side, we have this upcoming Windows on Snapdragon market, maybe, hopefully, that's going to pick up way more now. What's the biggest path for opportunity right now?Well, I think the PC market for us as supplementary market. Because if I can't grow in mobile, and show technologies in mobile, then I think the PC side will suffer. So I think volume wise, hundred percent of the mobile market is the biggest market by far.But I think, the most lucrative part of the mobile market, just like almost any other market is, in fact, the premium tier and the high tier. As technologies become better and better, that high tier part is also getting better and better. Like, the high tier part we have this year is better than our premium part tier we had two years ago. So that improvement by itself is kicking it up a notch. And you know, those high tier equivalent type of devices, they're also going to lower tier PCs.So I look at the PC market as a supplementary market. And think of it this way, if I can capture 10% of that market, 15% of that market, it's still a small player, but it's a huge growth for us. So I definitely see the opportunity in both, but absolutely the mobile market is the bigger one.AF: What about on the low end side? How cheap can we get 5G devices in 2021? Are we going to see a $250 5G phone?AK: Absolutely. $250 is is definitely a target. If you look at the 400-series part that we announced with 5G - one hundred percent going into that market. I also think you're probably gonna see $300 PCs with LTE.AF: $300 PCs with LTE? That would be quite something.AK: Education type PCs, and, you know, lower lower end, affordable PCs. Yes.AF: Last question, what happened to the Snapdragon 875? Explain that!AK: (laughs) Look, the number eight is the representation for premium tier for the past 10 years, there's there's no doubt behind that.And that has gained so much momentum with consumers, just purely by marketing, by us, marketing by our OEM partners. So the eight has got a big attraction. And since we have, I think, probably the best part we've done this year, we wanted to, assign a premium tier number and name to it, and 888 is a good one.We thank Alex Katouzian and the Qualcomm team for their time and answers.Related Reading:Qualcomm Details The Snapdragon 888: 3rd Gen 5G & Cortex-X1 on 5nmQualcomm Tech Summit 2020: Day One LiveBlog (10:00 ET, 15:00 UTC)Qualcomm’s New 3rd Generation Snapdragon X60 5G Modem, Built on 5nmQualcomm Announces Snapdragon 865+: Breaking the 3GHz ThresholdThe Snapdragon 865 Performance Preview: Setting the Stage for Flagship Android 2020Qualcomm Announces Snapdragon 865 and 765(G): 5G For All in 2020, All The DetailsThe Snapdragon 855 Performance Preview: Setting the Stage for Flagship Android 2019The Qualcomm Snapdragon 855 Pre-Dive: Going Into Detail on 2019's Flagship Android SoC\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16273/interview-with-alex-katouzian\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Qualcomm Details The Snapdragon 888: 3rd Gen 5G & Cortex-X1 on 5nm\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-12-02T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/16271/qualcomm-snapdragon-888-deep-dive\n",
      "Content: This year although we’re not reporting from Hawaii, Qualcomm’s Tech Summit is still happening in digital form, representing the company’s most important launch event of the year as it showcases the new flagship products that will power next year’s smartphones. Qualcomm yesterday announced the new Snapdragon 888 SoC and platform, and today we’re going in-depth into the specifications and features of the new silicon design.The Snapdragon 888 is a big leap for Qualcomm, so much so that they’ve veered off from their usual naming scheme increments this generation and even skipped the 87x series altogether. The 888 number is not there only for marketing purposes as it represents fortune and luck in Chinese, but the new SoC has some substantial generational changes that sets it apart from the usual yearly improvements of past.Featuring the first ever implementation of a Cortex-X1 CPU core as its performance engine, new Cortex-A78 cores for efficiency, a massive +35% boost in GPU performance, a totally new DSP/NPU IP redesigned from the ground up, triple camera ISPs, integrated 5G modem, all manufactured on a new 5nm process node, the new Snapdragon 888 touches and updates almost every part of the SoC design with significant uplifts in performance and capabilities. There is a lot to cover, so let’s go over the details piece by piece:Qualcomm Snapdragon Flagship SoCs 2020-2021SoCSnapdragon 865Snapdragon 888CPU1x Cortex-A77@2.84GHz 1x512KB pL23x Cortex-A77@2.42GHz 3x256KB pL24x Cortex-A55@ 1.80GHz 4x128KB pL24MB sL31xCortex-X1@ 2.84GHz 1x1024KB pL23xCortex-A78@ 2.42GHz 3x512KB pL24x Cortex-A55@ 1.80GHz 4x128KB pL24MB sL3GPUAdreno 650 @ 587 MHzAdreno 660@ ?MHz+35% perfDSP / NPUHexagon 69815 TOPS AI(Total CPU+GPU+HVX+Tensor)Hexagon 78026 TOPS AI(Total CPU+GPU+HVX+Tensor)MemoryController4x 16-bit CH@ 2133MHz LPDDR4X / 33.4GB/sor@ 2750MHz LPDDR5 / 44.0GB/s3MB system level cache4x 16-bit CH@3200MHz LPDDR5/ 51.2GB/s3MB system level cacheISP/CameraDual 14-bit Spectra 480 ISP1x 200MPor64MP with ZSLor2x 25MP with ZSL4K video & 64MP burst captureTriple 14-bit Spectra 580 ISP1x 200MP or 84MP with ZSLor64+25MP with ZSLor3x 28MP with ZSL4K video & 64MP burst captureEncode/Decode8K30 / 4K120 10-bit H.265Dolby Vision, HDR10+, HDR10, HLG720p960 infinite recording8K30 / 4K120 10-bit H.265Dolby Vision, HDR10+, HDR10, HLG720p960 infinite recordingIntegrated Modemnone(Paired withexternal X55only)(LTE Category 24/22)DL = 2500 Mbps7x20MHz CA, 1024-QAMUL = 316 Mbps3x20MHz CA, 256-QAM(5G NR Sub-6 + mmWave)DL = 7000 MbpsUL = 3000 MbpsX60 integrated(LTE Category 24/22)DL = 2500 Mbps7x20MHz CA, 1024-QAMUL = 316 Mbps3x20MHz CA, 256-QAM(5G NR Sub-6 + mmWave)DL = 7500 MbpsUL = 3000 MbpsMfc. ProcessTSMC7nm (N7P)Samsung5nm (5LPE)Re-integration of the 5G modem into the SoCThe most important aspect for this year’s design is the fact that Qualcomm is going back to an fully integrated modem design, contrastinglast year’s surprising choice of the Snapdragon 865 not containing any modem at alland having instead to rely on the external X55 modem.Last year’s rationale of going with an external modem was said to have been a practical one, stemming from the fact that 5G was still in its early stages and that many vendors had to make a lot of design efforts when designing their new handsets for 5G. A external 5G modem such as the X55 helped the 5G transition as it was available to vendors earlier than the Snapdragon 865 SoC itself, allowing them to design their RF systems before having access to the newest SoC.This year, the market has evolved and is more mature, and Qualcomm chose to re-integrate the modem into the same silicon die as the SoC. The new X60 modem subsystem is the company’s 3rdgeneration 5G design and brings new capabilities in terms of carrier aggregation and 5G frequency band interoperability.The platform’s reabsorption of the modem into the SoC die should signify better power efficiency, lower platform cost as well as lower PCB complexity for smartphone vendors.2020 certainly was the year that 5G became a mainstream feature amongst device vendors, with essentially everybody adopting the new standard into their flagship and even mid-range devices. The new X60 modem will further mature the 5G experience by providing more flexibility to network operators in terms of frequency band support.mmWave in particular has been a rather contentious aspect of 5G in 2020 as network deployments has been rather scarce and limited to US cities, with users reporting spotty reception with a larger impact on battery life. mmWave network expansion is progressing at a steady pace, and Qualcomm states that the new Snapdragon 888 platform completely solves the power efficiency concerns around mmWave usage. Hopefully 2021 will be the year where mmWave becomes a lot more useful and practical for users.Whilst mmWave is expected to still be relatively niche for the vast majority of users, Sub-6GHz will be the workhorse of 5G, and here we’re seeing rapid expansion and deployments in countries all over the world. The new X60’s modem capability of allowing for carrier aggregation between FDD (Frequency Division Duplex, dedicated frequency bands between upload & download) and TDD (Time Division Duplex, upload & download in the same frequency band) means that network carriers will be able to mix and match more available Sub-6GHz spectrum together for even greater bandwidth.DSS, or dynamic spectrum sharing, is also going to be a key technology enabling network operators to migrate existing LTE frequency bands to 5G NR dynamically based on the organic LTE/5G user demand – meaning that the frequency spectrum doesn’t need to be segregated for each technology, thus allowing more actual usable bandwidth for both types of users in the first few years and consumers switch over to 5G-capable handsets.Manufactured on Samsung 5nm / 5LPEThe new Snapdragon 888 is making the transition from 7nm to 5nm, but the new design doesn’t merely make a process shift, it’s also making a foundry shift. After being with TSMC for the 7nm generations of the Snapdragon 855 and Snapdragon 865, Qualcomm is now switching back to Samsung Foundry and their new 5LPE process node for the new Snapdragon 888.Qualcomm in recent years had been dual-sourcing from both TSMC and Samsung depending on the SoC design and product range, but in the high-end flagship SoC segment the company seems to have always chosen the technologically superior node as it had larger implications for the competitiveness of those parts. N7 and N7P were clear winning choices for the S855 and S865 as Samsung’s own 7LPP process was kind of late, and didn’t seem to be quite as good as TSMC’s variants. Qualcomm notably still used the 7LPP node on this year’s Snapdragon 765 SoC which has seen a lot of success in the premium range of device designs, however we had noted earlier in the year thatit didn’t appear to be nearly as efficient as the TSMC-manufactured flagship SoC.This year’s choice of switching back to a Samsung process for the flagship SoC seems to be a vote of confidence in the new process node- as otherwise Qualcomm likely wouldn’t have made the switch. Versus 7LPP, Samsung promises a 20% decrease in power consumption at the same performance, or a 10% increase in performance at the same power, together with a +-20% area reduction. How these figures will translate over to practical improvements for the new Snapdragon 888 remains to be seen.Another rationale for the foundry switch could be manufacturing capacity. As Apple is eating up a lot of TSMC’s early 5nm capacity with the A14 and M1, Qualcomm probably saw Samsung’s 5LPE as the safer choice this year as the new Snapdragon 888 may be manufactured in the newdedicated EUV V1 line at Hwaesong.It’ll be hard to gauge the process node switch for this generation as we don’t expect to see a similar design on TSMC’s 5nm node – unless MediaTek somehow has a new Cortex-X1 SoC in the pipeline for next year.Powered by Cortex-X1 and Cortex-A78 CPUsThe Snapdragon 888 is the first publicly announced SoC powered by the new Cortex-X1 and Cortex-A78 CPU IPs by Arm. The Cortex-X1 in particular is the first of a new generation of CPU IP by Arm that focuses on maximising performance at the cost of lesser power efficiency, while the Cortex-A78 being the same-generation design but which still prioritises a balance between performance, power and area.The new X1 core, based on Arm’s numbers, promised a +30% uplift in IPC over the last generation Cortex-A77 which was also deployed in the Snapdragon 865. Qualcomm advertises a 25% uplift over the Snapdragon 865, but that’s likely due to a few configuration differences on the part of the new Snapdragon 888 compared to Arm’s own internal figures.The S888 continues to use a 1+3+4 CPU setup this generation, with the big difference being that instead of using the same CPU IP with a different physical implementation, the new 1+3 big cores are actually of different microarchitectures.The “prime” performance core as Qualcomm likes to call it is the new Cortex-X1 design, clocking in at the same 2.84GHz as the Snapdragon 865’s prime core. The new core is configured with the maximum 1MB of L2 cache.What stood out for me during our briefing of the new chip is that the clock frequency of the new design isn’t all very aggressive at all. Qualcomm’s 25% performance boost is in comparison to the vanilla Snapdragon 865 which also came at the same frequency. Compared to the Snapdragon 865+ which clocks in at 3.09GHz, this performance advantage should reduce to only 13%, which is less impressive.Qualcomm’s 25% generational boost is also less than Arm’s advertised 30% as the new S888 continues to use a 4MB L3 cache for the CPU cluster, versus Arm’s envisioned 8MB configuration for a high-end 5nm SoC with the new X1 cores. Qualcomm explained to us that this was simply a balance between cost, implementation effort, and diminishing returns of a higher cache configuration design.What this all means is that there’s a high chance that the Snapdragon 888 won’t be holding the Android CPU performance crown next year if Samsung’s next-gen Exynos SoC is even a little more aggressive in terms of clocks or cache configurations.The high-performance X1 cores is joined by three Cortex-A78 cores clocking in at up to 2.4GHz, serving as the every-day workhorse CPUs for most computational tasks. In terms of cache, the new cores see their L2 doubled up from 256KB to 512KB.One aspect I was interested in finding out is whether the new design still continues Qualcomm choice of fitting all the big cores together on a single voltage plane, which oddly enough, also seems to be the case for the new Snapdragon 888. This means that while the X1 and A78 cores can run at different frequencies, they’re all powered by the minimum voltage of either operating frequency at any one time. Qualcomm explains that this is again a practical choice surrounding the design complexity of the power delivery system, particularly mentioning that the X1 core can take advantage of the increased capacitance available from the larger shared power plane. Whilst this has worked well for the Snapdragon 855 and 865, I wonder that given the new X1 core’s increased performance and dynamic range, if the company isn’t leaving further performance or efficiency gains on the table for the sake of lower power delivery design cost. It’ll be interesting to see how other SoC vendors tackle their X1 implementations.Finally, the big cores are again accompanied by four Cortex-A55 cores. This year the company yet again clocks them at 1.8GHz, which makes this the 4thgeneration SoC with an essentially identical configuration of little cores, which is a bit disappointing. Qualcomm can’t do much here as there’s simply a need for a new little core CPU IP, something which we’ll hopefully see released next year in 2021 for 2022 SoCs.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16271/qualcomm-snapdragon-888-deep-dive\n",
      "Title: The iPhone 12 & 12 Pro Review: New Design and Diminishing Returns\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-11-30T13:30:00Z\n",
      "URL: https://www.anandtech.com/show/16192/the-iphone-12-review\n",
      "Content: The new iPhone 12’s have been out for a while now, and while we’ve had our hands on them for a few weeks, Apple’snews bombardment of the new Apple Silicon announcementandrelease of new Apple M1 Mac deviceshas meant the iPhones have had to be put on the back burner for a little while.Having already covered Apple’snew A14 architecture in-depthin our coverage of the M1, it’s time to fill in the missing pieces for the actual new generation of iPhones.The new iPhone 12 generation of devices mark a new design restart for Apple, moving away from the design that had been started with the iPhone X in late 2017. Re-gaining the flat side-frame look that was originally found in past iPhone generations of the 4, 4S, 5 & 5S series, Apple is making the old new again.The new iPhone 12 series is also Apple’s widest range release ever, with a total of four new iPhones: the iPhone 12 mini, a new compact form-factor at the lower range, the iPhone 12, the “standard” iPhone part, and continuing to offer the Pro models in the form of the iPhone 12 Pro and 12 Pro Max. We’ll be focusing on the iPhone 12 and 12 Pro for today’s review.Apple iPhone 12 Series SpecificationsiPhone 12 miniiPhone 12iPhone 12 ProiPhone 12 Pro MaxSoCApple A14 Bionic2 × Firestorm4 × IcestormDRAM4GB6GBDisplay5.42\" OLED2340 x 1080625nits peak6.06\" OLED2532 x 1170625nits peak6.06\" OLED2532 x 1170800nits peak6.68\" OLED2778 x 1284800nits peakSizeHeight131.5 mm146.7 mm160.8 mmWidth64.2 mm71.5 mm78.1 mmDepth7.4 mm7.4 mm7.4 mmWeight135g164g189g228gBattery Life2227 mAh-12% videovs 112815 mAh+-0%vs 112815 mAh-5.6% videovs 11 Pro3687 mAh+-0%vs 11 Pro MaxWireless ChargingMagSafe Wireless Charging up to 15WQi Compatible (7.5W)Rear CamerasMain12MP 1.4µm26mm eq.f/1.6Optics OIS12MP 1.7µm26mm eq.f/1.6Sensor-shift OISTele-Photo-12MP52mm eq.f/2.0OIS12MP65mm eq.f/2.2OISUltra-Wide12MP13mm eq.f/2.4Front Camera12MPf/2.2Storage64GB128GB256GB128GB256GB512GBI/OApple LightningWireless (local)802.11axWi-Fi with MIMO + Bluetooth 5.0 + NFCCellular5G (sub‑6 GHz and mmWave**)Gigabit LTE with 4x4 MIMO and LAA**US models onlySplash, Water, Dust ResistanceIP68up to 6m, up to 30 minutesDual-SIMnano-SIM + eSIMLaunch Price64 GB:$699£699€809128 GB:$749£749€859256 GB:$849£849€97964 GB:$799£799€909128 GB:$849£849€959256 GB:$949£949€1079128 GB:$999£999€1159256 GB:$1099£1099€1279512 GB:$1299£1299€1509128 GB:$1099£1099€1259256 GB:$1199£1199€1379512 GB:$1399£1399€1609Starting with the innards, the new iPhone 12 series are powered by Apple’s new A14 SoC. The new chip is powered by two high performance cores and four power efficiency cores, as well as a 4-core GPU. We’ll be going into a bit more details on the SoC in a later page, but by now based on our coverage of the Apple Silicon M1, we should also be familiar with the capabilities of the smaller A14 sibling.In terms of DRAM, Apple fits the iPhone 12 mini and iPhone 12 with 4GB of LPDDR4X, whilst the Pro models are getting a larger 6GB pool. NAND storage this generation hasn’t changed all that much for the lower-tier models which are sticking to 64GB base, with configuration upgrades 128 or 256GB, however the Pro models do now start out with a 128GB base model, with larger configurations at 256 and 512GB.The big new feature of this year’s new iPhones is the 5G connectivity. Thanks to the usage of a Qualcomm sourced modem, Apple is now enabling 5G connectivity across its whole new range. It’s to be noted that for users in most countries, this still only means sub-6GHz 5G NR as mmWave antennas are only deployed in the US models. What’s also interesting is that it seems that these mmWave modules are designed by Apple themselves and not sourced from Qualcomm – which makes the new iPhones the first devices on the market to have such a non-Qualcomm antenna solution.In terms of design, the new iPhone 12 are a mix of the old and the new. What’s new on all new devices is their screens, with the “standard” sized models we’re reviewing here having 6.06” 2532 x 1170 OLED displays. What’s particularly interesting here is of course the fact that the iPhone 12 shared almost the same display specifications as the iPhone 12 Pro, something which couldn’t be said of last year’s iPhone 11 which still came with a lower resolution LCD display and a generally different form-factor as the iPhone 11 Pro.The new iPhone 12’s only difference to the 12 Pro in terms of screen specifications is that it doesn’t get as bright as the Pro model, being listed at 625 vs 800 nits peak brightness.On the back of the phones, although hard to notice on these white models, one thing of note besides the different camera setup is that the Pro model again comes with a frosted glass back cover whereas the regular iPhone 12 still uses a glossy glass finish.The new design is quite a bit of departure from the past 3 years of iPhones. Apple had noted that they’ve reduced the bezel of the screen while still maintaining a symmetric look on all the sides (besides the notch of course). This gives the visual impression that the new iPhone 12/12Pro is narrower than the iPhone 11 Pro, even though that’s not actually true – though it is a few millimeters taller.I’m still not too sure what to make of Apple’s decision to go back to a flat-edged frame as on older generation iPhones. To be honest the very first impression upon unboxing the new devices I had was that this was just a horrible design and a massive step backwards in terms of ergonomics. Although as I noted the new phone’s width isn’t wider than that of the iPhone 11 Pro, because it has right angle edges, it actually has a larger circumference compared to the rounded-off iPhones, and it translates into a larger-feeling device even though they’re the exact same form-factors.That first bad impression isn’t quite as prevalent after a few weeks of usage as you can still somehow get used to it, but as soon as I go back to the 11 Pro or another rounded frame phone it’s immediately striking how much better it feels in the hand.In terms of button layout, we’re seeing the same setup as previous generation iPhones, two volume buttons on the left side beneath a silent mode switch, with the power button on the right side – so nothing inherently new there. It’s interesting that even now several years after the first under-screen fingerprint sensors and quite mature and accurate implementations out be competing vendors that Apple still hasn’t attempted it on the iPhone line-up – I think many would like to see the return of TouchID in such a manner, at least as an option alongside FaceID.The iPhone 12 Pro comes with a steel frame with a special mirror finish, while the iPhone 12 comes in a matte aluminium frame build. Apple has been using steel frames for quite some time now with the iPhone X designs, but it hasn’t been quite as striking as the design here on the new 12 Pro series. It’s a highly subjective matter and many may feel that the steel frame is more premium, but I do vastly prefer the aluminium variant due to the fact that it’s nowhere near the fingerprint magnet – just looking at the 12 Pro I have here on the desk looks quite disgusting and messy while the 12 at least appears to be clean.Another big difference between the two phones is the fact that the steel frame (alongside the added camera) of the 12 Pro adds in another 25g to the weight of the phone to 189g vs 164g, a difference that is very much immediately noticeable.While I prefer the matte frame of the 12, the frosted glass on the 12 Pro is just simply much better and feels much more premium to the regular glossy finish on the 12 – again, because of fingerprints and dirt.For the new iPhone 12 mini, 12 and 12 Pro, the main camera doesn’t appear to change in terms of sensor versus the iPhone 11 series, although that’s still perfectly fine. It’s a 12MP sensor with 1.4µm pixels and full sensor dual-pixel coverage, however the new camera modules employ a larger f/1.6 aperture lens which allows for 27% more light.For the iPhone 12 mini and iPhone 12, the second camera module is the ultra-wide, which again appears to be the same as on the 11 series, featuring a 12MP sensor with an f/2.4 aperture and a large 13mm equivalent or 120° angle FOV. The novelties for this module this year lie on the software side of things with Apple now enabling various new features such as computational night mode on this camera as well.For the iPhone 12 Pro, the third camera module is a telephoto unit with again an apparently similar module to last year, a 12MP sensor on an 52mm equivalent (2x optical magnification) optics with f/2.0 and OIS.The Pro models also receive what Apple calls the LIDAR module, which is essentially a ToF sensor coupled with a structured IR light emitter, allowing for 3D depth sensing.The iPhone 12 Pro Max hasa more interesting camera setup, however we’ll be reviewing this at a later date.In general, my impression and design of the iPhone 12 and 12 Pro are two-fold, depending on the model.Starting off with the 12 Pro, I generally don’t like the new design as the right-angle frame edges and mirror finish are both not very ergonomic and also quite messy. It’s a highly subjective opinion but it just doesn’t do it for me at all, and I vastly prefer the 11 Pro over this, even with the larger screen bezels.Whilst I still don’t like the edges on the iPhone 12, because it’s a lighter phone and the general better feel of the matte aluminium, it’s actually the phone I prefer this generation. I would have liked the matte frosted glass on the back as well, but I guess you can’t have everything. What’s important for the iPhone 12 is that this year it’s major upgrade in terms of display compared to the iPhone 11, sporting a much higher resolution and also switching over from an LCD to an OLED. This was a major gripe of mine with the 11 and now the 12 essentially almost matches the display quality of the 11 Pro and 12 Pro devices, which is something that can’t be understated.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article already exists: https://www.anandtech.com/show/16192/the-iphone-12-review\n",
      "Title: The Corsair Gaming K100 RGB Keyboard Review: Optical-Mechanical Masterpiece\n",
      "Author: E. Fylladitakis\n",
      "Date Published: 2020-11-19T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16256/the-corsair-gaming-k100-rgb-optical-mechanical-keyboard-review\n",
      "Content: Corsair is a company that originally built its reputation on memory-related products, but they have long since diversified and established a significant presence in several other market segments over the past couple of decades, turning them into a globally renowned colossus of PC components and peripherals. Today, Corsair produces dozens of PC-related products, from RAM modules to CPU coolers and from headphones to chairs.One of their most successful product segments outside of their memory roots has been advanced gaming keyboards. The company was one of the first and most enthusiastic developers of modern, consumer-focused mechanical keyboards. Their exclusive deals with Cherry, the most reputable manufacturer of mechanical key switches, allowed Corsair to slightly outpace their competition.Over the last several years, innovation surrounding the PC keyboard market has been somewhat stagnant. There were hundreds of mechanical keyboards in the market but, aside from aesthetic improvements, we have seen very few advancements over the last few years. Most manufacturers were staying true to established solutions, developing keyboards with the same electronics and mechanical switches as the majority of their competitors, then basing the competitiveness of their keyboards on aesthetics, software, and value/price. So the market for mechanical keyboards is ripe for a shakeup – or at least a more significant advancement than we've seen in the past few years.For today's review we are taking a look at Corsair’s latest gaming keyboard, the K100 RGB. As its name suggests, it is the successor to the renowned K95 RGB, one of the better – and most expensive – gaming keyboards Corsair has ever released. And for this successor, Corsair isn't just putting on a new coat of paint and releasing the keyboard anew. Instead, the K100 RGB features new opto-mechanical switches and completely new electronics, making it a one-of-a-kind keyboard in today’s commodify keyboard market.Packaging and bundleWe received the K100 RGB Gaming keyboard in a thick, sturdy cardboard box. The artwork is dark with yellow accents and aesthetically focused on a picture of the keyboard itself, following the same theme as the rest of Corsair’s peripherals lineup.Inside the box, we found a basic manual and warranty leaflets, two sets of textured keycaps, and a plastic keycap puller. The extra ten keycaps are grey and have contoured, textured top surfaces that are supposed to assist tactile feedback while gaming. The first set is for FPS gamers and the second for MOBA gamers. Both sets are correspondingly contoured and textured. As a result, two keycaps, the W and the D, exist in both sets but have different contours.Corsair supplies a full-size wrist rest with the K100 RGB, with the company taking a much different approach than with any other keyboard they released to this date. The wrist rest is now padded, with a very soft textured synthetic fabric covering, and is magnetically attached to the keyboard. This magnetic coupling makes the insertion and removal of the wrist rest a seamless, split-second process, but is not strong enough for the wrist rest to stay in place if the keyboard is lifted or moved harshly. Its soft padded top is much more comfortable than most wrist rests that we have tried to this date but also is more susceptible to permanent damage, much like the scars our sample received during transport.The Corsair K100 RGB Optical-Mechanical Gaming KeyboardCorsair’s latest K100 RGB is similar to the K95 RGB Platinum that the company released a couple of years ago, at least as far as its size and number of macro keys are concerned. It is not externally identical to the older K95 RGB Platinum, but it is based on the same aesthetic design, featuring an anodized brushed aluminum chassis and having the keys attached directly to it, creating a floating keycaps effect. This design makes it very simple to clean, as a simple blow would remove most debris from the aluminum surface of the keyboard.Most of the keyboard remains visually the same. Corsair’s backlit logo is still at the top of the chassis, on a highly glossy strip. The designer also placed the indicator LEDs on that strip, which are completely hidden while the keyboard is unpowered. Even when the indicator lights are on, their lights are so subtle that will never catch anyone’s attention if they're not looking for them.The Corsair K100 RGB Optical-Mechanical is based on a standard 104 keys keyboard, but expanded with a six additional keys to the left. In a bold move, Corsair made the K100 RGB compliant with the standard ANSI layout, something they have not done for any of their mechanical keyboards in the past. The bottom row of the keyboard has a 6.5× Spacebar and seven 1.25× keys (ALT, CTRL, WIN/Menu, and Fn keys) bottom row keys. The PBT keycaps have large, futuristic characters printed on them. Both the primary and secondary characters are found at the top of the keycap, as the designer wanted to take full advantage of the keyboard’s LED lighting. The six G keys to the left of the main keyboard are grey and contoured, yet not textured like the extra ten keycaps that Corsair supplies for gaming.A metallic volume control wheel and a mute button can still be found at the right top side of the keyboard, with additional media buttons between them and the numeric keypad section. The volume control wheel is now wider and firmer, feeling better than every previous version of Corsair’s gaming keyboards.The major design difference lies at the top left corner of the keyboard, in the form of a second metallic wheel. This second wheel can be used to perform a variety of actions, such as application switching and LED brightness control, selectable via the tactile button in its center and programmable in Corsair’s iCUE software. As we will see while exploring the new options in Corsair’s software, the secondary wheel can dramatically improve the flexibility of the keyboard, be it for productivity or for fun. Two more tactile buttons, one for profile switching and one for locking specific keys, can be seen on either side of the rotary wheel.Beneath the keycaps we find Corsair’s latest OPX Optical-Mechanical switches. These switches are not mechanical switches that Corsair fiddled with in order to change their mechanical properties – these are optical switches that Corsair modified to operate similarly to mechanical switches. The only mechanical part of these switches is the return spring, as there are no electrical contacts at all. This approach eradicates all of the disadvantages that mechanical contacts have, such as debounce and wear, at the expense of tactile feedback that you can completely feel. The OPX switches actuate much faster than any mechanical switch, at just 1 mm below their resting point, and their total travel is also reduced down to 3.2 mm. It is worthwhile to note that the K100 RGB is also available with Cherry MX Speed switches, for people who prefer the classic tactile feeling.The underside of the K100 RGB is riddled with wire pathways, allowing users to route the wire of a mouse or headset underneath the keyboard. There are four very large anti-skid pads, firmly holding the keyboard still on any surface. The grip of the pads is so strong that the tilt feet are being forced to retract if someone tries to push the keyboard sideways while it is sitting on a desk.A single USB 2.0 port can be found at the rear of the K100 RGB. Meanwhile, not just one, but two USB connectors can be found at the tail end of the keyboard's connector cable. As it turns out, the keyboard's power requirements technically exceed the capacity of a USB 2.0 port, so if the keyboard isn't hooked up to a more powerful USB 3.0 (or later) port, Corsair needs to pull power from a second port.That said, the second connector also serves another purpose: driving the USB host port on the keyboard itself. Corsair has opted to implement a full pass-through solution (rather than using an internal USB hub), so the second USB connector needs to be inserted (regardless of power needs) in order to supply a connection to the host port. The final product is a bit trickier than it otherwise needs to be, but such is the case when ensuring the keyboard will be fully backwards-compatible with USB 2.0 hosts.Opening the K100 RGB Optical-Mechanical keyboard is going to cause permanent visual damage to it, as it was not designed to ever be disassembled by an end user. There are screws beneath the glossy top part of the keyboard that cannot be removed and reinstalled, as well as foam glue on its sides. Regardless, there are practically no serviceable parts inside, as the switches have no contacts and the optical sensors would require an expert to remove/replace them.On the white PCB, we find the heart of the K100 RGB, an NXP LPC54605J512 microprocessor. The ARM Cortex M4 microprocessor has a frequency of 180 MHz, as well as 512 kB of flash and 200 kB of SRAM embedded to it. This certainly is a much more powerful processor than the ones we are used to seeing in gaming keyboards, but unfortunately it doesn't lend itself to analysis on software level to assess how Corsair’s engineers implemented multi-threading with it.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16256/the-corsair-gaming-k100-rgb-optical-mechanical-keyboard-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The 2020 Mac Mini Unleashed: Putting Apple Silicon M1 To The Test\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-11-17T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested\n",
      "Content: Last week, Apple made industry news by announcing new Mac products based upon the company’s new Apple Silicon M1 SoC chip, marking the first move of a planned 2-year roadmap to transition over from Intel-based x86 CPUs to the company’s own in-house designed microprocessors running on the Arm instruction set.During the launch we had prepared an extensive article based on the company’s already related Apple A14 chip, found in the new generation iPhone 12 phones. This includes a rather extensive microarchitectural deep-dive into Apple’s new Firestorm cores which power both the A14 as well as the new Apple Silicon M1, I would recommend a read if you haven’t had the opportunity yet:Apple Announces The Apple Silicon M1:Ditching x86 - What to Expect, Based on A14Since a few days, we’ve been able to get our hands on one of the first Apple Silicon M1 devices: the new Mac mini 2020 edition. While in our analysis article last week we had based our numbers on the A14, this time around we’ve measured the real performance on the actual new higher-power design. We haven’t had much time, but we’ll be bringing you the key datapoints relevant to the new Apple Silicon M1.Apple Silicon M1: Firestorm cores at 3.2GHz & ~20-24W TDP?During the launch event, one thing that was in Apple fashion typically missing from the presentation were actual details on the clock frequencies of the design, as well as its TDP which it can sustain at maximum performance.We can confirm that in single-threaded workloads, Apple’s Firestorm cores now clock in at 3.2GHz, a 6.66% increase over the 3GHz frequency of the Apple A14. As long as there's thermal headroom, this clock also applies to all-core loads, with in addition to 4x 3.2GHz performance cores also seeing 4x Thunder efficiency cores at 2064MHz, also quite a lot higher than 1823MHz on the A14.Alongside the four performance Firestorm cores, the M1 also includes four Icestorm cores which are aimed for low idle power and increased power efficiency for battery-powered operation. Both the 4 performance cores and 4 efficiency cores can be active in tandem, meaning that this is an 8-core SoC, although performance throughput across all the cores isn’t identical.The biggest question during the announcement event was the power consumption of these designs. Apple had presented several charts including performance and power axes, however we lacked comparison data as to come to any proper conclusion.As we had access to the Mac mini rather than a Macbook, it meant that power measurement was rather simple on the device as we can just hook up a meter to the AC input of the device. It’s to be noted with a huge disclaimer that because we are measuring AC wall power here, the power figures aren’t directly comparable to that of battery-powered devices, as the Mac mini’s power supply will incur a efficiency loss greater than that of other mobile SoCs, as well as TDP figures contemporary vendors such as Intel or AMD publish.It’s especially important to keep in mind that the figure of what we usually recall as TDP in processors is actually only a subset of the figures presented here, as beyond just the SoC we’re also measuring DRAM and voltage regulation overhead, something which is not included in TDP figures nor your typical package power readout on a laptop.Starting off with an idle Mac mini in its default state while sitting idle when powered on, while connected via HDMI to a 2560p144 monitor, Wi-Fi 6 and a mouse and keyboard, we’re seeing total device power at 4.2W. Given that we’re measuring AC power into the device which can be quite inefficient at low loads, this makes quite a lot of sense and represents an excellent figure.This idle figure also serves as a baseline for following measurements where we calculate “active power”, meaning our usual methodology of taking total power measured and subtracting the idle power.During average single-threaded workloads on the 3.2GHz Firestorm cores, such as GCC code compilation, we’re seeing device power go up to 10.5W with active power at around 6.3W. The active power figure is very much in line with what we would expect from a higher-clocked Firestorm core, and is extremely promising for Apple and the M1.In workloads which are more DRAM heavy and thus incur a larger power penalty on the LPDDR4X-class 128-bit 16GB of DRAM on the Mac mini, we’re seeing active power go up to 10.5W. Already with these figures the new M1 is might impressive and showcases less than a third of the power of a high-end Intel mobile CPU.In multi-threaded scenarios, power highly depends on the workload. In memory-heavy workloads where the CPU utilisation isn’t as high, we’re seeing 18W active power, going up to around 22W in average workloads, and peaking around 27W in compute heavy workloads. These figures are generally what you’d like to compare to “TDPs” of other platforms, although again to get an apples-to-apples comparison you’d need to further subtract some of the overhead as measured on the Mac mini here – my best guess would be a 20 to 24W range.Finally, on the part of the GPU, we’re seeing a lower power consumption figure of 17.3W in GFXBench Aztec High. This would contain a larger amount of DRAM power, so the power consumption of Apple’s GPU is definitely extremely low-power, and far less than the peak power that the CPUs can draw.Memory DifferencesBesides the additional cores on the part of the CPUs and GPU, one main performance factor of the M1 that differs from the A14 is the fact that’s it’s running on a 128-bit memory bus rather than the mobile 64-bit bus. Across 8x 16-bit memory channels and at LPDDR4X-4266-class memory, this means the M1 hits a peak of 68.25GB/s memory bandwidth.In terms of memory latency, we’re seeing a (rather expected) reduction compared to the A14, measuring 96ns at 128MB full random test depth, compared to 102ns on the A14.Of further note is the 12MB L2 cache of the performance cores, although here it seems that Apple continues to do some partitioning as to how much as single core can use as we’re still seeing some latency uptick after 8MB.The M1 also contains a large SLC cache which should be accessible by all IP blocks on the chip. We’re not exactly certain, but the test results do behave a lot like on the A14 and thus we assume this is a similar 16MB chunk of cache on the SoC, as some access patterns extend beyond that of the A14, which makes sense given the larger L2.One aspect we’ve never really had the opportunity to test is exactly how good Apple’s cores are in terms of memory bandwidth. Inside of the M1, the results are ground-breaking: A single Firestorm achieves memory reads up to around 58GB/s, with memory writes coming in at 33-36GB/s. Most importantly, memory copies land in at 60 to 62GB/s depending if you’re using scalar or vector instructions. The fact that a single Firestorm core can almost saturate the memory controllers is astounding and something we’ve never seen in a design before.Because one core is able to make use of almost the whole memory bandwidth, having multiple cores access things at the same time don’t actually increase the system bandwidth, but actually due to congestion lower the effective achieved aggregate bandwidth. Nevertheless, this 59GB/s peak bandwidth of one core is essentially also the speed at which memory copies happen, no matter the amount of active cores in the system, again, a great feat for Apple.Beyond the clock speed increase, L2 increase, this memory boost is also very likely to help the M1 differentiate its performance beyond that of the A14, and offer up though competition against the x86 incumbents.Page 1: Apple Silicon M1: Recap, Power ConsumptionPage 2: Benchmarks: Whatever Is AvailablePage 3: M1 GPU Performance: Integrated King, Discrete RivalPage 4: SPEC2006 & 2017: Industry Standard - ST PerformancePage 5: SPEC2017 - Multi-Core PerformancePage 6: Rosetta2: x86-64 Translation PerformancePage 7: Conclusion & First Impressions\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NVIDIA Announces A100 80GB: Ampere Gets HBM2E Memory Upgrade\n",
      "Author: Ryan Smith\n",
      "Date Published: 2020-11-16T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/16250/nvidia-announces-a100-80gb-ampere-gets-hbm2e-memory-upgrade\n",
      "Content: Kicking off a very virtual version of the SC20 supercomputing show, NVIDIA this morning is announcing a new version of their flagship A100 accelerator. Barely launched 6 months ago, NVIDIA is preparing to release an updated version of the GPU-based accelerator with 80 gigabytes of HBM2e memory, doubling the capacity of the initial version of the accelerator. And as an added kick, NVIDIA is dialing up the memory clockspeeds as well, bringing the 80GB version of the A100 to 3.2Gbps/pin, or just over 2TB/second of memory bandwidth in total.The 80GB version of the A100 will continue to be sold alongside the 40GB version – which NVIDIA is now calling the A100 40GB – and it is being primarily aimed at customers with supersized AI data sets. Which at face value may sound a bit obvious, but with deep learning workloads in particular, memory capacity can be a strongly bounding factor when working with particularly large datasets. So an accelerator that’s large enough to keep an entire model in local memory can potentially be significantly faster than one that has to frequently go off-chip to swap data.NVIDIA Accelerator Specification ComparisonA100 (80GB)A100 (40GB)V100FP32 CUDA Cores691269125120Boost Clock1.41GHz1.41GHz1530MHzMemory Clock3.2Gbps HBM2e2.4Gbps HBM21.75Gbps HBM2Memory Bus Width5120-bit5120-bit4096-bitMemory Bandwidth2.0TB/sec1.6TB/sec900GB/secVRAM80GB40GB16GB/32GBSingle Precision19.5 TFLOPs19.5 TFLOPs15.7 TFLOPsDouble Precision9.7 TFLOPs(1/2 FP32 rate)9.7 TFLOPs(1/2 FP32 rate)7.8 TFLOPs(1/2 FP32 rate)INT8 Tensor624 TOPs624 TOPsN/AFP16 Tensor312 TFLOPs312 TFLOPs125 TFLOPsTF32 Tensor156 TFLOPs156 TFLOPsN/AInterconnectNVLink 312 Links (600GB/sec)NVLink 312 Links (600GB/sec)NVLink 26 Links (300GB/sec)GPUGA100(826mm2)GA100(826mm2)GV100(815mm2)Transistor Count54.2B54.2B21.1BTDP400W400W300W/350WManufacturing ProcessTSMC 7NTSMC 7NTSMC 12nm FFNInterfaceSXM4SXM4SXM2/SXM3ArchitectureAmpereAmpereVoltaDiving right into the specs, the only difference between the 40GB and 80GB versions of the A100 will be memory capacity and memory bandwidth. Both models are shipping using a mostly-enabled GA100 GPU with 108 active SMs and a boost clock of 1.41GHz. Similarly, the TDPs between the two models remain unchanged as well. So for pure, on-paper compute throughput, there’s no difference between the accelerators.Instead, the improvements for the A100 come down to its memory capacity and its greater memory bandwidth. When the original A100 back in May, NVIDIA equipped it with six 8GB stacks of HBM2 memory, with one of those stacks disabled for yield reasons. This left the original A100 with 40GB of memory and just shy of 1.6TB/second of memory bandwidth.For the newer A100 80GB, NVIDIA is keeping the same configuration of 5-out-of-6 memory stacks enabled, however the memory itself has been replaced with newer HBM2E memory. HBM2E is the informal name given to the most recent update to the HBM2 memory standard, which back in February of this year defined a new maximum memory speed of 3.2Gbps/pin. Coupled with that frequency improvement, manufacturing improvements have also allowed memory manufacturers to double the capacity of the memory, going from 1GB/die to 2GB/die. The net result being that HBM2E offers both greater capacities as well as greater bandwidths, two things which NVIDIA is taking advantage of here.With 5 active stacks of 16GB, 8-Hi memory, the updated A100 gets a total of 80GB of memory. Which, running at 3.2Gbps/pin, works out to just over 2TB/sec of memory bandwidth for the accelerator, a 25% increase over the 40GB version. This means that not only does the 80GB accelerator offer more local storage, but rare for a larger capacity model, it also offers some extra memory bandwidth to go with it. That means that in memory bandwidth-bound workloads the 80GB version should be faster than the 40GB version even without using its extra memory capacity.Being able to offer a version of the A100 with more memory bandwidth seems to largely be an artifact of manufacturing rather than something planned by NVIDIA – Samsung and SK Hynix only finally started mass production of HBM2E a bit earlier this year – but none the less it’s sure to be a welcome one.Otherwise, as mentioned earlier, the additional memory won’t be changing the TDP parameters of the A100. So the A100 remains a 400 Watt part, and nominally, the 80GB version should be a bit more power efficient since it offers more performance inside the same TDP.Meanwhile, NVIDIA has also confirmed that the greater memory capacity of the 80GB model will also be available to Multi-Instance GPU (MIG) users. The A100 still has a hardware limitation of 7 instances, so equal-sized instances can now have up to 10GB of dedicated memory each.As far as performance is concerned, NVIDIA is throwing out a few numbers comparing the two versions of the A100. It’s actually a bit surprising that they’re talking up the 80GB version quite so much, as NVIDIA is going to continue selling the 40GB version. But with the A100 80GB likely to cost a leg (NVIDIAalready bought the Arm), no doubt there’s still a market for both.Finally, as with the launch of the original A100 earlier this year, NVIDIA’s immediate focus with the A100 80GB is on HGX and DGX configurations. The mezzanine form factor accelerator is designed to be installed in multi-GPU systems, so that is how NVIDIA is selling it: as part of an HGX carrier board with either 4 or 8 of the GPUs installed. For customers that need individual A100s, NVIDIA is continuing to offer the PCIe A100, though not in an 80GB configuration (at least, not yet).Along with making the A100 80GB available to HGX customers, NVIDIA is also launching some new DGX hardware today as well. At the high-end, they’re offering a version of the DGX A100 with the new accelerators, which they’ll be calling the DGX A100 640GB. This new DGX A100 also features twice as much DRAM and storage as its predecessor, doubling the original in more than one way.Meanwhile NVIDIA is launching a smaller, workstation version of the DGX A100, which they are calling the DGX Station A100. The successor to the original, Volta-based DGX Station, the DGX Station A100 is essentially half of a DGX A100, with 4 A100 accelerators and a single AMD EPYC processor. NVIDIA’s press pre-briefing didn’t mention total power consumption, but I’ve been told that it runs off of a standard wall socket, far less than the 6.5kW of the DGX A100.NVIDIA is also noting that the DGX Station uses arefrigerantcooling system, meaning that they are using sub-ambient cooling (unlike the original DGX Station, which was simply water cooled). NVIDIA is promising that despite this, the DGX Station A100 is whisper quiet, so it will be interesting to see how much of that is true given the usual noise issues involved in attaching a compressor to a computer cooling loop.Both of the new DGX systems as in production now. According to NVIDIA, the systems are already being used for some of their previously-announced supercomputing installations, such as the Cambridge-1 system. Otherwise commercial availability will start in January, with wider availability in February.Gallery:NVIDIA SC20 Press Deck\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16250/nvidia-announces-a100-80gb-ampere-gets-hbm2e-memory-upgrade\n",
      "Title: Apple Intros First Three ‘Apple Silicon’ Macs: Late 2020 MacBook Air, 13-Inch MacBook Pro, & Mac Mini\n",
      "Author: Ryan Smith\n",
      "Date Published: 2020-11-11T01:45:00Z\n",
      "URL: https://www.anandtech.com/show/16235/apple-intros-first-three-apple-silicon-macs-late-2020-macbook-air-13inch-macbook-pro-mac-mini\n",
      "Content: As previously announced by Apple this summer, the company is embarking on a major transition within its Mac product lineup. After almost a decade and a half of relying on Intel’s x86 processors to serve at the heart of every Mac, the company is going to be shifting to relying on its own, in-house designed Arm processors to power their now un-PC computers. At the time Apple set the start of the transition at the end of this year, and right on cue, todayApple announced the first three Apple Silicon-powered Macs: the Late 2020 editions of the MacBook Air, the 13-Inch MacBook Pro, and the Mac Mini.Three of the lower-end devices within the Mac family, Apple is starting small for their Arm transition. The Mac Mini is of course the smallest and most integrated of Apple’s desktop-style computers. Meanwhile the MacBook Air and MacBook Pro are Apple’s two 13.3-inch laptops, focused on portability and performance respectively. Fittingly, these are also the areas where performance-per-watt is generally the most critical, as Apple is very strongly power-constrained on these platforms, and thus performance-limited as well.Before diving into the individual Macs, it’s interesting to note just how little is changing on the outside. The new machines are all clearly reusing the designs of their x86-based predecessors, if not parts of the shell in some cases. And unlike the transition to x86 that started in 2006, Apple isn’t even cleaning the slate with product naming – the Arm-based Macs all continue the MacBook/Mac Mini naming rather than being the progenitors of new families of machines. Apple wants to make the transition to Arm Macs as seamless as possible, and this goes right on down to their design and product names. So although guts will be entirely different, these new products are still designed to be Macs in every appreciable way.Powering all three of these new machines will be Apple’s first Mac-focused Apple Silicon chip, the Apple M1. Our own Andrei Frumusanu has adetailed breakdown on the new M1 chips and what we expect from them based on Apple’s A14 SoCused in the recently-launched iPhone 12 family. But in short, M1 looks a lot like a beefier A14, with four of Apple’s high-performance Firestorm cores – 2 more than on A14 – joined by four high-efficiency Icestorm cores. Meanwhile the GPU design has been bulked up to 8 cores there as well, doubling A14. Finally, joining this are various other functional blocks for the SoC, including a flash storage controller, a secure enclave, Apple’s neural engine (NPU), and a Thunderbolt 3 + USB 4 controller. All of this, in turn, is made on a 5nm process – no double the same TSMC 5nm process that Apple used for the A14.MacBook Air (Late 2020)Starting things off, we have the MacBook Air. The most recently (re)introduced member of Apple’s Mac lineup, the Air is the company’s smallest and cheapest laptop. The final Intel revision of the laptop was launched in March, where it shipped with Intel’s Ice Lake-Y processors, topping out with the quad CPU core i7-1060NG7. The newest model picks up where the previous one left off, swapping out Intel’s silicon for Apple’s own M1 processor.MacBook Air SpecificationsModelLate 2020Mid 2020Mid 2019 (Base)Late 2018 (Base)DimensionsWidth30.4 cmDepth21.2 cmHeight0.41 - 1.61 cm0.41 - 1.61 cm0.41 - 1.56 cmWeight2.8 lbs (1.29 kg)2.8 lbs (1.29 kg)2.75 lbs (1.25 kg)CPUApple M14C/4T Highi-Perf +4C/4T High-EffCore i3-1000NG42C/4T1.10 - 3.20 GHzCore i5-1030NG74C/8T1.10 - 3.50 GHzCore i7-1060NG74C/8T1.20 - 3.80 GHz1.6 GHz (3.60 GHz Turbo)Core i52 CPU CoresGPUApple M1 Integrated(7 or 8 Cores)Intel Iris PlusIntel UHD Graphics 617Display13.3-inch 2560x1600 IPS LCDP3with True Tone13.3-inch 2560x1600 IPS LCDsRGB with True Tone13.3-inch 2560x1600 IPS LCDsRGBMemory8 - 16 GB LPDDR4X-42668 - 16 GB LPDDR4X-37338 GB LPDDR3-2133SSD256 GB - 2 TB256 GB - 2 TB128 GB PCIeI/O2xUSB4Type-Cw/Thunderbolt 33.5mm AudioTouch ID2x USB 3.1 Type-Cw/Thunderbolt 33.5mm AudioTouch IDBattery Capacity49.9 Wh50.3 WhBattery Life15 - 18 Hours11 - 12 Hours12 HoursPrice$999$999$1099$1199The star of the show is of course Apple’s M1. Because the company has changed so little outside (there are a few things, which we’ll get to), the bulk of Apple’s focus in promoting the new MBA is based on performance and battery life – which is to say performance, power consumption, and performance-per-watt. Among figured quoted by the firm include up to 3x better CPU and 5x better GPU performance than a last-generation Intel-based MBA, and perhaps more directly relevant, enough performance to play back multiple 4K ProRes streams without dropping a frame. In other words, the new MacBook Air should be significantly more powerful than the previous one.Driving this laptop are two versions of the M1 chip. The more expensive, $1249 model of the MBA gets a fully-enabled 8 CPU core plus 8 GPU core version of the M1. Meanwhile the entry-level $999 model gets a reduced 8 CPU/7 GPU version of the chip. This is apparently where Apple is sending binned/salvaged M1 chips, as TSMC’s 5nm process is no doubt temperamental in its early days. Unfortunately, not published in any Apple documentation are clockspeeds, so we have no idea how the two models will compare to each other in terms of performance beyond the influence of an 8thGPU core.The MacBook Air is now fanless, harkening back to the retired 12-inch MacBook. It’s not clear whether Apple has set the M1 to a nominally lower TDP than the Intel CPUs (10 Watts), or if they’re simply more confident about the boost and throttle mechanics of their own chip. Either way, this does mean that we expect the MacBook Air to throttle under heavy, sustained workloads, not unlike the iPhones. This is the cost of having a fanless, passively-cooled design.The benefit to that, of course, being that there are no fans to make noise, break, or get clogged with dust. This kind of reliability was one of the great thinks about the old MacBook and was a notable loss with the previous MacBook Air, so it’s good to see that Apple has brought it back. This also helps to further differentiate the MacBook Air from the similarly-sized 13-inch MacBook Pro, as it means there should be a meaningful performance difference between the devices, and there will definitely be a noise difference.Like all of the M1-based Macs, Apple is offering two memory configurations and four storage configurations. The base model gets 8GB of LPDDR4X memory and 256GB of flash storage, and this can be dialed up to 16GB of memory and 2TB of flash. It goes without saying that all of this is soldered down – in the case of the RAM, directly on to the M1 package – so there are no end-user upgrade options.Apple rates the battery life on the MBA at up to 15 hours of web browsing and 18 hours of movie playback, which is 4 and 6 hours longer than their rating on the Intel model. Respectively. The laptop comes with the same capacity 49.9 watt-hour battery as the previous model, so all of these measurements are being done against equivalent batteries. However, as Ian Cutress noted, Apple doesn’t seem to be entirely equalizing for brightness in their testing – setting the two Macs to the same brightness setting rather than ensuring they’re at the same measured brightness in nits – so it’s not clear if that plays a role. In either case, however, Apple is clearly positioning the new MBA to last a lot longer than the previous model.The switch to M1 also means that the Air’s I/O functionality gets an upgrade. The SoC supports an odd combination of Thunderbolt 3 and the new USB 4 (but not the all-encompassing Thunderbolt 4), so the laptop’s two USB-C ports inherit this functionality. And, as required by USB 4, this includes DisplayPort alt mode functionality. On which note, the laptop also inherits M1’s support for a single external display, up to 6K@60Hz (i.e. Apple’s XDR display). Meanwhile, on the right side of the laptop, Apple continues to provide a 3.5mm combo headset jack.Meanwhile, Apple has also given the MacBook Air an unexpected display upgrade. The laptop, which previously used a 2560x1600 pixel sRGB-gamut display now offers a 2560x1600 pixel P3-gamut display, the same gamut as offered by the MacBook Pro family. Apple does not publish complete gamut coverage specifications, so it’s not clear how much of the P3 gamut that the new MBA does cover, and how that compares to the MacBook Pros. Apple is normally very good here, but that will be something worth checking out. This also means it’s unclear whether Apple has just upgraded the laptop’s backlighting to drive the wider gamut, or if the panel itself has been updated as well.Finally, it’s worth noting that in today’s presentation, Apple also brought up the matter of image quality from their integrated, front-facing Facetime camera. According to the company, they’re putting the M1’s superior ISP to good use here in order to offer better noise reduction, a higher dynamic range, and a better white balance. However the hardware spec sheet still lists the laptop as using a 720p Facetime camera, so at a minimum it’s safe to say that Apple’s resolution hasn’t improved; otherwise we’ll have to see if there are any other hardware changes driving the rest of those improvements.As with all three Apple Silicon-based Macs, the Late 2020 MacBook Air is set to be available next week, on November 17th.13-Inch MacBook Pro (Late 2020)Counterpoint to the light and ultraportable MacBook Air is the 13-inch MacBook Pro. Based around an identical-sized 13.3-inch panel as the MBA, the 13-inch MBP is Apple’s more productivity-focused Mac laptop, generally offering better performance and more I/O options than the MBA. With the switch to Apple’s Arm-based M1 processor, this is still the case, though how exactly the laptops differ has changed some.As with the MacBook Air, the smaller of the Pro laptops is almost entirely an upgrade to the guts of the machine, with no notable external changes. Apple isn’t updating the screen, Apple isn’t changing the size of the laptop, or even changing the battery; this is largely just the M1-ization of the MBP. Of particular note here, the Late 2020 13-inch MacBook Pro is technically only a replacement for the lower-end 13-inch MBP, commonly referred to as theTwo Thunderbolt 3 portsmodel, which relative to everything else in Apple’s lineup was fairly dated. Apple is continuing to sell the higher-end model from earlier this year, which uses more recent Intel chips.MacBook Pro 13-Inch (2 Port) SpecificationsModelLate 2020Mid 2020Mid 2019CPUApple M14C/4T Highi-Perf +4C/4T High-Eff1.4 GHz/3.9 GHzCore i5-8275U4C/8T(Coffee Lake)1.4 GHz/3.9 GHzCore i5-8275U4C/8T(Coffee Lake)GPUApple M1 Integrated (8 Cores)Intel Iris Plus Graphics 645(128MB eDRAM)Intel Iris Plus Graphics 645(128MB eDRAM)Display13\" 2560 x 1600 IPS LCDP3 Gamut, True Tone, 500 nitsMemory8 - 16 GB LPDDR4X-42668 - 16 GB LPDDR3-21338 - 16 GB LPDDR3-2133SSD256 GB - 2 TB256 GB - 2 TB256 GB - 2 TBTouch BarYesI/O2x USB4 Type-Cw/Thunderbolt 33.5mm AudioTouch IDWi-Fi 62x USB 3.1 Gen 2 Type-Cw/Thunderbolt 33.5mm AudioTouch IDWi-Fi 5Battery Capacity58.2 WhBattery Life17 - 20 hours10 HoursDimensions1.56 cm x 30.41 cm x 21.24 cmWeight3.0 lbs (1.4 kg)3.1 lbs (1.4 kg)3.02 lbs (1.37 kg)Launch Price$1299$1299$1299Since this new MBP is designed to replace the low-end model, the change on the CPU front is especially significant. Apple is going from Intel’s Core 8000 series (Coffee Lake-U) chips to the M1, so the M1 is a leapfrog in both expected CPU performance and graphics performance. In this case it’s 8 CPU cores (4B + 4L) versus Intel’s 4 CPU cores, and Apple’s 8 core GPU design against Intel’s eDRAM-backed Gen9.5 iGPU, which used 48 EUs. These old Intel CPUs were still based on their 14nm process, so going from that to the 5nm-based M1 means that Apple gets both an architectural upgrade and a big manufacturing boost at the same time.As with the MBA, there are no clockspeeds disclosed, so we have no idea how that compares to the MBA or the new Mac Mini. But the MBP it replaces was nominally a 15 Watt TDP part, and I’d expect the new model to be similarly tuned. On which note, the 13-inch MBP is actively cooled with a fan, so unlike the passively cooled MBA, expect the MBP to do better under sustained loads, as it has the necessary cooling to keep itself from throttling as much under heavy workloads.There is only one SoC configuration being offered here: a full-fat M1 with 8 CPU cores and 8 GPU cores. So upgrade options are limited to changing out the RAM and storage. Here Apple offers 8GB or 16GB of LPDDR4X RAM (another improvement over the previous MBP), and storage starts at 256GB for the $1299 model, with prices going much, much higher ($2099) for 2TB of flash storage.Rounding out the package is an identical capacity 58.2 watt-hour battery. So like the MBA, battery life comparisons are reasonable apples-to-apples to the previous generation laptop. In which case Apple is claiming that the M1-based MBP will get up to 17 hours of web browsing and 20 hours of video playback, which is an improvement of 7 and 10(!) hours respectively. At least with M1-based devices, it would seem the bigger the battery, the greater the battery life gains.Finally, as this model replaces the old two-port MBP, the new MBP also gets two ports. Once again both on the left side, the laptop features two USB-C ports capable of Thunderbolt 3 and USB4, being driven by the M1 SoC. Given that no M1 machine ships with more than this – not even the Pro-focused MBP – it’s reasonable to assume that two ports is a limitation of the M1 SoC. The new MBP is no worse than the old one, but it’s also no better – and it’s easy to see why Apple is keeping the higher-end Intel model around for now.As for the display, unlike the MBP it doesn’t appear that Apple has made any changes here. The Late 2020 MBP is listed as offering the same 2560x1600 P3 display as the previous model, both with a maximum brightness of 500 nits. External display users will want to make note, however, that unlike the Intel MBPs, this new one can only support a single external display. The good news is that it can go up to 6K@60Hz, but if you were accustomed to driving a pair of displays off of a MBP, then the new M1-based model won’t quite measure up.The Late 2020 13-inch MacBook is set to be available next week, on November 17th. Prices start at $1299 for the 8GB/256GB model, and $1499 for the 8GB/512GB model.Mac Mini (Late 2020)Last but certainly not least among the new Arm-based Macs is the sole desktop of the group, the Mac Mini. Apple used a modified Mini as the basis for the Developer Transition Kits (DTKs), so it’s only fitting that the Mini is also one of the first retail systems to ship with an M1 SoC. Besides being Apple’s cheapest Mac, the Mini sits in an interesting place in Apple’s lineup as the only true stand-alone desktop other than the pricey Mac Pro, so it has developed an eclectic following of users, ranging from desktop users to server admins. This also means that despite being little more than a body for the parts inside, it sees some of the most significant changes of any of the Macs in getting its M1 SoC upgrade.Apple Mac Mini SpecificationsLate 2020Late 2018CPUApple M14C/4T Highi-Perf +4C/4T High-EffCore i3-8100B4C/4T3.6 GHzCore i5-8500B6C/6T3.0 - 4.1 GHzCore i7-8700B6C/12T3.2 - 4.6 GHzGraphicsApple M1 Integrated (8 Cores)Intel UHD Graphics 630Memory8 - 16 GB LPDDR4X-42668 GB DDR4-2666 (SO-DIMMs)Configurable to 16 GB, 32 GB or 64 GB DDR4-2666Storage256 GB - 2 TB128 GB PCIe SSDConfiguratble to 256 GB, 512 GB, 1 TB, or 2 TB SSDEthernet1 GbE1 GbE or 10 GbEI/O2x USB4 Type-Cw/Thunderbolt 32x USB 3.2 Gen 1 Type-A(5 Gbps)3.5mm AudioWi-Fi 64 × USB 3.1 Gen 2 (10 Gbps) Type-Cw/ Thunderbolt 32 × USB 3.0 Type-A (5 Gbps)HDMI 2.03.5mm AudioWi-Fi 5DimensionsWidth19.7 cm19.7 cmHeight3.6 cm3.6 cmDepth19.7 cm19.7 cmPSU~ 150 W (internal)~ 150 W (internal)Prior to today, the Mini was another system running relatively old Intel hardware. The system hadn’t been updated since 2018, at which point it received Intel’s eighth-generation Coffee Lake CPUs. While the chip options here included four and six CPU cores, the graphics side was notably weaker than the more powerful options found in the 13-inch MBP, never mind discrete graphics. So the M1 stands to be a significant upgrade over those earlier offerings, especially when it comes to graphics. And while I have no doubts the M1 can surpass the older Intel chips here, it will be interesting to see whether only 4 high-performance cores is enough to go up against the 6 cores offered by the higher-end Intel configuration.On which note, Apple is continuing to sell the 6 core Intel model. The other, cheaper models have been retired with the introduction of the Late 2020 Mac Mini, however there is still one final Intel configuration to last for now. And while Apple doesn’t comment on otherwise trivial matters such as these, given the hardware differences, I strongly suspect the Intel model is being kept around (for now) for customers that have extensive RAM or I/O needs.For the M1-based Mac Mini then, the RAM and storage options are the same as the other M1-based Macs. This means just 8 or 16GB of memory, and anywhere between 256GB and 2TB of flash storage. The base configuration starts out at 8/256 for $699, while the higher-end SKU gets 8/512 for $899. And because this is the M1, this is all once again soldered down.The lack of more thorough expandability options is a notable departure from the earlier Mac Mini. That Mac used SO-DIMMs for memory, not only allowing users to install their own memory upgrades, but to install greater amounts of memory altogether – up to 64GB. But the use of soldered, on-package memory with the M1 means that expandable memory is no longer an option. Not that you can get LPDDR4X in SO-DiMM form, either.The Late 2020 Mac Mini’s I/O options aren’t quite as robust, either. Whereas the 2018 model offered three USB-C supports, all supporting Thunderbolt 3 and USB 3.1 Gen 2 (10Gbps), the newer model has two ports sporting Thunderbolt 3 and USB4. USB4 itself is a welcome upgrade, but it won’t help if you need more USB-C ports. Otherwise, the rest of the port collection remains unchanged, with an HDMI 2.0 port, two USB Type-A ports, and a 1Gbps Ethernet jack. Ultimately, we’re seeing what I suspect are the limitations of the M1; Apple kept things simple by only relying on the ports the SoC could directly drive, and it can only directly drive two such high-speed ports.Between all of those ports, in turn, the Mac Mini can drive two displays. One can be a DisplayPort/Thunderbolt-type display hooked up to the USB-C ports, and the other needs to be an HDMI display hooked up to the aforementioned port. Like the MBP, this is a step forward in regards to the best display supported (now up to 6K@60Hz), but a step backwards in the total number of displays supported, since the older Mac Mini could drive a total of three displays.Other than this, however, the Mac Mini may just end up being the best performing of the new M1-based Macs. With the same chassis as the previous 65W TDP model, including a fan for active cooling, the new Mac Mini has plenty of cooling headroom to work with for the M1 SoC. So with both power and cooling properly provided for, this is the Mac that has the best chance of sustaining the M1’s highest clockspeeds under a full load.But as always, the we’ll have to see if real-world results can live up to Apple’s claims. The Mac Mini ships alongside the rest of the Macs on November 17th, so we shouldn’t be waiting too long to see what the new Mini and the rest of the new Apple Silicon-based Macs are capable of.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16235/apple-intros-first-three-apple-silicon-macs-late-2020-macbook-air-13inch-macbook-pro-mac-mini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Apple Announces The Apple Silicon M1: Ditching x86 - What to Expect, Based on A14\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-11-10T20:00:00Z\n",
      "URL: https://www.anandtech.com/show/16226/apple-silicon-m1-a14-deep-dive\n",
      "Content: Today, Apple has unveiled their brand-new MacBook line-up. This isn’t an ordinary release – if anything, the move that Apple is making today is something that hasn’t happened in 15 years: The start of a CPU architecture transition across their whole consumer Mac line-up.Thanks to the company’s vertical integration across hardware and software, this is a monumental change that nobody but Apple can so swiftly usher in. The last time Apple ventured into such an undertaking in 2006, the company had ditched IBM’s PowerPC ISA and processors in favor of Intel x86 designs. Today, Intel is being ditched in favor of the company’s own in-house processors and CPU microarchitectures, built upon the Arm ISA.The new processor is called the Apple M1, the company’s first SoC designed with Macs in mind. With four large performance cores, four efficiency cores, and an 8-GPU core GPU, it features 16 billion transistors on a 5nm process node. Apple’s is starting a new SoC naming scheme for this new family of processors, but at least on paper it looks a lot like an A14X.Today’s event contained a ton of new official announcements, but also was lacking (in typical Apple fashion) in detail. Today, we’re going to be dissecting the new Apple M1 news, as well as doing a microarchitectural deep dive based on the already-released Apple A14 SoC.The Apple M1 SoC: An A14X for MacsThe new Apple M1 is really the start of a new major journey for Apple. During Apple’s presentation the company didn’t really divulge much in the way of details for the design, however there was one slide that told us a lot about the chip’s packaging and architecture:This packaging style with DRAM embedded within the organic packaging isn't new for Apple; they've been using it since the A12. However it's something that's only sparingly used. When it comes to higher-end chips, Apple likes to use this kind of packaging instead of your usual smartphone POP (package on package) because these chips are designed with higher TDPs in mind. So keeping the DRAM off to the side of the compute die rather than on top of it helps to ensure that these chips can still be efficiently cooled.What this also means is that we’re almost certainly looking at a 128-bit DRAM bus on the new chip, much like that of previous generation A-X chips.On the very same slide, Apple also seems to have used an actual die shot of the new M1 chip. It perfectly matches Apple’s described characteristics of the chip, and it looks looks like a real photograph of the die. Cue what's probably the quickest die annotation I’ve ever made:We can see the M1’s four Firestorm high-performance CPU cores on the left side. Notice the large amount of cache – the 12MB cache was one of the surprise reveals of the event, as the A14 still only featured 8MB of L2 cache. The new cache here looks to be portioned into 3 larger blocks, which makes sense given Apple’s transition from 8MB to 12MB for this new configuration, it is after all now being used by 4 cores instead of 2.Meanwhile the 4 Icestorm efficiency cores are found near the center of the SoC, above which we find the SoC’s system level cache, which is shared across all IP blocks.Finally, the 8-core GPU takes up a significant amount of die space and is found in the upper part of this die shot.What’s most interesting about the M1 here is how it compares to other CPU designs by Intel and AMD. All the aforementioned blocks still only cover up part of the whole die, with a significant amount of auxiliary IP. Apple made mention that the M1 is a true SoC, including the functionality of what previously was several discrete chips inside of Mac laptops, such as I/O controllers and Apple's SSD and security controllers.The new CPU core is what Apple claims to be the world’s fastest. This is going to be a centre-point of today’s article as we dive deeper into the microarchitecture of the Firestorm cores, as well look at the performance figures of the very similar Apple A14 SoC.With its additional cache, we expect the Firestorm cores used in the M1 to be even faster than what we’re going to be dissecting today with the A14, so Apple’s claim of having the fastest CPU core in the world seems extremely plausible.The whole SoC features a massive 16 billion transistors, which is 35% more than the A14 inside of the newest iPhones. If Apple was able to keep the transistor density between the two chips similar, we should expect a die size of around 120mm². This would be considerably smaller than past generation of Intel chips inside of Apple's MacBooks.Road To Arm: Second Verse, Same As The FirstSection by Ryan SmithThe fact that Apple can even pull off a major architectural transition so seamlessly is a small miracle, and one that Apple has quite a bit of experience in accomplishing. After all, this is not Apple’s first-time switching CPU architectures for their Mac computers.The long-time PowerPC company came to a crossroads around the middle of the 2000s when the Apple-IBM-Motorola (AIM) alliance, responsible for PowerPC development, increasingly struggled with further chip development. IBM’s PowerPC 970 (G5) chip put up respectable performance numbers in desktops, but its power consumption was significant. This left the chip non-viable for use in the growing laptop segment, where Apple was still using Motorola’s PowerPC 7400 series (G4) chips, which did have better power consumption, but not the performance needed to rival what Intel would eventually achieve with its Core series of processors.And thus, Apple played a card that they held in reserve: Project Marklar. Leveraging the flexibility of the Mac OS X and its underlying Darwin kernel, which like other Unixes is designed to be portable, Apple had been maintaining an x86 version of Mac OS X. Though largely considered to initially have been an exercise in good coding practices – making sure Apple was writing OS code that wasn’t unnecessarily bound to PowerPC and its big-endian memory model – Marklar became Apple’s exit strategy from a stagnating PowerPC ecosystem. The company would switch to x86 processors – specifically, Intel’s x86 processors – upending its software ecosystem, but also opening the door to much better performance and new customer opportunities.The switch to x86 was by all metrics a big win for Apple. Intel’s processors delivered better performance-per-watt than the PowerPC processors that Apple left behind, and especially once Intel launched the Core 2 (Conroe) series of processors in late 2006, Intel firmly established itself as the dominant force for PC processors. This ultimately setup Apple’s trajectory over the coming years, allowing them to become a laptop-focused company with proto-ultrabooks (MacBook Air) and their incredibly popular MacBook Pros. Similarly, x86 brought with it Windows compatibility, introducing the ability to directly boot Windows, or alternatively run it in a very low overhead virtual machine.The cost of this transition, however, came on the software side of matters. Developers would need to start using Apple’s newest toolchains to produce universal binaries that could work on PPC and x86 Macs – and not all of Apple’s previous APIs would make the jump to x86. Developers of course made the jump, but it was a transition without a true precedent.Bridging the gap, at least for a bit, was Rosetta, Apple’s PowerPC translation layer for x86. Rosetta would allow most PPC Mac OS X applications to run on the x86 Macs, and though performance was a bit hit-and-miss (PPC on x86 isn’t the easiest thing), the higher performance of the Intel CPUs helped to carry things for most non-intensive applications. Ultimately Rosetta was a band-aid for Apple, and one Apple ripped off relatively quickly; Apple already dropped Rosetta by the time of Mac OS X 10.7 (Lion) in 2011. So even with Rosetta, Apple made it clear to developers that they expected them to update their applications for x86 if they wanted to keeping selling them and to keep users happy.Ultimately, the PowerPC to x86 transitions set the tone for the modern, agile Apple. Since then, Apple has created a whole development philosophy around going fast and changing things as they see fit, with only limited regard to backwards compatibility. This has given users and developers few options but to enjoy the ride and keep up with Apple’s development trends. But it has also given Apple the ability to introduce new technologies early, and if necessary, break old applications so that new features aren’t held back by backwards compatibility woes.All of this has happened before, and it will all happen again starting next week, when Apple launches their first Apple M1-based Macs. Universal binaries are back, Rosetta is back, and Apple’s push to developers to get their applications up and running on Arm is in full force. The PPC to x86 transition created the template for Apple for an ISA change, and following that successful transition, they are going to do it all over again over the next few years as Apple becomes their own chip supplier.A Microarchitectural Deep Dive & BenchmarksIn the following page we’ll be investigating the A14’s Firestorm cores which will be used in the M1 as well, and also do some extensive benchmarking on the iPhone chip, setting the stage as the minimum of what to expect from the M1:Page 1: Replacing x86 - The Next Big StepPage 2: Apple's Humongous CPU MicroarchitecturePage 3: Dominating Mobile PerformancePage 4: From Mobile to Mac: What to Expect?Page 5: Apple Shooting for the Stars: x86 Incumbents Beware\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16226/apple-silicon-m1-a14-deep-dive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Apple Fall 2020 Mac Event Live Blog: 10am PST (18:00 UTC)\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-11-10T16:23:00Z\n",
      "URL: https://www.anandtech.com/show/16233/the-apple-fall-2020-mac-event-live-blog-10am-pt-1800-utc\n",
      "Content: 12:55PM EST- Today Apple is expected to pull the trigger on new ‘Apple Silicon’ Macbooks. Years in the making, today we should be hearing about a lew of new devices from the Cupertino company which ditch x86 processors in favour of their own in-house designs.12:55PM EST- We don’t know exactly what Apple has in store for us, but an upsized chip variant of the A14, maybe an A14X, is going to be a likely bet. Whatever Apple presents today, following the event, expect an in-depth microarchitectural exploration of the A14 and the Firestorm cores – with us attempting to put into context Apple’s big bet on Apple Silicon and how the competitive landscape might look like.12:56PM EST- It goes without saying that Apple's transition from x86 to Arm chips is a significant move. Not one without precedence (see: PPC->x86), but a major one none the less. Not even Apple changes CPU ISAs frequently12:57PM EST- Just as in 2006, Apple is coming to a crossroads in terms of CPU performnace. Long-time supplier Intel has struggled to keep moving forward. Meanwhile Apple's in-house team, responsible for developing their A-series chips for iOS devices, have been able to put together increasingly powerful hardware12:58PM EST- In fact it's outright surprising in some respects how far Apple has come12:58PM EST- Apple's latest CPU cores have IPCs higher than Intel's chips, and while IPC isn't everything (clockspeed matters as well), it's evidence of a very strong architecture design12:59PM EST- So for as messy as an ISA transition is, it's one that makes sense for Apple. They think they can do better than Intel's chips, and they're probably right01:00PM EST- In fact there's little Iif any) doubt in the hardware side of matters. The bigger question on everyones' minds seems to be the software side: backwards compatibility, bootcamp, x86 virtual machines, etc01:00PM EST- And with that said, here we go01:02PM EST- Starting as always with Tim Cook01:02PM EST- This is Apple's third major event in two months (we've noticed, Tim!)01:02PM EST- Cook is quickly recapping the past two announcements: iOS 14, macOS 11, new iPhones, iPads, and Watches01:03PM EST- \"There is just one more thing\"01:03PM EST- \"It's time to talk about the Mac\"01:04PM EST- Apple's Mac business grew by 30% last quarter01:04PM EST- Now rolling a video celebrating Mac users01:05PM EST- \"The Mac has always been about innovation and bold change\"01:05PM EST- Now recapping this summer's announcement of the Apple Silicon transition01:06PM EST- That day is finally here01:06PM EST- Now up, John Ternus01:06PM EST- For the past several years Apple has been working on building the next generation of Macs01:07PM EST- At the heart of this is Apple's SoCs, also known as Apple Silicon01:07PM EST- Announcing their first chip designed specifically for the Mac01:07PM EST- Apple M101:07PM EST- Designed for low-power, portable systems01:07PM EST- Now up, Johny Srouji on M101:08PM EST- \"M1 delivers a giant leap in performance-per-watt\"01:08PM EST- With M1, Apple doesn't just have their own chip, but they're able to go SoC-style and integrate what was previously multiple chips into a single chip01:09PM EST- Built on 5nm01:09PM EST- And offers a unified memory pool01:09PM EST- 16B transistors01:09PM EST- 8 core CPU: 4 perf cores, 4 efficiency cores01:09PM EST- \"World's fastest CPU core\"01:09PM EST- 192KB I-Cache, 128KB D-Cache, 12MB L2 cache01:10PM EST- Meanwhile the efficiency cores have their own 4MB L2 cache01:10PM EST- \"World's best CPU performance per watt\"01:11PM EST- M1 delivers 2x the performance of the \"latest PC laptop chip\" at 10 Watts, the MacBook Air's TDP01:11PM EST- And 3x performance per watt elsewhere01:11PM EST- Now on to GPUs01:12PM EST- Johny is talking up the benfits of an integrated GPU versus a discrete GPU01:12PM EST- 8 GPU cores01:12PM EST- 2.6 TFLOPs; nearly 25K threads at once01:12PM EST- Again 2x performance versus an unnamed PC laptop chip01:13PM EST- M1 has a neural engine as well with 16 cores01:13PM EST- And Apple's latest secure enclave01:13PM EST- Thunderbolt/USB 4 support01:14PM EST- \"M1 is by far the best chip we've ever created\"01:14PM EST- macOS Big Sur has been built to maximize M101:15PM EST- Now up, Craig Federighi01:15PM EST- Recapping everything introduced in Big Sur01:15PM EST- And wasting no time into getting into what the M1 Macs will be like01:15PM EST- iPhone-style instant-on01:16PM EST- Safari is 1.9x more responsive01:16PM EST- And once again bringing up the unified memory architecture01:16PM EST- Which means Apple doesn't have to copy data around from the CPU memory pool to the GPU (or in reverse)01:17PM EST- Craig is also touting better battery life01:17PM EST- iOS-style security is also coming to the M1 Macs01:17PM EST- (For better or worse)01:18PM EST- Apple has of course optimized all of their Mac apps for M101:18PM EST- Universal apps will offer binaries for both x86 and Arm processors01:18PM EST- So the same app will run on all Macs01:19PM EST- Developers in turn will be bringing universal versions of their apps01:19PM EST- Big Sur also has Rosetta 2 to run x86 apps on M1 Macs01:19PM EST- Apple claims some programs even perform better under Rosetta 2 on M1 than they did x86 Macs01:20PM EST- And M1 Macs can directly run iPhone/iPad apps01:20PM EST- Now rolling a video about apps that have been updated for Arm01:23PM EST- Developers talking about what they've been doing with their dev kits (at a very high level and rapid paced)01:23PM EST- Back to John01:24PM EST- Now introducing the Macs themselves01:24PM EST- First out of the gate: the new MacBook Air (with M1)01:25PM EST- Now up, Laura Metz01:25PM EST- MacBook Air is Apple's most popular Mac01:26PM EST- Up to 3.5x faster CPU than the previous-generation MBA01:26PM EST- Up to 5x faster graphics performance01:26PM EST- Up to 3x faster than the best-selling Windows laptops in its class01:27PM EST- 9x faster machine learning performance than the previous MBA01:27PM EST- Even the SSD is 2x faster. M1 has its own storage controller, and Apple is using the latest flash technology01:27PM EST- And the MBA is now fanless01:28PM EST- Up to 18 hours of video playback; 6 hours longer than before01:28PM EST- And 2x the battery life on conference calls01:28PM EST- Laura is also touting the M1's ISP to offer better front-facing camera image quality01:29PM EST- P3 wide color support for the display01:29PM EST- (No idea if Apple has actually improved the physical camera, however)01:29PM EST- Starting at $999 (and $899 for education)01:29PM EST- Up to 16GB of RAM, 2TB of flash storage01:31PM EST- Next up: Mac Mini01:31PM EST- Julie Broms to present the M1-powered Mac Mini01:32PM EST- Up to 3x faster CPU perf than the previous quad-core Mac Mini01:32PM EST- 6x faster graphics01:33PM EST- Up to 5x faster than the \"top-selling PC desktop\"01:34PM EST- The Mac Mini does have a fan01:34PM EST- But this means it's capable of sustaining its performance01:34PM EST- Two USB-C supports with Thunderbolt and USB4 support01:34PM EST- Can even drive Apple's XDR display01:34PM EST- Starts at $69901:35PM EST- $100 lower than the old intro price01:35PM EST- It's notable that Apple isn't clarifying whether this is Thunderbolt 3 or Thunderbolt 401:35PM EST- \"We're still not done\"01:36PM EST- The MacBook Pro 13-inch is also going M101:36PM EST- Shruti Haldea to present the 13-inch MBP01:37PM EST- 5x faster graphics01:37PM EST- (I'm really curious what the TDP is like)01:38PM EST- As with the MBA, Apple is talking up all of the creative tasks that can be done with the laptop, and the benefits of an NPU01:38PM EST- The MBP has a fan, of course01:39PM EST- 17 hours of wireless web browsing, and 20 hours of video playback (10 hours more than before)01:39PM EST- Also has \"studio-quality\" mics in a 3 microphone array01:40PM EST- And like the Mac Mini, it can drive the XDR display at full resolution01:40PM EST- Starting at $1299 ($1199 education)01:40PM EST- Up to 16GB RAM, 2TB SSD01:41PM EST- \"The ultimate expression of what the M1 chip can do\"01:41PM EST- Now back to John, recapping the benefits of M1 and Big Sur01:41PM EST- Performance, battery life, and security01:42PM EST- All three Macs available for order today01:42PM EST- They will be available next week01:42PM EST- Meanwhile macOS 11 Big Sur launches this week01:43PM EST- Recapping that the Arm transition will take a couple of years to complete01:43PM EST- And one last video to roll before turning things back over to Tim Cook01:45PM EST- \"The M1 chip is by far the most powerful chip we've ever created\"01:45PM EST- Cook is expressing his pride in Apple's product teams01:46PM EST- Looking forward to 2021 and \"bringing even more amazing experiences\"01:46PM EST- John Hodgman is back01:46PM EST- Apple is back to not being a PC, after all01:47PM EST- And that's a wrap! Check back a bit later today for our A14 deep dive, and what we expect from the M1\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16233/the-apple-fall-2020-mac-event-live-blog-10am-pt-1800-utc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Compute eXpress Link 2.0 (CXL 2.0) Finalized: Switching, PMEM, Security\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-11-10T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/16227/compute-express-link-cxl-2-0-switching-pmem-security\n",
      "Content: One of the more exciting connectivity standards over the past year has been CXL. Built upon a PCIe physical foundation, CXL is a connectivity standard designed to handle much more than what PCIe does – aside from simply acting as a data transfer from host to device, CXL has three branches to support, known as IO, Cache, and Memory. As defined in the CXL 1.0 and 1.1 standards, these three form the basis of a new way to connect a host with a device. The new CXL 2.0 standard takes it a step further.CXL 2.0 is still built upon the same PCIe 5.0 physical standard, which means that there aren’t any updates in bandwidth or latency, but adds some much needed functionality that customers are used to with PCIe. At the core of CXL 2.0 are the same CXL.io, CXL.cache and CXL.memory intrinsics, dealing with how data is processed and in what context, but with added switching capabilities, added encryption, and support for persistent memory.CXL 2.0 SwitchingFor users who are unfamiliar with PCIe switches, these connect to a host processor with a number of lanes, such as eight lanes or sixteen lanes, and then support many more lanes downstream to increase the number of supported devices. A standard PCIe switch for example might connect with 16x lanes to the CPU, but offer 48 PCIe lanes downstream to enable six GPUs connected at x8 apiece. There is an upstream bottleneck, but for workloads that rely on GPU-to-GPU transfer, especially on systems with limited CPU lanes, using a switch is the best way to go. CXL 2.0 now enables the standard for switching.Modern PCIe switches do more than just ‘add lanes’. Should one of the end-point devices fail (such as an NVMe SSD), then the switch ensures the system can still run and disable that lane so it doesn’t affect the rest of the system. Current switches in the market also support switch-to-switch connectivity, allowing a system to scale out downstream devices.One of the bigger updates in recent switch products has been the support for multiple upstream hosts, such that if a host fails, the downstream devices still have another host to connect to. Combined with switch-to-switch connectivity, a system can have a series of pooled hosts and pooled devices. Each device can work specifically with a host in the pool in a 1:1 relationship, or the devices can work with many hosts. The new standard, with CXL Switching Fabric APIs, enable up to 16 hosts to use one downstream CXL device at once. On top of this, Quality of Service (QoS) is a part of the standard, and in order to enable this the standard packet/FLIT unit of data transfer is unaltered, with some of the unused bits from CXL 1.1 being used (this is what extra bits are used for!).The one element not present in CXL 2.0 is multi-layer switch topologies. At present the standard and API only supports a flat layer. In our briefing, the consortium members (some of whom already create multi-layer PCIe switch fabrics) stated that this is the first stage of enabling switch mechanics, and the roadmap will develop based on customer needs.CXL 2.0 Persistent MemoryAnother notch in enterprise computing in the last few years is persistent memory – something almost as fast as DRAM but stores data like NAND. There has always been a question of where such memory would sit: either as small fast storage through a storage-like interface, or as slow high-capacity memory through a DRAM interface. The initial CXL standards did not directly support persistent memory, unless it already had a device attached to it, in the CXL.memory standard. This time however, CXL 2.0 enables distinct PMEM support as part of a series of pooled resources.The APIs enabling software to deal with PMEM support are built into the specification, allowing persistent memory pools to be accessed either in ‘App Direct’ mode as a separate storage tier, or as a ‘Memory Direct’ mode which expands the host DRAM pool. The goal here is more to do with tiered storage management with whichever way that CXL consortium member customers need for their application, so the goal is for the standard to support as much as possible, and then over time improve those that gain traction.The Persistent Memory aspect draws parallels with Intel’s DC Persistent Memory Optane products, and the goal here is to use them in more-or-less the same way, except this time there is direct support through CXL interfaces and CXL switches, making the pools of memory or storage available system wide, rather than just to a singular host (or through a PCIe interface to a pool).CXL 2.0 Security (optional)The last, but arguably most important feature update for some, is point-to-point security for any CXL link. The CXL 2.0 standard now supports any-to-any communication encryption through the use of hardware acceleration built into the CXL controllers. This is an optional element to the standard, meaning that silicon providers do not have to build it in, or if it is built in, then it can be enabled/disabled, however in the briefing I was told that it is expected that most if not all of the ecosystem that builds on CXL 2.0 will use it – or at least have it as an option to enable.When asked about hits to latency, I was informed there is a hit to latency, however it will depend on the exact use case if it is enabled. No hard numbers were given, but I was told that customers that need the highest latency, who can ensure safety in the system, will likely have it disabled, whereas those who need it to conform to customer requests, or for server-to-server communications, are likely to use it.CXL 2.0 and CXL RoadmapThe initial part of the discussion about CXL 2.0 with two of the consortium members started with explaining that the CXL roadmap is designed to both meet the demands and trends of the market, while still adhering to customer requests from consortium members. This means trends in server disaggregation, or more storage bandwidth, has led to features such as switching and persistent memory support. CXL is committed to enabling open industry standards, even as they scope out next-gen CXL.I asked about this physical layer tie in with PCIe, and where the consortium sees that evolving. I was told that currently there are no plans to deviate away from the PCIe physical specification – there would have to be a strong pull and a strong need to go and build another physical layer topology, which would also break backwards compatibility (which is apparently a cornerstone of CXL). It should be noted that all CXL devices are expected to have PCIe fallback, however some/most/all of the key features in PCIe mode might be lost, depending on the product.We have still yet to see any CXL products come to market, even 1.0/1.1, mostly due to the requirement of having a PCIe 5.0 physical interface. Intel has already stated that Sapphire Rapids will support CXL 1.1, however that is still a while away. Speaking with our consortium contacts, they stated that for any consortium member looking to build a CXL host or device or switch today, then it is expected that they will follow the CXL 2.0 specification. For a lot of consortium members, there is still learning to do with CXL silicon – proofs of concepts are still in the works. It should be noted that any CXL 2.0 product is backwards compatible with CXL 1.0/1.1 – the standard is designed this way.Next week is the annual Supercomputing conference, focusing on high-performance computing. This would be a key arena for some of the CXL consortium members to make announcements, should any of them start to talk about CXL products or CXL 2.0 roadmaps.Related ReadingSynopsys Demonstrates CXL and CCIX 1.1 over PCIe 5.0: Next-Gen In ActionCXL Consortium Formally Incorporated, Gets New Board Members & CXL 1.1 SpecificationArm Joins CXL ConsortiumAMD Joins CXL Consortium: Playing in All The InterconnectsCompute Express Link (CXL): From Nine Members to Thirty ThreeCXL Specification 1.0 Released: New Industry High-Speed Interconnect From Intel\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16227/compute-express-link-cxl-2-0-switching-pmem-security\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD Zen 3 Ryzen Deep Dive Review: 5950X, 5900X, 5800X and 5600X Tested\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-11-05T14:01:00Z\n",
      "URL: https://www.anandtech.com/show/16214/amd-zen-3-ryzen-deep-dive-review-5950x-5900x-5800x-and-5700x-tested\n",
      "Content: When AMD announced that its new Zen 3 core was a ground-up redesign and offered complete performance leadership, we had to ask them to confirm if that’s exactly what they said. Despite being less than 10% the size of Intel, and very close to folding as a company in 2015, the bets that AMD made in that timeframe with its next generation Zen microarchitecture and Ryzen designs are now coming to fruition. Zen 3 and the new Ryzen 5000 processors, for the desktop market, are the realization of those goals: not only performance per watt and performance per dollar leaders, but absolute performance leadership in every segment. We’ve gone into the new microarchitecture and tested the new processors. AMD is the new king, and we have the data to show it.New Core, Same 7nm, Over 5.0 GHz!The new Ryzen 5000 processors are drop-in replacements for the Ryzen 3000 series. Anyone with an AMD X570 or B550 motherboard today, with the latest BIOS (AGESA 1081 or above), should be able to buy and use one of the new processors without a fuss. Anyone with an X470/B450 board will have to wait until Q1 2021 as those boards are updated.As we’ve previously covered, AMD is launching four processors today for retail, ranging from six cores up to sixteen cores.AMD Ryzen 5000 Series ProcessorsZen 3 MicroarchitectureAnandTechCoresThreadsBaseFreqTurboFreqL3CacheTDPMSRPRyzen 9 5950X16c/32t3400490064 MB105 W$799Ryzen 9 5900X12c/24t3700480064 MB105 W$549Ryzen 7 5800X8c/16t3800470032 MB105 W$449Ryzen 5 5600X6c/12t3700460032 MB65 W$299**Comes with Bundled CPU CoolerAll the processors have native support for DDR4-3200 memory as per JEDEC standards, although AMD recommends something slightly faster for optimum performance. All the processors also have 20 lanes of PCIe 4.0 for add-in devices.The Ryzen 9 5950X: 16 Cores at $799The top processor is the Ryzen 9 5950X, with 16 cores and 32 threads, offering a base frequency of 3400 MHz and a turbo frequency of 4900 MHz – on our retail processor, we actually detected a single core frequency of 5050 MHz, indicating thatthis processor will turbo above 5.0 GHzwith sufficient thermal headroom and cooling!This processor is enabled through two eight core chiplets (more on chiplets below), each with 32 MB of L3 cache (total 64 MB). The Ryzen 9 5950X is rated at the same TDP as the Ryzen 9 3950X, at 105 W. The peak power will be ~142 W, as per AMD’s socket design, on motherboards that can support it.For those that don’t read the rest of the review, the short conclusion for the Ryzen 9 5950X is that even at $799 suggested retail price, it enables a new level of consumer grade performance across the board. The single thread frequency is crazy high, and when combined with the new core design with its higher IPC, pushes workloads that are single-core limited above and beyond Intel’s best Tiger Lake processors. When it comes to multi-threaded workloads, we have new records for a consumer processor across the board.The Ryzen 9 5900X: 12 Cores at $549Squaring off against Intel’s best consumer grade processor is the Ryzen 9 5900X, with 12 cores and 24 threads, offering a base frequency of 3700 MHz and a turbo frequency of 4800 MHz (4950 MHz was observed). This processor is enabled through two six-core chiplets, but all the cache is still enabled at 32 MB per chiplet (64 MB total). The 5900X also has the same TDP as the 3900X/3900XT it replaces at 105 W.At $549, it is priced $50 higher than the processor it replaces, which means that for the extra 10% cost it will have to showcase that it can perform at least 10% better.The Ryzen 7 5800X: 8 Cores at $449After AMD showcased a quad core processor under $100 in the last generation, it takes a lot of chutzpah to offer an eight core processor for $449 – AMD stands by its claims that this processor offers substantial generational performance improvements. The new AMD Ryzen 7 5800X, with eight cores and sixteen threads, is set to go up against Intel’s Core i7-10700K, also an eight core / sixteen thread processor.The Ryzen 7 5800X has a base frequency of 3800 MHz and a rated turbo frequency of 4700 MHz (we detected 4825 MHz), and uses a single eight-core chiplet with a total 32 MB of L3 cache. The single core chiplet has some small benefits over a dual chiplet design where some cross-CPU communication is needed, and that comes across in some of our very CPU-limited gaming benchmarks. This processor also has 105 W TDP (~142 W peak).The Ryzen 5 5600X: 6 Cores for $299The cheapest processor that AMD is releasing today is the Ryzen 5 5600X, but it is also the only one that comes with a CPU cooler in box. The Ryzen 5 5600X has six cores and twelve threads, running at a base frequency of 3700 MHz and a peak turbo of 4600 MHz (4650 MHz measured), and is the only CPU to be given a TDP of 65 W (~88 W peak).The single chiplet design means 32 MB of L3 cache total (technically it’s still the same that a single core can access as the Ryzen 9 parts, more on that later), and will be put up against Intel’s six-core Core i5-10600K, which also retails in a similar ballpark.Despite being the cheapest and technically the slowest processor of the bunch, I was mightily surprised by the performance of the Ryzen 5 5600X: similar to the Ryzen 9 5950X, in single threaded benchmarks, it completely knocks the socks off of anything Intel has to offer – even Tiger Lake.Why Ryzen 5000 Works: ChipletsAt a high level, the new Ryzen 5000 'Vermeer' series seem oddly familiar to the last generation Ryzen 3000 ‘Matisse’ series. This is actually by design, as AMD is fully leveraging their chiplet design methodology in the new processors.To introduce some terminology, AMD creates two types of chiplets. One of them has the main processing cores, and is called a core complex die or CCD. This is the one that is built on TSMC's 7nm process. The other chiplet is an interconnect die with I/O, known as an IO die or IOD - this one has the PCIe lanes, the memory controllers, the SATA ports, the connection to the chipset, and helps control power delivery as well as security. In both the previous generation and the new generation, AMD pairs one of its IO dies with up to two 8-core chiplets.Ryzen 3000 processor without heatspreader, showing two core chiplets and one IO die.This is possible because the new core chiplets contain the same protocols for interconnect, physical design, and power constraints. AMD is able to leverage the execution of the previous platform and generation such that when the core connections are identical, despite the different internal structures (Zen 3 vs Zen 2), they can still be put together and executed in a known and successful fashion.As with the previous generation, the new Zen 3 chiplet is designed with eight coresZen 3 is a New Core DesignBy keeping the new 8-core Zen 3 chiplet the same size and same power, this obviously means that AMD had to build a core that fits within those constraints but also affords a performance and performance efficiency uplift in order to make a more compelling design. Typically when designing a CPU core, the easiest thing to do is to take the previous design and upgrade certain parts of it – or what engineers call tackling ‘the low hanging fruit’ which enables the most speed-up for the least effort. Because CPU core designs are built to a deadline, there are always ideas that never make it into the final design, but those become the easiest targets for the next generation. This is what we saw with Zen 1/Zen+ moving on to Zen 2. So naturally, the easiest thing for AMD to do would be the same again, but with Zen 3.However, AMD did not do this. In our interviews with AMD’s senior staff, we have known that AMD has two independent CPU core design teams that aim to leapfrog each other as they build newer, high performance cores. Zen 1 and Zen 2 were products from the first core design team, and now Zen 3 is the product from the second design team. Naturally we then expect Zen 4 to be the next generation of Zen 3, with ‘the low hanging fruit’ taken care of.Inour recent interviewwith AMD’s Chief Technology Officer, Mark Papermaster, we were told that if you were to look at the core from a 100,000 foot level, you might easily mistake that the Zen 3 core design to be similar to that of Zen 2. However, we were told that because this is a new team, every segment of the core has been redesigned, or at the very least, updated. Users who follow this space closely will remember that the branch predictor used in Zen 2 wasn’t meant to come until Zen 3, showing that even the core designs have an element of portability to them. The fact that both Zen 2 and Zen 3 are built on the same TSMC N7 process node (the same PDK, although Zen 3 has the latest yield/consistency manufacturing updates from TMSC) also helps in that design portability.AMD has already announced the major change that will be obvious to most of the techies that are interested in this space: the base core chiplet, rather than having two four-core complexes, has a single eight-core complex. This enables each core to access the whole 32 MB of L3 cache of a die, rather than 16 MB, which reduces latency of memory accesses in that 16-to-32 MB window. It also simplifies core-to-core communication within a chiplet. There are a couple of trade-offs to do this, but overall it is a good win.In fact there are a significant number of differences throughout the core. AMD has improved:branch prediction bandwidthfaster switching from the decode pipes to the micro-op cache,faster recoveries from mispredicts,enhanced decode skip detection for some NOPs/zeroing idiomslarger buffers and execution windows up and down the core,dedicated branch pipes,better balancing of logic and address generation,wider INT/FP dispatch,higher load bandwidth,higher store bandwidth,better flexibility in load/store opsfaster FMACsA wide variety of faster operations (including x87?)more TLB table walkersbetter prediction of store-to-load forward dependenciesfaster copy of short stringsmore AVX2 support (VAES, VPCLMULQD)substantially faster DIV/IDIV supporthardware acceleration of PDEP/PEXTMany of these will be explained and expanded upon over the next few pages, and observed in the benchmark results. Simply put, this is something more than just a core update – these are genuinely new cores and new designs that required new sheets of paper to be built upon.A number of these features, such as wider buffers and increased bandwidth, naturally come with the question about how AMD has kept the power the same for Zen 3 compared to Zen 2. Normally when a core gets wider, that means more silicon has to be turned on all the time, and this influences static power, or if it all gets used simultaneously, then there is higher active power.When speaking with Mark Papermaster, he pointed to AMD’s prowess in physical implementation as a key factor in this. By leveraging their knowledge of TSMC’s 7nm (N7) process, as well as updates to their own tools to get the best out of these designs, AMD was able to remain power neutral, despite all this updates and upgrades. Part of this also comes from AMD’s long standing premium partner relationship with TMSC, being able to enable better design technology co-optimization (DTCO) between floorplan, manufacturing, and product.AMD’s ClaimsThe CPU marketing teams from AMD, since the launch of first generation Zen, have been very accurate in their performance claims, even to the point of understating performance from time to time. Aside from promoting performance leadership in single thread, multi-thread, and gaming, AMD promoted several metrics for generation-on-generation improvement.+19% IPCThe key metric offered by AMD was a +19% IPC uplift from Zen 2 to Zen 3, or rather a +19% uplift from Ryzen 5 3800XT to Ryzen 5 5800X when both CPUs are at 4.0 GHz and using DDR4-3600 memory.In fact, using our industry benchmarks, for single threaded performance,we observed a +19% increase in CPU performance per clock. We have to offer kudos to AMD here, this is the second or third time they've quoted IPC figures which we've matched.In multithreaded SPECrate, the absolute gain was only around 10% or so, given that faster cores also require more bandwidth to main memory, which hasn’t been provided in this generation. This means that there are some bottlenecks to which a higher IPC won’t help if more cores require the same resources.For real-world tests, across our whole suite, we saw an average +24% uplift. For explicitly multithreaded tests, we saw ranges from even performance up to +35%, while for explicitly single threaded tests, this ranged from even performance up to +57%. This comes down to execution/compute bound tests getting bigger speedups over memory bound workloads.Best GamingFor gaming, the number was given as a +5 to +50% uplift in 1920x1080 gaming at the high preset, comparing a Ryzen 9 5900X against the Ryzen 9 3900XT, depending on the benchmark.In our tests at CPU limited settings, such as 720p or 480p minimum, we saw an average +44% frames-per-second performance uplift comparing the Ryzen 9 5950X to the Ryzen 9 3950X. Depending on the test, this ranged from +10% to +80% performance uplift, with key gains in Chernobylite, Borderlands 3, Gears Tactics, and F1 2019.For our more mainstream gaming tests, run at 1920x1080 with all the quality settings on maximum, the performance gain averaged around +10%. This spanned the gamut from an equal score (World of Tanks, Strange Brigade, Red Dead Redemption), up to +36% (Civilization 6, Far Cry 5).Perhaps the most important comparison is the AMD Ryzen 9 5950X against the Intel Core i9-10900K. In our CPU limited tests, we get a +21% average FPS win for the AMD at CPU-limited scenarios, ranging from +2% to +52%. But in our 1080p Maximum settings tests, the results were on average neck-and-neck, swaying from -4% to +6%. (That result doesn’t include the one anomaly in our tests, as Civilization 6 shows a +43% win for AMD.)Head-to-Head Performance MatchupsBased on core counts and pricing, the new Ryzen 5000 series processors closely align with some of Intel’s most popular Comet Lake processors, as well as the previous generation AMD hardware.Q4 2020 MatchupsAMDRyzen 5000CoresSEPTray1kuCoresIntelCore 10th GenRyzen 9 5950X16C$799vs.$99918CCore i9-10980XE*Ryzen 9 5900X12C$549vs.$48810CCore i9-10900KRyzen 7 5800X8C$449vs.$45310CCore i9-10850K$3748CCore i7-10700KRyzen 5 5600X6C$299vs.$2626CCore i5-10600K*Technically a high-end desktop platform processor, almost unavailable at MSRP.Throughout this review we will be referencing these comparisons, and will eventually break-out each processor into its own analysis breakdown.More In This ReviewAs this is our Deep Dive coverage into Zen 3, we are going to go into some nitty-gritty details. Over the next few pages, we will go over:Improvements to the core design (prefetchers, buffers, execution units, etc)Our microbenchmark tests (core-to-core latency, cache hierarchy, turbo ramping)New Instructions, Improved instructionsSoC Power and Per-Core PowerSPEC2006 and SPEC2017 resultsCPU Benchmarks (Office, Science, Simulation, Rendering, Encoding, Web, Legacy)Gaming Benchmarks (11 tests, 4 settings per test, with RTX 2080 Ti)Conclusions and Final Remarks\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16214/amd-zen-3-ryzen-deep-dive-review-5950x-5900x-5800x-and-5700x-tested\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Xbox Series X Review: Ushering In The Next Generation of Game Consoles\n",
      "Author: Brett Howse\n",
      "Date Published: 2020-11-05T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/16217/the-xbox-series-x-review-ushering-in-next-gen\n",
      "Content: What makes a console generation? The lines have been blurred recently. We can state that the Xbox Series X, and its less-powerful sibling, the Series S, are the next generation consoles from Microsoft. But how do you define the generation? Just three years ago, Microsoft launched the Xbox One X, the most powerful console in the market, but also with full compatibility with all Xbox One games and accessories. With multiple tiers of consoles and mid-generation refreshes that were significantly more powerful than their predecessors – and in some cases, their successors – the generational lines have never been this blurred before.None the less, the time for a “proper” next generation console has finally arrived, and Microsoft is fully embracing its tiered hardware strategy. To that end, Microsoft is launching not one, but two consoles, with the Xbox Series X, and the Xbox Series S, each targeting a difference slice of the console market both in performance and price. Launching on November 10, 2020, the new Xboxes bring some serious performance upgrades, new designs, and backwards compatibility for not only the Xbox One, but also a large swath of Xbox 360 games and even a good lineup of games from the original 2001 Xbox. The generational lines have never been this blurred before, but for Microsoft the big picture is clear: it’s all Xbox.The Xbox Series X is the flagship console, and the one we have been teased about for over a year. When launched, it will be the most powerful console in the market. Microsoft learned some lessons from the original Xbox One launch, and they seem keen to not make those mistakes again, so they have partnered with AMD to deliver a console with eight Zen 2 CPU cores paired with an AMD RDNA 2 graphics processor with 12 TFLOPS of performance. With 16 GB of GDDR6 memory, and 1 TB of solid-state storage, the new Xbox Series X targets gamers looking for 4K gaming at 60 FPS, with up to 4K 120 FPS on some titles.The Xbox Series S goes a different route, with a much more cost-effective console. It still features AMD Zen 2 CPU cores, but a much smaller 4 TFLOP GPU, which is not even as powerful as the Xbox One X from 2017. With 10 GB of GDDR6 and 512 GB of solid-state storage, the Xbox Series S targets 1440p gaming, or, more realistically for televisions, 1080p at up to 120 FPS, and with a large number of people still owning 1080p televisions, the less-expensive console has a definite niche.Both consoles offer solid-state storage, which is one of the key features of the new generation. While not a new idea, solid-state offers a significant number of performance benefits as have been realized in the PC space for a decade or more, but cost has been prohibitive before.With any console generation, the glue that holds it together is the games, and Microsoft has chosen a very different course for the Xbox Series X|S. With full backwards compatibility, there is already a large library of games, and games developed for the new consoles will continue to be available on the Xbox One as well, at least for now. Exclusives that are just for the Xbox Series X|S do not exist, which is certainly not how most console launches go. Instead, Microsoft sees the future of gaming in the Xbox Game Pass, which is a subscription service to a buffet of gaming titles.Xbox Specification ComparisonXbox Series XXbox Series SXbox One XXbox One SCPU Cores8C/16T8C/16T8C/8T8C/8TCPU Frequency3.8 GHZ3.6 GHz w/SMT3.6 GHZ3.4 GHz w/SMT2.3 GHz1.75 GHzCPU µArchAMD Zen 2AMD Zen 2AMD JaguarAMD JaguarGPU CoresAMD RDNA 252 CUs3328 SPs1.825 GHzAMD RDNA 220 CUs1280 SPs1.565 GHzAMD GCN 240 CUs2560 SPs1172 MHzAMD GCN 212 CUs768 SPs914 MHzPeak Shader Throughput12 TFLOPS4 TFLOPS6 TFLOPS1.4 TFLOPSEmbedded MemoryNoneNoneNone32MB eSRAMEmbedded Memory BandwidthNoneNoneNone218 GB/sSystem Memory16GB GDDR610GB GDDR612GB GDDR58GB DDR3-2133System Memory Bus320-bit128-bit384-bit256-bitSystem Memory Bandwidth10 GB @ 560 GB/s6 GB @ 336 GB/s8 GB @ 224 GB/s2 GB @ 56 GB/s326 GB/s68.3 GB/sStorage1 TBSSD802 GB Free512 GBSSD364 GB Free1 TB HDD500 GB HDDManufacturing ProcessTSMC 7nmTSMC 7nmTSMC 16nmTSMC 16nmDimensions151mm x 151mm x 301mm151mm x 275mm x 65mm300mm x 240mm x 60mm295mm x 230mm x 65mmWeight4.44kg1.92kg3.81kg2.9kgPSU315W(Internal)165W(Internal)245W(Internal)120W(Internal)Optical DriveUHD Blu-RayNoneUHD Blu-RayUHD Blu-RayWireless2x2 802.11ac2x2 802.11ac2x2 802.11ac2x2 802.11acLaunch Price (USD)$499$299$499$299Launch Date11/10/202011/10/202011/07/201708/02/2016Microsoft has blurred the generational lines significantly with the Xbox Series X|S launch in quite a few ways, and accessory support is definitely one of them. In past console generations, new consoles would have new controllers and all new accessories would be required. That is not the case with the Xbox Series X|S. All Xbox One controllers are 100% compatible, as should be most other accessories such as headphones, and even the Xbox Adaptive Controller. There is one major accessory which is not compatible though, and that is Kinect. Kinect was a major focus of the Xbox One launch, but Microsoft quickly dropped the expensive accessory from being a requirement, and its use-case never materialized in any significant way. Xbox One games that require Kinect will not be compatible with the new Series X|S but all other Xbox One games will be through backwards compatibility. If you still use Kinect and enjoy it, this may seem like a step back, but from personal experience, Kinect will not be missed.Another loss moving into this generation is the HDMI input which is no longer available. As part of the media push for the Xbox One, it included an HDMI input so you could insert the Xbox between your cable box and TV, allowing cable to be controlled via an IR blaster which could be plugged into the Xbox One. This had some advantages, but the media usage with DVR recording functionality ultimately never materialized, and there were likely only a small group of people taking advantage of this feature.With the console features that define generations a short list indeed, let us now dig into the technical specifications of the new consoles so we can determine if these are indeed generational updates.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16217/the-xbox-series-x-review-ushering-in-next-gen\n",
      "Title: A Broadwell Retrospective Review in 2020: Is eDRAM Still Worth It?\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-11-02T16:00:00Z\n",
      "URL: https://www.anandtech.com/show/16195/a-broadwell-retrospective-review-in-2020-is-edram-still-worth-it\n",
      "Content: Intel’s first foray into 14nm was with its Broadwell product portfolio. It launched into the mobile market with a variety of products, however the desktop offering in 2015 was extremely limited - only two socketed desktop processors ever made it to retail, and in limited quantities. This is despite users waiting for a strong 14nm update to Haswell, but also because of the way Intel built the chip. Alongside the processor was 128 MB of eDRAM, a sort of additional cache between the CPU and the main memory. It caused quite a stir, and we’re retesting the hardware in 2020 to see if the concept of eDRAM is still worth the effort.eDRAM: The SaviorIn recent years, Intel has pushed hard its infamous ‘Pyramid of Optane’, designed to showcase the tradeoff between small amounts of cache memory close to the CPU being low latency, out to the large offline storage offered for at a significant ping time. When a processor requires data and instructions, it navigates this hierarchy, with the goal to have as much of what is required as close to the CPU (and therefore as fast) as possible.Traditional modern x86 processors contain three levels of caches, each growing in size and latency, before reaching main memory, and then out to storage. What eDRAM does is add a fourth layer between the last L3 cache on the processor. Whereas the L3 is measured in single digit megabytes, the eDRAM is in the 10s-100s of megabytes, and DRAM measures in gigabytes. Whereas the L3 cache is located on the processor die and low latency, the eDRAM is slightly higher latency, and the main memory is on modules outside the processor socket at the highest latency. Intel enabled an ‘eDRAM’ layer as a separate piece of silicon with the processor package, up to 128 MiB, offering latency and bandwidth between the L3 and main memory.This piece of silicon was built on Intel’s 22nm IO manufacturing process, rather than 22nm SoC or 14nm, due to Intel’s ability to drive higher 22nm frequencies at the time.By keeping the eDRAM as a separate piece of silicon, it allowed Intel to adjust stock levels based on demand – if the product failed, there would still be plenty of smaller CPU die for packaging. Even today, processors made with extra eDRAM use the same die as seen back in 2013-2015, showing the longevity of the product.The first eDRAM products were mobile under the 22nm Haswell microarchitecture, but Broadwell saw it come to desktop.On the Broadwell processors, this resulted in a memory access layer with the following performance:Broadwell Cache StructureAnandTechSizeTypeLatencyBandwidthL1 Cache32 KiB / corePrivate4-cycle880 GiB/sL2 Cache256 KiB / corePrivate12-cycle350 GiB/sL3 Cache6 MiBShared26-50 cycle175 GiB/seDRAM128 MiBShared< 150 cycle50 GiB/sDDR3-1600Up to 16 GiBShared200+ cycle25.6 GiB/sThe simplistic view of this eDRAM was as a ‘level 4’ cache layer – this is ultimately how it was described to us at the time, with the eDRAM layer acting as a victim cache accepting L3 evictions but enabled through a shadow tag system accessed through the L3. Data needed from the eDRAM would have to be moved back into L3 before going anywhere else, including the graphics or the other IO or main memory. In order to do this, these shadow tags required approximately 0.5 MiB/core of the L3 cache, reducing the L3 usefulness in exchange for lower latency extending out to 128 MiB. This is why Broadwell only had 1.5 MiB/core of L3 cache, rather than the full 2.0 MiB/core that the die shot suggested it should have.Haswell/Broadwell eDRAM LayoutThe eDRAM could be dynamically split on the fly for CPU or GPU requests, allowing it to be used in CPU-only mode when the integrated graphics are not in use, or full for the GPU when texture caching is required. The interface was described to us at the time as a narrow double-pumped serial interface capable of 50 GiB/s bi-directional bandwidth (100 GiB/s aggregate), running at a peak 1.6 GHz.In this configuration, in combination with the graphics drivers, allowed for more granular control of the eDRAM, suggesting that the system could pull from both the eDRAM and the DDR memory simultaneously, potentially giving a peak memory bandwidth of 75.6 GiB/s, at a time when mid-range graphics cards such as the GT650M had a bandwidth around 80 GiB/s.The second generation of the eDRAM design, as found in Skylake and future processors, moved the eDRAM out of the purview of the L3 cache, and enabled it as a purely transparent buffer between the system agent and the main DRAM memory controller, making it invisible to CPU/GPU accesses or IO accesses. This allows the cache to be accessed by all DRAM requests, enabling full coherency (although the drivers still allow it to be bypassed for textures larger than the eDRAM size), as well as removing the 0.5 MiB/core L3 cache reduction for shadow tags.Skylake-and-beyond eDRAM LayoutThere are arguments to be made about whether the eDRAM as an L4 victim cache or as a transparent buffer to DRAM is the correct direction to go – as a victim cache, Intel stated it allowed a cache hit rate over 95%, however in a number of scenarios in order to get the best performance it required software intervention, and a lot of software was not aware of such a configuration. As a buffer, it enabled seamless integration that all software can take advantage of, but it is not necessarily as optimizable as an L4 victim cache.‘Go Big or Go Home’For Broadwell’s eDRAM products, Intel enabled a 128 MiB implementation, quadruple that found on Xbox One silicon at the time. At the time, Intel said that a 32 MiB eDRAM L4 victim cache enabled substantial hit rates, but the company wanted the design to be futureproof as well as a long-term option in Intel’s product stack, so it was doubled, and doubled again just to be sure. The term was ‘go big or go home’, and in our initial review of the first Broadwell eDRAM products, Anand noted that it was very rare to see Intel be so ‘liberal’ with die area.The eDRAM silicon was built on the 22nm SoC process, as mentioned, one node behind Intel’s leading edge CPU designs. The 128 MiB design came in at a die size of ~77 mm2, contributing to over a third of the total die area used in the 14nm Broadwell Iris Pro quad-core processor package (182mm2+ 77mm2= 259 mm2).In the subsequent next generation Skylake generation, eDRAM models with 64 MiB were also offered.Under certain constraints, the system could save power by disabling the main memory controller entirely if all the data required over a period of time is available in the eDRAM. As part of the initial Broadwell launch, Intel described the extra power consumption of the eDRAM as under 1 watt at idle, moving up to a peak of 5 watts when operating at full bandwidth. Ultimately this means that at a chip level, less power is available to the cores should it be needed, but the trade-off will be better performance in memory limited scenarios. The power is meant to be tracked by the on-die PCU, or Power Control Unit, that can shift power budget between the CPU, GPU, eDRAM, as needed by performance counters or thermals.As part of this review, we are able to give at least some insights into this number. In our testing, we saw idle package power numbers for the following processors:Core i7-4790S (22nm Haswell 4 core 6 MiB L3): 6.01 WCore i7-5775C (14nm Broadwell 4 core 6 MiB L3 + 128 MiB eDRAM) 9.71 WCore i7-6700K (14nm Skylake 4 core 8 MiB L3): 6.46 WThese numbers would suggest that the effect of the eDRAM, at idle, is more akin to 3.3-3.7 watts, not the sub 1-watt that Intel suggested. Perhaps that sub 1-watt value was more for mobile processors? When running at a steady-state full load, the processors reported power values of their TDP, which doesn’t enable any insight.Broadwell’s eDRAM Flop?Intel had somewhat backed itself into a corner with its Broadwell launch. Due to the delays of Intel’s 14nm process at the time, the company had decided to follow its popular Haswell-based 22nm Core i7-4770K high-end processor with the launch of a higher binned ‘Devil’s Canyon’ processor, the Core i7-4790K. This processor offered +500 MHz, which at the time was a substantial jump in performance, despite the processors being launched 12 months apart.Devil’s Canyon Review: Intel Core i7-4790K and i5-4690KBecause Broadwell ‘wasn’t ready’, Devil’s Canyon was designed to be a stop-gap measure to appease Intel’s ever-hungry consumers and high-end enthusiasts. From the consumer point of view, Devil’s Canyon was at least a plus, but it gave Intel a significant headache.By bumping the clock speed of its leading consumer processor by a significant margin, Intel now had a hill to climb – the goal of a new product generation is that it should be better than what came before. By boosting its previous best to be even better, it meant the next generation had to do even more. This is difficult to do when the upcoming process node isn’t working quite right. This meant that in the land of the desktop processor, Intel’s reluctance to launch Broadwell with eDRAM was painful to see, and the company had to change strategy.Intel almost made Broadwell for desktops a silent launch, with very little fanfare. After the announcement, there was almost zero stock on shelves. At the time, Intel did not sample the processors for review – we were able to obtain units from other sources a few days in advance for our launch day coverage.The Intel Broadwell Desktop Review: Core i7-5775C and Core i5-5675C Tested (Part 1)The Intel Broadwell Review Part 2: Overclocking, IPC and Generational AnalysisBy launching Broadwell Core i7 as a 65 W processor rather than an 84-88 W processor, it meant that the lower frequency Broadwell wasn’t necessarily a direct comparison to Devil’s Canyon. It came out of the gate with a frequency deficit, however the presence of the eDRAM would enable some very careful wins in memory limited scenarios, and perhaps most importantly, gaming.Ultimately the stunted launch of desktop Broadwell in June 2nd2015 was very quickly followed by launch of Skylake on August 5th2015, and the top Core i7 processor was once again an 88+ watt unit and a true like-for-like competitor to Devil’s Canyon. Skylake also enabled DDR4 in the market, which was a significant upgrade on the memory front.Unfortunately Intel had another conundrum – the older Broadwell processors, due to the eDRAM, actually offered slightly better gaming performance than Skylake! It was title, resolution, and quality dependent, and some might argue there was only a few percentage points in it, but for those that wanted the best at gaming, Skylake wasn’t necessarily the answer. For pretty much all CPU tasks though, Skylake was the answer.Broadwell Still Available TodayUltimately, Intel’s foray into socketed Broadwell processors with eDRAM was a momentary blip in its line of consumer-focused Core products. At the time, the processors were hard to find for sale, and were quickly made old by the arrival of Skylake and DDR4. There were six different Broadwell processors that were socketable, two mainstream Core products and four Xeon E3 parts.Intel Broadwell eDRAM Socketable CPUsAnandTechCoresThreadsBaseFreqTurboFreqIGPIGPFreqTDPConsumer Corei7-5775C4C / 8T3300370048 EUs115065 Wi5-5675C *4C / 4T3100360048 EUs110065 W*Sometimes listed as Core i7-5675C as some ES had an incorrect CPUID stringEnterprise Xeon E3 v4E5-1285 v44C / 8T3500380048 EUs115095 WE5-1285L v44C / 8T3400380048 EUs115065 WE3-1270L v44C / 8T30003600--45 WE3-1265L v44C / 8T2300330048 EUs105035 WWe were able to also review three of the Xeons at the time.The Intel Broadwell Xeon E3 v4 Review: 95W, 65W and 35W with eDRAMMost of these processors are actually very easy to purchase today. The best place to find them are either on Aliexpress, or eBay, for as little as $104.Broadwell in 2020The main highlight of these processors was the high-speed eDRAM, coming up to 50 GiB/s bidirectional, at a time when the DDR3-1600 memory solution in dual channel could only offer 25.6 GiB/s. At some point in the future, it would be expected for the speed of normal DRAM to surpass this bandwidth offered, even if it can’t exactly match that latency.We actually reached that mark very recently.Intel’s best consumer-grade processor is the Intel Core i9-10900K, offering 10 cores up to a peak 5.3 GHz, but most importantly the memory side has official support for DDR4-2933, which in dual channel mode would enable 46.9 GiB/s.Current AMD Zen 2 processors have a peak supported frequency of DDR4-3200, which in dual channel mode would enable 51.2 GiB/s bandwidth.Intel’s mobile Tiger Lake processors support LPDDR4X-4266, which when fully populated would provide 68.2 GiB/s bandwidth.With the introduction of DDR5 set to come in the next couple of years, we are expecting to see DDR5-4800 as a possible entry point. This would enable 38.4 GiB/s per 64-bit channel, or 76.8 GiB/s in a standard consumer configuration.Perhaps it is difficult to wrap your head around the fact that only in 2020 are we matching bandwidth levels that were enabled back in 2015 by the addition of a simple piece of silicon. It might make you question why Broadwell was the only family of Intel’s socketable processors to get this innovation – all future eDRAM products were all for mobile devices that rely on integrated graphics, despite the benefits observed for discrete graphics configurations.It should be noted that because eDRAM offers a latency benefit in memory accesses from 6 MiB to 128 MiB, then as we approach the situation where a single core has access to 128 MiB of L3 cache, this benefit would also disappear. For consumer processors, we’re not there quite yet – while Intel processors offer up to 20 MiB (or 24 MiB in upcoming Tiger Lake 8-core processors), AMD’s future Zen 3 processors will offer access to 32 MiB of L3 for each core within a CCX. By that metric, we’re still very far behind.For this review, because we recently tested Intel’s Tiger Lake quad-core processors and graphics, I wanted to probe exactly where Broadwell will finally sit in the hierarchy of CPU performance and graphics performance. We recently announced anew benchmark and gaming suite, and Broadwell is always one of the interesting products to put on a new test suite.All integrated gaming tests (as well as gaming tests with an RTX 2080 Ti) will be under the respective game pages.Pages In This ReviewAnalysis and CompetitionTest Setup and #CPUOverload BenchmarksPower ConsumptionCPU Tests: Office and ScienceCPU Tests: SimulationCPU Tests: RenderingCPU Tests: EncodingCPU Tests: Legacy and Web TestsCPU Tests: SyntheticsCPU Tests: SPECCPU Tests: MicrobenchmarksGaming: ChernobyliteGaming: Civilization VIGaming: Deus Ex: MDGaming: Final Fantasy XIVGaming: Final Fantasy XVGaming: World of TanksGaming: Borderlands 3Gaming: F1 2019Gaming: Far Cry 5Gaming: Gears TacticsGaming: GTA 5Gaming: Red Dead Redemption 2Gaming: Strange BrigadeConclusions and Final Words\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16195/a-broadwell-retrospective-review-in-2020-is-edram-still-worth-it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Akasa Turing Fanless Case Review: Unrivalled Noiseless NUC\n",
      "Author: Ganesh T S\n",
      "Date Published: 2020-10-26T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/16191/akasa-turing-fanless-case-review-noiseless-nuc-nonpareil\n",
      "Content: Silent computing systems are preferable for a multitude of use-cases ranging from industrial applications (where dust and fans make for a troublesome configuration) to noiseless HTPCs (particularly for audiophiles).Akasahas been providing thermal solutions in multiple computing verticals for more than 20 years, with a particular focus on passive cooling. Akasa targeted the NUC form-factor early, with the introduction of theNewtonchassis for Ivy Bridge NUCs in early 2013. Last year, the company unveiled theTuring fanless casefor the Bean Canyon NUCs. It marked a complete re-design of their NUC solution. This review takes a look at the build process and performance characteristics of a NUC8i5BEK board in the Turing chassis.IntroductionUse-cases for many silent or decade long deployment computing systems require the complete absence of any moving parts. In industrial deployments, the reason may be the need to avoid performance loss due to cooling efficiency degradation resulting from dust build-up. For professional creators, it may be due to the need to avoid extraneous noise affecting the work output. The average home consumer may also prefer a silent system to better focus on the work at hand. For HTPCs, multimedia content can be enjoyed without distractions - an aspect that may be of paramount importance to audiophiles.Traditionally, passively cooled computing systems have either been woefully underpowered for general purpose use, or carried a significant premium in terms of both cost and physical footprint. Recent advancements in compute performance per watt and novel passive cooling chassis designs (that do not cost an arm and a leg to mass-produce) have combined to give consumers the ability to create powerful, yet affordable, fanless systems. Akasa has been offering passively-cooled cases for NUC boards since 2013.Akasa Fanless NUC Cases - A Brief HistoryAkasa introduced their first NUC fanless chassis for the Ivy Bridge NUC, and quickly expanded their offerings to include standard desktop, low-profile, and waterproof models. Since then, each generation has seen variants of the same chassis with a few tweaks. Once ever few years, the company has thrown in some interesting re-designs. Broadly speaking, the fanless NUC cases from Akasa fall into one of these families:NewtonTeslaPascal (IP65)Plato (low profile)TuringThe Plato models are low-profile (38.5mm in height), while the Pascal models are IP65-rated (waterproof). The Turing has a contemporary design. Almost all of the recent models support 2.5\" drive bays. The Akasa offerings for various NUCs are summarized in the table below.Akasa Fanless NUC CasesChassis ModelNUC GenerationNotesNewtonNewton VTeslaPascal3rdGen. (Ivy Bridge) NUCsV model for the vPro Ivy Bridge NUCNewton HTesla HNewton X4thGen. (Haswell) NUCsTesla H includes 2x 2.5\" drive baysNewton TTesla TBay Trail Atom (Embedded) NUCNewton T is taller, while Tesla T is widerNewton LBay Trail Celeron NUCNewton MCNewton SPlato MCPlatoPlato XMax SMax MTPascal MC5thGen. (Broadwell) NUCsMax S includes a rear serial port and an ODD bayMax MT includes a rear serial port and two 2.5\" SATA traysNewton MC includes a front serial portNewton S includes a rear serial portPlato low-profile cases support i3 and i5 models, X supports i7 in additionPascal MC supports only the i3 modelNewton PBraswell NUCsNewton S6Plato X6Max MT65thGen. (Broadwell) NUCs & 6thGen (Skylake) NUCsReplaceable front and rear panels to support both 5th and 6th Gen. non-i7 NUCsCharacteristics similar to the non-6 variantsNewton S6T6thGen (Skylake) NUCsSupport for non-i7 Skylake NUCs onlySimilar to Newton S6 except for the power switch and LED being on the top panel instead of the frontGalacticoSkull Canyon NUCNewton ACApollo Lake Celeron NUCNewton S7Plato X7Pascal MD7thGen (Kaby Lake) NUCsNewton S7DNewton D3Plato X7DPascal MC37thGen (Kaby Lake & Kaby Lake-R) NUCsD3 includes a front serial port, while S7D has it in the rear panelNewton JCGemini Lake NUCsPlato X8Pascal BCTuring8thGen (Coffee Lake) NUCsTuring is a contemporary re-imagination of a fanless NUC chassisPlato PXNewton PX8thGen (Whiskey Lake) Pro NUCsTuring FX10thGen (Comet Lake) NUCsContemporary Turing design with updated I/O panelsThe unit we are looking at today is the first Bean Canyon NUC Akasa Turing chassis. As mentioned in the Frost Canyon NUC review, the Bean Canyon NUC offers a better all-round package. With the introduction of the 10nm Ice Lake processors with a leap in graphics capabilities and the incoming mini-PCs based on that, the Bean Canyon models currently in the retail channel may offer excellent value for money (given that they are going to be discounted). As we shall see in the rest of the review, the Akasa Turing can act as the perfect case for users looking to silence the Bean Canyon NUC.Setting the StageAkasa had provided us with a review sample of the Turing from the first batch last year, and Intel sent us the NUC8i5BEK (Core i5-based Bean Canyon NUC) for use with the Turing. This provided us with the opportunity to look at the performance characteristics of the actively cooled version and compare it against the Turing-based passively cooled one for the same BIOS settings and internal hardware configuration.A judicious choice of build components tuned for low-power and energy-efficient operation is advisable for passively-cooled builds. Towards that, we chose DDR4 SODIMMs that had a maximum operating frequency corresponding to the qualified memory type for the NUC8i5BEK. On the storage side, we chose a DRAM-less entry-level NVMe SSD with good power efficiency.G.Skill Ripjaws DDR4-SODIMM (F4-2400C16S-8GRS)Western Digital SN500 PCIe 3.0 x2 NVMe SSDNote that these components are from the time of the build last year - Since then, WD has introduced the SN550 PCIe 3.0 x4 NVMe SSD as an update for the same entry-level segment.This review will not go into the hardware features of the Bean Canyon NUC. For that, readers may refer to thereview of the NUC8i7BEH- the version with a Core i7 processor. The Core i5 version being looked at today carries over all the features that matter - a 28W TDP processor with four cores and eight threads, Iris Plus Graphics with integrated eDRAM, USB 3.2 Gen 2 (10Gbps) support on all external Type-A ports, a single Thunderbolt 3 port, and 4Kp60 support with HDCP 2.2 on the HDMI port. This configuration will serve users well even with the Tiger Lake NUCs on the horizon, particularly for non-HTPC applications. Unless 8K playback and AV1 hardware acceleration are needed, the Bean Canyon NUCs can do an excellent job even for HTPCs.Size Comparison of NUC vs Akasa Turing SilentWe put the standard kit through our benchmarking process first. Following that, we disassembled the unit, and transferred the board to the Akasa Turing. The same benchmarks were processed again on the Turing build. The power consumption and thermal stress tests were performed on both units. In addition to the comparison between the actively-cooled and passively-cooled versions of the NUC8i5BEB, we also consider some of the other passively cooled PCs reviewed earlier, as well as a couple of other recent UCFF NUCs. In the table below, we have an overview of the various systems that we are comparing. The relevant configuration details of the machines are provided so that readers have an understanding of why some benchmark numbers are skewed for or against the Intel NUC8i5BEB (Akasa Turing) when we come to those sections.Comparative PC ConfigurationsAspectIntel NUC8i5BEB (Akasa Turing)Intel NUC8i5BEB (Akasa Turing)Intel NUC8i5BEK (Standard Kit)ECS LIVA Z2Intel NUC10i7FNH (Frost Canyon)Intel NUC8i7BEH (Bean Canyon)Zotac ZBOX CI660 nanoCPUIntel Core i5-8259UIntel Core i5-8259UGPUIntel Iris Plus Graphics 655Intel Iris Plus Graphics 655RAMG.Skill Ripjaws F4-2400C16-8GRS DDR4 SODIMM16-16-16-40 @ 2400 MHz2x8 GBG.Skill Ripjaws F4-2400C16-8GRS DDR4 SODIMM16-16-16-40 @ 2400 MHz2x8 GBStorageWestern Digital WD Blue WDS500G1B0C(500 GB; M.2 2280 PCIe 3.0 x2; SanDisk 64L 3D TLC)Western Digital WD Blue WDS500G1B0C(500 GB; M.2 2280 PCIe 3.0 x2; SanDisk 64L 3D TLC)Wi-FiIntel Dual Band Wireless-AC 9560(2x2 802.11ac - 1733 Mbps)(Not usable - Missing antennae)Intel Dual Band Wireless-AC 9560(2x2 802.11ac - 1733 Mbps)(Not usable - Missing antennae)Price (in USD, when built)$314 (barebones)$134 (Akasa Turing kit)$568 (as configured)$314 (barebones)$134 (Akasa Turing kit)$568 (as configured)Prior to a discussion of the performance characteristics of the passively-cooled configuration, it is worthwhile to take a look at the build process for the machine. This is followed by a couple of sections devoted to the benchmark numbers for various workloads in order to determine if going the fanless route entails leaving out some performance potential on the table. A section on the HTPC aspects and a detailed discussion of the power consumption and thermal performance of the build precedes the concluding remarks.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16191/akasa-turing-fanless-case-review-noiseless-nuc-nonpareil\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Intel Reports Q3 2020 Earnings: Still Very Profitable, But Challenging Times Ahead\n",
      "Author: Ryan Smith\n",
      "Date Published: 2020-10-23T01:00:00Z\n",
      "URL: https://www.anandtech.com/show/16189/intel-reports-q3-2020-earnings-still-very-profitable-but-challenging-times-ahead\n",
      "Content: Once again kicking off our earnings season coverage for the tech industry is Intel, who reported their Q3 2020 financial results this afternoon. The traditional leader of the pack in more than one way, Intel has been under more intense scrutiny as of late, particularly dueto their previously disclosed delay in their 7nm manufacturing schedule. None the less, Intel has been posting record revenues and profits in recent quarters – even with a global pandemic going on – which has been keeping Intel in good shape. It’s only now, with Q3 behind them, that Intel is starting to feel the pinch of market shifts and technical debt – and even then the company is still well into the black.For the third quarter of 2020, Intel reported $18.3B in revenue. A drop of $0.9B over the year-ago quarter. As previously mentioned, Intel has been setting a string of record revenues in previous quarters, but the boom is coming to an end as margins and revenues are slipping. Those declines are also having the expected knock-on effect to Intel’s profitability, with the company reporting $4.3B in net income, a 29% drop versus Q3’19.This also marks the second quarter where Intel’s overall gross margin has been noticeably soft. The company, normally known for its zeal for 60% margins, recorded a margin of just 53.1% for Q3, following last quarter’s 53.3%, and 58.9% a year ago. The drop in gross margins is a big part of Intel’s financial story for the most recent quarter: according to the company, average selling prices (ASPs) are down as customers gravitate towards cheaper products, and meanwhile costs are up as Intel further ramps up its 10nm plans.Intel Q3 2020 Financial Results (GAAP)Q3'2020Q2'2020Q3'2019Revenue$18.3B$19.7B$19.2BOperating Income$5.1B$5.7B$6.4BNet Income$4.3B$5.1B$6.0BGross Margin53.1%53.3%58.9%Client Computing Group Revenue$9.8B+4%+1%Data Center Group Revenue$5.9B-17%-8%Internet of Things Group Revenue$677M+1%-33%Mobileye Revenue$234M+60%+2%Non-Volatile Memory Solutions Group$1.2B-31%-11%Programmable Solutions Group$411M-18%-19%Breaking things down on a group basis, the majority of Intel’s internal reporting groups have seen revenue declines over the year-ago quarter. Though still not Intel’s largest segment, their data center group has been the company’s biggest star over the past year and a significant source of revenue. But revenues have begun slipping there, and for Q3’20 Intel booked $5.9B, which is down 8% from last year. Driving this decline has been a drop in ASPs, which dropped 15% versus the year-ago quarter. Breaking this down, Intel cites a significant drop in enterprise and government sales, coupled with a jump in 5G SoC sales that are dragging down the average.As for Intel’s client computing group, revenues have held just slightly better than steady, growing 1% over last year. Still Intel’s largest group by revenue, client computing has been the most exposed to changes in buying habits from the coronavirus shift, which is represented in Intel’s revenue mix. Desktop sales are down and notebook sales are up; unfortunately notebook ASPs are down, handicapping the revenue gains there. Overall Intel is reporting that customers have shifted to less expensive processors, which has driven the revenue declines.Meanwhile the biggest loser for Intel in Q3 has been their IoT group, which saw a 33% drop in revenue as Intel was impacted by both the pandemic and new US government export restrictions. Its counterpart Mobileye fared better, however, with revenue growing 2%.Finally, Intel’s Non-Volatile Memory Solutions Group has been a hot topic this week, and is so again today. The group, which covers Intel’s NAND and Optane businesses, saw its revenue decline 11% versus Q3’19. According to the company ASPs are up versus this time last year, but not enough to cover a decline in total volume.The marginally profitable group is about to undergo a split, asIntel slices out the NAND side of the business and, pending government approval, sells it to SK Hynix. The unstable, commodity nature of NAND has vexed Intel in recent years, so the company is letting go of the business in order to focus on more profitable opportunities elsewhere. Still, Intel won’t be fully divested of the business until 2025, so it will remain a part of Intel for years to come. In the meantime, the company had very little to add about the transaction in today’s earnings announcement; since Intel is the seller, the initiative to discuss the deal lies with the buyer, SK Hynix.Overall, Q3 marked another very profitable quarter for Intel. But it’s also clear that the company is going to face some new and recurring challenges over the coming quarters, and may not be out of the woods until 2023, if not later. The biggest drag being their 7nm delays, which have seen Intel’s initial shipments pushed out to at least the end of 2022, if not later. These delays have also led to Intel looking at outside foundries (e.g. TSMC) to make up the difference, which will likely hurt Intel’s margins and leave the company fighting with other fab customers for supplies. For this reason Intel is still hoping to leverage its own 7nm fabs first and foremost, though the company doesn’t expect to make (or at least, report) its most critical decisions there until early 2021.In the meantime, Intel has further ramped up its 10nm capacity (which wasn’t the original plan) in order to meet their needs over the next couple of years. According to the company, Intel now has 3 10nm fabs, following the recent addition of a 10nm fab in Arizona. As a result it has more capacity to handle 10nm products such as their recently-launched Tiger Lake CPUs, with Intel stating that they now expect to ship 30% more 10nm chips this year than their original plans from January of 2020 called for.Unfortunately, Intel’s overall product mix remains in an odd spot, especially on the data center side of matters. Previously Intel has stated that Ice Lake Server (Ice Lake-SP) would start production shipments this year. And while this has technically changed, Intel has now clarified this to mean that qualification is only now taking place at the end of Q4, with volume ramps to start in 2021. Intel’s continued reliance on 14nm Skylake-based cores has been an albatross around the company’s neck, and the company badly needs a 10nm server product to improve the number of cores per CPU they can ship. However at the rate things are going, Ice Lake Server may very well end up being a short-lived part; if Intel’s Sapphire Rapids remains on schedule, the 10nm Enhanced SuperFin-based part is supposed to ship in 2021.Speaking of future products, Intel has also confirmed that they’re now samplingAlder Lake, which will be their 2021 client CPU. Intel’s first x86 hybrid CPU, the company has previously said that it will be for both desktops and mobile, and it will also be built on their 10nm Enhanced SuperFin process. With that said, Intel traditionally samples new parts well in advance – especially for parts that will lead to major platform changes – so this shouldn’t be taken as a sign that Alder Lake is anything sooner than a Q3/Q4 2021 product.In the interim, Intel is likely to face some of the stiffest competition in at least a decade, if not more. AMD and several Arm customers are eyeing Intel’s client and server market share (and associated profits) with their improved products, and while the company continues to talk up its own products, it's increasingly clear that they don't expect to be able to return to their traditional leadership position until at least 2023. So for now, with Intel still working to adapt to their newfound flexible fab strategy, combined with finally correcting a troubled 10nm process, it’s clear that after a long period of record revenues some challenging times are ahead for the company.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16189/intel-reports-q3-2020-earnings-still-very-profitable-but-challenging-times-ahead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Huawei Announces Mate 40 Series: Powered by 15.3bn Transistors 5nm Kirin 9000\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-10-22T16:00:00Z\n",
      "URL: https://www.anandtech.com/show/16156/huawei-announces-mate-40-series\n",
      "Content: Today Huawei took the stage to unveil the new Mate 40 series of devices. In the form of the Mate 40, Mate 40 Pro and the Mate 40 Pro+, the new phones represent the company’s leading edge in terms of technology, mostly enabled by the new Kirin 9000 chipset which is manufactured on a new 5nm manufacturing node, promising great leaps in performance and efficiency.The new phones also feature an updated design with a different camera layout, differentiated display design and improved speakers and charging features.The new Kirin 9000 is at the core of the discussion – and it’s also Huawei’s biggest problem as the new silicon is no longer under production since September due to US sanctions on the company, representing a much more substantial threat than the already existing limitations on the company’s products, such as not being able to ship with Google Mobile Services.Huawei Mate 40 SeriesMate 40Mate 40 ProMate 40 Pro+SoCHiSilicon Kirin 9000(E)1x Cortex-A77 @ 3.13 GHz3x Cortex-A77 @ 2.56 GHz4x Cortex-A55 @ 2.05 GHzGPUMali G78MP24Mali G78MP22(E)DRAM8GB12GBDisplay6.5\" OLED2376 x 108090Hz240Hz Touch68° edge curve6.76\" OLED2772 x 134490Hz240Hz Touch88° edge curveSizeHeight158.6mm162mmWidth72.5mm75.5mmDepth8.8mm9.1mm8.8mmWeight188g212g230gBattery Capacity4100mAh (Rated)4200mAh (Typical)40W SuperCharge4300mAh (Rated)4400mAh (Typical)50W SuperChargingWireless Charging-66W SuperChargeRear CamerasMain50MP 1/1.28\" 2.44µm RYYB sensorf/1.923mm eq.+ OISTelephoto3x Optical8MPf/2.4 OIS85mm eq.-3x Optical8MPf/2.4 OIS70mm eq.PeriscopeTelephoto-5x Optical12MP RYYBf/3.4 OIS125mm eq.10x Optical8MPf/4.4 OIS240mm eq.Wide16MPf/2.217mm eq.20MPf/1.818mm eq.20MPf/2.414mm eq.zero distortionfree-form lensExtraLaser AFLaser AF+ ToFFront CameraYes13MP Wide-angle f/2.4+ TOF sensorStorage128GB256GB256GB+ proprietary \"nanoSD\" cardI/OUSB-C3.5mm jackUSB-CWireless (local)802.11ax(Wifi 6),Bluetooth 5.2Cellular4G + 5G NR NSA+SA Sub-6GHzSplash, Water, Dust ResistanceIP68(water resistant up to 1m)Dual-SIM2x nano-SIMLaunch OSAOSP 10 w/ EMUI 11without Google servicesLaunch Price8+128GB:899€8+256GB:1199€12+256GB:1399€As mentioned, the biggest news today was the official unveiling of the new HiSilicon Kirin 9000 SoC. Manufactured on TSMC’s brand new 5nm process node, the Kirin 9000 represents the second and likely only other chip design after Apple’s A14 to ship in 2020. Huawei had made a lot of parallels to Apple’s and Android SoC competitors such as Qualcomm – focusing on some important milestones that the competition hasn’t yet been able to achieve at the high-end segment, such as integrating the 5G modem within the SoC instead of relying on an external chip. In this regard, Huawei calls the Kirin 9000 the first and only 5nm 5G SoC.The chip is also of substantial complexity, as Huawei discloses if features 15.3 billion transistors, 30% more than the recently announced Apple A14 which “only” features 11.8bn. An explanation for the vastly larger die size of course is the inclusion of an on-die modem which is currently lacking in other SoCs from the competition (due to various design & cost reasons). However, the modem isn’t the only IP block that bloats up the die size, as HiSilicon opted for a quite gigantic GPU configuration:Featuring a Mali-G78MP24, we’re actually seeing the chip designers rely on the maximum configuration of the G78 IP that Arm offers. HiSilicon had previously skipped the Mali-G77 generation which actually had been a large architectural change for Mali GPUs so it’s not exactly comparable, but the previous generation Kirin 990 used an 16-core Mali-G76, which is dwarfed by the new 24-core GPU. A more valid comparison would by the Exynos 990 with its G77MP11 configuration – and here the new Kirin 9000 features essentially 2.2x the cores.Undoubtedly TSMC’s new 5nm node density allows for designers to employ more transistors in the same area, but this new GPU is still quite a freak that comes quite unexpectedly, given the company’s history on focusing on cost, rather than all-out performance. In terms of absolute performance metrics, the Kirin 9000 is quoted at being 52% faster than the Snapdragon 865+, which is a very large leap and should put the SoC and the Mate 40 series near the top positions in terms of performance.We don’t yet know the clock frequencies of the design, but I expect it to be very low given the large number of cores, representing an extreme case of a “wide and slow” configuration.The rest of the SoC also has seen updates, although not quite as large within the competitive landscape. The CPUs have been updated from Cortex-A76 cores to the newer Cortex-A77 IP, and HiSilicon has shifted from a 2+2+4 design to a 1+3+4 design, much like seen in recent generation Qualcomm chips. The new cores clock up to 3.13GHz on the fastest core, which is a slight lead over the Snapdragon 865+. HiSilicon’s product cycle means that it rarely manages to catch Arm’s new CPU IP release cycle, and as we expected doesn’t take advantage ofthe newer A78 or X1 CPUsthat we expect from upcoming Exynos and Snapdragon chipsets in a few months.Other improvements of the SoC include a new generation NPU – actually this here depends on the binning variant of the SoC as HiSilicon will have both a regular Kirin 9000 as well as a lower-end Kirin 9000E which will differ in terms of a slightly smaller GPU with 22 cores and only one big NPU core rather than two in the fastest SoC bin. It’s likely that the GPU takes up such a large amount of area on the chip that HiSilicon had to resort to functional binning of the chip to get out more working units, something we rarely see in the mobile space as power-binning is the preferred method of binning.The new chip features a new hybrid LPDDR4X/5 memory controller, and the Mate 40 series should feature the latter LP5 memory, although Huawei hasn’t yet confirmed the exact specifications.There’s also a new generation image signal processor which is said to improve HDR for both stills and video recording. Huawei also now boasts that this is the 3rdgeneration 5G modem, promising significant advantages over the competition such as the Qualcomm X55 modem.The 5nm manufacturing node also brings with it power efficiency improvements, and the most important disclosure here is the fact that Huawei is promising 25% better CPU efficiency versus the Snapdragon 865+. Given that both SoCs have almost the same CPU configuration at very similar clock frequencies, the majority of that 25% figure should be due to the process node improvements, which bodes well for future 5nm chipsets, as well as Apple’s A14 chip.The big elephant in the room is the fact that the Kirin 9000 is going to be a unicorn product– due to US sanctions TSMC no longer is allowed to produce silicon for Huaweiand has stopped delivering chipsets last September 15th. Speculations are that Huawei had only managed to received 3-5 million units – the actual number is unknown as Huawei refuses to comment on the matter.In terms of the actual devices, we’ve seen the announcements of three phones. The “regular” Mate 40 features a simpler design and reminds us more of a OnePlus 8 when viewed from the front, distinguished by a 6.58” OLED screen with a 2376 x 1080 resolution. Huawei opts for a mid-range 90Hz refresh rate but does offer a 240Hz touch sampling rate. Overall, this variant of phone seems pretty straightforward.The Mate 40 Pro and Pro+ on the other hand have a more exotic design as Huawei again resorted to a curved screen design that falls to the sides with an 88° curvature. Huaweihad used such a design before in the Mate 30 Pro, and frankly I found it to be annoying and a bad design that was just impractical. The new Mate 40 Pro’s don’t seem to showcase any significant improvements and thus I still have to get a hands-on with the devices, I’m pretty sceptical about the ensuing ergonomics of the phones.The screen is 6.76” – a little larger due to it flowing to the side of the phone, and the resolution is 2772 x 1344, featuring the same 90Hz refresh rate and 240Hz sampling rate.In general, this newer generation represents a bigger form-factor design than the Mate 30 Pro series as Huawei has increased the width of the phone from 73.1mm to 75.5mm, essentially a step-size bump in terms of device size, such as going from an S20+ to an S20 Ultra.Weight has consequently also gone up from 198g to 212g. Battery size is described at 4400mAh typical capacity which is actually 100mAh lower than the Mate 30 Pro. Huawei still says that due to the efficiency of the SoC and the overall device, it should last for longer than competing smartphones.In terms of camera setup, the new device’s hardware isn’t all too newcompared to the P40 series.All the models feature the same main camera module: a 50MP RYYB sensor with a 1/1.28” size and 1.22µm pixels, binning down to 2.44µm for 12.5MP images. The optics are a wide 23mm equivalent lens with f/1.9 aperture. The main problem of this module was that Huawei’s default camera settings always crops in to a 27mm equivalent field-of-view, producing outstretched native 10MP images that always upscaled up to 12.5MP which wasn’t always great. The benefits of the sensor come in low-light where Huawei’s phones are still unbeatable.It's to be noted that the main camera only features OIS on the Pro+ variant of the phone, which is a rather very bad differtiation to make in terms of product segmentation as it'll give a worse off experience for the majority of users.The ultra-wide on the Mate 40 is a 16MP unit with f/2.2, while on the Mate 40 Pro it’s an 20MP unit with f/1.8.The telephoto units are also the same as on the P40 series; the Mate 40 only comes with a 3x optical 12MP unit with f/2.4 aperture and OIS, whilst the P40 Pro uses the 5x optical 12MP f/3.4 unit with OIS. These result in respectively 85mm and 125mm equivalent focal lengths.The Mate 40 Pro+ switches the periscope unit to a 10x optical magnification with a 240mm equivalent focal length with a smaller 8MP sensor and f/4.4 aperture, with OIS, and includes a fourth module in the form of a 12MP 3x optical 70mm equivalent 12MP f/2.4 unit with OIS.The only real in interesting new announcement on the part of the camera setup was on the Mate 40 Pro+’s ultra-wide unit. If features the same 20MP sensor as on the Mate 40 Pro, however the optics now include a free-form lens which allows the camera to capture distortion-free and rectilinear images – something which all other smartphones today have to achieve via software corrections with large quality compromises towards the edges of the frame. Having a free-form lens should result in excellent image quality that’s above the competition – it’s just a pity that Huawei is only using this on the more expensive Mate 40 Pro+.In terms of camera image quality improvements, Huawei claims much better HDR processing that’s above the competition this generation, as well as just overall a greatly improve camera software experience.Other improvements of the phones include better speakers, achieving a claimed 150% better bass response, as well as the introduction of 66W wireless charging and 50W wired charging.Unclear Availability, Still High PricingThe Mate 40 comes in at 899€ in 8+128GB configuration, the Mate 40 Pro comes in at 1199€ for a 8+256GB config, and finally the Mate 40 Pro+ lands in at a whopping 1399€ for a 12+256GB model.In general, the Mate 40 series looks interesting on paper, however will still continue to be plagued by the fact that Huawei cannot ship the phones with Google Mobile Services. It’s been a year since Huawei first launched the Mate 30 series without the Google Play store, and has been making headways in their own in-house app store, the AppGallery, but even today it’s missing a ton of applications that a lot of users would consider critical to a mobile smartphone experience.Subjective design opinions aside, the Mate 40 seem to offer excellent hardware even though the premium models represent quite a large price bump.The biggest question for Huawei will be how and where they will be selling these phones. The launch event today includes prices in Euros, but it’s unclear exactly in which markets the company will actually be selling the phones, and which of the variants will be available. There’s also the pressing matter that Huawei only has a fixed and limited supplies of Kirin chips available – putting a cap on the amount of Mate 40 units the company can sell. Given all these uncertainties, Huawei has some very hard times ahead of it.Related Reading:TSMC Confirms Halt to Huawei Shipments In SeptemberMobile Flagship Phone Camera Overview 2020 H1: Still Picture BattleHuawei Announces P40, P40 Pro and P40 Pro+: A New Generation of CamerasThe Huawei Mate 30 Pro Review: Top Hardware without Google?\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16156/huawei-announces-mate-40-series\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD Zen 3: An AnandTech Interview with CTO Mark Papermaster\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-10-16T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16176/amd-zen-3-an-anandtech-interview-with-cto-mark-papermaster\n",
      "Content: The announcement of the new Ryzen 5000 processors, built on AMD’s Zen 3 microarchitecture, has caused waves of excitement and questions as to the performance. The launch of the high-performance desktop processors on November 5thwill be an interesting day.In advance of those disclosures, we sat down with AMD’s CTO Mark Papermaster to discuss AMD’s positioning, performance, and outlook.Dr. Ian CutressAnandTechMark PapermasterAMDWe’ve interviewed Mark a number of times before here at AnandTech, such asat the launch of second generation EPYCor looking atAMD’s 2020 prospects(and a couple of discussions that were never published). Mark is always very clear on what the vision of AMD’s roadmaps are, as always likes to highlight some of the key areas of AMD’s expertise that sometimes don’t hit the standard column inches.With the launch of Zen 3, and the Ryzen 5000 family, the key headline that AMD is promoting is an absolute desktop high-performance leadership, across workloads, gaming, and energy efficiency. It puts AMD in a position the company hasn’t held for at least 15 years, if the numbers are true. As part of the launch event, the AMD team reached out if we had some questions for Mark. Indeed we do.You can read our launch day coverage here:AMD Ryzen 5000 and Zen 3 on Nov 5th: +19% IPC, Claims Best Gaming CPUIC: When I interviewed Lisa at the crest of that first generation Ryzen launch, she mentioned how AMD’s positioning helped the company to think outside the box to develop its new high-performance x86 designs. Now that AMD is claiming market performance leadership, how do AMD’s engineering teams stay grounded and continue to drive that out-of-the-box thinking?MP:Of our team we are very proud - they are one of the most innovative engineering teams in the industry. So this is a hard fought battle to get into this leadership position with Zen 3 and I can tell you we have a very strong roadmap going forward. The team indeed is staying very very grounded - you look at the kind of approach that we took on Zen 3, and you know it wasn’t any one silver bullet that delivered the performance [uplift], it was really touching almost every unit across the CPU and the team did an excellent job of driving improvements in performance, improvements in efficiency, reducing the latency to memory, and providing a tremendous improvement in performance.[We achieved] a 19% in a single generation of instruction per clock over our previous generation, which was Zen 2 released just mid of last year. So it was a phenomenal achievement, and it’s that focus on what I’ll call ‘hardcore engineering’ that the team will continue going forward - it won’t be about silver bullets, it will be about continuing to provide real-world performance gains to our customers.IC: To highlight that 19% value: two of those highlights of AMD’s announcements include the +19% increase in raw performance per clock compared to Zen 2, but also this new core complex design with eight cores and 32 MB of L3 cache. To what extent is the larger core complex helping with the raw performance increase, or are there other substantial benefits in the design by moving to the combined CCX?MP:The change in the basic construct of the core complex was very very important in allowing us to realize reductions in latency to memory which is huge for gaming. Gaming is a huge market for us in high performance desktop and games typically have a dominant thread - and so that dominant thread, its performance is very dependent on the L3 cache available to it. This is because if it can hit in that local L3 cache, obviously it’s not traversing all the way out to main memory. So by reorganizing our core complex and doubling it to four cores that have direct access to 16 MB L3 cache, by now having eight cores that have direct access to a 32 MB of L3 cache you really - it’s the single biggest lever in reducing latency. Obviously when you hit in the cache you provide effective latency - it directly improves performance. It was a big lever for gaming, but we had a number of levers behind that - again we really touched every unit in the CPU.IC: Doubling that L3 cache access for each core, from 16 MB to 32 MB, is a sizable leap I’ll grant you that! It improves that overall latency up to 32 MB as you’ve said so we don’t have to go out to main memory. But has doubling the size affected the L3 latency range at all? Obviously there are tradeoffs when you double an L3 cache, even so when you have more cores accessing it.MP:The team did an excellent job on engineering, both logically and physically. That’s always the key - how to architect the reorganization, so to change the logic to support this new structure and equally focus on the physical implementation - how do you optimize layout so you’re not adding stages of delay that would effectively neuter the gains? It was tremendous engineering on the reorganization on the Zen 3 core that truly delivers the benefit in reduced latency.I’ll go beyond that - as we talk about physical implementation, the normal thinking would be when you add the amount of logic changes that we did to achieve that 19% IPC, normally of course the power envelope would go up. We didn’t change technology nodes - we stayed in 7nm. So I think your readers would have naturally assumed therefore we went up significantly in power but the team did a phenomenal job of managing not just the new core complex but across every aspect of implementation and kept Zen 3 in the power envelope that we had been in Zen 2.When you look at Ryzen as it goes out, we are able to stay in the same AM4 socket and that same power envelope and yet deliver these very very significant performance gains.IC: Speaking to that process node, TSMC’s 7nm as you said: we’ve specifically been told that it is this sort of minor process update that was used for Ryzen 3000XT. Are there any additional benefits that Ryzen 5000 is getting through the manufacturing process that perhaps we are not aware of?MP:It is in fact the core is in the same 7nm node, meaning that the process design kit [the PDK] is the same. So if you look at the transistors, they have the same design guidelines from the fab. What happens of course in any semiconductor fabrication node is that they are able to make adjustments in the manufacturing process so that of course is what they’ve done, for yield improvements and such. For every quarter, the process variation is reduced over time. When you hear ‘minor variations’ of 7nm, that is what is being referred to.IC: Moving from Zen 2 to Zen 3, the headline number in the improvement of performance per watt is 24% on top of the 19% IPC. This obviously means that there have been additional enhancements made at the power deliver level - can you talk to any of those?MP:We have a tremendous focus on our power management. We have an entire microcontroller and power management schema that we have across the entire CPU. We enhance that every generation, so we’re very proud of what the Zen 3 team has done to achieve this 24% power improvement. It is yet more advances in whole Precision Boost to give us more granularity in managing both frequency and voltage while constantly listening to the myriad of sensors that we have on the chip. It is yet more granularity and the adaptability of our power management to the workload that our users are running on the microprocessor. So it is more responsive, and being more responsive means that it also delivers more efficiency.IC: One of the points made on Zen 2 was a relatively high idle power draw of the IO die, anywhere from 13 W to 20 W. We’ve been told that this time around Zen 3 uses the same IO die as Zen 2 did. We just wanted to confirm that does Zen 3 change anything in this regard, given the focus on power efficiency and performance per watt, or is it the same design for the sake of compatibility or cost effectiveness?MP:These are incremental advancements on the IO die that allowed us to give our customers in high performance desktop, to leverage that AM4 socket while getting these performance gains - that was a very calculated move to deliver CPU performance while giving continuity to our customer base. We are constantly driving power efficiency - with Zen 3 the focus was on the core and the core-cache complex in driving the bulk of our power efficiency.IC: Can you talk about AMD’s goals with regards to IO and power consumption - we’ve seen AMD deliver PCIe Gen4 in 7nm but the IO die is still based in 12/14nm from Global Foundries. I assume it is a key target for improvements in the future just not this time around?MP:It’s generational - if you look to the future we drive improvements in every generation. So you will see AMD transition to PCIe Gen 5 and that whole ecosystem. You should expect to hear from us in our next round of generational improvements across both the next-gen core that is in design as well as that next-gen IO and memory controller complex.IC: Speaking about the chiplet itself, the AMD presentation gave us a high-level view of the increased core complex. We’ve noted that the off-chip communication for those chiplets has now been moved from the center of between the two core complexes to the edge. Are there any specific benefits to this, such as wire latency, or power?MP:You look at that optimization trade-off, marrying the logical implementation with the physical implementation. So the new cache core complex was designed to minimize latency from the CPU cores themselves into that cache complex. To put the control circuits being placed where they are means the longer wire lengths can go to the less latency sensitive circuits.IC: Over the last couple of years, AMD has presented that it has a roadmap when it comes to its Infinity Fabric design, such as striving towards the two typical areas of higher bandwidth and better efficiency. Does Zen 3 and the new Ryzen 5000 family have any updates to the IF over Ryzen 3000?MP:We do - we made improvements, you’ll see new security elements that will be rolling out. We boosted our security, [and] we are always tuning our Infinity Architecture. With Zen 3 the focus was on delivering the raw CPU performance. So in terms of our Infinity Architecture for Ryzen desktop it’s incremental, and we’ll be rolling out some of those details - we’ve very excited about it and it’s a great compliment to the main headline story, which is CPU performance leadership.IC: With AMD and Intel, we’re now seeing both companies binning the silicon from the fabs to within an inch of its maximum - leaving very little overclocking headroom just so users have that peak performance straight out of the box. From your perspective how do features such as the Precision Boost Overdrive, where frequencies go above the stated range on the box - how do features like that evolve or do they just slowly disappear as binning optimization and knowledge increases over time?MP:Of course our goal is to maximize what we support with our maximum boost frequency. With Zen 3 we do increase that to 4.9 GHz. We’re always focused on improving our binning - the way you should think about it is [that] we’ll always have the best boost frequency that we can provide, and it is tested across the full gamut of workloads. Our test suite tries to cover literally every type of workload that we believe our customers would be able to run on our CPU. But end-users are very smart, and they might have a segment of those applications, and our thought is that we will continue to provide overclocking so that the enthusiast that really understands their workloads and may have a workload that gives them an opportunity to run even faster given the unique nature of what they are interested in, of what they’re running, and we want to give them that flexibility.IC: We’ve touched upon security as it relates to changes in the Infinity Fabric - can you comment on AMD’s approaches to the major topics of security vulnerabilities, and if there are any new features inside Zen 3 or Ryzen 5000 to assist with this?MP:We’ll be rolling out more details on this, but it will continue the train we’ve been on. We’ve always been a security first approach to our design - we’re very very resilient to side channel attacks just based on the nature of our microarchitectural implementation, [and] the way we implemented x86 was very very strong. We have had great success and uptake of the encryption capability we have both across our whole memory space or our ability to encrypt unique instances of virtualization.We’ll continue that track with Zen 3. We will have more details forthcoming in the coming weeks, but what you’ll see is more enhancements that further protect against other rogue elements out there like Return Oriented Programming (ROP) and other facets that you have seen bad actors try to take advantage of out there in the industry.IC: Would you say the focus of those security enhancements is necessarily towards the enterprise rather than the consumer, just due to the different environments? Does AMD approach the markets separately, or is it more of a blanket approach?MP:We generally try and think about what is the best security we can provide across the full array of end applications. Of course, Enterprise is typically will have a higher focus on security, but I believe that has changed over time and everyone, whether you are running your CPU in a high performance application such as content creation, computation, gaming - I believe that security is foundational. So although historically it has been the focus of Enterprise, and it drives our approach of rolling out security enhancements as best we can across all of our products. We believe it is foundational.IC: Back to the 19% IPC uplift – in part of the presentation, AMD breaks down where it believes those separate percentages come from with different elements of the microarchitecture. It is clear that the updates to the load/store units and the front end contribute perhaps half of that benefit, with micro-op cache updates and prefetcher updates in the other half. Can you go into some slight detail about what has changed in load/store and front-end - I know you’re planning to do a deeper dive of the microarchitecture as we get closer to launch, but is there anything you can say, just to give us a teaser?MP:The load/store enhancements were extensive, and it is highly impactful in its role it plays in delivering the 19% IPC. It’s really about the throughput that we can bring into our execution units. So when we widen our execution units and we widen the issue rate into our execution units it is one of the key levers that we can bring to bear. So what you’ll see as we roll out details that we have increased our throughput on both loads per cycle and stores per cycle, and again we’ll be having more details coming shortly.IC: Obviously the wider you make a processor you start looking at a higher static power and active power - is this spending more focus on the physical design to keep the power down?MP:It’s that combination of physical design and logic design. What I think many people might miss in the story of Zen 3 as we roll it out, the beauty of this design is in fact the balance of bringing extensive changes to drive up the performance while increasing the power management controls and the physical implementation to allow the same typical power switching per cycle as we had in the previous generation - that’s quite a feat.IC: Zen 3 is now the third major microarchitectural iteration of the Zen family, and we have seen roadmaps that talk about Zen 4, and potentially even Zen 5. Jim Keller has famously said that iterating on a design is key to getting that low hanging fruit, but at some point you have to start from scratch on the base design. Given the timeline from Bulldozer to Zen, and now we are 3-4 years into Zen and the third generation. Can you discuss how AMD approaches these next iterations of Zen while also thinking about that the next big ground-up redesign?MP:Zen 3 is in fact that redesign. It is part of the Zen family, so we didn’t change, I’ll call it, the implementation approach at 100000 feet. If you were flying over the landscape you can say we’re still in the same territory, but as you drop down as you look at the implementation and literally across all of our execution units, Zen 3 is not a derivative design. Zen 3 is redesigned to deliver maximum performance gain while staying in the same semiconductor node as its predecessor.IC: While the x86 market for both client and enterprise is very competitive, there is increasing pressure from the Arm ecosystem in both markets, it’s hard to deny. At present, Arm’s own Neoverse V1 designs are promising a near-x86 level of IPC, and subsequent 30% year-on-year architectural uplift, at a fraction of the power that x86 runs at. While AMD’s goals so far have been achieving peak performance, like in Zen 3, but how does AMD intend to combat non x86 competition, especially as they are starting to promise in their roadmaps more and more performance?MP:We won’t let our foot of the gas pedal in terms of performance. It’s not about ISA (instruction set architecture) - in any ISA once you set your sight on high performance you’re going to be adding transistors to be able to achieve that performance. There are some differences between one ISA and another, but that’s not fundamental - we chose x86 for our designs because of the vast software install base, the vast toolchain out there, and so it is x86 that we chose to optimize for performance. That gives us the fastest path to adoption in the industry. We have historically have lived in nothing but a competitive environment - we don’t expect that to change going forward. Our view is very simply that the best defense is in fact a strong offence - we’re not letting up!IC: With the (massive) raw performance increases in Zen 3, there hasn’t been much talk on how AMD is approaching CPU-based AI acceleration. Is it a case of simply having all these cores and the strong floating point performance, or is there scope for on-die acceleration or optimized instructions?MP:Our focus on Zen 3 has been raw performance - Zen 2 had a number of areas of leadership performance and our goal in transitioning to Zen 3 was to have absolute performance leadership. That’s where we focused on this design - that does include floating point and so with the improvements that we made to the FP and our multiply accumulate units, it’s going to help vector workloads, AI workloads such as inferencing (which often run on the CPU). So we’re going to address a broad swatch all of the workloads. Also we’ve increased frequency which is a tide that, with our max boost frequency, it’s a tide that raises all boats. We’re not announcing a new math format at this time.IC: Has AMD already prepared accelerated libraries for Zen 3 with regard to AI workloads?MP:We do - we have math kernel libraries that optimize around Zen 3. That will be all part of the roll-out as the year continues.IC: Moving to competitive analysis, has the nature or approach of AMD’s competitive analysis changed since the first generation of Zen to where we sit today and where AMD is going forward?MP:We have consistently kept a clear focus on the competition. We look across our x86 competitors, and any emerging competitors using alternate ISAs. No change - one thing that we believe is you always have to do two things. One, listen to your customers, and understand where their workloads are going, where needs may be evolving over time, and secondly, and keep a constant eye on the competition. That is a key part of what got us to the leadership position with Zen 3, and an element of our CPU design culture that will not change going forward.IC: A lot of success of Zen 2 and both Ryzen and EPYC has been the chiplet approach: tiny chiplets, high yield, and can also be binned for frequency very well. However we’re starting to see large monolithic silicon being produced at TSMC now at 7nm, with some of TSMC’s customers going beyond the 600mm2 range. AMD is in a position now where revenues are growing, market share is growing, and now it comes out with Ryzen 5000 - where do AMD’s thoughts lie on producing CPU core chiplets on a larger scale - obviously [as core counts increase] you can’t end up with a million chiplets on a package!MP:We innovated in the industry on chiplets and as you saw last year as we rolled out our Zen 2 based products in both high-performance desktop and server, it gave us tremendous flexibility - it allowed us to be very very early in the 7nm node and achieve many manufacturing efficiencies but also design flexibilities. It is that flexibility going forward that you’ll to continue to see drive more adoption of chiplets. We will continue at AMD, and although some of our competitors derided us at our first release of chiplets, frankly you see most of them adopting this approach.It’s never going to be one size fits all, so I do believe, based on the market that you’re attacking and the type of interaction you have across CPU, GPU, and other accelerators will command whether the best approach is in fact a chiplet or monolithic. But chiplets are here to stay - they’re here to stay at AMD, and I believe they’re here to stay for the industry.IC: It’s funny you mention competitors, because recently they announced that they are moving to a very IP-block chiplet design scaling as appropriate. This means chiplets for cores, for graphics, for security, for IO - exactly how far down the chiplet rabbit hole to we go here?MP:There is always a balance - a great idea overused can become a bad idea. It’s really based on each implementation. Everyone in the industry is going to have to find their sweet spot. Of course, there is supply chain complexity that has to be managed - so every design that we do at AMD, we’re focused on how do we get the best performance in the best organization physically, how we implement that performance, and ensure that we can deliver it through our supply chain reliably to our customers. That’s the tradeoff that we make for each and every product architecture.IC: TSMC recently launched its 3D Fabric branding, covering all aspects of its packaging technology. AMD already implements a ‘simple’ CoWoS-S in a number of products, however (there) are other areas such as TSMC’s chip stacking or package-integrated interposers - I assume that AMD looks at these for consideration into the product stack. Can you talk about how AMD approaches the topic, or what’s being considered?MP:Our approach to packaging is to partner deeply with the industry - deeply with our foundry partners, deeply with the OSATs. I truly believe that we’re entering a new era of innovation in packaging and interconnect. It’s going to give chip design companies like AMD increasing flexibility going forward. It also creates an environment for increasing collaboration - what you’re seeing is the chiplet technology advance such that you can have more flexibility in co-packaging known good dies. This was always a dream in the industry, and that dream is now becoming reality.IC: We’ve seen AMD making inroads into other markets where it hasn’t had such a high market share, such as Chromebooks, and AMD’s first generation [Zen] embedded technologies. Does AMD continue to go specifically for these markets, or is there untapped potential in markets in AMD hasn’t particularly played in, such as IoT or Automotive?[Note this question was asked before the AMD-Xilinx rumors were publicized]MP:We continue to look at adjacent markets versus where we play in today. We’ve been in embedded, and we are growing share in embedded, so that certainly continues a focus at AMD. What we’re not doing is going after, what I’ll call, the markets that may have a lot of media attention but are not well matched to the kind of high performance and incredible focus that we have at AMD. We want to deliver high performance at a value to the industry, and so we will continue to putting our energies into our share in those that markets that really value what we can bring to the table.Many thanks to Mark and his team for their time.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16176/amd-zen-3-an-anandtech-interview-with-cto-mark-papermaster\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Imagination Announces B-Series GPU IP: Scaling up with Multi-GPU\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-10-13T08:00:00Z\n",
      "URL: https://www.anandtech.com/show/16155/imagination-announces-bseries-gpu-ip-scaling-up-with-multigpu\n",
      "Content: It’s almost been a year since Imagination had announced itsbrand-new A-series GPU IP, a release which at the time the company called its most important in 15 years. The new architecture indeed marked some significant updates to the company’s GPU IP, promising major uplifts in performance and promises of great competitiveness. Since then, other than a slew ofinternal scandals, we’ve heard very little from the company – until today’s announcement of the new next-generation of IP: the B-Series.The new Imagination B-Series is an evolution of last year’s A-Series GPU IP release, further iterating through microarchitectural improvements, but most importantly, scaling the architecture up to higher performance levels through a brand-new multi-GPU system, as well as the introduction of a new functional safety class of IP in the form of the BXS series.The Market Demands Performance: Imagination Delivers it through Multi-GPUIt’s been no secret that the current GPU IP market has been extremely tough on IP providers such as Imagination. Being the only other established IP provider alongside Arm, the company had been seeing an ever-shrinking list of customers due to several factors – one being Arm’s extreme business competitiveness in offering both CPU and GPU IP to customers, and the fact that there’s simply less customers which require licensed GPU IP.Amongst the current SoC vendors, Qualcomm and their in-house Adreno GPU IP is in a dominant market position, and in recent years had been putting extreme pressure on other vendors – many of these who fall back to Arm’s Mali GPU IP by default. MediaTek had historically been the one SoC vendor who had been using Imagination’s GPUs more often in their designs, however all of the recent Helio of Dimensity products again use Mali GPUs, with seemingly little hope for a SoC win using IMG’s GPU IP.WithApple using their architectural licensefrom Imagination to design custom GPUs, Samsung betting onAMD’s new aspirations as a GPU IP provider, and HiSilicon both designing their own in-house GPU as well ashaving an extremely uncertain future, there’s very little left in terms of mobile SoC vendors which might require licensed GPU IP.What is left are markets outside of mobile, and it’s here that Imagination is trying to refocus: High-performance computing, as well as lucrative niche markets such as automotive which require functional safety features.Scaling an IP up from mobile to what we would consider high-performance GPUs is a hard task, as this directly impacts many of the architectural balance choices that need to be made when designing a GPU IP that’s actually fit for low-power market such as mobile. Traditionally, this had been always a trade-off between absolute performance, performance scalability and power efficiency – with high performance GPUs simply being not that efficient, while low-power mobile GPUs were unable to scale up in performance.Imagination’s new B-Series IP solves this conundrum by introducing a new take on an old way of scaling performance: multi-GPU.Rather than growing and scaling a single GPU up in performance, you simply use multiple GPUs. Now, probably the first thing that will come to user’s minds are parallels to multi-GPU technologies from the desktop space such as SLI or Crossfire, technologies that in recent years have seen dwindling support due to their incompatibility with modern APIs and game engines.Imagination’s approach to multi-GPU is completely different to past attempts, and the main difference lies in the way workloads are handled by the GPU. Imagination with the B-Series moves away from a “push” workload model – where the GPU driver pushes work to the GPU to render, to a “pull” model, where the GPU decides to pull workloads to process. This is a fundamental paradigm shift in how the GPU is fed work and allows for what Imagination calls a “decentralised design”.Amongst a group of GPUs, one acts as a “primary” GPU with a controlling firmware processor that divides a workload, say a render frame, into different work tiles that can then the other “slave” GPUs can pull from in order to work on. A tile here is actually the proper sense of the word, as the GPU’s tile-based rendering aspect is central to the mechanism – this isn’t your classical alternate frame rendering (AFR) or split frame rendering (SFR) mechanism. Also, just how a single-GPU tile-based renderer can have varying tile sizes for a given frame, this can also happen in the B-Series’ multi-GPU workload distribution, with varying tile sizes of a single frame being distributed unevenly amongst the GPU group.The most importantly, this new multi-GPU system that Imagination introduces is completely transparent to the higher-level APIs as well as software workloads, which means that a system running a multi-GPU configuration just sees one single large GPU from a software perspective. This is a big contrast to current discrete multi-GPU implementations, and why Imagination’s multi-GPU technology is a lot more interesting.From an implementation standpoint, it allows Imagination and their customers a ton of new flexibility in terms of configuration options. From Imagination’s perspective, instead of having to design one large and fat GPU implementation, which might require more work due to timing closure and other microarchitectural scaling concerns, they can just design a more efficient GPU – and allow customers to simply put down multiple of these in an SoC. Imagination claims that this allows for higher-frequency GPUs, and the company projects implementations around 1.5GHz for high-end use-cases such as for cloud computing usages.For customers, it’s also a great win in terms of flexibility: Instead of having to wait on Imagination to deliver a GPU implementation that matches their exact performance target, it would be possible for a customer to just take one “sweet-spot” building block implementation and scale the configuration themselves all on their own during the design of their SoC, allowing higher flexibility as well as a smaller turn-around time. Particularly if a customer would be designing multiple SoCs for multiple performance targets, they could achieve this easily with just one hardware design from Imagination.We’ll get into the details of the scaling in the next page, but currently the B-Series multi-GPU support scales up to 4 GPUs. The other interesting aspect of laying down multiple GPUs on an SoC, in contrast to one larger GPU, is that they do not have to be adjacent or even near each other. As they’re independent design blocks, one could do weird stuff such as putting a GPU in each corner of an SoC design.The only requirements for the SoC vendor are to have the GPUs connected to the SoC’s standard AXI interconnect to memory – something that’s a requirement anyhow. Vendor might have to scale this up for larger MC (Multi-Core) configurations, but they can make their own choices in terms of design requirements. The other requirement to make this multi-GPU setup work is just a minor connection between the GPUs themselves: this are just a few wires that act as interrupt lines between the cores so that they can synchronise themselves – there’s no actual data traffic happening between the GPUs.Because of this, this is a design that’s particularly fit for today’s upcoming multi-chiplet silicon designs. Whereas current monolithic GPU designs have trouble being broken up into chiplets in the same way CPUs can be, Imagination’s decentralised multi-GPU approach would have no issues in being implemented across multiple chiplets, and still appear as a single GPU to software.Getting back to the initial point, Imagination is using this new multi-GPU approach to target higher performance designs that previously weren’t available to the company. They note that their more efficient mobile-derived GPU IP through multi-GPU scaling can compete with other current offerings from Nvidia and AMD (Imagination promotes their biggest configuration as reaching up to 6TFLOPs) in PCIe form-factor designs, whilst delivering 70% better compute density – a metric the company defines as TFLOPs/mm². Whilst that metric is relatively meaningless in terms of performance due to the fact that the upper cap on performance is still very much limited by the architecture and the MC4 top scaling limit on the current multi-GPU implementation of the B-Series, it allows for licensees to make for smaller chips that in turn can be extremely cost-effective.The B-Series covers a slew of actual GPU IP, with the company continuing a segmentation into different performance tiers – the BXT series being the flagship GPU designs, BXM series a more balanced middle-ground GPU IP, and the BXE series being the company’s smallest and most area efficient Vulkan compatible GPU IP. Let’s go over the various GPU implementations in more detail…\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16155/imagination-announces-bseries-gpu-ip-scaling-up-with-multigpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The NZXT N7 Z490 Motherboard Review: From A Different Direction\n",
      "Author: Gavin Bonshor\n",
      "Date Published: 2020-10-07T14:30:00Z\n",
      "URL: https://www.anandtech.com/show/16033/the-nzxt-n7-z490-motherboard-review-\n",
      "Content: It's been nearly two years to the day since NZXT last released a motherboard, which was the Z370N7. NZXT initially used ECS as its motherboard OEM,but has opted to use ASRock this time round for a new N7 model. This has thesame N7 infused armor,albeit using a combined metal and plastic instead of just metal which does reduce the overall cost. Aiming for the mid-range market, NZXT's N7 Z490 features 2.5 GbE, Wi-Fi 6, dual M.2, and four SATA ports, and we give it our focus in this review.The N7 Z490: Going For Mass EffectAt the beginning of September, NZXT reached out to me explaining that they intended to launch a new motherboard into an already cramped Z490 market. One of the elements NZXT wanted to address over its motherboard offerings was the firmware and overclocking, given that ECS isn't really a popular name on those two fronts. This time around, NZXT has leveraged ASRock's services for the underlying platform, on top of which NZXT has layered its own styling and form.Armed and equipped with a reasonable mid-ranged feature set, the NZXT N7 Z490 includes a premium HD audio codec, combined with 2.5 GbE and an Intel Wi-Fi 6 interface. Perhaps one of the most notable features is the boards full cover armor plating (plastic and metal), which users familiar with NZXT's previous motherboards will recognize. It stretches across the entirety of the PCB, only showing the CPU socket area, memory slots, and the top and bottom edges of the board.Looking at memory support closer, the N7 Z490 includes the capacity to install up to 128 GB, with speeds of up to DDR4-4266 supported. This is a couple of pegs lower than other models on the market, but the likelihood of regular users opting for higher speeds than this is likely to be slim due to cost. The board uses two PCIe 3.0 x4 M.2 slots for storage, with four SATA ports available. On the rear panel, the N7 Z490 uses just two USB 3.2 G2 ports, a single Type-C, and just one Type-A, with two USB 3.2 G1 Type-A and two USB 2.0 ports.In our performance testing, the N7 Z490 performed and behaved like any other Z490 board we've tested from ASRock. We saw the N7 Z490 do well in our Non-UEFI POST time test, get a good result in our DPC latency test, and observed competitive performance in our computational and gaming benchmarks. Overall, for any stock level performance, it was hard to find fault with the board.For overclocking, the NZXT N7 Z490 performed better than any NZXT board I've tested so far (I've owned all of them). Similar to other ASRock models, when adjusting the CPU VCore voltage in the firmware, it reverts the LLC profile to level 1 for tighter and aggressive VDroop control. It feels as if the firmware isn't quite polished to the standard we would expect for release - in our manual overclock testing, setting 1.250 V in the firmware at 4.8 GHz, all cores gave us a load VCore of 1.384 V, which is way too much. This presented even more problems at higher overclocks and caused a lot of downclocking, which skewed our results, with a possible issue within the board's loadline calibration profile settings. More info on this in the review.The NZXT N7 Z490 has an MSRP of $230, which puts it in direct competition with models such as the ASRock Z490 PG Velocita ($235), the GIGABYTE Z490 Aorus Elite AC ($220), and the ASUS Prime Z490-A ($230). All of the competing models include support for faster memory, more SATA ports, with the ASUS model offering better USB 3.2 G2 connectivity. What NZXT has squarely in its favor is a competitive networking combination, with a unique and uniformed aesthetic due to the PCB armor, which looks great. The crux of the matter is whether or not the NZXT N7 Z490 steps up to the challenge in our test suite.Read on for our extended analysis and comparison tests.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16033/the-nzxt-n7-z490-motherboard-review-\n",
      "Title: DDR5 is Coming: First 64GB DDR5-4800 Modules from SK Hynix\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-10-06T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/16142/ddr5-is-coming-first-64gb-ddr5-4800-modules-from-sk-hynix\n",
      "Content: Discussion of the next generation of DDR memory has been aflutter in recent months as manufacturers have been showcasing a wide variety of test vehicles ahead of a full product launch. Platforms that plan to use DDR5 are also fast approaching, with an expected debut on the enterprise side before slowly trickling down to consumer. As with all these things, development comes in stages: memory controllers, interfaces, electrical equivalent testing IP, and modules. It’s that final stage that SK Hynix is launching today, or at least the chips that go into these modules.DDR5 is the next stage of platform memory for use in the majority of major compute platforms. The specification (as released in July 2020) brings the main voltage down from 1.2 V to 1.1 V, increases the maximum silicon die density by a factor 4, doubles the maximum data rate, doubles the burst length, and doubles the number of bank groups. Simply put, the JEDEC DDR specifications allows for a 128 GB unbuffered module running at DDR5-6400. RDIMMs and LRDIMMs should be able to go much higher, power permitting.JEDEC DDR GenerationsDDR5DDR4DDR3LPDDR5Max Die Density64 Gbit16 Gbit4 Gbit32 GbitMax UDIMM Size(DSDR)128 GB32 GB8 GBN/AMax Data Rate6.4 Gbps3.2 Gbps1.6 Gbps6.4GbpsChannels per Module2111Total Width(Non-ECC)64-bits(2x32-bit)64-bits64-bits16-bitsBanks(Per Group)44816Bank Groups8/44/214Burst LengthBL16BL8BL8BL16Voltage (Vdd)1.1v1.2v1.5v1.05vVddq1.1v1.2v1.5v0.5vThere are four angles in the world of DDR that everyone involved in the specification wants to iterate on. Capacity is the obvious one, but also memory bandwidth plays a key role in performance scaling of common multi-core workloads in the large core-count servers we are seeing. The other two are power (an obvious goal), and the other is latency, another key metric for performance.With DDR5, one of the major changes to help drive this is the way the memory is seen by the system. Rather than being a single 64-bit data channel per module, DDR5 is seen as two 32-bit data channels per module (or 40 bits in ECC). The burst length has doubled, meaning that each 32-bit channel will still deliver 64 bytes per operation, but can do so in a more interleaved fashion. That means the standard ‘two 64-bit channel DDR4’ system will morph into a ‘quad 32-bit channel DDR5’ arrangement, although each memory stick provides a total of 64-bits but in a more controllable way. This also makes doubling the data rate, a key element in increasing peak bandwidth, easier, as well as a finer-grained bank refresh feature, which allows for asynchronous operations on the memory while it is in use, reducing latency.Voltage regulation is also being moved from the motherboard to the memory module, allowing the module to regulate its own needs. We already saw DDR4 adopt a per-chip Vdroop control, but this takes the whole idea a stage further for tighter power control and management. It also puts power management in the hands of the module vendor rather than the motherboard manufacturer, allowing the module manufacturer to size up what is required for faster memory – it will be interesting to see how different firmware cope with non-JEDEC standard gaming memory that will undoubtedly go above specification.SK Hynix’s announcement today is that they are ready to start shipping DDR5 ECC memory to module manufacturers – specifically 16 gigabit dies built on its 1Ynm process that support DDR5-4800 to DDR5-5600 at 1.1 volts. With the right packaging technology (such as 3D TSV), SK Hynix says that partners can build 256 GB LRDIMMs. Additional binning of the chips for better-than-JEDEC speeds will have to be done by the module manufacturers themselves. SK Hynix also appears to have its own modules, specifically 32GB and 64GB RDIMMs at DDR5-4800, and has previously promised tooffer memory up to DDR5-8400.SK Hynix has not provided information of the sub-timings of these modules. The JEDEC specification defines three different modes for DDR5-4800:DDR5-4800A: 34-34-34DDR5-4800B: 40-40-40DDR5-4800C: 42-42-42It is unclear which one of these that SK Hynix is using. The module says '4800E', however that appears to just be part of the module naming, as the JEDEC specification doesn't go beyond a CL value of 42 for DDR5-4800.For bandwidth, other memory manufacturers have quoted that for the theoretical 38.4 GB/s that each module of DDR5-4800 can bring, they are already seeing effective numbers in the 32 GB/s range. This is above the effective 20-25 GB/s per channel that we are seeing on DDR4-3200 today. Other memory manufacturershave already announcedthat they are sampling DDR5 with customers since the beginning of the year.As part of the announcement, it was interesting to see Intel as one of the lead partners for these modules. Intel has committed to enabling DDR5on its Sapphire Rapids Xeon processor platform, due for initial launch in late 2021/2022. AMD was not mentioned with the announcement, and neither were any Arm partners.SK Hynix quotes that DDR5 is expected to be 10% of the global market in 2022, increasing to 43% in 2024. The intersection point for consumer platforms is somewhat blurred at this point, as we’re probably only half-way through (or less than half) of the DDR4 cycle. Traditionally we expect a cost interception between old and new technology when they are equal in market share, however the additional costs in voltage regulation that DDR5 requires is likely to drive up module costs – scaling from standard power delivery on JEDEC modules up to a beefier solution on the overclocked modules. It should however make motherboards cheaper in that regard.Source:SK hynixRelated ReadingDDR5 Memory Specification Released: Setting the Stage for DDR5-6400 And BeyondSK Hynix: We're Planning for DDR5-8400 at 1.1 VoltsCadence DDR5 Update: Launching at 4800 MT/s, Over 12 DDR5 SoCs in DevelopmentSamsung to Produce DDR5 in 2021 (with EUV)Here's Some DDR5-4800: Hands-On First Look at Next Gen DRAMCES 2020: Micron Begins to Sample DDR5 RDIMMs with Server PartnersSK Hynix Details DDR5-6400Keysight Reveals DDR5 Testing & Validation SystemSK Hynix Develops First 16 Gb DDR5-5200 Memory Chip, Demos DDR5 RDIMMCadence & Micron DDR5 Update: 16 Gb Chips on Track for 2019Cadence and Micron Demo DDR5-4400 IMC and Memory, Due in 2019\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16142/ddr5-is-coming-first-64gb-ddr5-4800-modules-from-sk-hynix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Announces Cortex-A78AE, Mali-G78AE and Mali-C71AE Autonomous System IPs\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-09-29T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16114/arm-announces-cortexa78ae-malig78ae-and-malic71ae-autonomous-system-ips\n",
      "Content: Functional safety is an area of computing that is becoming ever more important as we see more and more embedded technologies integrated into our daily lives. Arm’s Automotive Enhanced (AE) line of IP had been launched back in 2018 with the release of the Cortex-A76AE.Fast-forward a few years, it’s time for a new set of AE IP, with Arm now introducing the new Cortex-A78AE, bringing a higher performance CPU core, and also for the first time introducing an AE class GPU and ISP in the form of the Mali-G78AE and Mali-C71AE. With the move, Arm also says that it is diversifying beyond just the automotive sector and widening the scope to industrial and other autonomous systems.Hercules-AE is Cortex-A78AEStarting off with the CPU, the new IP isn’t exactly new as we’ve first heard about the new Hercules-AE design last year duringNvidia’s announcement of their “Orin” automotive SoC.The new Cortex-A78AE, as its name implies, is based offthe Cortex-A78 microarchitecturewhich we’ve extensively covered in in our in-depth Tech Day article earlier this summer.Compared to the previous generation Corex-A76AE this means a 30% uplift in IPC and higher performance.What’s new this year in regards to the functional safety features of the IP is the introduction of a new operating “hybrid mode” that represents a new architecture for how to achieve ASIL-B compliance, but with a lesser performance impact compared to the existing Split Mode operating mode.Functional safety currently is achieved on AE CPUs by running in either “Split Mode” or “Locked Mode”. Locked mode is quite straightforward and includes running pairs of cores in lock-step with additional logic controlling that the computational results between the pairs are consistent at all times. Effectively this cuts your throughput in half as you are always duplicating work done.Split mode maintaining ASIL-B functional safety still requires the cores to be periodically checked for correct operation which makes them temporarily unavailable. The problem lies at the DSU-level (Dynamic Shared Unit – the L3 cache) as for this to be checked it will make the whole CPU cluster unavailable, and this has a larger performance impact on the system.The new hybrid mode adds additional logic on the part of the DSU to enable it failure detection without having to make it unavailable to the CPUs, and thus ensuring continuous operation and computational throughput. It’s to be noted that this redundancy on the part of the DSU means additional control logic, but does not include actually duplicating the L3 SRAMs which would incur a large area penalty.The new hybrid mode thus would represent a higher performing design configuration for ASIL-B workloads with a comparatively smaller cost in area overheard in the DSU. If a vendor chooses to implement Hybrid Mode or remains with the simpler Split Mode configuration is a design-time choice that takes into account the extra area requirements. ASIL-D operation in Locked Mode naturally continues to require the extra area investment.As noted, the Cortex-A78AE had already been licensed quite some time ago and Nvidia’s new Orin SoC with 12 cores is the first publicly known design to employ the new cores.Mali-G78AE - Finally introducing virtualisationAlongside the Cortex-A78AE, Arm is also for the first time announcing a functional safety capable GPU in the form of the new Mali-G78AE. Based onthe mobile Mali-G78 GPU core, we should be expecting similar performance and power efficiency figures from the IP- scaling up from 1 to 24 cores.The important addition of the new IP is the inclusion of full-fledged hardware virtualisation, a critical feature for autonomous systems that to date had been lacking in the company’s GPU IP.Hardware virtualisation is important to be able to separate safety critical software from other non-critical workloads, ensuring that if anything were to go wrong (such as for example some odd workload crashing the GPU driver), that the safety critical components continue to operate without issue.Samsung’sExynos Auto V9is an example of such a SoC design where previously it had to deploy 3 GPU clusters (MP12+MP3+MP3) to ensure independent workload operation that would not impact critical systems.With hardware virtualisation, a newer design with the Mali-G78AE wouldn’t require multiple GPU clusters and instead be able to use a single GPU, flexibly partitioning inner-GPU execution resources between concurrent multiple workloads. The IP supports four such partitions. Beyond the hardware partitioning, software virtualisation also allows time-split operation of workloads within the same partition.The new IP supports functional safety to an ASIL-B standard – this is both a design limitation, however Arm also says that this is currently what customers are demanding. It would be possible to achieve higher ASIL-D ability in a design if you put down two identical GPUs in your design and compare outputs.Beyond the announcement of the new CPU and GPU IP, we’re also seeingan extension of the company’s existing Mali-C71 ISP IPwith the new C71AE which adds support for ASIL-B and SIL2 integrity checking.Also part of the announcement today is Arm’s enablement for reference autonomous platforms:Beyond these new amazing hardware technologies, we are working to enable the developers of autonomous systems. For software developers we are enabling familiar cloud native technologies in autonomous applications to ease development, while Arm development solutions accelerate software development and validation while shortening the path to deployment. For developers of autonomous silicon, our physical IP, training and design reviews help reduce risk.Related Reading:Arm Unveils Arm Safety Ready Initiative, Cortex-A76AE ProcessorArm Announces Cortex-A65AE for Automotive: First SMT CPU CoreARM Announces Mali-C71: Their First Automotive-Grade Image Signal ProcessorNVIDIA Details DRIVE AGX Orin: A Herculean Arm Automotive SoC For 2022\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16114/arm-announces-cortexa78ae-malig78ae-and-malic71ae-autonomous-system-ips\n",
      "Title: ASRock Industrial 4X4 BOX-V1000M Ryzen Mini-PC Review: Finding Zen In The Small Things\n",
      "Author: Ganesh T S\n",
      "Date Published: 2020-09-28T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/16111/asrock-industrial-4x4-boxv1000m-ryzen-minipc-review\n",
      "Content: The miniaturization trend triggered by the ultra-compact form factor NUCs from Intel has emerged as a key driver in the growth of the PC market. Processor power efficiency is of paramount importance in this space, and AMD had been caught napping when the NUCs began to take flight. The introduction of the Zen microarchitecture in the Ryzen processors has scripted a remarkable turnaround for AMD. With leading core counts, the Ryzen processors have taken the HEDT market by storm. UCFF PC manufacturers, however, opted to play the wait and watch game, and it took a while before the embedded SoC versions of the first-generation Ryzens started appearing in the PC market. Last year, ASRock Industrial introduced one of the first Ryzen UCFF systems in the form of the 4X4 BOX-V1000M. This review attempts to figure out how the unit fares against the entrenched incumbents.Introduction and Product ImpressionsSmall form-factor (SFF) PCs and gaming systems have represented the bulk of the growing segment in the PC market over the last few years. Intel's NUC line-up has enjoyed unprecedented success. Despite the introduction of notebook processors using the first-generation Zen microarchitecture, AMD's power efficiency was not good enough for vendors (and even AMD themselves) to make a dent in Intel's success in the NUC space. The equation changed slightly with the launch of the AMD Ryzen Embedded processors in early 2018. Starting as a trickle with a mini-STX board from withSapphire, the uptake of the Ryzen Embedded series became averitable deluge late last yearwith UCFF systems from ASRock Industrial, EEPD, OnLogic, and SimplyNUC.ASRock Industrial has been at the forefront of AMD-based UCFF PCs, being one of the first tobring out UCFF systemsbased on the Ryzen Embedded APUs in mid-2019. In fact, their boards have been adopted by vendors such as OnLogic in theirML100G-40systems. The company offers three different 4X4 BOX configurations - the dual-core R1505G and R1606G variants in the R1000M and R1000V, and the flagship quad-core V1605B-based V1000M. The last of the three is the one we are looking at today.The 4X4 BOX-V1000M is ASRock Industrial's flagship AMD Ryzen Embedded offering with a 104mm x 102mm main-board housed in a 110mm x 118.5mm x 67.3mm plastic chassis. The system matches the Intel NUCs in the footprint department. The board comes with a soldered processor - the V1605B belonging to the AMD Ryzen Embedded V-Series. It is a quad-core processor with SMT enabled (4C/8T). It can operate with a TDP configurable between 12W and 25W.Switching to peripherals and networking, the board's WLAN component is M.2 card - the Intel AC3168. The system is otherwise barebones, providing with the flexibility to choose their own storage device and RAM. For best performance, a PCIe 3.0 x4 NVMe SSD can be used, and DDR4-2400 SODIMMs are supported. However, it must be noted that the system does NOT support M.2 2280 SSDs. Only M.2 2242 and 2260 Key M SSDs (both SATA and NVMe) are supported. We installed a Transcend MTS600 SATA SSD (one of the few M.2 2260 SSDs that we had access to during our testing process) along with vanilla DDR4 SODIMMs from the Team Group brand.The specifications of our ASRock 4X4 BOX-V1000M review configuration are summarized in the table below.ASRock Industrial 4X4 BOX-V1000M SpecificationsProcessorAMD Ryzen Embedded V1605BRyzen Embedded V-Series, 4C/8T, 2.0 (3.6) GHz2MB L2 + 4MB L3, 14nm, 12-25W TDPMemoryTeam Group TEAMGROUP-SD4-2666 DDR4 SODIMM16-16-16-39 @ 2400 MHz2x8 GBGraphicsAMD Radeon Vega 8 GraphicsDisk Drive(s)Transcend MTS600 TS256GMTS600(256 GB; M.2 Type 2260 SATA III; Micron 20nm MLC)(Silicon Motion SM2246EN Controller)NetworkingIntel Dual Band Wireless-AC 3168(1x1 802.11ac - 433 Mbps)2x Realtek RTL8111G Gigabit Ethernet ControllerAudio3.5mm Headphone JackCapable of 5.1/7.1 digital output with HD audio bitstreaming (HDMI)Miscellaneous I/O Ports2x USB 2.03x USB 3.2 Gen 2 Type-AOperating SystemRetail unit is barebones, but we installed Windows 10 Enterprise x64 (1909)Pricing (As configured)$390( $561 )Full SpecificationsASRock Industrial 4X4 BOX-V1000M SpecificationsThe ASRock Industrial 4X4 BOX-V1000M kit doesn't come with any pre-installed OS, but does come with a CD containing the drivers. In any case, we ended up installing the latest drivers downloaded off the product support page. In addition to the main unit, the other components of the package include a 96 W (12V @ 8A) adapter, a US power cord, a VESA mount (along with the necessary screws), a M.2 SSD heat-sink, a driver CD, user's manual and a quick-start guide.The gallery below takes us around the hardware in the main unit.Gallery:ASRock Industrial 4X4 BOX-V1000M - Hardware OverviewIn the table below, we have an overview of the various systems that we are comparing the ASRock 4X4 BOX-V1000M against. Note that they may not belong to the same market segment. The relevant configuration details of the machines are provided so that readers have an understanding of why some benchmark numbers are skewed for or against the ASRock 4X4 BOX-V1000M when we come to those sections.Comparative PC ConfigurationsAspectASRock 4X4 BOX-V1000MASRock 4X4 BOX-V1000MASRock DeskMini 310Intel NUC7PJYH (June Canyon)ASRock DeskMini A300Intel NUC10i7FNH (Frost Canyon)CPUAMD Ryzen Embedded V1605BAMD Ryzen Embedded V1605BGPUAMD Radeon Vega 8 GraphicsAMD Radeon Vega 8 GraphicsRAMTeam Group TEAMGROUP-SD4-2666 DDR4 SODIMM16-16-16-39 @ 2400 MHz2x8 GBTeam Group TEAMGROUP-SD4-2666 DDR4 SODIMM16-16-16-39 @ 2400 MHz2x8 GBStorageTranscend MTS600 TS256GMTS600(256 GB; M.2 Type 2260 SATA III; Micron 20nm MLC)(Silicon Motion SM2246EN Controller)Transcend MTS600 TS256GMTS600(256 GB; M.2 Type 2260 SATA III; Micron 20nm MLC)(Silicon Motion SM2246EN Controller)Wi-FiIntel Dual Band Wireless-AC 3168(1x1 802.11ac - 433 Mbps)Intel Dual Band Wireless-AC 3168(1x1 802.11ac - 433 Mbps)Price (in USD, when built)$390 (barebones)$561 (as configured)$390 (barebones)$561 (as configured)\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16111/asrock-industrial-4x4-boxv1000m-ryzen-minipc-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Announces Neoverse V1 & N2 Infrastructure CPUs: +50% IPC, SVE Server Cores\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-09-22T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/16073/arm-announces-neoverse-v1-n2\n",
      "Content: Arm’s ambitions for the server market has been a very long journey that’s taken years to materialise. After many doubts and false start attempts, today in 2020 nobody can deny that sever chips powered by the company’s CPU IP are not only competitive, but actually class-leading on several metrics.Amazon’s Graviton264-core Neoverse N1 server chip is the first of what should become awider range of designsthat will be driving the Arm server ecosystem forward and actively assaulting the infrastructure CPU market share that’s currently dominated by the x86 players such as Intel and AMD.The journey has been a long one, but has had its roots back inroadmaps publicly planned laid out by the company back in 2018. Fast-forward to 2020, not only have we seen products withthe first-generation Neoverse N1infrastructure CPU IP hit the market in commercial and publicly available form, but we’ve seen the company exceed their targeted 30% generational gain by a factor of 2x.The Neoverse V1: A New Maximum Performance Tier Infrastructure CPUToday, we’re ready to take the next step towards the next generation of the Neoverse platform, not only revealing the CPU microarchitecture previously known as Zeus, but a whole new product category that goes beyond the Neoverse N-series: Introducing the new Neoverse V-series and the Neoverse V1 (Zeus), as well as a new roadmap insertion in the form of the Neoverse N2 (Perseus).The new Neoverse V1 introduces the new V-series into Arm’s infrastructure IP portfolio, and essentially this represents the company’s push for higher absolute performance, no matter the cost.Earlier this spring we covered thecompany’s new mobile Cortex-X1 CPU IPwhich represented significant business model change for Arm: Instead of offering only a single one-fits-all CPU microarchitecture which licensees had to make due with in a wider range of designs and performance points, we’ve now seen a divergence of the microarchitectures, with one IP offering now focusing on pure maximum performance (Cortex-X1), no matter the area or power cost, while the other design (Cortex-A78) focuses on Arm’s more traditional maximised PPA (Power, Performance, Area) design philosophy.The Zeus microarchitecture in the form of the Neoverse V1 is essentially the infrastructure counterpart to what Arm has achieved in the mobile IP offering with the Hera Cortex-X1 CPU IP: A focus on maximum performance, with a lesser regard to power and area.This means that the V1 has significantly larger caches, cores structures, using up more area and power to achieve unprecedented performance levels.In terms of generational performance uplift, it’s akin to Arm throwing down the gauntlet to the competition, achieving a ground-breaking +50 IPC boost compared to Neoverse N1 that we’re seeing in silicon today. The performance uplift potential here is tremendous, as this is merely a same-process ISO-frequency upgrade, and actual products based on the V1 will also in all likelihood also see additional performance gains thanks to increased frequencies through process node advancements.If we take the conservatively clocked Graviton2 with its 2.5GHz N1 cores as a baseline, a theoretical 3GHz V1 chip would represent an 80% uplift in per-core single-threaded performance. Not only would such a performance uptick vastly exceed any current x86 competition in the server space in terms of per-core performance, it would be enough to match the current best high-performance desktop chips from AMD and Intel today (Though we have to remember it’ll compete against next-gen Zen3 Milan and Sapphire Rapids products).Neoverse N2 is Perseus – Continues the PPA FocusAlongside the Neoverse V1 platform, we’ve seen a roadmap insertion that previously wasn’t there. The Perseus design will become the Neoverse N2, and will be the effective product-positioning successor to the N1. This new CPU IP represents a 40% IPC uplift compared to the N1, however still maintains the same design philosophy of maximising performance within the lowest power and smallest area.It can be a bit confusing when it comes to the microarchitectural generations that we’re talking about here, so I made a graph to illustrate what we could call generational siblings between Arm’s mobile, and server CPU IP:Although this is just a general rough outline of Arm’s products, the important thing to note that there’s similarities between generations of Cortex and Neoverse products as they’ve being developed in tandem at similar moments in time during their design. The Neoverse N1 was developed in conjunction with the Cortex-A76, and thus the two microarchitectures can be regarded as sibling designs as they share a lot of similarities.The Neoverse V1 can be regarded as a sibling design to the Cortex-X1, likely sharing a lot of the supersized core structures that had been developed for these two flagship CPUs.The Neoverse N2 is a bit more special as it represents the sibling design to a next-generation Cortex-A core which is the follow-up to the A78. Arm says they’ll be licensing out this “Perseus” design by the end of the year and that customers already are engaging on beta RTL – we’re likely to hear more about this generation of products at next year’s TechDay event. The N2 would be lagging behind the V1 by one year and subsequently it'll take more time to see this in products.As a note, all of the above designs are all based in Austin and can be regarded as in the same microarchitecture family that had been started off with the Cortex-A76. If I’m not mistaken, next-generation “Poseidon” designs will be on a fresh new microarchitecture started by Arm’s Sophia-Antipolis design team – although Arm does note that there’s a lot more collaboration and blur between the different teams nowadays. Here Arm already notes a +30% IPC uplift for this generation of designs, likely to hit products in 2023.An Undisclosed Architecture with SVE: Armv9?One very notable characteristic of both the Neoverse V1 and N2 are the fact that these now supportSVE (Scalable Vector Extensions), with the V1 having two native 256-bit pipelines and the N2 being a 2x128-bit design. The advantage of SVE over other SIMD ISAs is the fact that code written in it can scale with the varying execution width of a microarchitecture, something that’s just not possible with today’s Neon or AVX SIMD instructions.Fujitsu’s A64FX chip and custom coremicroarchitecture had been to date the only CPU announced and available with SVE, meaning the V1 and N2 will be Arm’s first own designs actually implementing SVE.Today’s announcements around this part of the V1 and N2 CPUs raised more questions than it answered, as the company wasn’t willing to disclose whether this support referred to the first-generation SVE instruction set, or whether they already supported SVE2.In fact, the company wouldn’t confirm even the base architecture of the designs, whether this were Armv8 designs or one of the subsequent iterations. This is extremely unusual for the company as it’s traditionally transparent on such basic aspects of their IPs.What I think is happening here is thatthe V1 andN2 might bebothArmv9 designs, and the company will be publicly revealing the new ISA iteration sometime between today’s announcement and mid next year at the latest – of course this is all just my own interpretation of the situation as Arm refused to comment on the topic.Update:Actually it does seem that Arm had already publicly upstreamed theinitial compiler entries to GCC for Zeus back in June, confirming that at least the Neoverse V1 is an Armv8.4+SVE(1) design. I still think the N2 might be a v9+SVE2 design.At the end of the day, what we end up are two extremely compelling new microarchitectures that significantly push Arm’s positioning in the infrastructure market. The Neoverse N2 is an obvious design that focuses on Arm’s PPA metrics, and the company sees customers designing products that are primarily focused on “scale-out” workloads that requite a lot of CPU cores. Here we could see designs up to 128 cores.The Neoverse V1 will see designs with lesser core-counts as the CPUs are just bigger and more power hungry. Arm sees the 64-96 range being what’s most likely to be adopted by licensees. These are the premier products that will be going against the best of what Intel and AMD have to offer- and if the performance projections pan out (as they usually do for Arm), then we’re in for a brutally competitive fight unlike we’ve seen before.The first publicly known design confirmed to employ the new Neoverse V1 cores isSiPearl’s “Rhea” chipthat looks to feature 72 cores in a 7nm TSMC process node. Ampere’s “Siryn” design would also be a candidate for applying the V1 microarchitecture, targeted for a 2022 release on TSMC’s 5nm node.Today’s announcement has been more of a teaser or unveiling, with the company planning to go into more details about the architecture and microarchitectures of the designs at a later date.Arm's DevSummitis scheduled for October 6-8th - and might be where we'll hear a bit more about the new architecture.Related Reading:Arm Announces Neoverse N1 & E1 Platforms & CPUs: Enabling A Huge Jump In Infrastructure PerformanceArm's New Cortex-A78 and Cortex-X1 Microarchitectures: An Efficiency and Performance DivergenceAmazon's Arm-based Graviton2 Against AMD and Intel: Comparing Cloud ComputeNext Generation Arm Server: Ampere’s Altra 80-core N1 SoC for Hyperscalers against Rome and XeonArm Announces Neoverse Infrastructure IP Branding & Future Roadmap\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16073/arm-announces-neoverse-v1-n2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Intel’s Tiger Lake 11th Gen Core i7-1185G7 Review and Deep Dive: Baskin’ for the Exotic\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-09-17T13:35:00Z\n",
      "URL: https://www.anandtech.com/show/16084/intel-tiger-lake-review-deep-dive-core-11th-gen\n",
      "Content: The big notebook launch for Intel this year is Tiger Lake, its upcoming 10nm platform designed to pair a new graphics architecture with a nice high frequency for the performance that customers in this space require. Over the past few weeks, we’ve covered the microarchitecture as presented by Intel at its latest Intel Architecture Day 2020, as well as the formal launch of the new platform in early September. The missing piece of the puzzle was actually testing it, to see if it can match the very progressive platform currently offered by AMD’s Ryzen Mobile. Today is that review, with one of Intel’s reference design laptops.Like a Tiger Carving Through The IceThe system we have to hand is one of Intel’s Reference Design systems, which is very similar to the Software Development System (SDS) we tested for Ice Lake last year. The notebook we were sent was built in conjunction with one of Intel’s OEM partners, and is meant to act as an example system to other OEMs. This is slightly different to the software development system, which was mainly for the big company software developers (think Adobe) for code optimization, but the principle is still the same: a high powered system overbuilt for thermals and strong fans. These systems aren’t retail, and so noise and battery life aren’t part of the equation of our testing, but it also means that the performance we test should be some of the best the platform has to offer.Our reference design review sample implements Intel’s top tier Tiger Lake ‘Core 11thGen’ processor, the Core i7-1185G7. This is a quad core processor with hyperthreading, offering eight threads total. This processor also has the full sized new Xe-LP graphics, with 96 execution units running up to 1450 MHz.I haven’t mentioned the processor frequency or the power consumption, because for this generation Intel is deciding to offer its mobile processors with a range of supported speeds and feeds. To complicate the issue, Intel by definition is only publically offering it in the mix-max form, whereas those of us who are interested in the data would much rather see a sliding scale.Intel Core i7-1185G7 'Tiger Lake'CoresThreads4 Cores8 ThreadsBase Frequency at 12 W1200 MHzBase Frequency at 15 W1800 MHzBase Frequency at 28 W3000 MHz1C Turbo up to 50 W4800 MHzAll-core Turbo up to 50 W4300 MHzL2 Cache1.25 MB per core(non-inclusive)L3 Cache12 MB(non-inclusive)Integrated GraphicsXe-LP96 Execution Units1350 MHz TurboMemory Support32 GB LPDDR4X-4266or64 GB DDR4-3200In this case, the Core i7-1185G7 will be offered to OEMs with thermal design points (TDPs) from 12 W to 28 W. An OEM can choose the minimum, the maximum, or something in-between, and one of the annoying things about this is that as a user, without equipment measuring the CPU power, you will not be able to tell, as the OEMs do not give the resellers this information when promoting the notebooks.For this reference design, it has been built to offer both, so in effect it is more like a 28 W design for peak performance as to avoid any thermal issues.At 12 W, Intel lists a base frequency of 1.2 GHz, while at 28 W, Intel lists a base frequency of 3.0 GHz. Unfortunately Intel does not list the value that we think is most valuable – 15 W – which would enable fairer comparisons with the previous generation Intel hardware as well as the competition. After testing the laptop, we can confirm that the 15 W valueas programmed into the silicon (so we’re baffled why Intel wouldn’t tell us)is 1.8 GHz.In both 12 W and 28 W scenarios, the processor can turbo up to 4.8 GHz on one core / two threads. This system was built for thermals or power to not to be an issue, so the CPU can boost to 4.8 GHz in both modes. Not only that, but the power consumption while in the turbo modes is limited to 55 W, for any TDP setting. The turbo budget for the system increases with the thermal design point of the processor, and so when in 28 W mode, it will also turbo for longer. We observed this in our testing, and you can find the results in the power section of this review.The Reference DesignIntel sampled its Reference Design to a number of the press for testing. We had approximately 4 days with the device before it had to be handed back, enough to cover some key areas such as best-case performance on CPU and GPU, microarchitectural changes to the core and cache structure, and some industry standard benchmarks.There were some caveats and pre-conditions to this review, similar to our initial Ice Lake development system test, because this isn’t a retail device. The fans were fully on and the screen was on a fixed brightness.Intel also requested no battery life testing, because the system hasn't been optimized for power in the same way a retail device would - however as we only had a4 day review loan, that meant that battery life testing wasn’t possible anyway. Intel also requested no photography of the inside of the chassis, because again this wasn’t an optimized retail device. The silicon photographs you see in this review have been provided by Intel .When Intel’s regional PR teams started teasing the reference design on twitter (e.g. UK, FR), I initially thought this was an Honor based system due to the blue chamfered bezel like the Magicbook I reviewed earlier in the year. This isn’t an Honor machine, but rather one of the bigger OEMs known for its mix of business and gaming designs.Large keypad, chiclet style keys, and a 1080p display. For ports, this design only has two Type-C, both of which can be used for power or DisplayPort-over-Type C. The design uses the opening of the display to act as a stand for the main body of the machine.On the back is a big vent for the airflow in. Under the conditions of the review sample we’re not able to take pictures of the insides, however it’s clear that this system was built with an extra dGPU in mind. Intel wasn’t able to comment on whether the OEM it partnered with will use this as a final design for any of its systems, given some of the extra elements added to the design to enable its use as a reference platform.For the full system build, it was equipped with Intel’s AX201 Wi-Fi 6 module, as well as a PCIe 3.0 x4 Samsung SSD.Intel Reference Design: Tiger LakeCPUIntel Core i7-1185G7Four Cores, Eight Threads1200 MHz Base at 12 W1800 MHz Base at 15 W3000 MHz Base at 28 W4800 MHz Turbo 1C up to 50W4300 MHz Turbo nT up to 50WGPUIntegrated Xe-LP Graphics96 Execution Units, up to 1450 MHzDRAM16 GB of LPDDR4X-4266 CL36StorageSamsung 1 TB NVMe PCIe 3.0 x4 SSDDisplay14-inch 1920x1080, Fixed BrightnessIOTwo Type-C portsSupporting Charge, DP over Type-CWi-FiIntel AX201 Wi-Fi 6 CNVi RF ModulePower Modes15 W, no Adaptix28 W, no Adaptix28W, with AdaptixOthersBatterySpeakersFingerprint SensorThe first devices to market with the Core i7-1185G7 will have either LPDDR4X-4266 (32 GB) or DDR4-3200 (64 GB). Intel advertised these chips also supporting LPDDR5-5400, and we confirmed with the engineers that this initial silicon revision is built for LPDDR5, however it is still in the process to be validated. Coupled with the high cost of LPDDR5, Intel expects LP5 systems a bit later in the product cycle life-time, probably in Q1 2021.On storage: Tiger Lake technically supports PCIe 4.0 x4 from the processor. This can be used for a GPU or SSD, but Intel sees it mostly for fast storage. Given the prevalence of PCIe 4.0 SSDs on the market already, it was curious to see the reference designs without a corresponding PCIe 4.0 drive. Intel’s official reason for not equipping the system with such a drive was along the lines of ‘they’ve not been in the market for long and so we weren’t able to validate in time’. This is immediately and painfully laughable – PCIe 4.0 x4 enabled drives, built on Phison’s E16 controller, have been in the market for six months. We reported on them last year at Computex. To be clear, Intel’s argument here isn’t simply that it didn’t have enough time to validate it, it is the combination of validation time plus the argument that the drives haven’t been out in the market long enough for validation. This is wrong. If the drives had only been in the market for 6-8 weeks, perhaps I might agree with them, but to say it when the drives have been out for 24+ weeks amazes me.Therealreason why this system doesn’t have a PCIe 4.0 x4 drive is because the E16 drives are too power hungry. The E16 is based on Phison’s E12 PCIe 3.0 SSD controller, but with the PCIe 3.0 removed and PCIe 4.0 added, without much adjustment to the compute side of the controller or the efficiency point of the silicon. As a result, the E16-based drives can score up to 8 W for a peak throughput of 5 GB/s. A properly designed from-the-ground-up PCIe 4.0 x4 drive should be able to reach 8 GB/s at theoretical peak, preferably in that 2-4 W window.Adding an 8 W PCIe 4.0 SSD to a notebook, as we’ve said since they were launched, is a bad idea. Most laptops don’t have the cooling requirements for such a power hungry SSD, causing hot spots and thermal overrun, but also the effect on battery life would be easily noticeable. If Intel had said that ‘current PCIe 4.0 x4 drives on the market aren’t suitable due to the high power consumption of current solutions, however future drives will be much more suitable’, I would have agreed with them as a valid reason for not using one in the reference design. It makes sense – it certainly makes more sense than the reason first given about not being in the market long enough for validation.Beyond all this, by the time Tiger Lake notebooks come to market, new drives built on Phison’s E18 and Samsung’s Elpis PCIe 4.0 controllers are likely to be available. Whether these will be available in sufficient numbers for notebook deployment would be an interesting question, and so we are likely to see a mix of PCIe 3.0 and PCIe 4.0 enabled NVMe SSDs. I’m hopeful the OEMs and resellers will identify which are being used at the point of sale, or offer different SKU variants between PCIe 3.0 and PCIe 4.0, but I wouldn’t put money on it.Priority on PowerNormal operation on a notebook is for the processor to be offered at a specific thermal design point, and any changes to the power plan in the operating system will affect how long the system uses its turbo mode, or requirements to enter higher power states. This is because most notebooks are built to be optimized around that single thermal design point.In our Ice Lake development system (and in a few select OEM designs, like the Razer Stealth), the power slider while in the ‘Balanced’ power mode allowed us to choose between a 15 W power mode and a 25 W power mode, adjusting the base frequency (and subsequently the turbo budget) of the processor. The chassis was built for the higher power modes, and it allowed anyone using the development system to see the effect of the performance between the two thermal design points.For our Tiger Lake reference design, we have a similar adjustment at play. The power slider can choose either 15 W mode or 28 W mode (note that this is different to the 12 W to 28 W mode that Intel’s Tiger Lake is meant to offer, which I found odd for leaving out, but good in the sense that we could do 15W to 15W comparisons). There is also a third option: 28 W with Intel’s Dynamic Tuning enabled, also known as Adaptix.Intel’s Dynamic Tuning/Adaptix is a way for the system to more carefully manage turbo power and power limits based on the workload at hand. With Adaptix enabled, the idea is that the power can be more intelligently managed, giving a longer turbo profile, as well as a better all-core extended turbo where the chassis is capable. Intel has always stated that Adaptix is an OEM-level optimization, and it wasn’t enabled in our Ice Lake testing system due to that system not being optimized in the same way.However for our Tiger Lake system it has been enabled - at least in the 28 W mode anyway. Technically Adaptix could be enabled at any thermal design point, even at 12 W, but in all cases it should offer better performance in line with what the chassis can provide and the OEM feels safe. It still remains an OEM-enabled optimization tool, and Intel believes that the 28 W with Adaptix mode on the reference design should showcase Tiger Lake in its best light.More info later in the review.This ReviewAs a first look at Tiger Lake’s performance, our goal with this review is to confirm the claims Intel has made. The new platform has new features, and Intel has promoted its performance against the competition and previous generation. We’ll also go into microarchitectural details.Page two will be a brief primer on the fundamental updates on Tiger Lake: the transition to 10nm ‘SuperFin’ technology, the enhanced frequency, and the graphics. We’ll also cover the core as compared to Ice Lake, as well as the SoC level changes such as cache and updated hardware blocks.We’ll then move onto the new data. Page three will cover the minor changes in the core when it comes to instructions, as well as updates to security. We’ll also cover cache performance, latency, and a key part of modern computing in frequency ramping on page four.For the power consumption part of the coverage, I’m going to cover it into two brackets: how Intel compares to its own previous generation at 15 W, then moving onto the difference between a 15 W Tiger Lake and a 28 W Tiger Lake, which is going to be a running theme throughout this review.In Intel’s own announcement for Tiger Lake, the company pitted the 28 W version of Tiger Lake against the best power and thermal setting on an AMD 15 W processor; we’re going to see if those performance comparisons actually hold water, or if it’s simply a diversionary tactic to show Intel has the upper hand by using almost 2x the power.We’ll also cover our CPU gaming benchmark suite, tested at both 1080p maximum as well as 720p minimum. Intel made big claims about its new Xe-LP graphics architecture against AMD, so we will see how these measure up, both in 15 W Tiger Lake and 28 W Tiger Lake modes.PagesTiger Lake: Playing with Toe Beans10nm Superfin, Willow Cove, Xe, and new SoCNew Instructions and Updated SecurityCache Performance, Core-to-Core Latency, and Frequency RampingPower Consumption: Comparing 15 W TGL to 15 W ICLPower Consumption: Comparing 15 W TGL to 28 W TGLCPU Performance: SPEC 2006, SPEC 2017CPU Performance: Office and WebCPU Performance: Simulation and ScienceCPU Performance: Encoding and RenderingCPU Performance: Legacy and SyntheticXe-LP GPU Performance: Borderlands 3, Gears TacticsXe-LP GPU Performance: Final Fantasy XIV, Final Fantasy XVXe-LP GPU Performance: Civilization 6, Deus Ex Mankind DividedXe-LP GPU Performance: World of Tanks, Strange BrigadeConclusion: Is Intel Smothering AMD in Sardine Oil?\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16084/intel-tiger-lake-review-deep-dive-core-11th-gen\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: It’s Official: NVIDIA To Acquire Arm For $40 Billion\n",
      "Author: Ryan Smith\n",
      "Date Published: 2020-09-14T03:15:00Z\n",
      "URL: https://www.anandtech.com/show/16080/nvidia-to-acquire-arm-for-40-billion\n",
      "Content: Following a number of rumors and leaks, NVIDIA this evening announced that it is buying Arm Limited for $40 billion. The cash and stock deal will see NVIDIA buy the semiconductor and IP design firm from SoftBank and its associated SoftBank Vision Fund, with NVIDIA taking an eye towards expanding Arm’s IP licensing business while also using Arm’s technology to further pierce into the datacenter market. The deal is just being formally announced today and will likely not close for some time, as it is expected to be required to clear multiple regulatory hurdles in the UK, US, China, and other governments across the globe.The groundbreaking deal will see NVIDIA take over Arm Limited from SoftBank,who previously acquired the then-independent Arm in 2016 for $32 billion. At the time, SoftBank acquired Arm primarily as an investment vehicle, expecting the successful company to continue to grow as the number of chips shipped on the Arm architecture continued to explode. However, the investment firm has been under pressure in recent months as some of its other investments have taken big hits – particularly WeWork and Uber – and while SoftBank isn’t officially commenting on why it’s selling Arm after all of this time, there’s ample reason to believe that the firm is selling off one of its more valuable assets in order to shore up its balance sheets.The $40 billion transaction means that SoftBank will come out ahead on their investment, but only barely – their Arm investment has significantly underperformed relative to the broader technology industry. The deal will see SoftBank receive $12 billion in cash, along with $21.5 billion in NVIDIA stock. That transaction will give SoftBank a relatively sizable ownership stake in NVIDIA, though according to the companies the total stake is expected to be under 10 percent. Finally, the remaining $6.5B valuation of the deal will come from a further $1.5B in equity that NVIDIA will be paying out to Arm employees, as well as a $5B “earn-out” payment to be paid if Arm meets certain financial targets.As for NVIDIA, the Arm acquisition marks their largest acquisition to date, easily eclipsing the Mellanox acquisition that closed just a short few months ago. Over the last half-decade NVIDIA has undergone significant growth – both in regards to revenue and market capitalization – thanks in big part to NVIDIA’s newfound success in the server and datacenter market with their deep learning accelerators. While the company is well off of its 52-week high that it set earlier this month, NVIDIA now has a sizable $330B market cap that they are leveraging to make this deal possible.And according to the company, it’s that success in the server market that is driving their interest in and plans for Arm. NVIDIA expects the server market to remain a high-growth opportunity, and that by acquiring Arm they can leverage Arm’srecent success with Neoverseand other server products to reach an even bigger chunk of that market.To be sure, NVIDIA isn’t announcing any specific hardware plans today – the deal is easily still a year and a half off from closing – but NVIDIA has made it clear that following their success in the GPU/accelerator and networking markets, they see Arm as the perfect complement to their current product lineup, giving them a capable CPU architecture to round-out their technology portfolio. Even with Arm, NVIDIA will not be capable of complete vertical integration, but while the company today still has to rely on third-party vendors (e.g. AMD and Intel) for some of the most important silicon that goes into servers incorporating their accelerators, with an Arm-based server CPU, NVIDIA can offer a nearly complete package on its own.Of course, Arm is more than just a server CPU designer; the company has its fingers in everything from toasters to supercomputers thanks to its very broad range of IP, and any discussion about acquiring Arm has to include what happens to those businesses. Arm’s core business is licensing IP, and NVIDIA is telling the public (and partners) that this won’t change – that the company will continue to license out IP to other companies. The rationale for this is multifaceted – NVIDIA needs to win over everyone from regulators to customers to investors – but at the end of the day the company is in no position to compete with a lot of Arm’s customers, nor would they want to. Even in the server space NVIDIA couldn’t hope to address everything from microservers to supercomputers, never mind embedded controllers and smartphones. So NVIDIA is taking a complementary approach to the acquisition, using Arm’s server technology to augment their own, all the while continuing to license out IP.In fact, the company is looking at growing the amount of IP that Arm licenses by including IP currently held by NVIDIA; technologies such as GPUs, AI accelerators, and network processors. It’s an idea that NVIDIA hasplayed around with once beforewithout much success, but Arm comes with a much better business model and much more experience in licensing than NVIIDA ever had. Just what this expansion entails remains to be seen, but the obvious routes include licensing out GeForce graphics IP for use in SoCs (potentially replacing Arm’s Mali offerings), as well as licensing out bits and pieces of NVIDIA’s tensor core and InfiniBand technologies.Still, NVIDIA knows that they face an uphill battle in convincing Arm’s traditional customers that NVIDIA has their best interests at heart. The Arm deal is less than desirable for the industry as a whole, as Arm has traditionally only sold IP and related core designs, remaining fully divorced from full-scale chip design and sales. However with SoftBank seemingly set on selling Arm, there are few companies in a position to buy Arm, and even fewer that would be willing to take it on-board as a long-term investment. Ultimately, Arm being acquired by a chipmaker makes for strange bedfellows all around, and it falls on NVIDIA to convince customers that their acquisition of Arm will help the ecosystem by combining the companies engineering resources, and that they are earnest about continuing to design and sell top-shelf IP that other companies – even NVIDIA’s competitors – will get reasonable access to.The other (and perhaps more immediate) challenge for NVIDIA is convincing regulators across the globe to approve the deal. NVIDIA is pitching the deal as being complementary, combining two companies that otherwise have minimal overlap. None the less, minimal is not the same as “none”, and besides the immediate and obvious overlap with Mali and GeForce GPU technologies, regulators will no doubt take a great deal of interest in the future of IP licensing. The smartphone revolution of the past decade and a half has been built on top of Arm architectures – never mind the billions of devices with Arm-based microcontrollers – so many parties have a vested interest in keeping that going.To that end, while NVIDIA is just starting discussions with regulators – the deal was secret and not being discussed with partners nor regulators until this evening – the company isalready making concessions and guarantees to the British government to get its approval. This includes committing to keeping Arm headquartered in Cambridge, and continuing to do a significant amount of their engineering work there. The company is alsoannouncing that they will be building one of their AI “centers of excellence” in Cambridge. Besides providing an environment for cutting-edge AI research and training, NVIDIA will be building an Arm & NVIDIA-powered supercomputer at the site. While specific plans for the supercomputer are not being announced, the company recently finished building the world’s 7th-fastest supercomputer,Selene, using its DGX Pod infrastructure, so NVIDIA has significant capabilities here.Otherwise, no such overtures have been made to the US or China, however the situations in those countries are very different since they are not Arm’s traditional home. What (if anything) NVIDIA will need to do to sell regulators in those countries remains to be seen, but it’s worth noting that nothing about this deal resolves the current export impasse with China; even after the deal closes, Arm will still face the same restricts in exporting its technology to China.Finally, while this deal will see NVIDIA buying Arm wholesale, the two companies have confirmed that Arm’s ongoing efforts to sell off its IoT Services Group will continue. As a non-IP business NVIDIA has no interest in it, and as a result it will still be spun-off.Ultimately, the Arm deal will be a significant shift in the industry, one far bigger than the $40 billion price tag indicates. In one form or another, Arm and its IP are at the heart of billions of chips sold every year, a reach that few other companies can match. For NVIDIA, acquiring Arm will cement their position as a top-tier chip designer, all the while giving them an even larger technology portfolio to go after the server market, and new opportunities to sell IP to other vendors. But true success will likely hinge on how well NVIDIA can court the rest of the technology industry: Arm’s reach is only as big as its customers, and so it’s up to NVIDIA to convince it’s customers that they can still count on Arm’s neutrality even after the change in ownership.Gallery:NVIDIA To Acquire Arm: Press DeckSource:NVIDIA\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16080/nvidia-to-acquire-arm-for-40-billion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Armari Magnetar X64T Workstation OC Review: 128 Threads at 4.0 GHz, Sustained!\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-09-09T16:00:00Z\n",
      "URL: https://www.anandtech.com/show/16070/a-rendering-powerhouse-the-armari-magnetar-x64t-workstation-with-4-ghz-allcore-threadripper-3990x\n",
      "Content: Blitzing around a race track in a fast car only ever convinces you of one thing: I need to go around the track even faster. I need a better car, I need a better engine, better brakes, or better tires. I need that special go faster juice, and I need to nail the perfect run. The world of professional computing works the same, whether it comes down to rendering, rapid prototyping, scientific compute, medical imaging, weather modelling, or something like oil and gas simulation, the more raw horsepower there is, the more can be done. So enter the new Armari Magnetar X64T – an overclocked 64-core Threadripper 3990X that holds the new SPECworkstation3 world record. We got hold of one. It’s really fast.Playing with PerformanceAMD’s Threadripper 3990X is one of those crazy processors. It comes at you with some of the best of any processor statistics: it has 64 cores and 128 threads, it has 256 MB of L3 cache, it has a TDP of 280 W, which allows for a 2.9 GHz base frequency up to a 4.3 GHz turbo. It is overclockable, and so with the right system those frequencies can go even higher. With the best binned 7nm chiplets, paired with quad-channel DDR4-3200 memory, for multithreaded workloads it is one of the ultimate powerhouses anyone can build in a single socket with a socketable processor.In our initial review of the Threadripper 3990X, it blitzed any software that could take advantage of all those threads – the nearest competitors were the 32-core Threadrippers, or Intel’s 28-core Xeon-W processors. We even put it up against two of Intel’s $10000 28-core Xeons, and it won pretty much everything by a large margin.So what happens when we overclock it? There are those that want more, and not just those overclocking for fun – workstation customers, like animation studios, are always looking for ways in which they can rapidly render frames for upcoming projects. If a cooling system can be built to withstand it, and the power is available, then there’s always scope to get more out of the hardware that comes from the big players. This is what the Armari Magnetar X64T Workstation is designed to do – get more.To that end, today AMD and SPEC is announcing that the Magnetar X64T workstation, a system that you can buy, will off-the-shelf give the best performance in SPECworkstation3 ever seen.The Magnetar X64T: Performance ReimaginedThe key highlight from this review, should you not read any further, is that this system is built to blitz workloads. The Threadripper 3990X is usually fast enough in its own right, but Armari have gone above and beyond. The goal of this system is to be an off-the-shelf powerhouse that requires very little setup from its customers.Armari, perhaps a lesser well known system integrator, is a company that has in recent years focused on building systems for 3D animation, video editing, and scientific research. With over 20 years of experience, Armari’s hardware has gone into high performance computing solutions and clusters that have featured in the TOP500 lists, as well as rendering server farms for the top animation, VFX, and CGI studios in Soho, London.These are clients who want the best performance, and Armari positions itself not so much as a boutique system builder, but something between the big OEMs (like Dell/HP) and the main retailers to offer custom solutions by leveraging its network of cooling and hardware contacts around the world. This enables the company to build custom chassis, obtain optimized memory, order power supplies with custom connector configurations, and ensure consistency from batch-to-batch when ordering from its partners. In speaking to Armari’s Technical Director Dan Goldsmith, he mentioned that working with partner companies for so long has enabled them to get access to rapid prototyping and component consistency with continual feedback with partners such as EKWB, ASRock, Tyan, and many other ODM companies that Armari leverages on a regular basis.The Magnetar X64T, I was told, leverages the strong relationship Armari has with AMD. The Opteron was a popular range a decade ago, and that partnership has been maintained through today. The goal of the Magnetar project was to create a system that offers the best that Threadripper has to offer while still enabling the under-the-desk workstation platform. This project has been slightly delayed due to COVID, and AMD now has Threadripper Pro, but those processors are not overclockable – for those that want raw performance, AMD and Armari believe they are on a winner.The key to the system is in how Armari is cooling the processor, and the choice of components. The Magnetar X64T features a custom water cooling loop, which is perhaps not anything new in its own right, however the company has created a component chain to ensure consistency in its design, as well as using some of the most powerful options available.The water block is probably the best place to start, because this is a completely custom-for-Armari design built in partnership with EK Water Blocks. This block is specifically built for this one motherboard, the ASRock TRX40 Taichi, and applies cooling to both the processor and the power delivery. The block works in conjunction with the highest-quality thermal paste pads on the market, to ensure a flat connection with the water block. As it also covers the power delivery, Armari worked with ASRock to enable a consistent z-height of all the power delivery components, something that can vary during manufacturing, and maintain that consistency on a batch-by-batch basis. Pair this up with Armari’s custom FWL liquid cooling pump, reservoir, tubing, 3x140mm radiator, and fan combinations (many of which are custom from their respective ODMs), and we have a cooling capacity in excess of 700 W. The coolant is a special long-life coolant designed for 24/7 over three years, and the standard warranty comes with service during those three years, including collection and return, at no extra cost.Now, the ASRock TRX40 Taichi isn’t the top Threadripper motherboard on the market, and Armari fully admits that, however it points out that the best motherboard available costs twice as much. In working with ASRock, they were able to co-ordinate what was needed within the discrete motherboard component lists as well as enable a custom BIOS implementation for additional control. One of the tradeoffs I was told about is that a cheaper motherboard might mean slightly cheaper components, however Armari says that their cooling system and setup were co-operatively tuned to meet its customers’ demands.With this cooling arrangement, Armari have fired up the overclock.In our initial review of the Threadripper 3990X, we were observing ~3450 MHz during our sustained running with the CPU reaching its full 280 W. For the Armari Magnetar X64T, we have an all-core frequency from 3950-4100 MHz, depending on the workload. Users might scoff at the +400-550 MHz lift, but bearing in mind this is across all of the 64 cores simultaneously, and the cooling is built such that this frequency is sustained for renders or simulations that might take days. Further details of frequency and power later in the review.While having the overclocked CPU is great, the Magnetar X64T system we were delivered also had memory, graphics, and storage.Armari Magnetar X64T as shipped(X64T-RD1600G3-FWL)ProcessorAMD Ryzen Threadripper 3990XOverclocked to ~4.0 GHz All-Core TurboCoolingCustom Armari FWLv2 Liquid Cooling LoopCustom CPU+VRM Monoblock420x45mm EK Coolsense Radiator3 x EK-Vardar 140ER EVO 140mm fansHigh Performance PumpClear Coolant, Designed for 3yr operationGraphicsPNY NVIDIA Quadro RTX 6000 24 GBMotherboardASRock TRX40 TaichiMemory256 GB of DDR4-3200Power Supply1600W 80PLUS Gold 93%, rated to 50ºC0% fan under 40% load9x PCIe connectionsStorageASRock Hyper Quad M.2 PCIe 4.0 x16 add-in card1 x Corsair MP600 PCIe 4.0 x4 1 TB Boot Drive2 x Corsair MP600 PCIe 4.0 x4 1TB Striped ArrayNetworkingRealtek RTL8125 2.5 GbE (motherboard)Intel I211-AT 1 GbE (motherboard)Intel AX201 Wi-Fi 6 module (motherboard)AudioOnboard Realtek ALC1220 + ALC4050HFans3 x EK 140mm for radiator2 x Noctua 140mm for internal airflow1 x SanAce 80mm low noise for DRAMPrice as Built£10790 + tax(~$14200 + tax)Special launch price for SeptemberUK Warranty1 Year RTB3 Year Parts+LaborOne service/coolant replacement, inc collection/pickupLoaner systems available if bigger issues occurThe system as shipped came with an PNY NVIDIA RTX6000 graphics card, which is essentially an RTX 2080 Ti on steroids with 24 GiB of GDDR6, and the system can be configured with two of them. As Threadripper is not an ECC-qualified platform, the X64T comes with the peak configuration supported, 256 GB, but with custom SPD profiles to run up to DDR4-3600. Unfortunately due to how quickly this system was rebuilt for this review, the system I was sent was using DDR4-3200 at CL20, as some of the original memory was accidentally splashed with coolant, and Armari wanted to ensure I wouldn’t have any issues with the system.Storage comes in two forms, both of which are PCIe 4.0. As shipped, we were specified with a boot drive to the tune of a Corsair MP600 1 TB PCIe 4.0 x4 drive. Another two of these drives were provided inside an ASRock Hyper M.2 PCIe 4.0 card, plugged into one of the PCIe 4.0 slots. Armari says that as newer and bigger PCIe 4.0 drives come to market beyond the Phison E16 solutions, this should expand to higher capacity drives or faster drives as required.The power supply is a fully custom 1600W 80PLUS Gold unit, rated to run at 50 ºC with 93% efficiency. It has a custom fan profile directly from the OEM, and is set to only stir up the fans if the power required goes above 40% (640 W). The fully modular PSU has nine 8-pin connections and five 6-pin connections, providing 14 total, should any customer want to go above and beyond. The PSU on its own has a 10-year warranty.The motherboard has a 2.5 GbE wired network port and a 1 GbE wired network port, and Armari does offer a 10G upgrade (space permitting based on the PCIe slots). Wi-Fi 6 support comes as standard, as does the ALC1220 audio configuration.The chassis is the last custom part to discuss, with the system featuring the Magnetar naming on the front with the Armari logo. The chassis is big, but quite standard for a high-end workstation platform: 53cm x 22cm x 57cm (20.9-in x 8.7-in x 22.4-in), with a typical single GPU weight of 18 kg (39.7 lbs).The chassis comes with handles on top that fold away, making the system easy to move around as required. I love these.Inside there is lots of ‘space’ for directed airflow. The pump and reservoir is found in the bottom of the case, underneath the standard drive-bays, while the 3x140mm double thick radiator is at the top built into the side of the chassis. This is a special hinged mount, which makes the side panel easy to remove and the cooling apparatus easy to inspect.There is a PCIe retention bracket for any add-in card installed, and in the base of the chassis is the power supply, hidden away. The insides weren’t necessarily built to look aesthetically pleasing, however the system as provided by Armari has a nice clean look.Due to a couple of issues with arranging this system for review, I was told that normally Armari adds in some custom sealant to help with the liquid cooling loop, however as it requires 24 hours to set, they weren’t able to in this instance. The liquid cooling loop is pre-tested for every system they build at over 1 bar of pressure, along with full stability testing and thermal testing before shipping. For any reason if a system needs to be returned for warranty, Armari can supply a loaner system if required. As mentioned above, the standard warranty includes one full service and inspection, and the coolant can be replaced in order to give the customer another 3 years of ‘hassle free’ operation.The News Today: World RecordsToday AMD and Armari are announcing that the new Magnetar X64T has set a new world record in the SPECworkstation 3 benchmark. The system that achieved this test is, by and large, the system I am testing today (it was stripped down and rebuilt with an updated water block). For the customers that Armari typically services this one of the primary benchmarks they care about, and so getting a new world record for a commercially available system should put Armari’s offerings high on their list.Our testing, as shown over the next few pages, is similarly impressive. We already saw that the Threadripper 3990X with no overclock was a formidable beast in almost every one of our rendering and compute workloads. The only real comparison point we have to compare against is our W-3175X workstation that was provided when we reviewed that system.The Magnetar X64T-RD1600G3 FWL (the full name) system in our testing is ~£10790 ($14200) excluding tax . This includes a Windows 10 Professional 64-bit license, and Armari’s 3 year premium workstation warranty, with 1-year on site and 2/3rd year parts and labor, along with a loaner system for the duration of any repairs.Read over the next few pages for our testing on Performance and Power.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16070/a-rendering-powerhouse-the-armari-magnetar-x64t-workstation-with-4-ghz-allcore-threadripper-3990x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: TSMC Updates on Node Availability Beyond Logic: Analog, HV, Sensors, RF\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-08-25T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/16030/tsmc-updates-on-node-availability-beyond-logic-analog-hv-sensors-rf\n",
      "Content: Most of the time when we speak about semiconductor processes, we are focused on the leading edge of what is possible. Almost exclusively that leading edge is designed for logic circuitry where performance and power efficiency are key drivers of pushing the boundaries, but also there’s a strong market in it. Other markets use semiconductor technology where there are other factors to consider: power, analog capabilities, voltage, and memory, all use semiconductor fabs but they are rarely at the leading edge. Nonetheless, the pureplay foundry businesses aims to offer enough technologies and features to cater as needed, along with driving which markets can use which technologies. At TSMC’s Technology Symposium this week, the company gave is a holistic view of its offerings.As always, the key central pillar is logic. It also contributes most to revenue, and helps drive the rest of the company, so it’s an important arm of the business. Here TSMC offers all the way up to 5nm, which is currently in high-volume manufacturing ready for the first products by the end of the year. Companies that have 5nm products planned for this year and early next year already have test chips in house built on 5nm. The 3nm node is currently in development.RF (radio frequency) silicon, used in modems, often sits on the right hand of logic given its importance in the smartphone and mobile connectivity space. Companies have to decide whether to combine logic and RF onto a single monolithic design, or offer logic and RF on separate connected dies. As it stands, TSMC’s RF capabilities have already shown some RF solutions on the leading nodes, but TSMC still very much sees this as a work in progress. At the current time, TSMC is developing its N6 RF technologies for discrete RF solutions.For analog, TSMC has offerings on 22nm (22 Ultra-Low-Leakage), and is currently in the process of developing its N12e (efficient) offerings for this space. N12e will be a dedicated process node covering a lot of the IoT space as well.Other highlights include TSMC’s CMOS Image Sensing (CIS) nodes, which a number of high-profile camera sensor modules use. Moving from 40nm to 28nm will allow TSMC to offer 0.7 micron pixels and increase overall image sensor size, with TSMC expecting to work with partners to offer 100 megapixel sensors in 2020. On the MEMs side, TSMC is moving from 8-inch wafers to 12-inch wafers, and for High Voltage applications TSMC is investing in its Wafer-on-Wafer bonding options at 28nm.TSMC calls its non-logic business as its ‘specialty’ portfolio, which it says has increased in revenue at 17% CAGR since 2014. Its own capacity for specialty technology has grown 10% Year on Year when comparing equivalent wafer starts, and specialty products now account for 54% of the company’s production on 28nm and above.Related ReadingTSMC Details 3nm Process Technology: Full Node Scaling for 2H22 Volume ProductionTSMC To Build 5nm Fab In Arizona, Set To Come Online In 2024TSMC & Broadcom Develop 1,700 mm2 CoWoS Interposer: 2X Larger Than ReticlesTSMC Boosts CapEx by $1 Billion, Expects N5 Node to Be Major SuccessEarly TSMC 5nm Test Chip Yields 80%, HVM Coming in H1 2020TSMC: 5nm on Track for Q2 2020 HVM, Will Ramp Faster Than 7nmTSMC: N7+ EUV Process Technology in High Volume, 6nm (N6) Coming Soon\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16030/tsmc-updates-on-node-availability-beyond-logic-analog-hv-sensors-rf\n",
      "Title: ASRock B550 Taichi Review: The $300 B550 Motherboard with Chutzpah\n",
      "Author: Gavin Bonshor\n",
      "Date Published: 2020-08-21T19:30:00Z\n",
      "URL: https://www.anandtech.com/show/15941/the-asrock-b550-taichi-motherboard-review\n",
      "Content: Outside of its Aqua series of motherboards, which come with exquisitely crafted monoblocks, ASRock's Taichi brand has been a critical part of the company's offerings in the land of premium motherboards. The ASRock B550 Taichi sits at the top of its product stack and features an impressive quality feature set. Some of the most notable features include a large 16-phase power delivery, eight SATA ports, dual M.2 slots, an Intel 2.5 GbE Ethernet controller, and an Intel Wi-Fi 6 interface. At $300 it comes equal in price with the X570 version, which leaves questions on the table as to which one is actually worth the money.When AMD first released the B550 chipset, a lot of fanfare was made about the high launch price of some models. For what has been usually considered a 'budget' chipset, some of the more premium B550 models cost more than some of the X570 models, which featured full support for PCIe 4.0 to add-in cards and the chipset. The B550 chipset has less PCIe 4.0 support than X570, with only the top full-length PCIe slot and one PCIe M.2 slot operating at PCIe 4.0, while the rest of the slots and chipsetoperate at PCIe 3.0. The trade-off is downstream bandwidth from the chipset as well as power, because the X570 requires a fan to keep that PCIe 4.0 chipset cool. Ultimately X570 requires 'more' to be fully enabled than B550, so it comes as a bit of surprise when the B550 and X570 models sit on equal pricing.The ASRock B550 TaichiEven with what has been said above, the ASRock B550 Taichi has all the hallmarks of a premium AM4 model with a solid array of controllers, ports, and power delivery.Armed with its unique Taichi inspired design, the ASRock B550 Taichi follows a bronze and black color scheme with three customizable RGB LED zones.ASRock has installed the B550 Taichi with a large sixteen phase power delivery, which performs very well in our thermal testing. Unique to the ASRock range, the B550 Taichi is the only board where the top two PCIe slots support a PCIe 4.0 x8/x8 configuration - out of all other B550 boards, only two others have this feature.Another solid benefit of the Taichi is that it includes a BIOS Flashback controller which allows users to update the firmware without a CPU installed. This is useful for when AMD launches Ryzen 4000 and a firmware update will be required to use the new Zen 3 processors.Some of the board's other core features include 2.5 GbE networking, Wi-Fi 6, and a premium audio codec with an assisting amplifier designed to bolster the quality of the front panel audio header.After running our benchmarking suite, the ASRock B550 proved a consistent and solid performer in our testing. It displayed competitive performance in our system tests with strong power consumption figures in all three of our power tests, with a decent non-UEFI POST time of 20.6 seconds at default settings. In our CPU and gaming tests, the B550 Taichi performed competitively against other AM4 boards tested with our testbed Ryzen 7 3700X processor.Our overclocking testing proved resourceful with a maximum stable overclock of 4.3 GHz with our Ryzen 7 3700X processor. Overall VDroop control was consistent throughout our testing, as was the performance in our POV-Ray benchmark testing. We saw equally impressive performance in our VRM thermal testing with the Taichi beating some of the more advanced and much more expensive X570 models. This is due to a more efficient power delivery design, as well as a solid pair of heatsinks which are interconnected via a single heat pipe for better heat dissipation. This is ultimately the reason why, despite being B550, this Taichi costs the same as the X570: better thermals and better power delivery.The ASRock B550 Taichi as it stands is one of the most expensive B550 models with an MSRP of $300. This puts it up against ASRock's own X570 Taichi which currently costs $300 at Newegg. This puts the B550 Taichi in an awkward position in terms of value, with the X570 offering more raw features. However the B550 Taichi offers an alternative is the slightly larger power delivery (16-phase versus 14-phase), and an Intel 2.5 GbE Ethernet controller whereas the X570 Taichi is equipped with a standard Gigabit port.Read on for our extended analysis and comparison tests.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15941/the-asrock-b550-taichi-motherboard-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Hot Chips 2020 Live Blog: Intel 10nm Agilex FPGAs (8:30am PT)\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-08-18T14:50:00Z\n",
      "URL: https://www.anandtech.com/show/16001/hot-chips-2020-live-blog-intel-10nm-agilex-fpgas-830am-pt\n",
      "Content: 11:27AM EDT- First Session of Day 2 is on FPGAs, with Intel's 10nm Agilex up first11:27AM EDT- I saw one last year at a Stratix 10 briefing11:30AM EDT- And here we go, presentation about to begin11:31AM EDT- Ralph Wittig from Xilinx is the session chair11:32AM EDT- Agilex is Intel's first in-house FPGA11:32AM EDT- customized 10nm process11:32AM EDT- 40% higher perf, 40% lower power, compared to Stratix11:32AM EDT- Supports up to 116 Gbps11:33AM EDT- Variants will support CXL, DDR4/DDR5, HBM2e and Optane11:33AM EDT- Agilex is the Spatial component in its strategy - also supports OneAPI11:33AM EDT- Second gen EMIB11:33AM EDT- Disaggregated transceivers and HBM tiles11:34AM EDT- Meets specific customer needs11:34AM EDT- Programming logic is kept monolithic11:34AM EDT- Floorplan has improved11:34AM EDT- Smooth fabric grid without I/O disruptions11:34AM EDT- No notches in the fabric11:35AM EDT- Arm processor complex has been moved to the corners to eliminate those notches11:35AM EDT- Rectangular fabric simplifies customer routing11:35AM EDT- Identical resourcing across the fabric11:35AM EDT- Quad Arm A53 sub-system, ECC L1/L2 with snoop control11:36AM EDT- Secure Device Manager, triple redundant hard processor, crypto, SEU events, device boot order11:36AM EDT- Memory and GPIO11:36AM EDT- Up to 8TB support with HBM2e11:36AM EDT- *Up to 8TB with Optane, HBM2e is separate11:36AM EDT- EMIB bridges are 'AIB'11:37AM EDT- Five types of tiles for Agilex11:37AM EDT- Trancievers, PCIe 4, PCIe 5, Ethernet, CXL11:37AM EDT- Uses Intel's standard 10nm process with customizations11:37AM EDT- Metal stack improvments11:38AM EDT- wider poly pitch, Vt tuning, custom layouts and dummy fill enhancements11:38AM EDT- Each fabric sector has columns of logic and memory11:38AM EDT- Logic speed +50%11:38AM EDT- BF16, FP19, FP16, INT8 increased throughput11:39AM EDT- Programmable clock delays11:39AM EDT- Hyperflex ties all the blocks together11:40AM EDT- Old - Mux -> Buffer -> Signal across columns. Fan out to other muxes. High FI/FO didn't scale to 10nm11:40AM EDT- Now narrow low fan out muxes and redesigned routing pattern11:40AM EDT- C4 routing delay has a big improvement11:41AM EDT- Agilex has repartitioned crossbar, allows suppor of narrower faster muxes11:41AM EDT- 2.5x faster vs stratix10 crossbar delay11:42AM EDT- 40%+ geomean Fmax improvement11:42AM EDT- Using same code as Stratix without any change11:43AM EDT- Comparing GX1100 with Agilex AGF01411:43AM EDT- Five tile types11:43AM EDT- New R-Tile for Agilex - PCIe5 and CXL11:43AM EDT- 16 lanes of 32 Gbps NRZ per tile11:44AM EDT- Full support for CXL 1/2/3 configs11:44AM EDT- F-Tile is high speed networking11:44AM EDT- Two groups of transceiver PHYs11:44AM EDT- Up to 116G11:45AM EDT- Bifurcatable up to 400 GbE11:45AM EDT- PCIe 4 x1611:46AM EDT- 1e-7 BER in 116G, two order of magnitudes better than the standard requires11:46AM EDT- Different Agilex families11:47AM EDT- F-Series, I-Series, M-Series. The images at the beginning of the blog was F-Series11:48AM EDT- Now to software and OneAPI11:48AM EDT- Programmable registers at every stage11:48AM EDT- ASIC-style clocking architecture11:48AM EDT- Quartus software co-designed with the hardware11:49AM EDT- Quartus has been revamped to be timing-centric11:49AM EDT- Accurate delays without rerouting connections11:49AM EDT- Additional Fmax unlocks11:50AM EDT- Retiming aware that can fix critical paths11:50AM EDT- Design specific clock routing11:51AM EDT- Sequential circuit optimization technique for Fmax11:52AM EDT- This is a bit over my head. Hope you're getting something out of this :)11:55AM EDT- Fine grained clock timing of 3ns11:55AM EDT- Agilex has flexible clock skewing in the hardware, used by Quantus to help improve Fmax by 4% on average11:56AM EDT- Different types of developers: low level or high level11:56AM EDT- OneAPI offering is a compiler and toolchain for DC++ for direct programming as well as API programming11:56AM EDT- Layered on top of the OpenCL offering which has Quartus Prime Pro as its base11:57AM EDT- Q&A Time11:58AM EDT- Q: What process? A: 10nm SuperFin with enhancements, such as metal stack. Subsequent products may leverage new enhancements11:59AM EDT- Q: New plans to bring Xeon + FPGA in package? A: Nothing to disclose right now12:00PM EDT- Q: EMIB now, Foveros coming? A: EMIB for tiles and HBM - some other experimental projects announced to create a chiplet ecosystem. Intel has many packaging technologies, so we look at many different opportunities but nothing to disclose right now.12:00PM EDT- (Intel announced next-gen FPGA with Foveros at architecture day)12:01PM EDT- Q: Register everywhere strategy from Stratix 10? A: We optimized hyperflex registers by reducing overall count while keeping Fmax abilities. We have enough so we can exploit them all. We also optimized it for setup-and-hold characteristics. We matured the algorithms in Quartus too.12:02PM EDT- That's a wrap. Next up is Xilinx Versal, head on over to that live blog\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/16001/hot-chips-2020-live-blog-intel-10nm-agilex-fpgas-830am-pt\n",
      "Title: Hot Chips 2020 Live Blog: Alibaba Xuantie-910 RISC-V CPU (3:00pm PT)\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-08-17T22:00:00Z\n",
      "URL: https://www.anandtech.com/show/15991/hot-chips-2020-live-blog-alibaba-xuantie910-riscv-cpu-300pm-pt\n",
      "Content: 06:04PM EDT- This is the first talk on edge computing06:05PM EDT- Xuantie-910 of Alibaba06:06PM EDT- Innovating Cloud and Edge Computing by RISC-V06:06PM EDT- Xuantie refers to a heavy sword from Chinese folklore made of Iron06:07PM EDT- T-Head semiconductor - a young Alibaba organization specializing in circuit design specialising next gen compute for various areas with a strong commitment to Open Source06:08PM EDT- RISC-V is very attractive for the IoT era06:08PM EDT- Extensibility and modularity allows for customization for the domain specific workloads06:09PM EDT- RISC-V Mainline platform in Linux, fully supported in AlibabaOS06:09PM EDT- Xuantie goal is to contribute to the oepn source community06:09PM EDT- AI Vector Engine06:10PM EDT- Similar in performance to Arm 7306:10PM EDT- Xuantie-902 (M0+ like) with hardware TEE up to Xuantie-91006:10PM EDT- 903, 907,908 coming06:11PM EDT- 4 cores per cluster in 91006:11PM EDT- HMP cluster06:11PM EDT- Each core supports 32-64 KB L1 D and 32-64 KB L1 I06:11PM EDT- Each single core is 3-decode 8-issue OoO06:11PM EDT- Hybrid branch predictor06:11PM EDT- vector engine06:12PM EDT- One of the first commercial processors to use RISC-V vector extension proposals06:12PM EDT- Performance on Coremark 7.1 per MHz. This workload is a full cache hit only06:13PM EDT- Highest performance RISC-V on market now06:13PM EDT- SiFive has U84 processor which might be higher performance, but no details of yet06:13PM EDT- waiting for info to become available06:13PM EDT- X910 supports RISC-V 0.7.1 Vector Extension06:13PM EDT- FP16-FP64, INT8-INT6406:14PM EDT- MMX, Clint, PPC06:14PM EDT- MMU*06:14PM EDT- Supports unaligned memory data access06:14PM EDT- Supports custom extensions06:14PM EDT- RISC-V Turbo extensions06:15PM EDT- bit operations, memory access, core sync06:15PM EDT- Can be disabled to be completely compatible with RISC-V06:15PM EDT- but Alibaba toolchain can use the new instructions06:16PM EDT- Two vector pipes, 1 ALU/MUL, 1 ALU/DIV, 1 Branch, 1 dual issue Load/Store units06:16PM EDT- 128-bit instruction fetch unit06:16PM EDT- can fetch 8 instructions at once06:17PM EDT- Hybrid multi-mode branch prediction06:17PM EDT- Cache Way prediction06:17PM EDT- Loop accelerator06:18PM EDT- Can do one load and one store in parallel06:18PM EDT- 3-cycle load-to-use06:19PM EDT- Unique multi-mode and multi-stream prefetch mode for RISC-V by pattern matching and backfills the L1/L2 cache06:19PM EDT- 4 cores per cluster, up to 4 clusters06:20PM EDT- All Clusters shares L2, up to 8MB06:20PM EDT- Two 128-bit Vector ALU ops/cycle06:21PM EDT- More than 300 GFLOPs FP16 per cluster (32 FLOPs/core/cycle x 2.5 GHz x 4-cores)06:21PM EDT- FP32 perf is 0.5x FP1606:21PM EDT- So 150 GFLOP of FP32 per cluster - up to 600 GFLOP of FP32 in a 4-cluster design06:22PM EDT- Also integrated IDE with profiling for Xuantie-91006:22PM EDT- Compiler has been co-optimized for the hardware improvements06:22PM EDT- Compared to Arm A7306:23PM EDT- A73 CPU is from Huawei Kirin 97006:23PM EDT- Xuantie is configured to same L1 cache sizes06:24PM EDT- 'on par in this config'06:24PM EDT- Benchmarks doesn't mean that Xuantie-910 is up to the perfection of A73, as it's still new, and needs more collaboration06:25PM EDT- Here's an AI workload06:25PM EDT- on an FPGA simulation of X91006:25PM EDT- Here's a floor plan06:26PM EDT- TSMC 12FF06:26PM EDT- FPGA X910 already deployed in Alibaba cloud06:27PM EDT- FPGA runs at 200 MHz06:27PM EDT- 2020 July, 28 HPC version at 1.6 GHz, 0.3 mW/MHz06:27PM EDT- September, 12nm FinFET due06:28PM EDT- Help external customers with X910 with Wujian SoC platform06:30PM EDT- Now for Q&A06:32PM EDT- Q: What applications are you using it for?06:33PM EDT- A: It's a full chip - a high-end core for embedded SoCs06:34PM EDT- Q: Source code? A: we are actively working on open source procedures. It's not straight forward for a high performance core - legal required. We are talking to open source companies to find the best way to do this. Also repository management and such. Once it is available, we will let you know!06:34PM EDT- Q: plans to support RVV 1.0? A: 0.7.1 for now - when we designed, it was still at that level. We are following and working ont hat yes.06:36PM EDT- That's a wrap. My next live blog will be NVIDIA A100 at 5pm PT.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15991/hot-chips-2020-live-blog-alibaba-xuantie910-riscv-cpu-300pm-pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Hot Chips 2020 Live Blog: Intel's Raja Koduri Keynote (2:00pm PT)\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-08-17T20:50:00Z\n",
      "URL: https://www.anandtech.com/show/15990/hot-chips-2020-live-blog-intels-raja-koduri-keynote-200pm-pt\n",
      "Content: 04:54PM EDT- Hot Chips has gone virtual this year! Lots of talks on lots of products, including Tiger Lake, Xe, POWER10, Xbox Series X, TPUv3, and a special Raja Koduri Keynote. Stay tuned at AnandTech for our live blogs as we commentate on each talk. Intel recently had its own Architecture Day 2020, with Raja Koduri and other Intel specialists disclosing details about process and products. It will be interesting to see if Raja discusses anything akin to roadmaps in this keynote.04:58PM EDT- Raja M. Koduri, Senior Vice President, Chief Architect, and General Manager of Architecture, Graphics, and Software, Intel'05:01PM EDT- The title of the talk is 'No Transistor Left Behind'. Raja has had it on a t-shirt at a number of events05:04PM EDT- 'Raja has spent his career enhancing accelerate compute in the technology industry, across graphics, vector compute, consoles, and semi-custom designs'05:05PM EDT- First, paying tribute to Frances Allen, who recently passed away05:08PM EDT- The balance of software abstraction and performance hardware execution is the boundary that Frances worked on and we still work on today05:09PM EDT- A little of 20 years, Intel senior architects (hardware and software) got together to discuss heterogenity in Intel's roadmap and software roadmaps05:09PM EDT- They all knew each other, but many of them were meeting each other for the first time05:10PM EDT- That discussion is where the phrase 'No Transistor Left Behind' comes from05:10PM EDT- David Blythe is Xe senior architect05:11PM EDT- The role of hardware/software in our lives05:11PM EDT- COVID has shown how vital the progress of the decades of tech improvements has become05:11PM EDT- Technology has led disruptions05:12PM EDT- Predicting the future is tough, but we expect to see 100 billion devices - intelligent computing05:12PM EDT- Accessing data and compute from anywhere - exascale for everyone05:12PM EDT- like electricity05:12PM EDT- 10x growth opportunity for the industry05:14PM EDT- A balance of performance vs general purpose05:14PM EDT- Leveraging data to build intelligence - data that isn't analyzed isn't useful05:14PM EDT- We need more capacity and more bandwidth at every level05:14PM EDT- We need bandwidth to achieve exponential growth05:15PM EDT- Gaps between what we have for memory today for AI vs what we need05:15PM EDT- We need superhuman-style computing05:15PM EDT- Now Moore's Law05:16PM EDT- People have predicted the end of Moore's Law for decades05:16PM EDT- Moore's Law is how we've built the last two eras of computing05:16PM EDT- It has been harder and harder to deliver the required metrics05:16PM EDT- But it's definitely not over yet05:17PM EDT- There is plenty of room at the top05:17PM EDT- Software helps us to get there as much as hardware does05:17PM EDT- Python vs AVX51205:17PM EDT- Over 100x perf on the same CPU with software updates05:17PM EDT- New AI workloads allows vector optimization opportunities that weren't there before05:18PM EDT- Transistor scaling though isn't helping as much as it used to05:18PM EDT- Whatever we call the Moore Law in the modern age, we believe transistor density 50x easily05:18PM EDT- 3x in FinFET itself05:19PM EDT- x2 in Nanowire05:19PM EDT- Nanowire stackeds for another 3x05:19PM EDT- This is regular pitch scaling might stop05:19PM EDT- then wafer-to-wafer stacking for 2x05:19PM EDT- Then die on wafer stacking for 2x05:19PM EDT- All of this is happening in labs across the world05:19PM EDT- The vision will play out over the next decade or more05:20PM EDT- Heat dissipation is a challenge too05:20PM EDT- Room at voltage, capacity scaling, new pacakinbg, frew scaling, new architectures05:21PM EDT- Also packaging - the future of Foveros is hybrid bonding05:21PM EDT- Simpler interconnects with lower capacitance and lower power05:21PM EDT- Stacked SRAM test chip recently taped out05:22PM EDT- Significant investment allows Intel to drastically adjust its view on next gen packaging for end-user product05:22PM EDT- Now memory hierarchy05:23PM EDT- (the dreaded pyramid of optane)05:23PM EDT- And the inverse next gen pyramid05:23PM EDT- Need 10x improvement across the board05:24PM EDT- Brainstorm next gen requirements with Tim Sweeney about next gen MMO05:24PM EDT- Support 1000s of users or more at once with Hardware and Software05:24PM EDT- But also make general purpose and accessible to everyone05:25PM EDT- First, this is how hardware companies think:05:25PM EDT- This is the concept we were thinking05:25PM EDT- 25 cores per CPU - with density, go up 100x - 4x boards, then racks for 1million cores05:26PM EDT- It's all about the interconnect!05:26PM EDT- Now software05:26PM EDT- The grumpy person reminds Raja of Jim Keller05:27PM EDT- This contract between hardware/software is what matters05:27PM EDT- All about ISA + OS developers05:27PM EDT- It's all about performance and generality05:28PM EDT- Rich software stack on x86 today05:28PM EDT- The more abstraction, the more developers05:28PM EDT- Abstractions are very leaky05:29PM EDT- It's a Sisyphean effort05:29PM EDT- What are the hardware/software contracts of the future?05:29PM EDT- x86, Arm, RISC-V, AI, GPU, Memory, Network05:30PM EDT- Intel is adding heterogenity in the CPU socket05:30PM EDT- Beware of beyond Cooper Lake05:31PM EDT- 3-5 years to see adoption of new hetero ISA extensions05:31PM EDT- That's a broad software ecosystem statement05:31PM EDT- The key to this is to give developers performance at every level05:32PM EDT- Ninja developers at the low level can offer non-linear improvements higher up the stack05:32PM EDT- Any abstraction needs to be scalable - open and accessible to all, Have to retain productivity at all levels while also maintaining perf05:32PM EDT- Misconception that python isn't used for performance05:33PM EDT- Ninja programmers are rare, but very important for performance05:33PM EDT- Important to support ninjas05:33PM EDT- Scaling across every product05:33PM EDT- Level sub-zero05:34PM EDT- OneAPI05:34PM EDT- Still early days05:34PM EDT- OneAPI beta available on Intel Dev Cloud05:36PM EDT- Scale from sensors to edge to cloud05:36PM EDT- Where will be in 202105:36PM EDT- milliwatts to Megawatts05:37PM EDT- XeHP GPU !05:37PM EDT- 1000x in compute by 202505:38PM EDT- Exascale for everyone05:38PM EDT- Now time for Q&A05:39PM EDT- Actually a few more comments first05:45PM EDT- More complex hardware in the future05:45PM EDT- Now Q&A05:48PM EDT- Q: Integration between CPU and GPU A: We've been doing a lot time for the PC space, what hasn't been done yet is in the DC and at scale. The key is figuring out the programming model that scales - at the moment we see them a scalar/vector/matrix and it's all about combining them and building the programming model. Physical integration is also key, at high performance.05:48PM EDT- Q: Does intel plan the open source Xe dGPU code, as with the Gen11, or will it be closed stack? A: We are pretty active in Linux open source. Xe drivers in Linux will be Open Source.05:51PM EDT- Q: Does ISA matter in a future of accelerators? A: Great Question. It's the central thesis of the talk. DSA - do you need an ISA, or not? My thesis on ISA is important is for the general purpose, for the mass install base - architectural impact based on that hardware software contract. Lots of us have worked on DSAs, but when you talk generality, today, ISA still matters. If you move the contract up the stack, is that in the form of an ISA, how does it look? It's a trillion dollar question, I'm not proposing that I have an answer, but my talk is that we are working on it, and we will share what we find through OneAPI, and in some ways it's a call to action for the whole community. It has to cover the whole industry, not just one vendor or architecture.05:53PM EDT- Q: Security HW vs SW, direction in industry vs academia? A: Great Question. I could have spent more time on Security if I had more time and Intel's vision! It's super important. The surface area we are generating over all these layers of hierarchy - the security attack surface area is growing more than exponential. It's scary! It's a big call to action for the community to. Security is hardware as we move forward, not easier. Architecture opportunities and simplications, in both hw and sw is daunting.05:56PM EDT- Q: ML revolution - libraries or GP stack? A: Great question. We already have special paths in TF and pytorch - the inner loops have been phenominally accelerated in the last few years. As we do the analysis in the workload, we are seeing the bottlenecks shifting around as we optimize. The algorithm rate of change is quite high - with the community and our customers, I have a lot of conversations about generality of future approaches. It's a whack-a-mole. Right now the need for generality software stack is potent and there are lots of discussions that are active. Is there an API that develops a better scalable contract? It's hard to tell.06:00PM EDT- Q: Other approaches of wide purpose compute like OpenCL haven't succeeded. What makes OneAPI work? A: At one point there was abstractions of the GPU hardware, and even with all the limiotations a decade ago. I don't believe at OpenCL it wasn't really taken a step back to look at the overall compute problem. If you go back more than two decades ago, the work in high performance computing systems in languages there is a lot of golden nuggets and answers that sit on those infrastructures. That's one of the things we look at. Personally I also have been a big fan of abstraction of Apple's grand central dispatch. I know swift and concurrency models in swift have made some amazing progress, then there's Apache Spark too. If you look at those models on those software frameworks, there is somehting there for us a hardware community to pay attention to. I won't say OpenCL is a great example is a great example to cover all forms of parallelism (dense, sparse, async, task) and memory heterogenity is a big deal - how do we cover that? That's a harder problem in my opinion.06:01PM EDT- Q: About mem power efficiency how do you see 10x required BW/power scaling? A: The opportunities I see is that we have to get compute closer to memory (or memory closer to compute). As I alluded to, we're doing so interesting things with new products, like Rambo cache which we've announced. If you look at both capacity and latency at every level, you do see that 10x opportunity. 10x doesn't seem to hard, but memory is hard, because memory isn't just a hardware/tech problem, it's also a big business problem!06:04PM EDT- That's the end of the Q&A. Now onto the third session. First up, a RISC-V talk\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15990/hot-chips-2020-live-blog-intels-raja-koduri-keynote-200pm-pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Hot Chips 2020: Marvell Details ThunderX3 CPUs - Up to 60 Cores Per Die, 96 Dual-Die in 2021\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-08-17T20:30:00Z\n",
      "URL: https://www.anandtech.com/show/15995/hot-chips-2020-marvell-details-thunderx3\n",
      "Content: Today as part ofHotChips 2020we saw Marvell finally reveal some details on the microarchitecture of their new ThunderX3 server CPUs and core microarchitectures. The company had announced the existence of thenew server and infrastructure processor back in March, and is now able to share more concrete specifications about how the in-house CPU design team promises to distinguish itself from the quickly growing competition that is the Arm server market.We hadreviewed the ThunderX2 back in 2018– at the time still a Cavium product before the designs and teams were acquired by Marvell only a few months later that year. Ever since, the Arm server ecosystem has been jump-started by Arm’s Neoverse N1 CPU core and partner designs such as fromAmazon (Graviton2)andAmpere (Altra), a quite different set of circumstances and alongside AMD’s successful return in the market, a very different landscape.Marvell started off the HotChips presentation with a roadmap of its products, detailing that the ThunderX3 generation isn’t merely just a single design, but actually represents a flexible approach using multiple dies, with the first generation 60-core CN110xx SKUs using a single die as a monolithic design in 2020, and next year seeing the release of a 96-core dual-die variant aiming for higher performance.The use of a dual-die approach like this is very interesting as it represents a mid-point between a completely monolithic design, and a chiplet approach from vendors such as AMD. Each die here is identical in the sense that it can be used independently as standalone products.From a SoC-perspective, the ThunderX3 die scales up to 60 cores, with the 2-die variant scaling up to 96. The first thing question that comes to mind when seeing these figures is why the 2-die variant doesn’t scale up to the full 120-cores- Marvell didn’t cover this during the talk but there were a few clues in the presentation.Marvell had made the performance improvement claim of 2-3x over a ThunderX2 at equal power levels. This latter had a TDP of 180W – if the TX3 maintains this thermal envelope then it would mean that a dual-die design would have had to grow TDPs to up to 360W which far beyond what one can air cool in a typical server form-factor and rack in terms of power density. Assuming just a linear cut-down to 96 cores as advertised we’d end up around 288W – which is more in line with the current high-end server CPU deployments without water-cooling. Of course – this is all our own analysis and take of the matter.A single die supports 8 channels of DDR4-3200 which is standard for this generation of a server product and essentially in line with everybody else in the market. I/O wise, we see a disclosure of 64 lanes of PCIe 4.0 – which is again in line with competitors but half of what higher-end alternatives from Ampere or AMD can achieve.One big unknown right now is how the dual-die product will segment the I/O and memory controllers – if this is going to be a 50-50 split in terms of resources between the two dies, or whether we’ll see an imbalanced setup – or if the platform can actually handle the full resources from each die and transform itself into a 16-channel 128 lane beast?Comparison of Major Arm Server CPUsMarvellThunderX3110xxCaviumThunderX29980-2200AmpereAltraQ80-33AmazonGraviton2Process TechnologyTSMC7nmTSMC16 nmTSMC7 nmTSMC7nmDie TypeMonolithicorDual-Die MCMMonolithicMonolithicMonolithicMicro-architectureTritonVulcanNeoverse N1 (Ares)Cores60 (1 Die)Swiched 3x Ring96 (2 Die)32Ring bus80Mesh64MeshThreads240 (1 Die)384 (2 Die)1288064Max. number of sockets2221Base Frequency?2.2 GHz--Turbo Frequency3.1 GHz2.5 GHz3.3 GHz2.5 GHzL3 Cache90MB32 MB32 MB32 MBDRAM8-ChannelDDR4-32008-ChannelDDR4-26678-ChannelDDR4-32008-ChannelDDR4-3200PCIe lanes4.0 x 64(1 Die)3.0 x 564.0 x 1284.0 x 64TDP~180W (1 Die)(unconfirmed)180W250 W~110-130W(unconfirmed)On paper at least, the ThunderX3 seems quite similar to Amazon’s Graviton2 as they both share a similar amount of CPU cores and similar memory and IO configurations. The bigger differences that one can immediately point out to is that the ThunderX3 employs SMT4 in its CPU cores and thus supports up to 240 threads per die. There’s also a TDP difference, but I attribute this to the Graviton2 being conservative with its clock frequencies, whilst Ampere’s SKUs being more in line with the ThunderX3, particularlythe 64-core 3.0GHz 180W Q64-30being the closest match in specifications.Another thing that stands out for the ThunderX3 is the 90MB of L3 cache that dwarfs the 32MB of the previous generation as well as the 32MB configurations of Ampere and Amazon.Marvell here opted to evolve its own interconnect microarchitecture which has now evolved from a simple ring design, to a switched ring with three sub-rings, or columns. Ring stops consist of CPU tiles with 4 cores and two L3-slices with 3MB of cache. This gives a full die with 15 ring stops (3x5 columns) and the full 60 cores 90MB of total L3 cache which is a quite respectable amount.In the Q&A sessions, Marvell disclosed that their rationale for a switched ring topology versus a single ring, or a mesh design was that a single ring wouldn’t have been able to scale up in performance and bandwidth at higher core counts. A mesh design would have been a big change, and it would have required a reduction in core count. A switched ring represented a good trade-off between the two architectures. Indeed, if this is what enabled Marvell to include up to 3x the cache versus its nearest competitors, it seems to have been a good choice.One odd thing I noted is that the system is still using a snoop-based coherency algorithm which comes in contrast with other directory-based systems in the industry. This might reduce implementation complexity and area, but might lag behind in terms of power efficiency and coherency traffic for the chip.The memory controllers tap into the rings, and Marvell’s inter-socket/die CCPI3 interface here serves up to 84GB/s of bandwidth.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15995/hot-chips-2020-marvell-details-thunderx3\n",
      "Title: Hot Chips 2020 Live Blog: Marvell ThunderX3 (10:30am PT)\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-08-17T17:30:00Z\n",
      "URL: https://www.anandtech.com/show/15986/hot-chips-2020-live-blog-marvell-thunderx3-1030am-pt\n",
      "Content: 01:41PM EDT- ThunderX3, now owned by Marvell. They acquired Cavium for TX and TX201:41PM EDT- Rabin Sugumar is lead architect for TX301:41PM EDT- Sell into the same market as Intel and AMD01:42PM EDT- Recap of TX2, a 32-core Arm v8.1 design with SMT401:42PM EDT- Paved the way for a number of Arm Server CPU firsts01:42PM EDT- Industry leading perf on bandwidth intensive workloads when launched01:43PM EDT- Lots of learnings went into TX301:43PM EDT- Two versions of TX3 - single die and dual die01:43PM EDT- ThunderX4 in the works01:44PM EDT- Up to 60/96 cores, Arm v8.3 with other 8.4 and 8.5 features01:44PM EDT- 8x DDR4-3200, 64 PCIe 4.0 lanes01:44PM EDT- On-die monitoring and power management subsystem01:44PM EDT- Full IO, SATA, USB01:44PM EDT- 2x-3x perf over TX2 in SPEC01:44PM EDT- TSMC 7nm01:44PM EDT- SMT401:45PM EDT- Core block diagram01:45PM EDT- 64KB L1-I, up to 8 instructions/cycle01:45PM EDT- 8x decode01:45PM EDT- Most instructions map to a single micro-op01:46PM EDT- Skid buffer - private to each thread01:46PM EDT- most other structures are shared01:46PM EDT- Skid buffer is where the loop buffer is located01:46PM EDT- 4 ops/cycle dispatch to scheduler01:47PM EDT- 70 entry scheduler, 220 entry ROB01:47PM EDT- 7 execution ports, 4 are HP, 3 are Load/Store01:47PM EDT- 2 are load+store, 1 is store01:47PM EDT- 512 KB 8-way unified L201:47PM EDT- 64 KB I-cache01:48PM EDT- decoupled fetch (TX2 did not have this)01:48PM EDT- this allows use the path to walk and fetch cache lines01:48PM EDT- performance uplift on datacenter codes01:49PM EDT- Each thread has a 32-micro-op skid buffer, supports 8x four micro-op bundles01:49PM EDT- rename tries to bundle microops01:49PM EDT- Scheduler - 256 entry depths of ports01:50PM EDT- L1 TLB allow 0-cycle01:50PM EDT- L2 supports strides and regen01:50PM EDT- Here are all the improvements over TX2 for each change01:51PM EDT- larger OoO helps 5%, reduce micro-op expansion helps 6%01:51PM EDT- Early perf measurements on TX3 silicon01:51PM EDT- SPECint 30% from arch, rest is from frequency increase01:52PM EDT- FP gets slightly better gain than Int01:52PM EDT- Gains are actually better - these slides are a couple weeks old01:52PM EDT- As far as OS is concerned, each thread is a full CPU01:52PM EDT- so four CPUs per core01:53PM EDT- die area impact of SMT4 is ~5%01:53PM EDT- Arbitration to ensure high utilization01:54PM EDT- Here's the thread speedup01:55PM EDT- up to 2.21x going to SMT4 for MySQL, 1.28x for x26401:55PM EDT- (these are SPEC numbers)01:55PM EDT- MySQL: 1 thread to 60 cores and 240 threads offers 89x perf improvement01:56PM EDT- (that's ~1.5x ?)01:56PM EDT- Ring cache with a column01:56PM EDT- Logically shared but structurally distributed01:56PM EDT- non-inclusive01:57PM EDT- Snoop based coherence01:57PM EDT- Q&A time01:58PM EDT- Q: Who manufacture? A: TSMC 7nm01:59PM EDT- Q: No SVE? A: Didn't line up with dev schedule. Better fit for next gen TX. Coming then02:01PM EDT- Q: Any competitive comparison? What areas does the TX3 chip stand out? A: vs Intel on ST, Intel turbos to higher freq. Even though on a per-GHz TX3 is faster, overall on Intel single thread, Intel tends to be faster. But lower core count on Intel, so our throughput on SPECintrate, we are higher than Intel. On AMD, it's a little bit of a reverse. TX3 does better than AMD on single thread. On throughput, limited cache sharing, AMD does better. But code with more sharing, producers/consumers, then scaling seems to shop. TX3 is better on database performance codes. For Graviton2 - that's a really good chip. Frequency is low, no threading, but its a good chip.02:02PM EDT- Q: Why did you select switched ring topology vs mesh or single ring? A: tradeoff with area and design changes and performance. On TX2 we had a single ring. Extending that to TX3 didn't work because core count. Mesh would have been a big change, and we would have had to reduce core count. Switched ring seemed a good tradeoff. One of the metrics we use is Stream out of L3 cache - on this on TX3 we see 15%(50%?) more BW per core than TX2 using this topology.02:03PM EDT- That's a wrap. Next up is z15!\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15986/hot-chips-2020-live-blog-marvell-thunderx3-1030am-pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NUVIA Phoenix Targets +40-50% ST Performance Over Zen 2 for Only 33% the Power\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-08-11T17:00:00Z\n",
      "URL: https://www.anandtech.com/show/15967/nuvia-phoenix-targets-50-st-performance-over-zen-2-for-only-33-power\n",
      "Content: In November 2019, the company NUVIA broke out of stealth mode. Founded by former senior Apple and Google processor architects, John Bruno, Manu Gulati and Gerard Williams III, the company came crashing out of the gate with quite considerable goals to revamp the server market with an SoC that would provide ‘A step-function increase in compute performance and power efficiency’. Today NUVIA is putting more data behind those goals.The press release we received from NUVIA takes some time to cover some of the basics of the modern day server market, and it initially read almost like an AnandTech article, which is eerily scary. Suffice to say, NUVIA understands the current state of play of the server market, including where Intel and AMD stand with respect to each other, and how x86 offerings are squaring up against the other options on the market. As with most elements of the server market, different verticals often have different requirements, on compute, memory, IO, power, or physical constraints, as well as initial cost of hardware alongside total cost of ownership. To that end, NUVIA’s processor designs, according to the company, ‘an SoC that will deliver industry-leading performance with the highest levels of efficiency, at the same time’.With that, NUVIA is announcing that its first generation CPU core will be called Phoenix and be built upon the ARM architecture (likely Armv9) with an architecture license. Phoenix will be part of the Orion SoC, with NUVIA stating that they are implementing ‘a complete overhaul of the CPU pipeline’. Gerard William’s designs from Apple are known to be considerably different to what we’ve seen elsewhere in the market, so we suspect that this is going to be a big part of the secret sauce behind Orion and its Phoenix cores.NUVIA goes on to say that Phoenix is ‘a clean sheet design’, focusing on single core performance leadership and maximizing memory bandwidth and utilization. The Orion SoC will be built to focus on high utilization and sustained frequencies, without having to rely on high-turbo marketing numbers, to allow customers to make the best use of the hardware within allocated power and cooling budgets. Alongside this, NUVIA is stating that there will be hardware infrastructure built to specification ‘to support peak performance on real cloud workloads’.NUVIA’s NumbersThe big part of the press release is NUVIA’s performance-per-watt claims. To do this, NUVIA is using Geekbench5 as a performance indicator, along with direct power measuring, of current in-market x86 and Arm offerings. NUVIA is taking smartphone and mobile based cores, such as Intel Ice Lake, Qualcomm SD865, AMD Ryzen 4700U, as well as Apple’s A12Z Vortex and A13 Lightning, as starting points. The reason for this is that NUVIA believes there is starting to become no meaningful difference between smartphone/mobile cores and server cores when extrapolated – only if you start adding in massive vector engines for specific customers does that become relevant.According to NUVIA’s numbers, this is where the current market stands with respect to Geekbench 5. At every point, ARM’s results are more power efficient/higher performant than anything available on x86, even though at the high end Apple and Intel are almost equal on performance (for 4x the power on Intel).NUVIA notes that power of the x86 cores can vary, from 3W to 20W per core depending on the workload, however in the sub 5W bracket, nothing from x86 can come close to the power efficiency of high-performance Arm designs. This is where Phoenix comes in.NUVIA’s claim is that the Phoenix core is set to offer from +50% to +100% peak performance of the other cores, either for the same power as other Arm cores or for a third of the power of x86 cores. NUVIA’s wording for this graph includes the phrase ‘we have left the upper part of the curve out to fully disclose at a later date’, indicating that they likely intend for Phoenix cores to go beyond 5W per core.At this point, NUVIA is running simulations of its core designs in-house to get these numbers. This is a standard thing for any company developing a new SoC or a new core before actually going to the fab to get it made. It also helps investors analyze where things stand.What gives credibility to the new company’s lofty goals is the founder’s track record of their past designs. Apple’s silicon success over the last half decade has been one of the most impressive developments in the industry, and it seems NUVIA has been able to recruit top talent with the aim to reproduce such success in the datacentre market.Some users might consider that SPEC should have been used, given its relevance to NUVIA’s initial target markets on server, and I perhaps agree. I suspect that NUVIA believed that GB5 might be more accessible to a wider audience for core-to-core comparisons.The FutureNUVIA states with this press release that it will aim to have some of the highest performance and best efficiency CPU/SoC products in the market. The company reiterates that even if other vendors suddenly see a 20% year-over-year gain in raw performance, NUVIA still expects to be ahead of its main competitors. We shall have to wait and see what magic NUVIA has that others do not.Update: Initially this article said that NUVIA will have new products in the next 18 months. This was a simple misreading of NUVIA's press release and the relevent sentence has been removed.Related ReadingNUVIA: New Server CPU Startup Going After Intel and AMD\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15967/nuvia-phoenix-targets-50-st-performance-over-zen-2-for-only-33-power\n",
      "Title: OPPO's Reno3 5G vs Reno3 Pro vs Reno3 Pro 5G: Why Don't We See More MediaTek Dimensity 1000 Phones?\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-08-10T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/15963/oppos-reno3-vs-reno3-pro-vs-reno3-pro-5g-why-dont-we-see-more-mediatek-dimensity-1000-phones\n",
      "Content: The last 2 years have been quite exciting for the mobile silicon landscape – Arm’s release of the Cortex-A76 had marked a big jump in performance for smartphones, vastly raising the bar of what is possible to achieve in a mobile SoC. With this generation, we’ve also seen SoC vendors deploy some increasingly competitive designs that improved upon their predecessors – much to do with the new IP, but also a large part thanks to new 7nm manufacturing technologies.We’re all too familiar with Qualcomm’s Snapdragon 855 and 865 which power the majority of flagship devices in the market right now. Qualcomm has been steadily improving their execution of the last couple of years ever since the Snapdragon 835, and the new S865 is really a great balance between performance and power efficiency for 2020’s devices. We’ve also seen HiSilicon push out quite competitive products with the Kirin 980 and the 990 – although the future of Huawei’s in-house SoC designs are in grave limbo and run the risk ofno longer seeing successors in the future.Qualcomm’s and HiSilicon’s successes have however come at a cost in the market though – we’re all aware ofSamsung’s missteps with their Exynos series, although we do hope things will improve in the comings years, fingers crossed.The other victim to Qualcomm and Huawei’s success has been MediaTek. The Taiwanese SoC vendor had been traditionally been a larger player in the smartphone market, however it’s been losing market share to Qualcomm quarter after quarter for years now. Last year, the company had revealed that with the new 5G generation of “Dimensity” SoCs we’d be seeing a renewed push into the high-end market.Today, we’ll be taking a closer look at one of these Dimensity 1000 powered phones, the OPPO Reno3 5G. The Reno3 series has been quite the oddball device line-up this year, as the company has been selectively releasing quite different variants at a rapid pace in different markets – all with different SoCs and slightly differing device specs.Some OPPO Reno3'sReno3 5G(China only)Reno3 Pro(Global variant)Reno3 Pro 5Ga.k.aFind X2 NeoSoCDimensity 1000L4x Cortex A77 @ 2.2GHz4x Cortex A55 @ 2.0GHzMali-G77MP7 @ 695 MHzHelio P952x Cortex A75 @ 2.2GHz6x Cortex A55 @ 2.0GHzPowerVRGM 9446 @ 970MHzSnapdragon 765G1x Cortex A76 @ 2.4GHz1x Cortex-A76 @ 2.2GHz6x Cortex-A55 @ 1.8GHzAdreno 620Display6.4-inch OLED2400 x 1080 (20:9)6.4-inch OLED2400 x 1080 (20:9)6.5-inch OLED2400 x 1080 (20:9)90HzDimensions160.3 x 74.3 x 8 mm181 grams158.8 x 73.4 x 8.1mm175 grams159.4 x 72.4 x 7.7 mm171 gramsRAM8/12GB LPDDR4X8GB LPDDR48/12GB LPDDR4XNANDStorage128GB UFS 2.1128/256GB128/256GB UFS 2.1Battery4025mAh (15.57Wh) typ.3935mAh (15.22Wh) ratedVOOC Flash Charge 4.030WFront Camera32MP f/2.044MP f/2.4+ 2MP f/2.432MP f/2.0Primary Rear Camera64MP IMX686 1/1.73\" 0.8µmf/1.8 w/ OIS48MP IMX586 1/2.0\" 0.8µmf/1.7 w/OISSecondaryRear Camera8MP 1.4µmf/2.25119° Ultra-wideTertiaryRear Camera2MP depth portraitf/2.413MP 1/3.4\" 1.0µmTelephotoExtraCamera2MP black & white portraitf/2.42MP black & white portraitf/2.42MP black & white portraitf/2.44G / 5GModem5G Sub-6 Integrated4G LTE5G Sub-6 IntegratedSIM SizeDual nanoSIMWireless802.11a/b/g/n/ac/ax 2x2 MU-MIMO,BT 5.0 LE, NFC, GPS/Glonass/Galileo/BDS802.11a/b/g/n/ac 2x2 MU-MIMO,BT 5.0 LE, NFC, GPS/Glonass/Galileo/BDS802.11a/b/g/n/ac/ax 2x2 MU-MIMO,BT 5.0 LE, NFC, GPS/Glonass/Galileo/BDSConnectivityUSB Type-C3.5mm headsetUSB Type-CSpecial FeaturesIn-screen fingerprint sensorLaunch OSAndroid 10All in all – there’s 4 different variants of the “Reno3”, with the key characteristic being that we’re seeing quite different devices in the Chinese market compared to the international counterparts. We have had our hands on 3 of these for several months now:The Chinese market Reno3 5G is the most interesting device today as it’s the one phone that employs MediaTek’s newest Dimensity 1000 SoC. To be precise, this is a “1000L” lower-binned variant of the chip that slightly differs in clocks speeds and GPU configuration. Originally planned for 4x 2.6GHz Cortex-A77 cores with 4x 2.0GHz Cortex-A55 cores and a Mali-G77MP9 at 800+MHz, the new lower-binned chip only clocks the big cores at 2.2GHz, and disables two GPU cores to make it an MP7 configuration at 695MHz. Strangely enough, this Maywe saw MediaTek re-release the chip in the form of the Dimensity 1000+with the full specifications that were originally promised late last year.The Chinese market also received the Reno3 Pro 5G – a higher-end variant of the phone with a different design, powered by Qualcomm’s Snapdragon 765G SoC. This product positioning was quite weird, as on paper, you’d expect the regular Reno3 5G actually outperform the Pro 5G variant – and this is actually the most interesting topic we’ll be addressing today. The Snapdragon 765 features older Cortex-A76 cores, one at 2.4GHz, and one at 2.2GHz, alongside 6x Cortex-A55’s at 1.8GHz, and has the Adreno 620 as its GPU.The interesting thing also about the Reno3 Pro 5G is that a few months after the Chinese launch, we’ve actually seen the device come to international markets, but this time marketed as the Find X2 Neo.The Reno3 and Reno3 Pro in international markets are seemingly quite shadows of their Chinese counterparts. The Reno3 comes with a 2018 launched MediaTek P90 chipset, while the Reno3 Pro using a P95 that doesn’t really change much at all in the configuration of the chip beyond a slight clock boost to its AI engine. The 2x Cortex-A75 cores at 2.2GHz with 6x A55’s at 2.0GHz and an ImgTech PowerVR GM 9446 GPU at 970MHz are all quite outdated in 2020 – with the SoC being fabricated on an older 12nm process.The Reno3 Pro also has very little to do with the Reno3 Pro 5G other than their naming – the latter is clearly a higher-end device in terms of design and screen.The piece today isn’t much of a device review as it’s more focused on the SoC differences between the phones, with an emphasis on the MediaTek Dimensity 1000L on the Chinese Reno3 5G.The Reno3 phones are actually outdated by this point in time as the company has followed up with the Reno4 series in China – a slight design iteration on the previous gen, mixing and mashing specifications again whilst retaining the Snapdragon 765G and dropping MediaTek as a SoC source, making today’s article all the more intriguing.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15963/oppos-reno3-vs-reno3-pro-vs-reno3-pro-5g-why-dont-we-see-more-mediatek-dimensity-1000-phones\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Ampere Altra 1P Server Pictured: GIGABYTE’s 2U with 80 Arm N1 Cores, PCIe 4.0 and CCIX\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-08-03T21:00:00Z\n",
      "URL: https://www.anandtech.com/show/15949/ampere-altra-1p-server-pictured-gigabytes-2u-with-80-arm-n1-cores-pcie-40-and-ccix\n",
      "Content: With the news of Apple moving to Arm SoCs replacing Intel in a few key products, and the success of the new Graviton2 SoC in Amazon’s Web Services, the news-o-sphere is awash with excitement about a new era of Arm-based computing. One of the companies looking to deploy Arm into the cloud is Ampere, with its new Altra and Altra Max compute processors.We’ve coveredAltra in some detail, with the aim to offer better-than-Graviton performance and functionality to hyperscalers that don’t have access to Graviton2 (because it’s an Amazon only chip). In June,the company launched its processor list, going from 24 cores all the way up to 80 cores running at 3.3 GHz for 250 W. This processor list is quite possibly the easiest-to-follow naming scheme of any processor list in recent memory. Alongside all those high-performance cores there are 128 PCIe 4.0 lanes, support for CCIX, eight channel DDR4-3200 memory, and a 128-core version coming early next year. It’s a shot well past Graviton, aimed squarely at Xeon and Epyc.At the announcement of Altra, Ampere stated that it would be developing reference designs for Altra – a single socket called Mt. Snow, and a dual socket called Mt. Jade. GIGABYTE is the first OEM partner to showcase its single socket design, with a dedicated video on the product as part of theGIGABYTE Virtual Show 2020, which replaced its Computex plans.The R272-P30 server is the Mt. Snow single socket 2U design, built upon GIGABYTE’s MP32-AR0 motherboard, which is an EATX form factor. The motherboard is laid out in order to improve server airflow with a transposed socket, which also helps with supporting all of the sixteen DDR4-3200 memory slots. The Altra socket is a rather substantial LGA4926 socket, indicating 4926 pins within the bracket, with the bracket held on by five Torx screws (EPYC uses three by comparison). Supporting the socket is an 8-phase server-grade power delivery, which seems minuscule by consumer standards but is probably overkill here.The motherboard has two PCIe 4.0 x16 full-length slots and four PCIe 4.0 x8 full-length slots, plus another PCIe 4.0 x8 half-length slot. Management is through the popular Aspeed AST2500 BMC implementation, while onboard Ethernet uses a dual Intel I350 solution. There are four slimline U.2 ports, as well as eight min-SAS breakout headers and an OCP 2.0 PCIe 3.0 slot for add-in OCP solutions.The server as presented by GIGABYTE had seven PCIe breakout cards installed, leading to 24x PCIe 4.0 NVMe x4 storage at the front of the chassis. Ultimately this solution is for a fast-storage cloud deployment, and is one of the first Altra-based servers that GIGABYTE is developing. Ampere is a key partner with NVIDIA in order to provide CUDA-on-Arm solutions, so we suspect a GPGPU variant might be in the works as well.Ampere 1st Gen Altra 'QuickSilver'Product ListAnandTechCoresFrequencyTDPPCIeDDR4PriceQ80-33803.3 GHz250 W128x G48 x 3200?Q80-30803.0 GHz210 W128x G48 x 3200?Q80-26802.6 GHz175 W128x G48 x 3200?Q80-23802.3 GHz150 W128x G48 x 3200?Q72-30723.0 GHz195 W128x G48 x 3200?Q64-33643.3 GHz220 W128x G48 x 3200?Q64-30643.0 GHz180 W128x G48 x 3200?Q64-26642.6 GHz125 W128x G48 x 3200?Q64-24642.4 GHz95 W128x G48 x 3200?Q48-22482.2 GHz85 W128x G48 x 3200?Q32-17*321.7 GHz58 W128x G48 x 3200?Q32-17321.7 GHz45 W128x G48 x 3200?*With 4 TiB DRAM InstalledAmpere recently announced that the first cloud instances on Altra were starting to come online, startingwith Packet as part of the Early Access Program. Wider general availability is expected through the rest of the year. We’re already on the list for a review sample!Source:GIGABYTERelated ReadingNext Generation Arm Server: Ampere’s Altra 80-core N1 SoC for Hyperscalers against Rome and XeonAmpere’s Product List: 80 Cores, up to 3.3 GHz at 250 W; 128 Core in Q4Avantek's Arm Workstation: Ampere eMAG 8180 32-core Arm64 ReviewArm Development For The Office: Unboxing an Ampere eMag WorkstationAmazon's Arm-based Graviton2 Against AMD and Intel: Comparing Cloud Compute\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15949/ampere-altra-1p-server-pictured-gigabytes-2u-with-80-arm-n1-cores-pcie-40-and-ccix\n",
      "Title: Supermicro SuperServer E302-9D Review: A Fanless 10G pfSense Powerhouse\n",
      "Author: Ganesh T S\n",
      "Date Published: 2020-07-28T19:00:00Z\n",
      "URL: https://www.anandtech.com/show/15906/supermicro-superserver-e3029d-review-a-fanless-10g-pfsense-powerhouse\n",
      "Content: Intel launched the Xeon D-2100 SoCs in early 2018, with a feature set making them a fit for several verticals including edge servers, networking, and storage. One of the key advancements made in the Xeon D-2100 compared to the first-generation Xeon D-1500 series was the inbuilt support for two additional 10G network interfaces. With TDPs starting at 60W, the Xeon D-2100 SoCs lends itself to some interesting and unique server and edge procesing products. One such system is Supermicro's passively-cooledSuperServer E302-9Dsporting the Xeon D-2123IT SoC.As part of the evaluation efforts of different technologies and products, AnandTech editors are regularly tasked with the building or identification of suitable testbed systems. The requirements for these systems often mirror the requirements of software developers and homelab enthusiasts. The increasing adoption of 10G across various networking / network-attached storage product lines meant that we were on the lookout for a low-power system with multiple 10G ports to act as testbeds. We reached out to Supermicro after spotting theirX11SDV-4C-TP8F-01FlexATX board. Supermicro graciously agreed to loan us two SuperServers based on the board to take for a testdrive - the E302-9D in a passively-cooled desktop form factor (that we are taking a detailed look at today), and the 5019D-4C-FN8TP 1U rackmount version.IntroductionIntel's Xeon D product line targets servers used in power- and size-constrained scenarios (including edge compute). This includes applications across multiple domains such as storage, networking, and communication. The product line integrates server-class CPU cores along with the platform controller hub (PCH) in a single package. The first-generation Xeon D (1500 series) was based on Broadwell-DE cores along with the C220 server PCH. Ourlaunch coverageof the Xeon D-2100 series brought out the details of the updated server core (Skylake-DE) and PCH (Lewisburg C600-series). The relatively power-hungry PCH update and the addition of AVX512 capabilities in the Skylake cores meant that the minimum TDP went up from 20W in the D-1500 family to 60W in the D-2100. However, the updates also brought in welcome connectivity updates.The Supermicro SuperServer E302-9D / X11SDV-4C-TP8F-01 we are looking at in this review utilizes the Xeon D-2123IT with a 4C/8T configuration. It has the least TDP of all members in the D-2100 family, yet comes with support for up to four 10G ports. The 60W TDP of the SoC allows Supermicro to utilize it in a passively-cooled system. To the best of our knowledge, this is the only off-the-shelf x86 system that provides consumers with four 10G Ethernet ports in a fanless configuration.The Xeon D-2100 series offers support for up to 20 PCIe 3.0 lanes, 14 SATA 3.0 lanes, and 4 USB 3.0 ports. The D-2123IT can be equipped with up to 256GB of DDR-2400 ECC memory. In creating the X11SDV-4C-TP8F-01 board used in the E302-9D, Supermicro has worked around these features to create a compact board / system that appeals to developers and home-lab enthusiasts working on cutting-edge networking applications.The SuperServer E302-9D is marketed as an embedded system comprising of the CSE-E302iL chassis and the X11SDV-4C-TP8F-01 board. The power supply is an external 150W adapter. The chassis sports a power button and status LED in the front panel, with all the I/O ports in the rear. The chassis supports a low-profile PCIe card mounted horizontally. The dimensions come in a 205mm x 295.2mm x 73mm. The gallery below takes us around the external design of the system.Gallery:Supermicro SuperServer E302-9D External FeaturesThe table below presents the specifications of the system along with the details of the reviewed configuration.Supermicro E302-9D SpecificationsProcessorIntel Xeon D-2123ITSkylake Xeon D, 4C/8T, 2.2 (3.0) GHz8MB L2+L3, 14nm (optimized), 60W TDPMemoryUp to 4x DDR4-2400 DIMMs (256GB ECC/non-ECC RDIMM)Micron DDR4-2400 ECC DIMMs17-17-17-39 @ 2400 MHz2x 16 GBBaseboard Management Controller (BMC)ASpeed AST2500Disk Drive(s)Mushkin Atlas Vital MKNSSDAV250GB-D8(250 GB; M.2 Type 2280 SATA 3.0; MLC ; Sandforce SF2241)M.2 2280 slot also supports PCIe 3.0 x4 NVMe SSDsChassis supports 2x 2.5\" 7mm SATA drives (HDD or SSD)Networking1x Realtek RTL8211 Gigabit Ethernet (IPMI)4x Intel I350-AM4 Gigabit Ethernet2x Intel X722 10GbE Controller with X557-AT2 PHY for 10GBASE-T Ethernet2x Intel X722 10GbE SFP+Miscellaneous I/O Ports2x USB 3.2 Gen 1 (5 Gbps) Type-A (Rear)Operating SystemBarebones, configured for triple boot:Windows 2019 Server Standard (x64)Ubuntu 20.04 LTSpfSense 2.4.5-p1Pricing (As configured)$1483 ($1203+$230+$50)Full SpecificationsSupermicro SuperServer SYS-E302-9D SpecificationsIn the rest of this review, we first look at the detailed specifications of the board along with a look at the internals of the system. This is followed by some of our setup and usage impressions. In particular, we look at pfSense installation on the system along with some basic benchmarks. Finally, we take a look at the power consumption and temperature profiles before offering some concluding remarks.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15906/supermicro-superserver-e3029d-review-a-fanless-10g-pfsense-powerhouse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Marvell Unveils its Comprehensive Custom ASIC Offering\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-07-27T16:00:00Z\n",
      "URL: https://www.anandtech.com/show/15931/marvell-unveils-its-comprehensive-custom-asic-offering\n",
      "Content: Last week Marvell had updated us with an overview of the company’s new more extensive and comprehensive custom ASIC offerings, detailing the company’s design abilities for customers who are seeking to deploy differentiated products.People traditionally familiar with Marvell will know the company from their products in the processor and baseband chipset spaces, having also a large footprint in the storage device controller space as well as other networking space. Most recently we’ve reported on the company’s booming business in the 5G infrastructure processors withthe Octeon TX2 and Octeon Fusion products.Last year, Marvell had acquired Avera Semiconductor, which was prior to 2018 a custom ASIC design division of Globalfoundries and prior to that a longer and well-known history as part of IBM. With GlobalFoundries having exited the leading-edge process node manufacturing business, the division had been spun off as AveraSemi, and had a brief stint as a independent company before Marvell quickly showed interest in purchasing the lot.We briefly covered the topic of Marvell being able to develop custom variations of their Octeon processor line back a few months ago, and the team and resources which enable the company to achieve this are the ex-Avera people which have joined forces with Marvell’s own teams and extensive IP portfolio.In essence, what is happening here is Marvell is offering a unique business model in terms of offering customised product hardware design that are either developed by Marvell themselves, or integrate third-party IP by a given product.Customer specific products would be something akin to how AMD currently operates its semi-custom business in that they take requests and design requirements from a customer and design a specific product based on IP that the company also separately maintains for its own “vanilla” products. Marvell’s offering here in terms of custom products also leverages their IP portfolio, allowing designs opportunities with a large level of flexibility.Going further, and which is seemingly unique (or very rare), is the ability for Marvell to integrate custom customer IP blocks, which opens up a whole new business model for companies who wish to have custom silicon for specific tasks, but do not have the resources or the design ability to create a whole silicon design themselves.Marvell here showcases a vast variety of solution opportunities, with the Avera custom ASIC team showcasing the vastly expanded opportunities and abilities that have been enhanced by their integration into Marvell’s existing business, adding Marvell’s expertise in memory subsystems, programmable logic, accelerator IP, processing units and a very large scale of IP including unique opportunities such as custom CPU microarchitectures (They also offer Arm IP cores). Least to say, the capabilities have seen a huge leap in terms of breadth and quality of IP.The custom ASIC offering would be extremely interesting in areas of wireless or wired networking designs, or even custom data-centre silicon.There are various levels of engagement for customers based on their needs and resources. This gradient ranges from either fully custom turnkey designs by Marvell, down to various intermediate models where the design work is shared between the customer and the Marvell team. A customer for example could have their own IP design, but would hand it over to Marvell for synthesis and physical design. A more advanced customer who might have more design resources on their side might simply just share a hard macro of their IP block with Marvell left with doing the layout on the chip.In essence, Marvell’s new custom ASIC division would act as a design-house for a wide range of customers who are looking for custom silicon without having to invest resources of brining up a design team themselves. Imagine this being newcomers, or maybe even larger companies who want a more unique product and don’t have the design bandwidth to develop it.Design bandwidth of the team is probably the biggest advantage for Marvell, as the scale of the operations is what allows it to be profitable and justify itself in the first place. In smaller companies with a more serial design approach one doesn’t necessarily have the ability to do multiple projects at the same time. The custom ASIC team here would be able to work on several parallel projects in tandem, with a pipeline design approach of various teams such as RTL, verification, physical design and validations teams working all on different projects. The division has a quoted decades old track-record of working on 14 leading-edge process nodes and having produced over 2000 custom designs.With leading-edge process node designs ballooning in terms of costs and the ever-increasing barrier to entry, it seems a sensible decision to be able to out-source your chip design to a design house which has such versatile resources at hand.In terms of process nodes and foundries, the company’s 14nm offerings are obviously tied to Globalfoundries given the close history the team has had with the foundry in the past. Current and next-generation designs are developed for TSMC’s 7 and 5nm nodes (Although the company didn’t directly name TSMC, the insinuation was obvious).Related Reading:Marvell’s ThunderX3 Server Team Loses VP/GM and Lead ArchitectMarvell Announces ThunderX3: 96 Cores & 384 Thread 3rd Gen Arm Server ProcessorMarvell Announces OCTEON Fusion and OCTEON TX2 5G Infrastructure ProcessorsMarvell at FMS 2019: NVMe Over Fabrics Controllers, AI On SSDMarvell Announces Client SSD Controllers With PCIe Gen4Marvell to Acquire Avera Semiconductor from GlobalFoundries\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15931/marvell-unveils-its-comprehensive-custom-asic-offering\n",
      "Title: Launching the #CPUOverload Project: Testing Every x86 Desktop Processor since 2010\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-07-20T17:30:00Z\n",
      "URL: https://www.anandtech.com/show/11425/launching-the-cpu-overload-project-testing-every-x86-desktop-processor-since-2010\n",
      "Content: One of the most visited parts of the AnandTech website, aside from the reviews, is our benchmark databaseBench. Over the last decade we've placed in there as much benchmark data as we can for every sample we can get our hands on: CPU, GPU, SSD, Laptop and Smartphone being our key categories. As the Senior CPU editor here at AnandTech, one of my duties is to maintain the CPU part of Bench, making sure the benchmarks are relevant and the newest components are tested with benchmark data up to date as much as possible. Today we are announcing the start of a major Bench project with our new Benchmark suite, and some very lofty goals.What is Bench?A number of our regular readers will knowBench. We placed a link to easily access it at the top of the page, although given the depth of content it holds, is an understated part of AnandTech. Bench is the centralized database where we place all of the benchmark data we gather for processors, graphics, storage, tablets, laptops and smartphones. Internally Bench has many uses, particularly when collating review data to generate our review graphs, rather than manually redrawing full data sets for each review or keeping datasets offline.But the biggest benefit with Bench is to either compare many products in one benchmark, or compare two products across all our benchmark tests. For example, here are the first few results of our POV-Ray test.At its heart, Bench is a comparison tool, with the ability to square two products off side by side can be vital when choosing which one to invest in. Rather than just comparing specifications, Bench provides real world data, offering 3rd party independent verification of data points. In contrast to the benchmarks other companies that are invested in selling you the product might provide, we try and create benchmarks that actually mean something, rather than just list the synthetics.The goal of Bench has always been a regressive comparison, comparing what the user has to what the user might be looking at purchasing. As a result of a decade of data, that 3-5 year generational gap of benchmark information can become vital to actually quantifying how much of an upgrade a user might receive on the CPU alone. It all depends on what products already have benchmark data in the database, and if the benchmarks are relevant to the workflow (web, office, rendering, gaming, workstation tests, and so on).Bench: The BeginningBench originally started over a decade ago by the founder of AnandTech, Anand. On the CPU side of the database, he worked with both AMD and Intel to obtain a reasonable number of the latest CPUs of the day, and then spent a good summer testing them all. This happened back when Core 2 and Athlons were running the market, with a number of interesting comparisons. The beauty of the Bench database is that all the data from the 30 or so processors Anand tested way back then still exists today, with the core benchmarks of interest to the industry and readership at the time.With AMD and Intel providing the processors they did, testing every processor became a focal point for the data: it allowed users to search for their exact CPU, compare it to other models in the same family that differ on price, or compare the one they already have to a more modern component they were thinking of buying.As the years have progressed, Bench has been updated with all the review samples we could obtain and have time to put through the benchmarks. When a new product family is launched however, we rarely get to test them all - unfortunately official sampling rarely goes beyond one or two of the high end products, or if we were lucky, maybe a few more. While we’ve never been able to test full processor stacks from top to bottom, we have typically been able to cover the highlights of a product range, and it has still allowed users to perform general comparisons using the data and for users looking to upgrade their three year old components.Two main factors have always inhibited the expansion of Bench.Bench Problem #1: Actually Getting The HardwareFirst, the act of sourcing the components can be a barrier to obtaining benchmark data. If we do not have the product, we cannot run the benchmarks! Intel and AMD (and VIA, back in the day) have had different structures for sampling their products, depending on how much they want to say, the release time frame, and the state of the market. Other factors can include the importance of certain processors to the financials of a company, or level of the relationship between us and the manufacturers. Intel and AMD will only work with review websites at any depth if the analysis is fair, and our readers (that’s you) would only read the data if the analysis was unbiased as well.When it comes down to the base media sampling strategies, companies can typically take two routes. The nature of the technology industry is down to Press Relations (PR), and most companies will have both internal PR departments and also outsource local PR to companies that specialize in that region. Depending on the product, sampling can occur either direct from the manufacturer or via the local PR team, and the sampling strategy will be pre-determined at a much higher level: how many media websites are to be sampled, how many samples will be distributed to each region etc. For example, if a product is going to be sampled via local PR only, there might only be 3-5 units for 15+ technology media outlets, requiring that the samples be moved around when they have been tested. Some big launches, or depending on the relationship between the media outlet with the manufacturer, will be managed from the company internal global PR team, where samples are provided in perpetuity: essentially on long-term loans (which could be recalled).For the x86 processor manufacturers, Intel and AMD are the players we work with. Of late, Intel’s official media sampling policy provides the main high-end processor in advance of the processor release, such as the i7-4770K, or the i7-6700K. On rare occasions, one of the lower down parts down the stack are provided at the same time, or made available for sampling after the launch date. For example, with the latest Comet Lake, we were sampled both the i9-10900K and the i5-10600K, however these are both high-impact overclockable CPUs. This typically means that if there's an interesting processor down the stack, such as an i3-K or a low cost Pentium, then we have to work with other partners to get a sample (such as motherboard manufacturers, system integrators, or OEMs), or outright purchase it internally.For AMD’s processors, as demonstrated over the last 4-5 years, the company does not often release a full family stack of CPUs at one time. Instead, processors are launched in batches, with AMD choosing to do two or three every few months. For example, AMD initially launched Ryzen with the three Ryzen 7 processors, followed by four Ryzen 5 processors a few weeks later and finally two Ryzen 3 parts. With the past few generations from AMD, depending on how many processors are in the final stack of CPUs, AnandTech is usually sampled most of them, such as with 1stGen Ryzen where we were sampled all of them. Previously with the Richland and Trinity processors, only around half the stack were initially offered for review, and less chance of being sampled for the lower value parts, or some parts were offered through local PR teams a couple of months after launch. AMD still today launches OEM parts for specific regions - it tends not to sample those to press either, especially if the press are not in the region for that product.With certain processors, they target certain media organizations that prioritize different elements of testing, which lends to an imbalance of which media get which CPUs. Most manufacturers will rate the media outlets they work with into tiers, with the top tier ones getting earlier sampling or more access to the components. The reason for this is that if a company sampled everyone everything every time, suddenly 5000 media outlets (and anyone who wants to start a component testing blog) would end up with 10-25 products on their doorstep every year and it would be a mammoth task to organize (for little gain from the outlets with fewer readers).The concept of tiering is not new – it depends on the media outlets readership reach, the demographic, and the ability to understand the nuance of what is in their hands. AMD and Intel can't sample everyone everything, and sometimes they have specific markets to target, which will also shift focus on who will get what samples. A website focused on fanless HTPCs for example would not be a preferred sampling vector for workstation class processors. At AnandTech, we cover a broad range of topics, have educated readers, and have been working with Intel and AMD for twenty years. On the whole, we generally do well when it comes to processor sampling, although there are still limits - going out and asking for a stack of next generation Xeon Gold CPUs is unlikely to be as simple as overnight shipping.Bench Problem #2: The March of TimeThe second problem with the benchmark database is timing and benchmarks. This comes down to manpower – how many people are running the benchmarks, and the timeframes for which the benchmarks we do test remain relevant for the segments of our readers that are interested in the hardware.Take graphics card testing, for example: GPU drivers change monthly, and games are updated every few months (and the games people are playing also change). To keep a healthy set of benchmark data, it requires retesting 5 graphics cards per GPU vendor generation, 4-5 generations of GPU launches, from 3-4 different board partners, on 6-10 games every month at three different resolutions/settings per game (and testing each combination enough to be statistically accurate). That takes time, significant effort, and manpower, and I’m amazed Ryan has been able to do so much in the little time he has being the Editor-in-Chief. Picking the highest numbers out of those ranges gives us 5 (GPUs) x 2 (vendors) x 5 (generations) x 4 (board partners) x 10 (games) x 3 (resolutions) x 4 (statistically significant) results, which comes to 24000 benchmark runs, out of the door each month, in an ideal scenario. You could be halfway through and someone issues a driver update, making the rest of the data for naught. It’s not happening overnight, and arguably that could be work for at least one full time employee if not two.On the CPU side of the equation, the march of time is a little slower. While the number of CPUs to test can be higher (100+ consumer parts in the last few generations), the number of degrees of freedom is smaller, and the rate of our CPU benchmark refresh cycles can be longer. These parameters depend on OS updates and drivers like the GPU testing, but it means that some benchmarks can still be relevant several years later with the same operating system base. 30 year old legacy Fortran code still in use is likely going to stay 30 year old legacy Fortran code in the near future. Or even benchmarks like CineBench R15 are still quoted today, despite the Cinema4D software on which it is based is several generations newer. The CPU testing ends up ultimately limited by the gaming tests, and depends on which modern GPUs are used, what games are being tested, what resolutions are relevant, or when new benchmarks enter the fray.When Ryan retests a GPU, he has a fixed OS, system ready to go, updates the drivers, and puts the GPU back into the slot. Preparing a new CPU platform for new benchmarks means rebuilding the full system, reinstalling the OS, reinstalling the benchmark suite, and then testing it. However, with the right combination of hardware and tests, a good set of data can last 18 months or so without significant updates. The danger is that whenever there is a full benchmark refresh, which especially revolves around updates to newer operating systems. Due to how OS updates and scheduling with the software stack effects the new operating system, all the old data cannot be compared and the full set of hardware has to be retested on the new OS with an updated benchmark suite.With our new CPU Overload project (stylized as #CPUOverload in our article titles, because social media is cool?), the aim is to get around both of these major drawbacks.What is #CPUOverload?The seeds of this project were initially sown several years ago in 2016. Despite having added our benchmark data to Bench for several years, I had kind of known our Benchmark database was a popular tool, but I didn't really realize how much it was used, or more precisely, under optimized, until recently when I was given access to be able to dig around in our back-end data.Everyone shopping for a processor wants to know how good the one they're interested in is, and how much of a jump in performance they'll get from their old part. Reading reviews is all well and good, but due to style and applicability, only a few processors are directly compared in a review to a different part specifically, otherwise the review could be a hundred pages long. There have been many times where Ryan has asked me to scale back from 30000 data points in a review!Also it’s worth noting that reviews are often not updated with newer processor data, as there would be a factual disconnect with the textual analysis underneath.This is why Bench exists. We often link in each review toBenchand request users go there to compare other processors, or for legacy benchmarks / benchmark breakdowns that are not in the main review.But for #CPUOverload, with the ongoing march of Windows 10, and the special features therein (such as enabling Speed Shift on Intel processors, new scheduler updates for ACPI 6.2, and having the driver model to support DX12), it has been getting time for us to update our CPU test suite. Our recent reviews were mostly being criticized for still using older hardware, namely the GTX 1080s that I was able to procure, along with having some tests that didn’t always scale with the CPU. (It is worth noting that alongside sourcing CPUs for testing, sourcing GPUs is somewhat harder - asking a vendor or the GPU manufacturer for two or three or more of the same GPU without a direct review is a tough ask.) The other angle is that in any given month, I will get additional requests to benchmark specific CPU tests – users today would prefer seeing their workload in action for comparison, rather than general synthetics, for obvious reasons.There is also a personal question of user experience on Bench, which has not aged well since our last website layout update in 2013.In all, the aims of CPU Overload are:Source all CPUs. Focus on ones that people actually useRetest CPUs on Windows 10 with new CPU testsRetest CPUs on Windows 10 with new Gaming testsUpdate the Bench interfaceFor the #CPUOverload project, we are testing under Windows 10, with a variety of new tests, including AI and SPEC, with new gaming tests on the latest GPUs, and more relevant real world benchmarks. But the heart of CPU Overload is this:We want to have every desktop CPU since 2010 tested on our new benchmarks. By my count, there are over 900.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/11425/launching-the-cpu-overload-project-testing-every-x86-desktop-processor-since-2010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Dell XPS 13 (9300) Review: Return of the King\n",
      "Author: Brett Howse\n",
      "Date Published: 2020-07-16T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/15911/the-dell-xps-13-9300-review-return-of-the-king\n",
      "Content: Dell changed the Windows laptop market in a single stroke with the launch of the updated XPS 13 back in 2015, ushering in the world of the InfinityEdge display, and moving the entire industry forward. We were fortunate enough to get a chance to check out the precursor to the new XPS 13 back in November, witha review of the XPS 13 2-in-1. Dell had chosen not to rest on their laurels, and the 2-in-1 proved to be one of the best notebooks around if you needed a compact and powerful convertible laptop. Today we are evaluating the traditional clamshell version of the XPS 13, and while it offers many of the same features and design touches, it does so in a more familiar form factor that many customers are going to prefer.For the 2020 refresh, Dell has made the refreshing move to taller displays, as we saw with the XPS 13 2-in-1. As a result the XPS 13 uses 13.4-inch display panel with a 16:10 aspect ratio, offering more vertical space for getting work done, and some convenient padding to place controls when watching 16:9 content. The larger display fits into a chassis that is actually 2% smaller than the outgoing design, with the new XPS 13 offering a 91.5% screen to body ratio.This is actually the second time that Dell has refreshed the XPS 13 within the last year. The company previously updated the XPS 13 in August 2019 to use Intel's 10th generation Core processors, but presumably due to limited supply of Intel’s then-new Ice Lake platform, Dell opted to launch that iteration with Comet Lake-U processors. And under more normal circumstances we would have expected Dell to stick with an annual cadence – and thus Comet Lake – for an entire year. Instead, to some surprise, Dell gave the XPS 13 a further mid-generation refresh, launching the Ice Lake-based XPS 13 9300 model that we are reviewing today, and bringing the clamshell XPS 13 to parity with the 2-in-1 version.The switch from Comet Lake to Ice Lake, in turn, is a significant one. it means the XPS 13 gets Intel’s new Sunny Cove CPU architecture, as well as the much-improved Gen 11 graphics. Dell offers Core i3, i5, and i7 models, with the Core i3 and i5 offering G1 graphics, meaning 32 Execution Units (EUs), and the top-tier Core i7-1065G7 featuring the full 64 EUs on the GPU side. Just as a comparison, the Comet Lake-U only offered 24 EUs of Gen 9.5 graphics, so even the base Ice Lake models still offer a 33% larger (and much newer) GPU than the outgoing models.The move to Ice Lake also brings some badly-needed LPDDR4X support, which in turn means a 32 GB maximum memory option in the XPS 13 9300, up from 16 GB previously. Although Dell still lists a paltry 4 GB option on their specifications sheet, a quick look at the Dell.com site shows that, at least in the USA, it appears that 8 GB is the new minimum, and that is a welcome change. Offering just 4 GB of RAM in a premium Ultrabook was always a poor choice, even if it did allow Dell to hit a slightly lower price bracket. On the storage front there is more good news, with 256 GB the new minimum, with up to 2 TB available, and all drives are PCIe x4 NVMe offerings.Specifications of the Dell XPS 13 9300-SeriesGeneral SpecificationsAs Tested: Core i7-1065G7 / 16GB / 512GB / 1920x1200LCDDiagonal13.4-inchResolution1920×12003840×2400Brightness500 cd/m²500 cd/m²Contrast Ratio1800:11500:1Color Gamut100% sRGB100% sRGB90% P3FeaturesDolby VisionDolby VisionTouch Supportwith or without touchYesProtective GlassCorning Gorilla Glass 6 in case of touch-enabled modelCPUIntel Core i3 1005G1 (4MB cache, up to 3.4GHz)Intel Quad Core i5 1035G1 (6MB cache, up to 3.6GHz)Intel Quad Core i7 1065G7 (8MB cache, up to 3.9GHz)GraphicsIntel UHD GraphicsIntel Iris Plus GraphicsRAM4 - 32 GB LPDDR4X-3733 DRAM (soldered/onboard)Storage256 GB PCIe 3.0 x4 SSD512 GB PCIe 3.0 x4 SSD1 TB PCIe 3.0 x4 SSD2 TB PCIe 3.0 x4 SSDWirelessKiller AX1650 Wi-Fi 6 + Bluetooth 5.0 (based on Intel's silicon)Killer AX500 Wi-Fi 6 + Bluetooth 5.0 (based on Qualcomm's silicon)USB3.12 × TB 3/USB Gen 3.1 Gen 2 Type-C3.0-Thunderbolt2 × TB 3 (for data, charging, DP displays)CamerasFront720p HD webcamOther I/OMicrophone, 2 stereo speakers, audio jackBattery52 Wh | 45 W AC Adapter (USB Type-C)DimensionsWidth295.7 mm | 11.64 inchesDepth198.7 mm | 7.82 inchesThickness14.8 mm | 0.58 inchesWeightnon-touch 1.2 kilograms | 2.64 poundstouch-enabled 1.27 kilograms | 2.8 poundsLaunch PriceStarting at $999.99Dell has gone all-in on USB-C with the new XPS 13, with one port on each side of the notebook. Both feature Thunderbolt 3 with 4 lanes, as well as power delivery for charging. The lack of a Type-A port may inconvenience some, but Dell does include an adapter in the box to assist. Wireless is the Killer AX1650, which based on the latest Intel AX200 wireless adapter – and withIntel purchasing Killerthis partnership seems like it is not going anywhere.If you read our review of the 2-in-1 version of this laptop, you will undoubtedly notice a lot of similarities. As they are from the same product line, that is not an accident: Dell has now refreshed their entire XPS series of laptops with a similar design philosophy. Let’s take a peek at what is new.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15911/the-dell-xps-13-9300-review-return-of-the-king\n",
      "Title: Qualcomm Announces Snapdragon 865+: Breaking the 3GHz Threshold\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-07-08T13:30:00Z\n",
      "URL: https://www.anandtech.com/show/15893/qualcomm-announces-snapdragon-865-breaking-the-3ghz-threshold\n",
      "Content: Today Qualcomm is announcing an update to its extremely successful Snapdragon 865 SoC: the new Snapdragon 865+. The Snapdragon 865 had already seen tremendous success with over 140 different design wins, powering some of the best Android smartphone devices this year. We’re past the hectic spring release cycle of devices, and much like last year with the S855+, for the summer and autumn release cycle, Qualcomm is providing vendors with the option for a higher-performance binned variant of the chip, the new S865+. As a bit of a arbitrary, but also important characteristic of the new chip is that this is the first ever mobile silicon to finally pass the 3GHz frequency mark.Qualcomm Snapdragon Flagship SoCs 2020SoCSnapdragon 865Snapdragon 865+CPU1x Cortex A77@ 2.84GHz 1x512KB pL23x Cortex A77@ 2.42GHz 3x256KB pL24x Cortex A55@ 1.80GHz 4x128KB pL24MB sL3 @ ?MHz1x Cortex A77@3.1GHz1x512KB pL23x Cortex A77@ 2.42GHz 3x256KB pL24x Cortex A55@ 1.80GHz 4x128KB pL24MB sL3 @ ?MHzGPUAdreno 650 @ 587 MHzAdreno 650 @ ?+10% PerfDSP / NPUHexagon 69815 TOPS AI(Total CPU+GPU+HVX+Tensor)MemoryController4x 16-bit CH@ 2133MHz LPDDR4X / 33.4GB/sor@ 2750MHz LPDDR5 / 44.0GB/s3MB system level cacheISP/CameraDual 14-bit Spectra 480 ISP1x 200MP64MP ZSL or 2x 25MP ZSL4K video & 64MP burst captureEncode/Decode8K30 / 4K120 10-bit H.265Dolby Vision, HDR10+, HDR10, HLG720p960 infinite recordingIntegrated Modemnone(Paired withexternal X55only)(LTE Category 24/22)DL = 2500 Mbps7x20MHz CA, 1024-QAMUL = 316 Mbps3x20MHz CA, 256-QAM(5G NR Sub-6 + mmWave)DL = 7000 MbpsUL = 3000 MbpsMfc. ProcessTSMC7nm (N7P)We’ve come to know the Snapdragon 865 quite well over the last few months, detailing the performance of the chipset in ourinitial benchmark articlesas well as a moreextensive deep-dive in our Galaxy S20 review. The new Snapdragon 865+ is a new binned variant of the same chipset with higher peak frequencies on the part of the “prime” CPU as well as the GPU, promising +10% performance on both aspects.The First Mobile Silicon to Break Past 3GHz – 3.1GHz EvenWhilst in relative terms the new chipset’s +10% clock improvement isn’t all that earth-shattering, in absolute terms it finally allows the new Snapdragon 865+ to be the first mobile SoC to break past the 3GHz threshold, slightly exceeding that mark at a peak 3.1GHz frequency.Ever since the Cortex-A75 generationwe’ve seen Arm make claims about their CPU microarchitectures achieving such high clock frequencies – however in all those years actual silicon products by vendors never really managed to quite get that close in commercial mass-production designs.We’ve had a chat with Qualcomm’s SVP and GM of mobile business Alex Katouzian, about how Qualcomm achieved this, and fundamentally it’s a combination of aggressive physical design of the product as well as improving manufacturing yields during the product’s lifecycle. Katouzian explained that they would have been able to achieve these frequencies on the vanilla Snapdragon 865 – but they would have had a lower quantity of products being able to meet this mark due to manufacturing variations. Yield improvements during the lifecycle of the Snapdragon 865 means that the company is able to offer this higher frequency variant now.For context, in the mobile world, usually SoC SKUs are binned not by performance (clock-frequency), but by power (voltage variations). This comes in contrast to the desktop and server world where one single silicon design is binned by different performance SKUs, varying in frequencies or even functional blocks. In a sense, Qualcomm’s 855+ and 865+ are SKUs that expand the product line in the way that usual PC silicon vendors do. Other mobile vendors such as MediaTek for example also take advantage of such product segmentation by releasing a single silicon design as multiple product SKUs.As to what this means for the power and efficiency of the new Snapdragon 865+: There will be a power increase to reach the higher frequencies, however this will only be linear with the increased clock speed, meaning energy efficiency of the new SoC will maintain the same excellent levels of that of the Snapdragon 865, so battery life will not be affected.More + Designs This YearThis mid-year refresh was only introduced last year with the Snapdragon 855+, and while we’ve seen some vendors opt for the upgrade in their latest device releases, uptake was rather limited, with only a few handful more prominent devices such as theASUS ROG Phone II.This year, Qualcomm tells us that we should be expecting more adoption for the refreshed silicon, with more design wins. Amongst the publicly announced platforms today is naturally the AUSS ROG Phone 3, with full details on the phone to follow in the next couple of weeks. Lenovo is also part of the launch partners, promising to bring to market a smartphone under the Lenovo Legion branding.Amongst other new novelties of the Snapdragon 865+ platform is the ability for vendors to bundle with the new FastConnect 6900 Wi-Fi chips from Qualcomm, the company’s new Wi-Fi 6 chipsets with 6GHz band capability (Wi-Fi 6E).We’re looking forward to devices with the new Snapdragon 865+ in the coming weeks and months.Related Reading:Qualcomm Announces Snapdragon 855 Plus: A Higher Bin SKUQualcomm Announces Snapdragon 865 and 765(G): 5G For All in 2020, All The DetailsThe Snapdragon 865 Performance Preview: Setting the Stage for Flagship Android 2020The Samsung Galaxy S20+, S20 Ultra Exynos & Snapdragon Review: Megalomania DevicesThe Snapdragon 855 Phone Roundup: Searching for the Best Implementations\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15893/qualcomm-announces-snapdragon-865-breaking-the-3ghz-threshold\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Hot Chips 32 (2020) Schedule Announced: Tiger Lake, Xe, POWER10, Xbox Series X, TPUv3, Raja Koduri Keynote\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-07-08T13:01:00Z\n",
      "URL: https://www.anandtech.com/show/15806/hot-chips-32-2020-schedule-tiger-lake-xe-power10-xbox-series-x-tpuv3-jim-keller\n",
      "Content: This article has been updated with the latest schedule for the Hot Chips conference. Due toJim Keller leaving Intelrecently, the main change has been the Day One keynote - Raja Koduri is presenting on behalf of Intel in Jim's stead.I’ve said it a million times and I’ll say it again – the best industry conference I go to every year is Hot Chips. The event has grown over the years, to around 1700 people in 2019 if I remember correctly, but it involves two days of presentations about the latest hardware that has hit the market. This includes new and upcoming parts that change the industry we work in, including deep dives into some of the most important silicon at play in the market today. There are also extensive keynote presentations from the most prominent members of the industry that give insights into how these people (and the companies) work, but also where the future is going.This week the lid was lifted on the provision Hot Chips 2020 schedule. With COVID-19 in mind, this year will also be the first year the conference will be offered online-only for attendees. Hot Chips 2020 is scheduled for August 16thto August 18th.Hot Chips is traditionally over three days: a first day of tutorials, and two days of presentations. Times are given as Pacific Time (PT).Tutorial DayThis year the tutorials are covering Machine Learning and Quantum Computing.Hot Chips 32 (2020): Tutorial DayAnandTechSpeakerCompanyInfo08h30 - 13h00Machine Learning Scaleout SystemsMichael HoustonNVIDIADXG A100 SuperPODSameer KumarGoogleGoogle TPU PodDehao ChenNatalia VassilievaCerebrasCerebras CS-1Machine Learning Scaleout TrainingMohammed ShoeybiNVIDIAMegatron Language ModelWeijie ZhaoBaiduDistributed Parameter Server for Massive Recommender SystemZhifeng ChenGoogleGShard: Scalaing Giant Models with Conditional Computation and Automatic ShardingNoam Shazeer14h00 - 17h15Quantum ComputingJohn MartinisUSCBQuantum Supremacy Using a Programmable Superconducting ProcessorJarrod McCleanGoogleApplications and Challenges with Near-Term Quantum HardwareMatthias SteffenIBMUnderneath the Hood of a Superconducting Qubit Quantum SupercomputerOliver DialJames S. ClarkeIntelTowards a Large-Scale Quantum Comptuer Using Silicon Spin QubitsDavid ReillyMicrosoftIf Only We Could Control Them: Challenges and Solutions in Scaling the Control Interface of a Quantum ComputerThe first presentation deals with a popular topic of machine learning being scaled across large systems. The ability to train a model across a supercomputer, like an NVIDIA DGX, or across across a large cluster, like a TPU pod, is essential in order to get the appropriate linear speedups that using multiple units of hardware requires. This presentation (presenters TBD) will cover both software for parallelizing a neural network and also the infrastructure required for companies that want to scale out. Some basic knowledge of the principles of ML will likely be required.The second tutorial is on Quantum Computing. There is a lot of research into the state of Quantum Computing, as well as a lot of talk about achieving a level of ‘Quantum Supremacy’ to tasks that are actually worthwhile for computational problems today, or for ten years in the future. The speakers for this presentation will cover the range from the different types of Quantum computing, to the infrastructure required to enable it, plus also the challenges in actually making it happen and applying programs to them. It should make for a densely packed 3 ¼ hour tutorial.Day One: MorningDay One is going to be very busy, and is split into five sessions, from 9:15am to 7pm PT.As with any conference, the opening minutes are spent detailing the conference, what’s new for the year, and some of the rules (such as no streaming). I suspect there will be a large discussion about how the COVID situation will affect the presentations, what to do if one of the presentations fails, or such. I actually hope that the presentations are pre-recorded so that doesn’t happen.The first session is on Server Processors.Hot Chips 32 (2020): Day One, Session 1Server ProcessorsAnandTechSpeakerCompanyInfo09h15Opening Remarks09h30Irma Esmer PapzianIntelNext Generation Intel Xeon Scalable Server Processor: Ice Lake-SP10h00William StarkeIBMIBM's POWER10 ProcessorBrian W Thompto10h30Rabin SugumarMarvellMarvell ThunderX3 Next Generation Arm-Based Server Processor11h00Antony SaporitoIBMThe 5.2 GHz IBM z15 ProcessorStraight off the bat we have a 30 minute talk on Intel’s Ice Lake-SP hardware. Never mind Cooper Lake, Intel is keen to promote its next generation Xeon Scalable processor line. This talk is likely to recap all the new features that the Ice Lake core brings to the table, but also adjustments that have been made for the server processor, updates to the mesh, core counts, PCIe layout, and perhaps even some platform infrastructure. We have not heard yet if Intel will announce Ice Lake-SP before Hot Chips, so some of this we might know already by then.Intel’s Sailesh Kothapalli with an Ice Lake Xeon CPU in Dec 2018The second talk is from IBM, on its newest POWER10 processor. IBM has teased this processor in roadmapsat previous Hot Chips conferences, with this latest design. Last year the company said that POWER10 will feature PCIe 5.0, up to 800 GB/s of memory bandwidth, and be based on a new microarchitecture and a new process technology. The finer details aren’t clear, but for POWER9 the company did implement a new configurable memory system called OMI, so it will be interesting to see if that makes it into the new design.From IBM’s Power9 AIO talk at Hot Chips 2019The third talk is on Marvell/Cavium’s ThunderX3 processor design, somethingwhich we’ve reported on recentlyas Marvell has announced details. The new chip uses 96 custom Arm v8.3+ cores, built on TSMC 7nm, with up to four threads per core. There’s 8 channels of DDR4-3200, 64 lanes of PCIe 4.0, four 128-bit SIMD units per core, and support for dual socket operation. This is the upgrade to the ThunderX2, which has been seen in a number of mid-range deployments for HPC. With support for CUDA on Arm, we expect to hear a lot about it at Hot Chips, along with a detailed guide of the microarchitecture hopefully.The final talk of the session is again from IBM, this time on the z15 processor line. The title of the talk is listed as 5.2 GHz, indicating how fast these processors can go, and we expect this talk to be an extension of the talk given at ISSCC earlier this year. I still want to write this processor analysis up in detail, when there’s a break in the barrage of product releases so far this month.The second session of the day is on Mobile (aka Notebook) processors.Hot Chips 32 (2020): Day One, Session 2Mobile ProcessorsAnandTechSpeakerCompanyInfo12h00Sonu AroraAMDAMD Next Generation 7nm Ryzen 4000 APU12h30Xavier VeraIntelInside Tiger Lake: Intel's Next Generation Mobile Client CPUTalk one is on AMD’s Ryzen 4000 APUs, codenamed Renoir.We have gone into a deep dive on this processorwhen it was announced, as well asa numberof reviewson productsthat have come out. I’m hoping that this presentation might go into some of the design choices that had to be made with a monolithic 7nm design between Zen 2 and an updated Vega 8 pairing, as well as the decoupling of the Infinity Fabric to main memory. For users expecting to see a large discussion about desktop versions of Renoir, Hot Chips isn’t going to speak specifically about that – it is more of a conference about the platform design.The second talk of the session is going to be highly anticipated, with Intel going into detail about its new Tiger Lake processor for notebooks. We’ve had asneak peak at a wafer, along with some close-up die shots to get a feel for what should be a quad-core design with 96 execution units with the new Xe graphics architecture. Again, we’re not sure if Intel is going to use this event as a big announcement point, or if we’re going to get some information before the event. What I can hope for is that Hot Chips is a platform where Intel can talk about some of the 10nm improvements they’ve had to make in order to get Tiger Lake to work.Tiger Lake. Just ask Carole Baskin.Day One: KeynoteAfter the first two sessions comes the keynote, headed up by Intel’s Raja Koduri, Senior VP, Chief Architect, and GM of Architecture, Graphics and Software.The keynotes at Hot Chips aren’t necessarily a ground for announcing new items, but with Raja on the stage I expect him to go into more detail about how things have changed at Intel to get into graphics. He’s been there just over three years, which is enough time to bring about a lot of change. Hopefully this keynote will enable Raja to go into some specifics, though given the secretive nature of what he does, perhaps not – but it will enable Raja to define some elements of the industry and where he has been taking Intel along that path.Initially Jim Keller was set to preset this keynote, however he has left Intel due to personal issues. We wish Jim the best. Aaron Pressman of Fortunerecently published a detailed look into Jim Keller, along with comments and quotes from a number of the industry covering his past, his present, and some goals for the future. If you have access, it is well worth a read.Day One: AfternoonThe first afternoon session on day one is on Edge Computing and Sensing.Hot Chips 32 (2020): Day One, Session 3Edge Computing and SensingAnandTechSpeakerCompanyInfo15h00Yu PuAlibabaXuantie-910: Innovating Cloud and Edge Computing by RISC-V15h30Allan SkillmanArmA Technical Overview of the Arm Cortex-M55: Arm's Most AI-capable Cortex-M ProcessorTomas Edso16h00Glenn G. KoHarvardUniversityPGMA: A scalable Bayesian Inference Accelerator for Unsupervised LearningThe first talk on RISC-V has the potential to be interesting if there’s some serious applications to how RISC-V is developing. The second talk on the Arm Cortex M55 is probably the most relevant to us, and Armannounced the M55 core back in February, so hopefully this talk will be a chance to look into the architecture of the design.The second afternoon session on day one is another exciting one: GPUs and Gaming ArchitecturesHot Chips 32 (2020): Day One, Session 4GPUs and Gaming ArchitecturesAnandTechSpeakerCompanyInfo17h00Jack ChoquetteNVIDIANVIDIA's Next Generation GPU: Performance and Innovation for GPU Computing17h30David BlytheIntelThe Xe GPU Architecture18h00Jeff AndrewsMicrosoftXbox Series X System ArchitectureMark GrossmanTalk one is from NVIDIA, discussing the ‘Next Generation GPU’. We can all come to the conclusion that this means Ampere, which was recently announced for HPCin the form of the A100. However as with the Renoir discussion earlier, this talk isn’t specifically going to go into consumer products – these Hot Chips talks will focus on a design philosophy and talk about the different variants therein, so while we expect to hear about some of the optimizations that might be made for consumer graphics, a lot of the talk will be going towards what is already out there (HPC-focused A100) and then optimizations made for either compute or for graphics.The second talk of the session is from Intel, on the Xe GPU architecture. Now there are a couple of ways Intel could swing this talk – it could either be a discussionsimilar to the one by Raja Koduriat Supercomputing 2019, talking about some of the different ways in which Xe can do SIMD and SIMT, or it could be a chance for the company to go into more detail about configurations and infrastructure. This is only a 30 minute talk, so we have to be wary here – I suspect at least 5-10 minutes will be dedicated to OneAPI, leaving only a little time to talk silicon. As with the NVIDIA talk, users looking for consumer graphics related details aren’t likely to find anything specific here.The final talk in a very long day is from Microsoft, about the Xbox Series X system architecture. This is likely going to focus on the infrastructure design on the console, the collaboration with AMD on the processor, perhaps some insight into new features we’re going to find in the console, and how the chip is going to drive the next 4-8 years of console gaming. I’m actually a bit 50-50 on this talk, as we’ve had presentations like this at events before (e.g. Qualcomm’s XR) which didn’t actually say anything that wasn’t already announced. There’s the potential here for Microsoft to not say anything new, but I hope that they will go into more detail.Day Two: MorningThe second day starts with a session on FPGAs and Reconfigurable DevicesHot Chips 32 (2020): Day Two, Session 1FPGAs and Reconfigurable ArchitecturesAnandTechSpeakerCompanyInfo8h30Ilya GanusovIntelAgilex Generation of Intel FPGAs9h00Saheer AhmadXilinxXilinx Versal Premium Series9h30Ljubisa BajicTenstorrentNeurons vs NAND Gates vs Networks: Finding the Right Compute Substrate for Artificial IntelligenceThe first talk is on Intel’s Agilex generation of FPGAs. These are Intel’s biggest 10nm silicon, and although they’ve been out for a while. Intel’s 14nm family of Stratix silicon scales all the way from small to large dies, so perhaps here we will get an insight into just how large Agilex might get, power depending. We’ve only seen Agilex-F in the market so far, and we’re waiting for Agilex-I with 112G transceivers and Agilex-M with PCIe 5.0 with Optane support.Next is a presentation from Xilinx or its Versal Premium series. We didn’t cover these when they were first announced, but the new Xilinx Versal Premium is the realisation of its vision for an ACAP – an adaptive compute acceleration platform based on an FPGA and hardened silicon that can operate through libraries rather than bitstreams. The Premium models are the top tier parts, with PCIe 5.0, 112G trancievers, HBM options, DSP Engines, Ethernet cores, and high-speed crypto engines. As with the Agilex presentation, we’re guessing that this Hot Chips presentation will align with a product being ready for demonstration perhaps.Versal Premium ACAPThe final talk of the session is from Tenstorrent, on Neurons vs NAND Gates vs Networks. This is likely going to be a study in finding which of these is going to be the right compute element for a variety of applications.The second session on the day is for Networking and Distributed SystemsHot Chips 32 (2020): Day Two, Session 2Networking and Distributed SystemsAnandTechSpeakerCompanyInfo10h30Anurag AgrawalIntel / BarefootTofino2 - A 12.9 Tbps Programmable Ethernet SwitchChanghoom Kim11h30Francis MatusPensandoPensando Distributed Services Architecture12h00Pradeep SinghuFungibleThe DPU: A New Category of Microprocessor12h00Justin SongAlibabaHigh-density Multi-tenant Bare-metal Cloud with Memory Expansion SoC and Power ManagementXiantao ZhangI don’t have much to say here – the Alibaba presentation at the end about a multi-tenant bare metal cloud SoC is likely to be interesting to watch.Day Two: KeynoteThe keynote for the second day is by Dan Belov, Distinguished Engineer at Deepmind.Deepmind is the company that created the AlphaGo program that played professional Go champion Lee Sedol in 2016, with the final score of 4-1 in favor of the artificial intelligence. This will likely be an update to what’s going on at Deepmind (now owned by Alphabet) and what they’re planning for the future of AI. We might get some insight as to how the company is working with other departments inside Alphabet – it has been cited that Deepmind has used its algorithms toincrease the efficiency of cooling inside Google’s datacenters, for example.Day Two: AfternoonThe afternoon of the final day is dedicated to Machine Learning. The first session of the afternoon will be for Training.Hot Chips 32 (2020): Day Two, Session 3Machine Learning, TrainingAnandTechSpeakerCompanyInfo14h30Thomas NorrieGoogleGoogle's Training Chips Revealed:TPUv2 and TPUv3Nishant Patil15h00Sean LieCerebrasThe Second Generation Cerebras Wafer Scale Engine15h30Florian ZarubaETH ZurichManticore: A 4096-core RISC-V Chiplet Architecture for Ultra-efficient Floating-point ComputingFabian SchuikiThe first talk is from Google, who will *finally* lift the lid of its TPUv2 and TPUv3 chips used internally for training its algorithms. Googleinitially disclosed the design of TPUv1at Hot Chips 2017, and I believe this talk has been highly requested by a lot of the attendees for the past couple of years, so I hope that there’s an opportunity to go into some level of complex detail here.TPU v3 blade, as seen at Supercomputing 19The second talk of the session is from Cerebras, known famously for creating its Wafer Scale Engine (WSE) for machine learning. The fact that Cerebras are already going to talk about its second generation version of its WSE is quite impressive, given that the first generation WSE talkwas only last year. If we’re lucky, then the 2ndgeneration of the WSE is going to be at the same stage the first generation was, and the company will go into as much detail as it did before.Wafer Scale chips never looked so tastyThe final talk of the session is from ETH Zurich, showing off a 4096-core RISC-V chiplet architecture for ML. The fact that this talk says ‘architecture’ might suggest that they don’t have silicon to show off, so it will be interesting to see if they do. Also, given it’s a chiplet, I wonder if it is part of the industry effort to provide unified interfaces for chiplet design, and in that regard could be added to other silicon in order to provide acceleration.The final session of the event is on Inference.Hot Chips 32 (2020): Day Two, Session 4Machine Learning, InferenceAnandTechSpeakerCompanyInfo16h30Jian OuyangBaiduBaidu Kunlun - An AI Processor for Diversified Workloads17h00Michael XuSenseTimeSTPU: SenseTime Processing UnitWenquang Wang17h30Carl RamayLightmatterSilicon Photonics for Artificial Intelligence Acceleration18h00Yang JiaoAlibabaHanguang 800 NPU - The Ultimate AI Inference Solution for Data CentersI can honestly say I don’t have much to go on here. We’ve heard of Baidu’s Kunlun a few months ago due to a press release from the company and Samsung stating that the silicon was making use of Interposer-Cube 2.5D packaging, as well as HBM2, and packing 260 TOPs into 150 W.The talk featuring Alibaba’s Hanguang 800 NPU will also be interesting, as we saw results submitted to MLPerf back in November, though not a lot of information about the chip itself.In SummaryWith Hot Chips 32 (2020) going on-line only this year, it promises to be more accessible than previous iterations – a success this year will enable Hot Chips to offer online versions of its conference for those that do not want to attend in person. As a personal preference, I prefer going to the conference – there are plenty of side discussions and things that don’t always come across just in the presentations, so I’m looking forward to 2021 where it will hopefully avail itself to in-person attendance.Due to the nature of the conference this year,the price for attending is at an all-time low. The maximum price for any attendant is $160 for non-IEEE members booking late and wanting access to both the tutorials and the presentations. Booking today until August 1st, the early booking price is $125. (In previous years, it has been $300+ due to venue and lunch being provided).For anyone with a passing interest in how this silicon works, I highly recommend taking part – not only to help fund future events, but signing up and paying the fee gives you access to all the PDFs of all the presentations months before they are given to the public, and also early access to video recordings before they are made available online.I thought last year’s Hot Chips was exciting – this year’s event will seem to at the very least match it, if not go one better. If any of the presenters at this years conference wants to get in touch before the event and take us through their presentation in advance, with the potential for a proper article on their disclosures, then please get in contact at ian@anandtech.com.Ian Cutress (AnandTech), David Schor (wikichip), Charlie Demerjian (SemiAccurate), Paul Alcorn (Tom's Hardware), Andrea Pellegrini (Arm), Francois Piednoël (Mercedes-Benz AG)Taken at Hot Chips 31 (2019)\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15806/hot-chips-32-2020-schedule-tiger-lake-xe-power10-xbox-series-x-tpuv3-jim-keller\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Intel Lakefield Deep Dive: Everything To Know About the First x86 Hybrid CPU\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-07-02T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15877/intel-hybrid-cpu-lakefield-all-you-need-to-know\n",
      "Content: For the past eighteen months, Intel has paraded its new ‘Lakefield’ processor design around the press and the public as a paragon of new processor innovation. Inside, Intel pairs one of its fast peak performance cores with four of its lower power efficient cores, and uses novel technology in order to build the processor in the smallest footprint it can. The new Lakefield design is a sign that Intel is looking into new processor paradigms, such as hybrid processors with different types of cores, but also different stacking and packaging technologies to help drive the next wave of computing. With this article, we will tell you all you need to know about Lakefield.Part Smartphone, Part PCWhen designing a processor, there are over a thousand design choices to be made. The processor can be built to tackle everything, or it can be aimed at a niche. For high performance computing, there might be a need for a high power, high performance design where cooling is of no consideration – compare that to a processor aimed at a portable device, and it needs to be energy efficient and offer considerable battery life for a fixed battery size. There is also the cost of designing the product, how much to invest into research and development, how many units are expected to sell, and thus how many should be produced and what size the product should be. What the price range of the target market is can be a huge factor, even before putting pen to paper.The New Samsung Galaxy Book SThis is all why we have big multi-core processors with lots of compute acceleration in servers, more moderate power and core counts in home machines that focus on single core performance and user experience, and why smartphone processors have to physically fit into a small design and offer exceptional battery life.Laptop processors have always sort of fit into the middle of the PC and smartphone markets. Laptop users, especially professionals and gamers, need the high performance that a desktop platform can provide, but road warriors need something that is superbly efficient in power consumption, especially at idle, to provide all-day battery life as if they were on a good smartphone. Not only this, but the more energy efficient and the smaller the footprint of the processor and its features, the thinner and lighter the laptop can be, offering a premium design experience.As a result, we have seen the ultra-premium notebook market converge from two directions.From the top, we have AMD and Intel, using their laptop processor designs in smaller and smaller power envelopes to offer thin and light devices with exceptional performance and yet retain the energy efficiency required for battery life. For the most premium designs, we see 12-15+ hours of laptop battery life, as well as very capable gaming.From the bottom, we have Qualcomm, building out its high-performance smartphone processor line into larger power envelopes, in order to offer desktop-class performance with smartphone-class connectivity and battery life. With the designs using Qualcomm’s processors, a user can very easily expect 24+ hours of battery life, and with regular office use, only charge the system once every couple of days. Qualcomm still has an additional barrier in software, which it is working towards.Both of these directions converge on something in the middle – something that can offer desktop-class performance, 24hr+ battery life, capable gaming, but also has a full range of software support. Rather continue with trying to bring its processors down to the level it requires, Intel has decided to flip its traditional processor paradigm upside down, and build a smartphone-class processor for this market, matching Qualcomm in its bottom up approach while also looking into novel manufacturing techniques in order to do so.This processor design is called ‘Lakefield’.Lakefield at the Core, and the AtomFor the past two decades, Intel has had two different types of x86 CPU design.The Big ‘Core’ CPUIntel calls its high power/high performance x86 design the ‘Core’ family. This can make it very confusing, to differentiate between the general concept of a processor core and a ‘Core’-based processor core.Over the years, Core-based processor cores have been designed for power envelopes from low-power laptops all the way up to the beefiest of servers. The Core line of processor cores implement more complex logic in order to provide additional acceleration, at the expense of physical size and power.The Small ‘Atom’ CPUThe second type of x86 design from Intel is its more energy efficient implementation, called ‘Atom’. With the Atom cores, Intel simplifies the design in order to maximise efficiency for a given power or a given performance. This makes the design smaller, cheaper to manufacturer, but has a lower peak performance than the Core design. We typically see Atom designs in power restricted scenarios where performance is not critical, such as IoT, or low cost laptop designs.Where Core Meets AtomNormally we characterise a processor core design in terms of this power and performance. Due to the variation in the design, we see where some designs work best, at various points for a given power or for a given performance. In the case of Intel’s latest generation of Core and Atom hardware, it looks something like this, if we compare one thread against one thread:Modified from Intel’s SlidesFrom this graph, which measures Performance on the bottom axis and power on the side axis, there is a crossover point where each design makes the best sense. When the demand for performance is below 58%, the Atom design is the most power efficient, but above 58% then a Core design is preferred.Homogenous CPUs (all the same) vsHeterogeneous CPUs (mix of different)Now in modern processors, especially in laptops, desktops, and servers, we only experience one type of core design. We either have all Core or all Atom, and the performance is designed to scale within those homogeneous designs. It becomes a simple curve to navigate, and when more parallel performance is required, more of those types of cores are fired up to serve the needs of the end user. This has been the case for these markets for the last 30-50 years.The smartphone space, for the last decade, has been taking a different approach. Within the smartphone world, there are core designs listed as ‘big’ and core designs listed as ‘little’, in the same way that Intel has Core and Atom designs.These smartphone processors combine numbers of big cores with numbers of small cores, such that there is an intrinsic benefit to running background tasks on the little cores, where efficiency is important, and user experience related elements on the big cores, where latency and performance is important.The complexity of such a heterogeneous smartphone-like design has many layers. By default most items will start on the little cores, and it is up to either the processor or the operating system to identify when the higher performance mode during a user experience moment is needed. This can be tricky to identify.Then also comes the matter when a workload has to actually move from one type of core to the other, typically in response to a request for a specific level of performance – if the cores are designed significantly different, then the demands on the memory can likely increase and it is up to the operating system to ensure everything works as it should. There is also an additional element of security, which is a larger topic outside of the scope of this article.Ultimately building a design with both big cores and little cores comes down a lot to what we call the scheduler. This is a program inside the operating system that manages where different background processes, user experience events, or things like video editing and games, get arranged. The smartphone market has been working on different types of schedulers, and optimizing the designs, for over a decade as mentioned. For the land of Intel and AMD, the push for heterogeneous schedulers has been a slow process by comparison, and it becomes very much a chicken and egg problem – there is no need for an optimized heterogeneous scheduler if there is never a heterogeneous processor in the market.So why bring all this up?Lakefield is the first x86 heterogeneous processor.In its marketing, Intel calls this a ‘hybrid’ CPU, and we will start to see logos identifying this as such. At the heart of its design, Lakefield combines one of the big Core designs with a cluster of four smaller Atom designs, all into one single piece of silicon. In normal x86 processor talk, this is essentially a ‘penta-core’ design, which will commonly be referred to as a 1+4 implementation (for one big core and four small cores).Intel’s goal with Lakefield is to combine the benefits of the power efficient Atom core with the better user-experience elements provided by the more power hungry but better peak performing big Core. As a result, it sits in the middle of Intel’s traditional homogeneous designs which only contain one type of x86 design – somewhere above the ‘all Atom’ 0+4 design and somewhere below the ‘all Core’ 4+0 design (in actual fact, it’s closer to 0+4).Based on our conversations with Intel, and the small demonstrations we have seen so far, the best way to consider the new Lakefield processor is to consider it similar to one of the older quad-core Atom processors, with the benefits of the single core performance of a big Core. The cluster of four smaller Atom CPUs will take care of the heavy lifting and parallel performance requests, because there are four of them, while the big Core will respond when the user loads an application, or touches the screen, or scrolls a web browser.Being a new form of x86 hybrid CPU is not the only thing that Lakefield brings to the table.Now, just for some form of clarification, we have already had some experience with these sorts of hybrid CPU designs on operating systems like Windows. Qualcomm’s Windows on Snapdragon laptops, like the Lenovo Yoga, use a 4+4 design with the Snapdragon smartphone chips, and Qualcomm has had to work extensively with Microsoft to develop an appropriate scheduler that can manage workloads between the different CPU designs.The main difference to what Qualcomm has done and what Intel is doing with Lakefield is in software support – Qualcomm processors run ‘Arm’ instructions, while Intel processors run ‘x86’ instructions. Most Windows software is built for x86 instructions, which has limited Qualcomm’s effectiveness in penetrating the traditional laptop market. Qualcomm's design actually allows for ‘x86 translation’, however its scope is limited and there is a performance penalty, but is a work in progress. The point being is that while we have not had a hybrid CPU scheduler for Windows on an x86 system previously, there has been a lot of work put in by Microsoft to date while working with Qualcomm.Visualising Heterogeneous CPU DesignsNot to any sort of scaleHere are some examples of mobile processors, from Intel and Qualcomm, with the cores in green. On the left is Intel's own Ice Lake processor, with four big cores. In the middle is Intel's Lakefield, which has two stacked silicon dies, but it's the top one that has one big core and four small ones. On the right is Qualcomm's Snapdragon 8cx, currently used in Windows on Snapdragon devices, which uses four performance cores and four efficiency cores, but also integrates a smartphone modem onboard.In this article, over the following pages, we'll be looking at Intel's new Lakefield processor in detail, covering the new multi-core design, discussing chiplets and Intel's new die-to-die bonding technology called Foveros, the implications of such a design on laptop size (as well as looking at the publicly disclosed Lakefield laptops coming to market), die shots, supposed performance numbers, thermal innovations, and the future for Lakefield. Data for this article has come from our research as well as interviews with Intel's technical personnel and Intel's own presentations on Lakefield at events such asHotChips,Architecture Day,CES,IEDM, andISSCC. Some information is dissected with helpful input fromDavid Schor of Wikichip. We also cover some of Intel’s innovations with the scope of other semiconductor companies, some of which may be competitors.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15877/intel-hybrid-cpu-lakefield-all-you-need-to-know\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD Succeeds in its 25x20 Goal: Renoir Crosses the Line in 2020\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-06-25T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15881/amd-succeeds-in-its-25x20-goal-renoir-zen2-vega-crosses-the-line-in-2020\n",
      "Content: One of the stories bubbling away in the background of the industry is the AMD self-imposed ‘25x20’ goal. Starting with performance in 2014, AMD committed to itself, to customers, and to investors that it would achieve an overall 25x improvement in ‘Performance Efficiency’ by 2020, which is a function of raw performance and power consumption. At the time AMD was defining its Kaveri mobile product as the baseline for the challenge – admittedly a very low bar – however each year AMD has updated us on its progress. With this year being 2020, the question on my lips ever since the launch of Zen2 for mobile was if AMD had achieved its goal, and if so, by how much? The answer is yes, and by a lot.In this article we will recap the 25x20 project, how the metrics are calculated, and what this means for AMD in the long term.Renoir 2020: New Silicon, Goal AchievedThe announcement today from AMD confirms the company has reached its goal of 25x performance efficiency by the end of 2020, starting from the Kaveri baseline. Here’s the important graph:As we can see using this metric, there were big jumps from Kaveri to Carrizo, Bristol Ridge to Raven Ridge, then a series of stagnation over Zen/Zen+, before finally a bump up to Renoir. This gives three distinct jumps:Kaveri to Carrizo was 3.5x,Bristol to Raven was 2.2x,then Picasso to Renoir was 2.92xThe base value for AMD’s goal is on its Kaveri mobile processors, which by the standards of today set a very low bar. As AMD moved to Carrizo, it implemented new power monitoring features on chip that allowed the system to offer a better distribution of power and ran closer to the true voltage needed, not wasting power. After Carrizo came Bristol Ridge, still based on the older cores, but used a new DDR4 controller as well as lower powered processors that were better optimized for efficiency.A big leap came with Raven Ridge, with AMD combining its new highly efficient Zen x86 cores and Vega integrated graphics. This heralded a vast improvement in performance due to doubling the cores and improving the graphics, all within a similar power window as Bristol Ridge. This boosted up the important 25x20 metric and keeping it well above the ‘linear’ gain.From 2017-2019, this was ultimately a lull in AMD’s strategy, namely because there were no significant design changes. The versions of 2017/2018 Raven Ridge come down to slight SKU differences used for the metric, but ultimately when it came time to measure the systems AMD was a little out of cycle here. Moving from Raven Ridge to Picasso was a shift from using GlobalFoundries 14nm to 12nm, which affords a slight power gain but not so much on the performance. Going from 2017 to 2019 still yielded a 23.5% gain within the same product family, mainly due to minor manufacturing updates and better binning or power algorithms. It was around the Picasso time when OEM’s started taking AMD’s notebook platform more seriously, especially as the lead up to 2020’s Renoir.The jump from Picasso to Renoir has been well documented. Our first use of the CPUs,reviewed in the ASUS Zephyrus G14, left us with our mouths open, almost literally. We called it a ‘Mobile Revival’, showcasing AMD’s transition over from Zen+ to Zen2, from GF 12nm to TSMC 7nm, along with a lot of strong design and optimization on the graphics side. The changes from the 2019 to the 2020 chip include doubling the core count, from four to eight, improving the clock-for-clock performance by 15-20%, but also improving the graphics performance and frequencies despite moving down from an silicon design that had 11 compute units down to 8. This comes in line with a number of power updates, adhering to AHCI specifications, and as we discussed with Sam Naffziger, AMD Fellow, supporting the new S0ix low power states has helped tremendously. The reduction in the fabric power, along with additional memory bandwidth, offered large gains.The jump from Picasso to Renoir is 2.92x, taking AMD to 31.77x over the original target. Goal achieved, and kudos to AMD’s teams that have succeeded at this ambitious target.How did AMD define how the target will be measured? That’s in the fine print.Calculating X: Get Me Some X FactorAMD calls the value it calculates as X, defined as the ratio between a performance metric C and an efficiency metric E. In 2017, it gave detailed notes on how it calculates these values:Overall performance efficiencyXisCdivided byEPerformanceCis a 50:50 average pf CPU and GPU performance compared to Kaveri- CPU Performance fromCinebench R15 nT Score- GPU Performance from3DMark 11 P ScoreEnergy UseEis defined by ETEC 'Typical Energy Consumption from Notebooks' as per Energy Star Program Requirements, Rev 6.1 Oct-2014Kaveriis thebaselinewhereX = 1The secret sauce is based on how you calculateCandE. The headline equation is as stated above:The compute metricCis relatively easy to understand. Here AMD takes the 50-50 weighted average of CPU and GPU performance with theCinebench R15 multi-threadedtest and3D Mark 11 Pfull benchmark.Using Kaveri as a base result of 1.0, Carrizo scores 1.23, Bristol Ridge scores 1.36, and Raven Ridge 2017 scored 2.47 etc.The efficiency metricEis vastly more complicated. It relies on a ‘typical energy use’ model defined by ETEC Energy Star program that adds weights based on sleep power, idle power, and some loading power. The equation looks a little like this:The PT(x) options are the power consumed in those modes. The main thing to bring up about this metric is that it ends up being highly dependent on the device or laptop the processor is being used in. If you want the best result, you need a device that has a low powered, preferably low resolution but efficient display, a small efficient SSD, as few controllers as possible, and as much thermal headroom as possible. The best environment becomes this odd hybrid of premium components but low specifications.For this metric AMD uses their internal reference platforms, which is often based on one of the first devices to launch with the new product. This is where we initially believe that AMD’s improvements kick in – the first devices in 2017 with Raven Ridge were, not to sugar coat it, rather middle-of-the-road. As reported by our sister website Laptop Mag, the HP Envy x360 with Raven Ridge was a repurposed chassis from HP’s catalogue, rather than something hyper optimized. It is likely that AMD’s reference design mirrors this unit a lot, as AMD and HP work very close together. But clearly room for some improvement.For those keeping track, again the base line for this value is referred back to Kaveri. Kaveri also sets a low bar here, being a 19W TDP processor to begin with, and Carrizo improved the metric a lot through its much more optimized power monitoring and delivery. The goal here is for a lower value, so while Kaveri scored a value ofEof 1.00 as the baseline, Carrizo was 0.35, Bristol Ridge was 0.34, and Raven Ridge was 0.28, and also gave double the performance of Bristol Ridge. When it came time to Renoir 2020, performance was +75% over Picasso 2019, but also offered 40% lower power as measured by this metric, giving that 2.92x overall gain.For anyone wondering, the equation for the ‘goal’ line approximates to:The Final DataFor this disclosure, AMD has given us all the data as collected. We had to have a couple of emails back and forth, because some of the data that AMD provided was different to what it had given us in previous 25x20 disclosures – beyond that, there was some mathematical errors in a number of places. AMD took some time to reconfirm the correct numbers, and admitted that in light of some of the issues I’d found, previous data may have been incorrect as well (at one point they had moved a data point from a 19W CPU to a 35W CPU, among other issues).With that being said, here are all the platforms that AMD has used for the 25x20 goal:AMD 25x20 SystemsCPUuArchBase/TurboGPUCUsGPUFreqDDRFreqTDPKaveri2014FX-7600P2 xSteamroller270036008 x R76862 x 4 GBDDR3L-160035 WCarrizo2015FX-8800P2 xExcavator210034008 x R78002 x 2 GBDDR3-186635 WBristol2016FX-9830P2 xExcavator+300037008 x R79002 x 4 GBDDR4-213335 WRaven2017Ryzen 72700U4 xZen22003800Vega1013002 x 4 GBDDR4-240015 WRaven2018Ryzen 72800H4 xZen33003800Vega1113002 x 8 GBDDR4-320035 WPicasso2019Ryzen 73750H4 xZen+23004000Vega1014002 x 8 GBDDR4-240035 WRenoir2020Ryzen 74800H8 xZen229004200Vega816002 x 8 GBDDR4-320035 WNote that some of these platforms are not running at their standard TDP designations. This was done for unity across the performance years (which explains some variance in the results over the years), with Raven Ridge 2017 being the sole 15 W data point due to where the product stack was at the time. The Ryzen 7 4800H, used for the Renoir testing, was run in 35W mode (essentially a 4800HS).And here are the raw results, with the key columns highlighted:AMD 25x20 ScoresAnandTechCBR15 nT3DM11Compute(C)ETECEnergy(E)PerformanceEfficiency (X)Kaveri201423221421.000.9311.001.00xCarrizo201527727091.230.3270.353.50xBristol201627932341.360.3180.343.97xRaven201766744252.470.2610.288.81xRaven201875448772.760.2610.289.86xPicasso201977251912.880.2460.2610.88xRenoir2020172755465.020.1470.1631.77xOverall AMD has achieved a 5.02x performance gain with a 6.33x idle efficiency, which the company is wrapping up into a combined 31.77x performance efficiency metric.In speaking with AMD’s Sam Naffziger, he mentioned that when this project started, the company had created what it assumed would be the year-on-year targets for both the CPU and the GPU. Ultimately in 2014 AMD was very big on the heterogenous system architecture, attempting to meld GPU compute in with the CPU. While GPU acceleration has made it into some aspects of a standard laptop-style device, it perhaps isn’t as ubiquitous as was originally envisioned, however the ultimate end-point ended up being a distinct CPU and GPU gain anyway. Sam told me that based on those original targets back in 2014/2015, AMD exceeded his projects in CPU by some considerable margin, which offset some of the GPU projections.Sam mentioned that one of the key elements to helping achieve this metric was the work AMD has done in idle power management, which has a direct consequence on standard laptop use battery life. Because the 'efficiency' part of the calculation is heavliy weighted towards idle, decreasing the latency for a CPU to enter and exit a turbo mode helps a machine power to idle quicker. Also, optimizing the voltage characteristics of what defines an idle state amd supporting the S0ix power states was also a big leap in that metric. The ACPI standards have helped define some of that roadmap, and some of the requirements imposed by Microsoft in order to enable certain features have driven the design forward.AMD and x86 vs ArmAs an aside, I did want to get Sam’s thoughts on how AMD is approaching the increasing competition from Arm based designs. Note that we had this briefing well before Apple announced its recent news. Sam stated that Arm designs still have to push both frequency and performance at the high-end, which is going to require some extensive work. He pointed to the tribal knowledge of driving x86 at high-performance and at scale – although he did concede that Arm’s partners have a number of impressive core and SoC designs, and they are keeping tabs on what that market is doing. On top of the core work, Arm’s partners still have the ISA/software porting task, and architecture transitions have to enable significant benefits and lots of investment to be taken advantage of. Sam’s point of view is that AMD has no intention of letting any advantage materialize from the Arm space, and aim to stay several steps ahead at all times. Sam was keen to point out that he believes competition is healthy, and not to dismiss Arm, but to acknowledge that AMD aims to be ahead of the curve if any competition does arise.‘The Next Five Years Are Going To Be Fun’At this point I asked Sam if AMD has a similar goal to 25x20 in mind for the next 5-10 years. He was admittedly coy, saying that there is another efficiency goal in the plan, however it would be applied in a much broader context, especially with how the world has changed when it comes to compute requirements. Such a goal would consider a number of performance aspects, perhaps relating to AI acceleration, but also go beyond notebooks into the desktop and the server space – these markets have different performance/energy co-optimization efforts. Sam stated that ultimately AMD plans to focus on what matters most to end-users, what drives to lower energy consumption, and what can offer impressive environmental gains in the future.I was told that AMD is not going to sit on its laurels any time soon, regardless of where it sits in the competitive landscape. The goal of projects like 25x20 is to create paradigm shifts, either internally or externally, to drive a level of continuous innovation by pushing boundaries. ‘The next five years are going to be fun’, Sam said.AMD also highlighted its work with TSMC on 7nm, with Sam stating that without it, achieving the goal would not have been possible. Back when the project first started, there wasn’t a clear indication of what TSMC’s 7nm was going to perform like, but by being an early TSMC partner and going deep into design and technology co-optimization, AMD has been able to extract what they need from the process – without that work, Sam predicts that AMD would have barely hit 20x in its performance efficiency goal. Going forward, AMD plan to continue to be lead partners on upcoming process node technologies.Related ReadingAMD’s Mobile Revival: Redefining the Notebook Business with the Ryzen 9 4900HS (A Review)AMD Updates its 25x20 Goal: Progress in a GenerationAMD's Progress on Its 25x20 Goal: The Task AheadAn Interview with AMD’s CTO Mark Papermaster: ‘There’s More Room At The Top’\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15881/amd-succeeds-in-its-25x20-goal-renoir-zen2-vega-crosses-the-line-in-2020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Marvell’s ThunderX3 Server Team Loses VP/GM and Lead Architect\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-06-23T21:00:00Z\n",
      "URL: https://www.anandtech.com/show/15872/marvells-thunderx3-server-team-loses-vp-gm\n",
      "Content: One of the key drivers in the Arm server space over the last few years has been the cohesion of the different product teams attempting to build the next processor to attack the dominance of x86 in the enterprise market. A number of companies and products have come and gone (Qualcomm’s Centriq) or been acquired (Annapurna by Amazon, Applied Micro by Ampere), with varying degrees of success, some of which is linked to the key personnel in each team. One of our readers has recently highlighted us to a recent movement in this space: Gopal Hegde, the VP/GM of the ThunderX Processor Business Unit at Marvell, has now left the company.ThunderX was originally under the banner of Cavium, and the company released two generations of products built on Arm: ThunderX, and ThunderX2. We reviewed both systems, withThunderX in 2016andThunderX2 in 2018, the latter being noted as a potential competitor in defined workloads as well as cloud and hosting providers, given the right price. We’ve seen ThunderX2 have some success in container deployments,as well as HPC. However, during the deployment of ThunderX2,Cavium was acquired by Marvellfor $5.5 billion USD, with the company looking to bolster its CPU, networking, and security assets. That acquisitionwas completed in July 2018.It was noted that before the acquisition, Cavium was keen to highlight it had a regular roadmap planned for its ThunderX line of processors and servers. ThunderX3 was understood to be an aggressive design suited for a wide range of markets. However, after the acquisition, limited information was provided beyond what was said by Cavium. It was with some personal frustration that attempting to discuss anything about ThunderX3 at the Marvell booth at Supercomputing 2019 was met with a wall of silence. It is typically very odd for a company to stay silent for so long, especially on a product where details had been expected a few months prior, based on previous release cycles. Conversing with peers at the time, we came to the same realization that ThunderX3 was, perhaps, later than expected.However, much to our surprise, Marvell reached out to us in March this yearto discuss the initial steps of ThunderX3. While it wasn’t a deep dive into the architecture, we were certainly very glad to be told the highlights – a 3rdgeneration custom microarchitecture on top of Arm v8.3+, using four threads per core, scaling up to 96 cores or 384 threads per chip. This was combined with 8x DDR4-3200 memory controllers, 64 lanes of PCIe 4.0, and four 128-bit SIMD units per core. This is all to be built on TSMC 7nm. In our briefing, we were told that a critical element to the design of the chip is that it remains monolithic, and that the chip is to target a significant number of cloud-based workloads and HPC workloads with its differentiation (such as SMT4) to allow for higher utilization of the underlying hardware. We are expectingmore information about ThunderX3 at Hot Chips in August, as well as an update on the roadmap.Heading up Cavium and Marvell’s efforts for the ThunderX project was Gopal Hegde. Gopal cites his experience in helping build the engineering teams across several sites worldwide, and as a core and platform engineer, helped define the specifications, the silicon, the brand, and the roadmap as part of a wider team. This was a position held by Gopal since 2014, and before that he was COO of Calxeda, according to his LinkedIn.Gopal Hegde, Image from LinkedInIt is worth noting that Dr. Shubu Mukherjee,Lead Engineer, also left the company in December 2019, to work at SiFive.Gopal Hegde has since transitioned to SVP of Engineering and Operations at SiMa.ai, a startup looking to provide energy efficient machine learning edge compute.Having two key personnel losses in short succession inevitably put up a few question marks. A few of my industry key leads pointed to Gopal moving position as perhaps an inflection point in Marvell’s ThunderX project strategy. At this point we’re aware that while ThunderX2 has been generating revenue, we’re unsure if it has actually provided a positive return at all, let alone sufficient revenue for Marvell since the acquisition to justify the purchase. Development of ThunderX3, a large chip on a leading process node, is going to take a fair amount of investment from Marvell in order to execute both on time and at scale. Usually when we consider the development of new chips by a startup, we talk about the ‘cash burn’ of the company as a way of measuring how much money it is going through until a product is launched. We rarely talk about it for the bigger names, the Intels, the Qualcomms, or the Marvells, but it might be poignant here. Marvell doesn't make these numbers public for obvious reasons.The loss of two big names might give cause for Marvell to reconsider the feasibility of the project in its entirety. Qualcomm famously put the lid on its Centriq server project as a way of removing parts of the company that weren’t generating sufficient revenue, and the start of that process publically began with the announcement ofthe senior VP of the business unitleaving the company. With that being said, Marvell's ThunderX3 is still on the list of presentations at Hot Chips in August and the company still seems positively bouyant about the ThunderX3 release.We reached out to Marvell for commentary about the move and the future of the ThunderX processor family, and spoke to Raghib Hussain, Marvell Chief Strategy Officer and EVP of the Networking and Processors Group, and Raj Singh. Raghib is a co-founder of Cavium, and took on a senior role at Marvell after the acquisition. Raghib's background has primarily involved engineering roles.“We made the strategic decision to integrate the marketing and engineering teams for all of our processor businesses, including OCTEON and ThunderX, under Raj Singh [back in September 2019]. Looking at the data infrastructure market moving forward, we see more similarities and synergies between our compute-focused segments. Under Raj’s leadership, this organization now represents the world’s largest and most scalable Arm-based infrastructure processor business, with solutions scaling from a few cores all the way up to server-class. Marvell’s Arm processor portfolio is unsurpassed for the data infrastructure market and is bringing optimized power, performance and TCO advantages to applications from enterprise appliances to 5G base stations, servers in the data center, and in the near future – new and emerging edge data center applications.”Raj Singh is also an ex-Cavium employee, who has servers as part of his mandate as Marvell's EVP of the Processor Business Group under Raghib Hussain since September 2019.Marvell also gave us the opportunity to ask questions on the business as well as the structure of where ThunderX fits in. We were told that the ThunderX project fits under the banner of 'strategic bets', and by grouping the processor line under the same banner as other products such as automtive ethernet and investments in the datacenter, Marvell aims to offer a datacentric solution to its customers and is in it for the long haul. At this point in time, immediate return isn't so much the main metric of the project, but the potential for a return down the road, which if a chip design is run more like a startup, might not be a feasible strategy if revenue has to start coming in immediately.Compared to other companies in this space, Marvell explained that it has the ability to build IP that can be used across a dozen products in its portfolio, such as a memory controller that might appear across Octeon, ThunderX, Avera ASICs, and this allows it to amortize the cost of research and development across many projects from the mature businesses, the growth businesses, and the strategic bets. Marvell points to its growing R&D spend as a function of revenue, citing it as one of the highest percentages in the fabless semiconductor space.The next big announcement for ThunderX3 is at Hot Chips in August. Marvell seem very keen to engage with us for a microarchitecture disclosure, so stay tuned for that.Source:SiMa.ai Press ReleaseRelated ReadingMarvell to Acquire Cavium for $5.5 BillionMarvell Completes Acquisition of Cavium, Gets CPU, Networking & Security AssetsAssessing Cavium's ThunderX2: The Arm Server Dream Realized At LastInvestigating Cavium's ThunderX: The First ARM Server SoC With AmbitionMarvell Announces ThunderX3: 96 Cores & 384 Thread 3rd Gen Arm Server ProcessorHot Chips 32 (2020) Schedule Announced: ThunderX3 Included\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15872/marvells-thunderx3-server-team-loses-vp-gm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Ampere’s Product List: 80 Cores, up to 3.3 GHz at 250 W; 128 Core in Q4\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-06-23T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15871/amperes-product-list-80-cores-up-to-33-ghz-at-250-w-128-core-in-q4\n",
      "Content: With the advent of higher performance Arm based cloud computing, a lot of focus is being put on what the various competitors can do in this space. We’ve covered Ampere Computing’s previous eMag products, which actually came from the acquisition of Applied Micro, but the next generation hardware is called Altra, and after a few months of teasing some high performance compute, the company is finally announcing its product list, as well as an upcoming product due for sampling this year.Ampere’s Altra is a realized version of Arm’s Neoverse N1 enterprise core, much like Amazon’s Graviton2, but this time in an 80-core arrangement. Where Graviton2 is designed to suit Amazon’s needs for Arm-based instances, Ampere’s goal is essentially to supply a better-than-Graviton2 solution to the rest of the big cloud service providers (CSPs). Of the companies that have committed to an N1 based design, so far on paper Ampere is publically the biggest and fastest on the books.The Ampere Altra range, as part of today’s release, will offer parts from 32 cores up to 80 cores, up to 3.3 GHz, with a variety of TDPs up to 250 W. As we’ve described in our previousnews items on the chip, this is an Arm v8.2 core with a few 8.3+8.5 features, offers support for FP16 and INT8, supports 8 channels of DDR4-3200 ECC at 2 DIMMs per channel, and up to 4 TiB of memory per socket in a 1P or 2P configuration. Each CPU will offer 128 PCIe 4.0 lanes, 32 of which can be used for socket-to-socket communications implemented with the CCIX protocol over PCIe. This means 50 GB/s in each direction, and 192 PCIe 4.0 lanes in a dual socket system for add-in cards. Each of the PCIe lanes can bifurcate down to x2.Ampere 1st Gen Altra 'QuickSilver'Product ListAnandTechCoresFrequencyTDPPCIeDDR4PriceQ80-33803.3 GHz250 W128x G48 x 3200?Q80-30803.0 GHz210 W128x G48 x 3200?Q80-26802.6 GHz175 W128x G48 x 3200?Q80-23802.3 GHz150 W128x G48 x 3200?Q72-30723.0 GHz195 W128x G48 x 3200?Q64-33643.3 GHz220 W128x G48 x 3200?Q64-30643.0 GHz180 W128x G48 x 3200?Q64-26642.6 GHz125 W128x G48 x 3200?Q64-24642.4 GHz95 W128x G48 x 3200?Q48-22482.2 GHz85 W128x G48 x 3200?Q32-17*321.7 GHz58 W128x G48 x 3200?Q32-17321.7 GHz45 W128x G48 x 3200?*With 4 TiB DRAM InstalledI must credit Ampere here. This is by far theeasiestproduct naming scheme I’ve ever seen. Intel could learn a million things from this naming scheme alone. The ‘Q’ stands for QuickSilver, the codename of the underlying SoC, followed by a core count and a frequency.Previously Ampere had stated they were going for 80 cores at 3.0 GHz at 210 W, however the Q80-33 is pushing that frequency another 300 MHz for another 40 W, and we understand that the tapeout of silicon from TSMC performed better than expected, hence this new top processor.It’s worth doing some basic metrics on power efficiency. If we take the TDP as solely the power for the cores, and do some math on Watts per Core, then GHz per Watt, the top Q80-33 SKU scores 1.06, around the middle of the pack (most CPUs score 0.95-1.25 GHz/W). The highlight of the list by this metric is the Q64-24, offering the most frequency for the least power: 1.62 GHz per Watt.Also, just because we have the numbers, AMD’s big Rome CPUs consume about 3 W per core at full load, and run at approximately 3.0 GHz on all CPUs. Altra, by comparison, uses 2.6 W per core on the Q80-30. These Altra CPUs have no turbo mechanism, and thus the TDP metrics being given by Ampere are for the literal peak power consumption numbers, so what is listed above is merely a design point for chassis building, rather than a full representation of power consumption when deployed in the cloud.Ampere states they have a number of ODMs on board that will be ready to provide Altra systems, including Gigabyte and Wiwynn, with a couple of second tier players also in the mix. These systems should be more readily available in August and September.When we asked Ampere about the interest for these chips, the company stated that most of the interest from CSPs was actually at the high end dual socket deployments, for the highest core counts and the highest frequencies. Even though Ampere isn’t announcing pricing publically, the company states that their pricing has not been an obstacle for CSP deployments, with major customers testing the hardware for up to 2 months already. Current announced customers include Packet and CloudFlare, with Packet offering early access for its key clients.Ampere is also one of the lead partners for CUDA on Arm, and is set to offer full CUDA support for Altra when paired with NVIDIA graphics accelerators.Altra MaxIf that wasn’t enough, Ampere dropped a sizeable nugget into our pre-announcement briefing. The company is set to launch a 128-core version of Altra later this year.This will be a new silicon design, beyond Ampere's initial layout of 80 cores for Altra, however Ampere states that while they are using the same platform as the regular Altra, they have done extensive tweaking and optimizations within the mesh interconnect for Altra Max to hide the additional contention that might occur when using the same main memory speeds.Altra Max will be socket and pin-compatible with Altra, also support dual socket deployments, and Ampere states that the silicon will be ready for early sampling with partners in Q4, and is looking to move into high volume in mid-2021.The 128-core design was given the code-name Mystique, and so we might expect to see these CPUs start with the letter M.Update on 5nm SirynThe next generation of Ampere’s product line, as previously reported, is going to use the codename Siryn (sire-inn) and be built on TSMC’s 5nm process, set for sampling in late 2021. Ampere stated in our briefing that test chips that use IP meant to be adopted in Siryn have already taped out - the actual Siryn chip will tape out sometime in the next year.Siryn will likely be marketed as ‘2ndGeneration Altra’, and if the naming convention of CPUs stays the same, these will start with ‘S80’ etc. Ampere has stated that the Siryn platform will be new, especially because of new technologies (PCIe 5.0 and DDR5 were mentioned, but not confirmed for Siryn).Related ReadingNext Generation Arm Server: Ampere’s Altra 80-core N1 SoC for Hyperscalers80-Core N1 Next-Gen Ampere, ‘QuickSilver’: The Anti-Graviton2Avantek's Arm Workstation: Ampere eMAG 8180 32-core Arm64 ReviewArm Development For The Office: Unboxing an Ampere eMag WorkstationArm Server CPUs: You Can Now Buy Ampere’s eMAG in a WorkstationAmpere eMAG in the Cloud: 32 Arm Core Instance for $1/hrAmpere Computing: Arm is Now an InvestorArm Announces Neoverse N1 & E1 Platforms & CPUs: Enabling A Huge Jump In Infrastructure Performance\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15871/amperes-product-list-80-cores-up-to-33-ghz-at-250-w-128-core-in-q4\n",
      "Title: The Next Phase: Apple Lays Out Plans To Transition Macs from x86 to Apple SoCs\n",
      "Author: Ryan Smith\n",
      "Date Published: 2020-06-22T23:00:00Z\n",
      "URL: https://www.anandtech.com/show/15875/apple-lays-out-plans-to-transition-macs-from-x86-to-apple-socs\n",
      "Content: After many months of rumors and speculation, Apple confirmed this morning during their annual WWDC keynote that the company intends to transition away from using x86 processors at the heart of their Mac family of computers. Replacing the venerable ISA – and the exclusively-Intel chips that Apple has been using – will be Apple’s own Arm-based custom silicon, with the company taking their extensive experience in producing SoCs for iOS devices, and applying that to making SoCs for Macs. With the first consumer devices slated to ship by the end of this year, Apple expects to complete the transition in about two years.The last (and certainly most anticipated) segment of the keynote, Apple’s announcement that they are moving to using their own SoCs for future Macs was very much a traditional Apple announcement. Which is to say that it offered just enough information to whet developers (and consumers’) appetites without offering too much in the way of details too early. So while Apple has answered some very important questions immediately, there’s also a whole lot more we don’t know at the moment, and likely won’t known until late this year when hardware finally starts shipping.What we do know, for the moment, is that this is the ultimate power play for Apple, with the company intending to leverage the full benefits of vertical integration. This kind of top-to-bottom control over hardware and software has been a major factor in the success of the company’s iOS devices, both with regards to hard metrics like performance and soft metrics like the user experience. So given what it’s enabled Apple to do for iPhones, iPads, etc, it’s not at all surprising to see that they want to do the same thing for the Mac. Even though the OS itself isn’t changing (much), the ramifications of Apple building the underlying hardware down to the SoC means that they can have the OS make full use of any special features that Apple bakes into their A-series SoCs. Idle power, ISPs, video encode/decode blocks, and neural networking inference are all subjects that are potentially on the table here.Apple SoCs: Market Leading Performance & EfficiencyAt the heart of this shift in the Mac ecosystem will be the transition to new SoCs built by Apple. Curiously, the company has carefully avoided using the word “Arm” anywhere in their announcement, but the latest macOS developer documentation makes it clear Apple is taking their future into their own hands with the Arm architecture. The company will be making a series of SoCs specifically for the Mac, and while I wouldn’t be too surprised if we see some iPad/Mac overlap, at the end of the day Apple will want SoCs more powerful than their current wares to replace the chips in their most powerful Mac desktops.And it goes without saying that Apple’s pedigree in chip designs is nothing less than top-tier at this point. The company has continued to iterate on its CPU core designs year after year, making significant progress at a time when x86 partner Intel has stalled, allowing the company’s latest Lightning cores to exceed the IPC of Intel’s architectures, while overall performance has closed in on their best desktop chips.Apple’s ability to outdo Intel’s wares is by no means guaranteed, especially when it comes to replacing the likes of the massive Xeon chips in the Mac Pro, but the company is coming into this with a seasoned design team that has done some amazing things with low-power phone and tablet SoCs. Now we’re going to get a chance to see what they can do when the last of the chains come off, and they are allowed to scale up their designs to full desktop and workstation-class chips. Apple believes they can deliver better performance at lower power than the current x86 chips they use, and we’re all excited to see just what they can do.Though from an architecture standpoint, the timing of the transition is a bit of an odd one. As noted by our own Arm guru,Andrei Frumusanu, Arm is on the precipice of announcing the Arm v9 ISA, which will bring several notable additions to the ISA such as Scalable Vector Extension 2 (SVE2). So either Arm is about to announce v9, and Apple’s A14 SoCs will be among the first to implement the new ISA, otherwise Apple will be setting the baseline for macOS-on-Arm as v8.2 and its NEON extensions fairly late into the ISA’s lifecycle. This will be something worth keeping an eye on.Selling x86 & Arm Side-by-Side: A Phased TransitionWhile for obvious reasons Apple’s messaging today is about where they want to be at the end of their two-year transition, their transition is just that: around two years long. As a result, Apple has confirmed that there will be an overlapping period where the company will be selling both x86 and Arm devices – and there will even be new x86 devices that the company has yet to launch.In the near term, it will take Apple some time to build new devices around their in-house SoCs. So even if Apple doesn’t introduce any new device families or form factors over the next two years, the company will still need to refresh x86-based Macs with newer Intel processors to keep them current until their Arm-based successors are ready. And although Apple hasn’t offered any guidance on what devices will get replaced first, it’s as reasonable a bet as any that the earliest devices will be lower-end laptops and the like, while Apple’s pro gear such as the Mac Pro tower will be the last parts to transition, as those will require the most extensive silicon engineering.This also means that Apple is still on the clock as far as x86 software support goes, and will continue to be so well after they complete their hardware transition. In part a practical statement to avoid Osborning themselves and their current x86-based systems, Apple has confirmed that they will continue supporting x86 Macs for years to come. Just how long that will be remains to be seen, of course, but unless Apple accelerates the retirement x86 Mac support, the company as of late has been supporting Macs with newer OSes and OS updates for several years after their initial launch.x86 Compatibility: Rosetta 2 & VirtualizationMeanwhile, in order to bridge the gap between Apple’s current software ecosystem and where they want to be in a couple of years, Apple will once again be investing in a significant software compatibility layer in order to run current x86 applications on future Arm Macs. To be sure, Apple wants developers to recompile their applications to be native – and they are investing even more into the Xcode infrastructure to do just that – but some degree of x86 compatibility is still a necessity for now.The cornerstone of this is the return of Rosetta, the PowerPC-to-x86 binary translation layer that Apple first used for the transition to x86 almost 15 years ago. Rosetta 2, as it’s called, is designed to do the same thing for x86-to-Arm, translating x86 macOS binaries so that they can run on Arm Macs.Rosetta 2’s principle mode of operation will be to translate binaries at install time. I suspect that Apple is eyeing distributing pre-translated binaries via the App Store here (rather than making every Mac translate common binaries), but we’ll see what happens there. Meanwhile Rosetta 2 will also support dynamic translation, which is necessary for fast performance on x86 applications that do their own Just-in-Time compiling.Overall Apple is touting Rosetta 2 as offering “fast performance”, and while their brief Maya demo is certainly impressive, it remains to be seen just how well the binary translation tech works. x86 to Arm translation has been a bit of a mixed bag, judging from Qualcomm & Microsoft’s efforts, though past efforts haven’t involved the kind of high-performance chips Apple is aiming for. At the same time, however, even with the vast speed advantage of x86 chips over PPC chips, running PPC applications under the original Rosetta was functional, but not fast.As a result, Rosetta 2 is probably best thought of as a backstop to ensure program compatibility while devs get an Arm build working, rather than an ideal means of running x86 applications in the future. Especially since Rosetta 2 doesn’t support high-performance x86 instructions like AVX, which means that in applications that use dense, performance-critical code, they will need to fall back to slower methods.On which note, right now it’s not clear how long Apple will offer Rosetta 2 for macOS. The original Rosetta was retired relatively quickly, as Apple has always pushed its developers to move quickly to keep up with the platform. And with a desire to have a unified architecture across all of its products, Rosetta 2 may face a similarly short lifecycle.Meanwhile, macOS Big Sur (11.0), the launch OS for this new Mac ecosystem, will also be introducing a new binary format called Universal 2. Apple has ample experience here with fat binaries, and Universal 2 will extend that to cover Arm binaries. Truth be told, Apple already has the process down so well that I don’t expect this to be much more than including yet another folder in an application bundle with the necessary Arm binaries.Finally, rounding out the compatibility package is an Apple-developed virtualization technology to handle things such as Linux Docker containers. Information on this feature is pretty light – the company briefly showed it off as part of Parallels running Linux in the keynote – so it remains to be seen just what the tech can do. At a minimum, and appropriate for a developers conference, the fact that they have a solution in place for Linux and Docker is an good feature to show off, as these are features that are critical to WWDC’s software developer crowd.But it leaves unanswered some huge questions about Windows support, and whether this tech can be used to run Windows 10 similar to how Parallels and other virtualization software can run Windows inside of macOS today. As well, Apple isn’t saying anything about BootCamp support at this time, despite the fact that dual-booting macOS and Windows has long been a draw for Apple’s Mac machines.Dev Kits: A12Z As A Taste of Things To ComeFinally, in order to prepare developers to launch native, Arm-compiled software later this year when the first Arm Macs ship, Apple has also put together a developer transition kit, which the company will be loaning out to registered developers. The DTK, as it’s called, was used in Apple’s keynote to demonstrate the features of macOS Big Sur. And while it’s essentially just an iPad in a Mac Mini’s body, it’ll be an important step in getting developers ready with native applications by giving them actual hardware to test and optimize against.Overall, the DTK is based on Apple’s A12Z processor, and includes 16GB of RAM as well as a 512GB SSD. I wouldn’t be the least bit surprised if the machine is also clocked a bit higher than iPads as well, thanks to the device’s larger form factor, but in an interesting twist of fate it’s still likely to be slower than the iPhone 11 series of devices, which use the newer A13 SoC. The upside, at least, is that the A12Z sets a rather high low for performance, and conversely encourages developers to make efficient applications. So if developers can get their applications running well on an A12Z device, then they should have no problems whatsoever in running those apps on future A14-derived silicon.And although the A12Z SoC inside the DTKs is a known quantity at this point, like their other beta programs, Apple will be keeping a tight lid on performance. The DTK license agreement bans public benchmarking, and even though developers will pay $500 to take part in the program, the DTKs remain the property of Apple and must be returned. So while leaks will undoubtedly drip out over the coming months, it would seem that we’re not going to get the chance to do any kind of extensive, above-the-board performance testing of Mac-on-Arm hardware until the final consumer systems come out late this year.Closing ThoughtsWhile the confluence of events that have led to Apple’s decision may have removed any surprise from today’s announcement, there is no downplaying the significance of what Apple has decided to do. To go vertically integrated – developing their own chips and controlling virtually every aspect of Mac hardware and software – is a big move on its own. But the fact that Apple will be doing this while simultaneously transitioning the macOS and the larger Mac software ecosystem to another Instruction Set Architecture makes it all the more monumental. A large number of things have to go right for the company in order to successfully make the transition, both at the hardware and the software level. Every group within the Mac division will be up to bat, as it were.The good news is that, as a fixture of the personal computing industry since the very beginning, there are few companies more experienced in these sorts of transitions than Apple. The move to x86 almost 15 years ago more or less created the playbook for Apple’s move to Arm over the next two years, as many of the software and hardware challenges at the same. By not tying themselves down with legacy software – and by forcing developers to keep up or perish – Apple has remained nimble enough to pull off these kinds of transitions, and to do so in a handful of years instead of a decade or longer.Overall, I’m incredibly excited to see what Apple can do for the Mac ecosystem by supplying their own chips, as they have accomplished some amazing things with their A-series silicon thus far. However it’s also an announcement that brings mixed feelings. Apple’s original move to x86 finally unified the desktop computing market behind a single ISA, letting one system natively run macOS to Windows, and everything in between. But after 15 years of a software compatibility utopia, the PC market is about to become fractured once again.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15875/apple-lays-out-plans-to-transition-macs-from-x86-to-apple-socs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD Scores First Top 10 Zen Supercomputer… at NVIDIA\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-06-22T22:30:00Z\n",
      "URL: https://www.anandtech.com/show/15874/amd-scores-first-top-10-zen-supercomputer-at-nvidia\n",
      "Content: One of the key metrics we’ve been waiting for since AMD launched its Zen architecture was when it would re-enter the top 10 supercomputer list. The previous best AMD system, built on Opteron CPUs, was Titan, which held the #1 spot in 2012 but slowly dropped out of the top 10 by June 2019. Now, in June 2020, AMD scores a big win for its Zen 2 microarchitecture by getting to #7. But there’s a twist in this tale.Measuring success by the TOP500 list is not so much for scoring revenue, but for scoring prestige. On the database are systems that were built over a decade ago, so a chance to put something into the list on the latest and greatest at a fraction of the size and power ends up being a big promotional opportunity for the company whose hardware is involved (as well as where it ends up being based). Obviously since AMD started introducing its new Zen-based processors, as a return to the high-end of performance after several years, we’ve been wondering how long it would take for a large scale AMD deployment.AMD has had HPC success in the past, most notably withthe Titan supercomputer, built on a mixture of Opteron 6274 CPUs paired with NVIDIA K20x accelerator cards. The machine hit #1 in 2012, and still sits at #12 today. This was a sizeable deployment, coming in at 17.6 PetaFLOPs for 8.2 MegaWatts.Anand back in the day event went for a look around:Inside the Titan Supercomputer: 299K AMD x86 Cores and 18.6K NVIDIA GPUsBack in 2012When it comes to AMD’s Zen designs, the two main CPUs we have to look for areNaples(1stGen EPYC) andRome(2ndGen EPYC). That latter has been getting a lot of attention for having up to 64 high performance cores as well as a lot of memory bandwidth and heaps of connectivity for storage and add-in cards.However, the first Zen system on the top 500 was technically neither of those.The Hygon joint venture actually provided the first Zen based supercomputer to join the listin November 2018 at #38. This was a system built at Sugon, the company distributing the Hygon systems, to showcase the hardware. It used 5120 of the Hygon 32 core CPUs. We’vereviewed and done a deep diveinto the Hygon hardware. The Hygon joint venture has since dissolved, but the supercomputer it's based on is still running at #58.It wasn’t until late 2019 that systems based on AMD EPYC show up. In November’s list that we saw two AMD Naples and two AMD Rome systems push AMD’s total up to six (5 based on EPYC, one on older Opterons). For the June 2020 announcement this week, another seven AMD Rome systems are in the list, making Rome the 10thmost popular processor family for supercomputers. But it’s Selene at #7 that’s making the headlines.Selene is the name ofthe new supercomputer sitting at #7. For host processors, it is using AMD’s Rome 7742 parts, which are the highest performing commercial parts available that aren’t for specialized markets – technically a list price of $6950 each. What makes Selene a bit odd for an AMD win is that it is part of a supercomputer built with NVIDIA A100 accelerators. And it’s also built for NVIDIA to use at NVIDIA.When NVIDIA announced its new A100 Ampere accelerator card for compute, it also announced the concept of a DGX A100 ‘SuperPod’, connecting 140 DGX A100 nodes and 1120 A100 GPUs to supply up to 700 PetaOPs of AI-based performance. It turns out that this concept of a SuperPOD also just happens to hit #7 in the TOP500 supercomputer list, which uses more traditional LINPACK FP64 FLOPs, straight off the bat. Each of the DGX A100 nodes contains two AMD EPYC CPUs and eight A100 accelerators.Selene scores a performance of 27.6 PetaFLOPs of FP64 throughput, for 1.3 MegaWatts of power. Compared to the previous Titan supercomputer, which had Opterons and K20x accelerators, that’s 57% more performance for only 16% of the power, making it almost 10x more efficient. Selene uses NVIDIA’s Mellanox HDR Infiniband for connectivity, and has 560 TiB of memory installed.At launch, NVIDIA said that a DGX A100 node would cost $199k. This makes the hardware deployment for Selene (minus switches, install cost, cabling) somewhere around $28 million. It’s worth noting that this is technically only 280 EPYC CPUs paired with 1120 A100 GPUs, combined together for 277760 ‘cores’. It seems odd to suggest that 'this is all that is needed' to reach #7.The wins for AMD on Zen are now (with Rmax):#7,Selene, an EPYC 7742 + A100 system for NVIDIA (27.6 PF)#30,Belenos, an EPYC 7742 system for Meteo France (7.7 PF)#34,Joliot-Curie Rome, an EPYC 7H12 system for CEA in France (7.0 PF)#48,Mahti, an EPYC 7H12 system for CSC in Finland (5.4 PF)#56,Betzy, an EPYC 7742 system for Sigam2 AS in Norway (4.44 PF)#58,PreE, a Hygon C86 system for Sugon, China (4.32 PF)#124,Freeman, an EPYC 7542 system for ERDC DSRC (2.5 PF)#172,Betty, an EPYC 7542 system for the US Army Research Laboratory (2.1 PF)#268,Cara, an EPYC 7601 system for German Aerospace Center (1.75 PF)#292, an EPYC 7501 + Vega 20system for Pukou Advanced Computing Center, China (1.66 PF)#483,Spartan, an EPYC 7H12 system for Atos, France (1.26 PF)All of which are new in the past year, except for #58 the Hygon system.The two main upcoming supercomputers for AMD are both part of the US Exascale project.Frontieris set to have 1.5 ExaFLOPs of EPYC and Radeon Instinct in a 30 MegaWatt design at Oak Ridge, built by Cray (HPE), for 2021.El Capitanis set to 2.0 ExaFLOPs of EPYC and Radeon Instinct in a 30 MegaWatt design at Lawrence Livermore National Laboratory, built by Cray (HPE), for early 2023.The other US Exascale project in the US isAurora, with 1.0 Exaflops of Xeon and Xe, for the Argonne National Laboratories, due in late 2021.US Department of Energy Exascale SupercomputersEl CapitanFrontierAuroraCPU ArchitectureAMD EPYC \"Genoa\"(Zen 4)AMD EPYC(Future Zen)Intel Xeon ScalableGPU ArchitectureRadeon InstinctRadeon InstinctIntel XePerformance (RPEAK)2.0 EFLOPS1.5 EFLOPS1 EFLOPSPower Consumption<40MW~30MWN/ANodesN/A100 CabinetsN/ALaboratoryLawrence LivermoreOak RidgeArgonneVendorCrayCrayIntelYear202320212021AMD is still fervent in meeting its goal of hitting 10% market share for EPYC by the middle of the year. Given that the middle is usually somewhere in Q2/Q3, and we’re set to enter Q3, we should be hearing more about that target soon, and how COVID-19 may have adjusted those expectations.Related ReadingEl Capitan Supercomputer Detailed: AMD CPUs & GPUs To Drive 2 Exaflops of ComputeUS Dept. of Energy Announces Frontier Supercomputer: Cray and AMD to Build 1.5 Exaflop MachineAMD Confirms Zen 4 EPYC Codename, and Elaborates on Frontier Supercomputer CPUNew #1 Supercomputer: Fujitsu’s Fugaku and A64FX take Arm to the Top with 415 PetaFLOPsAn Interview with AMD’s CTO Mark Papermaster: ‘There’s More Room At The Top’An Interview with AMD’s Forrest Norrod: Naples, Rome, Milan, & GenoaIntel’s 2021 Exascale Vision in Aurora: Two Sapphire Rapids CPUs with Six Ponte Vecchio GPUs\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15874/amd-scores-first-top-10-zen-supercomputer-at-nvidia\n",
      "Title: Apple Announces iOS 14 and iPadOS 14: An Overview\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-06-22T21:00:00Z\n",
      "URL: https://www.anandtech.com/show/15873/apple-announces-ios-14-and-ipados-14-an-overview\n",
      "Content: Amongst today’s Apple’s WWDC historic announcements, such asthe company’s switch from x86 to Arm processor architectures, we also saw the launch of the new iOS 14 and iPadOS 14 which bring new features to the company’s mobile devices.Home screen: App Library and WidgetsStarting off with the home screen, for the first time in many generations we’re seeing some substantial changes to this part of iOS and iPhones. For many years Apple had been relatively stubborn in implementing a more flexible app management system that’s a bit more organised than simply having all applications spread over several home screens. Apple attempts to solve this issue by introducing the new “App Library” section of the home screen. This is essentially an extra page where the launcher will organise your applications by groups defined by their application type. On top of this UI addition, we’re also seeing now a linear alphabetical list of all applications that can be called up from the new app search UI, also allowing you to type in the name of the app you want to find and launch.Another big addition is the addition of home-screen widgets. Undoubtedly some people will make fun of Apple for being more than a decade late to this feature, but now finally we can also have customizable widgets directly on the home screen. One question which I asked myself during the presentation is as to whether Apple will allow “empty space” in the home screen or if things will still be filled top-to-bottom left-to-right in terms of widgets and app icons.iMessage Updates – Better Group Chats & User taggingiMessage has also seen very large updates that vastly improve the usability of the platform and bring it more up to date with other messaging apps. Group conversations can now be pinned at the top of the app, up to 9 of the most frequent chats can now be favourited with quick access to them instead of having to search through the list. We’ve seen big improvements in the customisations of the group conversations, naming or giving the group dedicated thumbnails, as well as new usability options as to show the most recent group members who wrote something.Users will now also be able to tag people in the chats. This serves to directly address a person in a group chat all whilst remaining transparent to the other members. One big advantage of such a functionality is that a member can customise his notification settings, for example muting the conversation unless he’s been tagged in the chat.It’s now also possible to do in-line replies to a specific message, creating conversation threads instead of a single on-going linear chat history.Maps UpdatesMaps has been updated with the introduction of cycling directions, with the ability for navigation not only through roads but any cyclable path up to your destination. There’s extra information such as elevation changes, any obstacles in the way such a stairway you’d have to take that are accounted into your computed route. Routing has also been improved for electric vehicles, and Apple Maps can now route you properly through a longer trip by considering charging stations on the way.Guides are now a curated discovery mechanism in the app for locations, venues, shops or restaurants that you can visit.Offline Translation App & Inline Browser TranslationApple now offers a new translation app that thanks to the new machine learning silicon and models that the company now includes in the app, will be able to work offline. The new interface is extremely simplistic and straightforward in order to facilitate easy communication with whomever you are using it with, and will be able to automatically detect which person is talking to make for a more seamless conversation.Another important translation addition is the ability for in-line webpage translation. Usually this was a feature that was previously only possible in Google’s Chrome browser as I’m not aware of any other popular browser that made use of Google’s paying API for this feature. The new Safari features essentially works very similarly but Apple’s implementation looks to be even a bit more seamless. We’ll have to see how the translation quality holds up.Phone as an NFC Car KeyApple will be partnering with BMW to allow your phone to act as the car’s keys. Using the NFC connectivity of the phone, iOS14 will now be able to securely store a digital car key within your wallet. Whilst I don’t necessarily see this by itself to be any game-changer, the possibilities of what this would enable for car-sharing go a bit further, as you can share your digital car key with others. You can also set the permissions that this person would have in the car which is also interesting, but also raises a lot of questions as to the security aspect of such a feature.App Clips – Apple’s take on Progressive Apps?Another big addition to the App Store and the iOS ecosystem is the introduction of “App Clips”. These are light-weight applications that are treated essentially as more substantive “links” that can be embedded in websites, shared through messaging, or included in real life locations through either NFC tags or QR codes.The difference to a full-blown app is their light-weight nature and the fact that they’re meant to immediately bring you to their intended experience. Apple here has strict restrictions as to the complexity of the app, such as not allowing anything exceeding 10MB in size.Privacy Improvements: More Fine-Grained Permissions & TransparencyApple puts a big focus on privacy and has expanded on the way that applications are able to handle permissions. Essentially Apple is taking a note of Android’s latest permissions management, showcasing what permission each app is requesting. iOS14 goes further in that it also showcases data it uses to track you for identifications as well as data that is linked to you.Applications which for example use the phone’s microphone will now have a red recording light in the notification bar show up, letting the user know that the mic is active.Applications will have to disclose their requested permissions in the app listings in the App Store so users will know in advance about the app’s data gathering.iPadOS - Better Side-Panel OrganisationNew application sidebar designs in iPad apps will now vastly improve navigation of different sections of more complex apps. Apple has added more pull-down menu to let you quickly access more app functionality.Some general improvements of the iPad UI also include doing away with some actions taking up the full screen. Calls no longer prompt a full-screen window but just a smaller top compact call pop-up, which by the way is also present for iOS. The new compact call also works for third-party apps.Search no longer takes up the full screen either and instead will also be a overdrawn pop-up widow on top of your current activities.Pencil as a First-Class Input MethodApple’s pencil certainly has bene useful in the past, but it’s never been something that you could exclusively use your iPad with, always having to switch back to the keyboard input for typing in something in a field for example.This now changes in iPadOS 14 as the pencil becomes a first-class input method, with the ability to write in any input field on the OS and have the device OCR it into text.A huge problem with this is that it’s quite hard or near impossible to select handwritten text. Not anymore, as the new update now uses machine learning to detect and handle such texts as if they were typed in. Selecting a piece of text or a word essentially now just requires you to encircle that word with the pencil to be able to bring up the usual copy-paste menus, and you’re now also able to highlight larger pieces of handwritten text that you can just copy from there and paste into any other text input field in an app.There’s also context awareness to handwritten content, with the OS being to recognise phone numbers, dates, or addresses which when selected directly brings up your address book, calendar or the maps application.Public Beta in July - Available This FallAs always with new iOS releases, Apple will make developer releases available soon with a public beta in July followed by widespread final releases of iOS 14 and iPadOS 14 later this fall.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15873/apple-announces-ios-14-and-ipados-14-an-overview\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: New #1 Supercomputer: Fugaku in Japan, with A64FX, take Arm to the Top with 415 PetaFLOPs\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-06-22T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/15869/new-1-supercomputer-fujitsus-fugaku-and-a64fx-take-arm-to-the-top-with-415-petaflops\n",
      "Content: High performance computing is now at a point in its existence where to be the number one, you need very powerful, very efficient hardware, lots of it, and lots of capability to deploy it. Deploying a single rack of servers to total a couple of thousand cores isn’t going to cut it. The former #1 supercomputer, Summit, is built from 22-core IBM Power9 CPUs paired with NVIDIA GV100 accelerators, totaling 2.4 million cores and consuming 10 MegaWatts of power. The new Fugaku supercomputer, built at Riken in partnership with Fujitsu, takes the top spot on the June 2020 #1 list, with 7.3 million cores and consuming 28 MegaWatts of power.The new Fugaku supercomputer is bigger than Summit in practically every way. It has 3.05x cores, it has 2.8x the score in the official LINPACK tests, and consumes 2.8x the power. It also marks the first time that an Arm based system sits at number one on the top 500 list.Due to the onset of the Coronavirus pandemic, Riken accelerated the deployment of Fugaku in recent months. On May 13th, Riken announced that more than 400 racks, each featuring multiple 48-core A64FX cards per server, were deployed. This was a process that had started back in December, but they were so keen on getting the supercomputer up and running to assist with the R&D as soon as possible – the server racks didn’t have their official front panels when they started working. There are still additional resources to add, with full operation scheduled to begin in Riken’s Fiscal 2021, suggesting that Fugaku’s compute values on the top 100 list are set to rise even higher.Alongside being #1 in the TOP500, Fugaku enters the Green500 List at #9, just behind Summit, and below the Fugaku Prototype installation which sits at #4.At the heart of Fugaku is the A64FX, a custom Arm v8-A CPU-based chip optimised for compute. The total configuration uses 158,976 of these 48+4-core cards, running at 2.2 GHz peak performance (48 cores for compute, 4 for assistance). This allows for some substantial Rpeaknumbers, such as 537 PetaFLOPs of FP64, the usual TOP500 metric. But A64FX also supports quantized models with lower precision, which is where we get into some fun numbers for Fugaku:FP64: 0.54 ExaFLOPsFP32: 1.07 ExaOPsFP16: 2.15 ExaOPsINT8: 4.30 ExaOPsDue to the design of the A64FX, it also allows for a total memory bandwith of 163 PetaBytes per second.To date, the A64FX compute card is the only implementation of Arm’s v8.2-A Scalable Vector Extensions (SVE). The goal of SVE is to allow Arm’s customers to build hardware with vector units ranging from 128-bit to 2048-bit, such that any software that is built to run on SVE will automatically scale regardless of the SVE execution unit size. A64FX uses two 512-bit wide pipes per core, with 48 compute cores per chip, and also adds in four 8 GiB HBM2 links per chip in order to feed the units for 1 TiB/s of total bandwidth into the chip.As listed above, the unit supports INT8 through FP64, and the chip has an on-board custom Tofu interconnect, supporting up to 560 Gbps of interconnect to other A64FX modules. The chip is built on TSMC’s N7 process, and comes in at 8.79 billion transistors. 90% execution efficiency is claimed for DGEMM type workloads, and additional mechanisms such as combined gather and unaligned SIMD loading are used to help keep throughput high. There is also additional tuning that can be done at the power level for optimization, and extensive internal RAS (over 128k error checkers in silicon) to ensure accuracy.Details on the A64FX chip weredisclosed at Hot Chips in 2018, andwe saw wafers and chipsat Supercomputing in 2019. This chip is expected to be the first in a series of chips from Fujitsu along a similar HPC theme.Work done on Fugaku to date includes simulations about Japan’s COVID-19 track and tracing app. According to Professor Satoshi Matsuoka,predictions calculated by Fugakusuggested a 60% distribution on the app development in order to be successful.Droplet simulationshave also been performed on virus activity. Deployment of A64FX is set to go beyond Riken, withSandia Labs to also have an A64FX systembased in the US.Source: TOP500Related ReadingA Success on Arm for HPC: We Found a Fujitsu A64FX WaferHot Chips 2018: Fujitsu's A64FX Arm Core Live Blog\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15869/new-1-supercomputer-fujitsus-fugaku-and-a64fx-take-arm-to-the-top-with-415-petaflops\n",
      "Title: The Apple WWDC 2020 Keynote Live Blog (Starts at 10am PT/17:00 UTC)\n",
      "Author: Ryan Smith\n",
      "Date Published: 2020-06-22T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15870/the-apple-wwdc-2020-keynote-live-blog\n",
      "Content: While COVID may have put a crimp on the tech industry, for Apple the show must still go on. Join us at 10am Pacific/17:00 UTC for our live blog coverage of this year's Apple WorldWide Developer's Conference (WWDC), which like so many other shows is taking a uniquely virtual tack this year.The morning keynote for the developer-focused show is typically a rapid-fire two-hour run through Apple's ecosystem, covering everything from macOS and iOS to individual Apple applications and more, and it sounds like Apple will be sticking to that strategy for their virtual show. Meanwhile there's always the lingering question over whether we'll also see a new hardware announcement this year – Apple tends to be about 50/50 with hardware at WWDC – something which has taken on an even greater significance this year as Apple is widely believed to be working on transitioning the Mac platform to its own Arm-based SoCs. Even if we don't get hardware details at this year's WWDC, even confirmation of that project and Apple's transition plans would mark the kick-off point for a huge shift in the Apple ecosystem, and an event that could reverberate into the PC ecosystem as well.12:58PM EDT- Welcome to this year's coverage of Apple's WWDC keynote12:58PM EDT- Apple's keynote is expected to begin momentarily01:00PM EDT- As with every year, the developer-focused conference is going to lean heavily into Apple's operating systems/ecosystems, including macOS, iOS, watchOS, and tvOS01:01PM EDT- On the macOS front, Apple is coming off the recent retirement of 32-bit applications, as well as their efforts to increase iOS/macOS application portability01:02PM EDT- And it looks like we're starting01:02PM EDT- As always, Tim Cook kicks things off01:02PM EDT- \"I can assure you we have a great show ahead of us\"01:04PM EDT- Cook is starting with Apple's backing of social justice initiatives, which has long been near and dear to the bay area company's heart01:06PM EDT- Now going into the conference itself, with Cook recapping what Apple is planning on doing with the newly-virtual conference01:06PM EDT- First up is Craig Federighi with iOS01:06PM EDT- Who of course is here to talk about iOS 1401:07PM EDT- Discussing the home screen01:07PM EDT- \"We've rethought some of the core elements of iOS\" to reflect that iPhones are doing more than ever before01:08PM EDT- Now rolling a short video01:08PM EDT- The home screen is receiving a revamp01:08PM EDT- Apple is introducing a feature called the \"App Library\"01:09PM EDT- A single view that organizes all of the apps on a phone01:09PM EDT- For helping users find apps, which Apple finds to be harder to follow after the first couple of pages01:10PM EDT- Among the categories in the App Library: suggested apps, recently added, social, Apple arcade, etc01:10PM EDT- Widgets are getting updated as well01:11PM EDT- Widgets now come in variable sizes01:11PM EDT- Widgets can now be placed on the home screen as well01:12PM EDT- Also a new widget, called the smart stack, which can flip between multiple widgets01:13PM EDT- Picture-in-picture mode updates01:13PM EDT- PiP videos are becomming fairly freeform on iOS, and can be placed pretty mucn anywhere and hover over applications01:14PM EDT- Siri updates01:14PM EDT- Apple has redesigned the graphical elements of Siri01:14PM EDT- Rather than a dedicated Siri screen, Siri now acts more like a notification01:15PM EDT- Now on stage: Yael Garten, discussing more about Siri01:15PM EDT- (Apple is decidedly trailing the AI assistant race, so the company needs to catch up)01:16PM EDT- Apple is introducing a new translation app. Named \"Translate\", of course01:17PM EDT- Voice and text translation between 11 languages, including English, Mandarin, and Russian01:17PM EDT- Includes a conversation view for two people to use on a single device01:17PM EDT- Now back to Craig and the Messages app01:18PM EDT- Now on stage: Stacey Lysik01:18PM EDT- Discussing how Apple has updated Messages and tracking conversations01:18PM EDT- Conversations can now be pinned01:18PM EDT- Memojis are back01:19PM EDT- Hair, face coverings, and more customization options01:19PM EDT- Messages is getting inline replies for group conversations01:20PM EDT- And Mentions as well. Type their name to direct a message01:20PM EDT- New visual indicators for group chats as well01:20PM EDT- Back to Craig01:21PM EDT- Now on to Maps01:21PM EDT- Recapping Apple's recent updates to their underlying map, which launched this last year01:22PM EDT- The new map base is coming to more countries, including the UK, Ireland, and Canada01:22PM EDT- Now on stage: Meg Frost01:22PM EDT- Discussing new Maps features in iOS 1401:22PM EDT- Focusing right now on discoverability and recommendations01:23PM EDT- iOS 14 Maps is adding support for cycling directions01:24PM EDT- Lets users know about hazards such as stairs, large hills, etc01:24PM EDT- Cycling will start with the major cities, and be added to more locations from there01:24PM EDT- iOS 14 Maps is also getting more information for electric vehicles, with \"EV routing\" to take into account EV charger access01:25PM EDT- Congestion zone and green zone info is being added as well01:25PM EDT- Back to Craig with Carplay01:25PM EDT- Now available on 97% of new cars sold in the US, and 80% worldwide01:26PM EDT- Now on stage: Emily Schubert01:26PM EDT- Discussing using Apple tech to replace car keys01:27PM EDT- Open and start your car with your iPhone01:27PM EDT- These digitial keys are kept in the iPhone's secure element01:28PM EDT- And can be integrated with things like parental controls to limit access01:28PM EDT- And this is being added to iOS 13 as well01:28PM EDT- Expecting to see support for the new standard in cars starting next year01:28PM EDT- Now on to the App Store01:28PM EDT- Rolling a video01:29PM EDT- New feature: App Clip01:30PM EDT- \"A small part of an app\" that's light and fast, and easy to discover01:30PM EDT- It sounds like fast micro-apps for specific purposes01:30PM EDT- App Clips are mostly contextual based, but can also be launched from the App Library01:31PM EDT- App Clips can be discovered and launched from the web01:31PM EDT- Apple is making their own QR code system to indicate/trigger App Clip support, which they're calling an App Clip code01:32PM EDT- App Clips need to be less than 10MB in size to launch quickly, but have full SDK access01:32PM EDT- And that's iOS 1401:32PM EDT- Now quickly recapping the new iOS 14 features01:32PM EDT- Switching gears to iOS's sibling, iPadOS01:33PM EDT- iPadOS 1401:34PM EDT- Continued emphasis on making better use of the iPad's large display and other unique features01:34PM EDT- Now on stage: Josh Shaffer01:34PM EDT- Discussing iPadOS 14 enhancement01:35PM EDT- Demonstrating the latest version of the Photos app01:35PM EDT- Photos is getting a sidebar to access the core functionality of the app01:35PM EDT- The sidebar has been added to other apps as well, such as Notes and Files01:36PM EDT- Also some UI tweaks, such as moving some controls to the top of apps01:36PM EDT- Back to Craig01:36PM EDT- Discussing Siri updates for iPadOS01:37PM EDT- Meanwhile the calls interface is being updated as well01:37PM EDT- Incoming calls are displayed as notifications instead of full-screen events01:37PM EDT- This change is coming to iOS as well01:37PM EDT- And works for native calls, Facetime, and third-party apps01:38PM EDT- Search has been rebuilt \"from the ground up\"01:38PM EDT- Demonstrating using the updated search to launch apps, find contacts, etc01:39PM EDT- And of course, iPadOS 14 will include all of iOS 14's updates01:39PM EDT- Now on to Apple Pencil01:39PM EDT- Craig is recapping the benefits of freely inked text and drawings01:40PM EDT- For this year, Apple is improving written text support01:40PM EDT- Now on stage: Jenny Cheng01:40PM EDT- Demonstrating the new pencil feature updates01:40PM EDT- Auto shapes when drawing01:41PM EDT- Handwriting recognition01:41PM EDT- And scribble to use handwriting to fill in a text field that can be typed in01:42PM EDT- Scribble recognizes English as well as Chinese characters01:43PM EDT- Apple's come a long way from the days of Eat Up Martha01:43PM EDT- Now on to AirPods01:43PM EDT- Now on stage: Mary-Ann Ionascu01:43PM EDT- AirPods can now seamlessly move between Apple devices01:44PM EDT- So Mac to iPad to iPhone and back again01:44PM EDT- Spatial audio is coming to AirPods Pro01:44PM EDT- Sounds like Apple is adding audio virtualization/HRTF support01:46PM EDT- Apple is using the gyro and accelerometer to allow the audio field to shift with the user's head01:46PM EDT- Spatial audio works with 5.1, 7.1, and Dolby Atmos content01:46PM EDT- Now on stage: Kevin Lynch01:46PM EDT- Discussing Apple Watch01:47PM EDT- Now over 20,000 watchOS apps in the App Store01:47PM EDT- In watchOS 7, apps can support multiple complications01:47PM EDT- So displaying several complications on a watch face01:48PM EDT- New feature: watch face sharing01:48PM EDT- Quickly demoing the feature01:49PM EDT- Developers can offer pre-configured watch faces from their apps01:49PM EDT- Now to Maps on watchOS01:49PM EDT- Cycling directions is coming to this version of Maps as well01:50PM EDT- Now on stage: Julz Arney01:50PM EDT- Discussing workout enhancements for watchOS 701:50PM EDT- The workout app now tracks dancing01:51PM EDT- Dance uses both accel and gyro data as well as heartrate data to try to keep better track of motions and calories burnt01:51PM EDT- All of which can be tracked in the Activity app01:51PM EDT- The app has been renamed to Fitness01:52PM EDT- Another new watchOS 7 feature: sleep tracking01:52PM EDT- New feature: Wind Down01:53PM EDT- To help users transition towards going to bed01:53PM EDT- Can turn on do not disturb mode on the phone, putting an Apple Watch into sleep mode, etc01:54PM EDT- All of these sleep enhancements are also available on iOS 14, without a watch01:54PM EDT- watchOS 7 is adding hand washing detection01:55PM EDT- Figure out if the user is washing their hands based on motions and audio01:56PM EDT- Back to Craig01:56PM EDT- Next subject: Privacy01:57PM EDT- Apple has taken a very strict tack on maintaining user privacy, and they are keeping this going01:57PM EDT- Quickly recapping Apple's privacy stance on data collection, control, and where it's stored01:57PM EDT- Also recapping Sign In With Apple, which was launched last year. 200mil accounts created thus far01:58PM EDT- Now on stage: Katie Skinner01:59PM EDT- Adding the ability to share your approximate location with apps, instead of just the precise location01:59PM EDT- New tracking control features for apps01:59PM EDT- Apple is going to require developers to self-report their privacy practices to the App Store01:59PM EDT- The info will be published on App Store product pages02:00PM EDT- Now on to Home technology02:00PM EDT- Which has taken on a new importance with so many people stuck at home02:01PM EDT- Apple's Home focus: ease of use, private, and working together seamlessly02:01PM EDT- Now on stage: Yah Cason with HomeKit02:02PM EDT- Apple has partnered with Google, Amazon, and others to create a new home automation standard02:02PM EDT- iOS 14 is adding suggested automations when adding a new IoT device02:03PM EDT- iOS 14 is adding Adaptive Lighting for RGB lightbulbs; changing the color over the day02:03PM EDT- HomeKit cameras will get activity zones and face recognition02:04PM EDT- HomeKit camera integration with tvOS 14 as well02:04PM EDT- Now on stage: Cindy Lin with tvOS02:04PM EDT- \"Apple TV goes beyond video\"02:05PM EDT- Expanding multi-user support for games on Apple TV02:05PM EDT- Adding support for Xbox Elite 2 and Xbox adaptive controllers02:05PM EDT- tvOS is getting the same Picture-in-Picture enhancements from iOS 1402:06PM EDT- Now pitching Apple's TV streaming service02:06PM EDT- Apple+ coming to Sony and Vizio TVs this summer02:06PM EDT- And of course, Apple will be adding new content02:07PM EDT- (We're AnandTech, not AnandTV. But when you say Foundation, you have my attention)02:08PM EDT- Apple's Foundation adaption is coming out next year02:09PM EDT- Now on to what's likely to be the big subject of the day: macOS02:09PM EDT- \"What should we call [the next version of macOS]?\"02:10PM EDT- macOS Big Sur02:10PM EDT- New designs and major updates to many of the apps on the platform02:10PM EDT- Design: making the biggest change since the introduction of Mac OS X02:10PM EDT- Rolling a video02:11PM EDT- \"Our goal was to bring even more clarity to the software\"02:12PM EDT- Focusing on consistency, clarity, and simplicity02:12PM EDT- (This video is sure making Big Sur look a lot like Big iOS)02:13PM EDT- Craig is back to demo the new macOS02:13PM EDT- Starting with the Dock02:13PM EDT- New/refined icons for a lot of apps02:14PM EDT- Quickly flipping through Finder, Mail, and Photos02:14PM EDT- Refreshed the designs of all of the apps02:14PM EDT- The Menu Bar is now translucent02:14PM EDT- macOS gets Control Center02:15PM EDT- Display brightness, volume, dark mode, etc02:15PM EDT- Items from Control Center can be added to the Menu Bar as well02:15PM EDT- iOS 14's redesigned widgets are coming to macOS as well02:16PM EDT- Now on to individual app updates for the Mac02:16PM EDT- Messages is getting new features such as search and a redesigned photo picker02:17PM EDT- Memoji can now be created/edited on the Mac as well02:17PM EDT- So it's reaching parity with the iOS version of Messages02:17PM EDT- macOS is getting a new version of the Maps app02:18PM EDT- It looks like a fresh port of the iOS version02:18PM EDT- On to Mac Catalyst02:19PM EDT- Latest version of Catalyst will give developers more options to use the full real estate of the Mac desktop02:19PM EDT- And confirming that the new version of macOS Maps is a Catalyst app02:19PM EDT- And Messages02:20PM EDT- Now for Safari02:20PM EDT- \"The biggest update to Safari since it was first introduced\"02:20PM EDT- Apple has continued to work on improving Safari's JS execution speed02:20PM EDT- And they're claiming 50% faster page loading than Chrome02:21PM EDT- Safari is adding more privacy reporting info to help inform users02:21PM EDT- And is checking saved passwords against known compromised passwords/sites02:22PM EDT- Apple is adding more fine-grained controls for extensions as well, so that they don't need such wide-ranging access02:22PM EDT- Now on stage: Beth Dakin02:23PM EDT- New feature: customizable start page02:23PM EDT- iCloud tabs and reading lists can be added to the start page02:23PM EDT- Demonstrating Web Extension support02:24PM EDT- Allow extensions to be enabled for just a day, just for a site, etc02:25PM EDT- (Google is undoubtedly thankful that most desktop users don't use Safari)02:25PM EDT- Getting improved tab control as well. Very Firefox-like02:26PM EDT- And that's macOS Big Sur02:26PM EDT- \"But these changes are just the beginning\"02:26PM EDT- Back to Tim Cook02:27PM EDT- \"Today is going to be a truly historic day for the Mac\"02:27PM EDT- The Mac has had 3 major transitions in its history: PPC, Mac OS X, and then Intel/x8602:28PM EDT- Confirmed: the Mac is transitioning to Apple's silicon (Arm)02:28PM EDT- And so it begins02:28PM EDT- Tim is talking up the advantages of being vertically integrated02:29PM EDT- (Apple gets to control it all. Software, the hardware, the app ecosystem)02:29PM EDT- First up: talking about the history of Apple's SoC development02:29PM EDT- Apple's teams have delivered 10 generations of A-series SoCs02:30PM EDT- Apple reckons CPU performance has improved 100x over that period02:30PM EDT- And Apple's side-family of AX-series of SoCs for the iPad as well02:30PM EDT- \"The iPad core is faster than the vast majority of PC laptops\"02:31PM EDT- No new information quite yet; Apple is laying out the case for why their hardware engineering is so solid, and why it will enable them to pull this switch off02:31PM EDT- \"Will give a whole new level of performance\"02:32PM EDT- Apple wants to deliver desktop performance with notebook-level power consumption02:32PM EDT- And they believe they can do it with their own SoCs02:32PM EDT- Especially with the help of their highly integrated power management tech02:33PM EDT- This also moves various A-series features like the ISP and neural engine to first-class status in the Mac ecosystem02:33PM EDT- Apple is making a series of SoCs specifically for the Mac02:33PM EDT- Which will have a common architecture with the other Apple SoCs02:34PM EDT- Now back to Craig to talk about the tech in macOS Big Sur to help with the transition02:34PM EDT- Apple has made sure all of their own apps are up and running as Arm apps02:35PM EDT- Ideally, developers will be able to get apps up and running in days02:35PM EDT- Apple has introduced a new binary format, Universal 2, to include both x86 and Arm binaries (fat binary's revenge)02:36PM EDT- The Apple Development Platform is using the A12Z02:36PM EDT- All of the Big Sur features demonstrated earlier were being run on the development platform02:36PM EDT- Now showing an Arm build of Microsoft Word02:36PM EDT- And Excel02:37PM EDT- Basically confirming that Apple's most critical application developers are on-board with the ISA change02:37PM EDT- And here's Photoshop02:38PM EDT- Final Cut Pro is running on the dev platform, too02:38PM EDT- Craig is showing off several features and emphasizing how smooth the performance is02:40PM EDT- Apple is also taking steps to ensure the transition is seamless, even for apps that haven't been updated02:40PM EDT- App translation at installation, and support for dynamic JIT02:41PM EDT- Docker support as well02:41PM EDT- Now on stage: Andreas Wendker02:41PM EDT- Running Maya02:41PM EDT- Running via Rosetta 202:42PM EDT- And running the Mac App Store release of Shadow of the Tomb Raider02:42PM EDT- (I'm curious what this means for games using Wine)02:42PM EDT- Now showing off a Linux VM using Parallels Desktop02:43PM EDT- And because they're Apple SoCs, iPad and iPhone apps can be run directly on macOS02:44PM EDT- Most iOS apps will just work without any further work from developers02:44PM EDT- Now recapping everything: Universal 2, Rosetta 2, and Virtualization tech02:45PM EDT- Apple is launching a quick start program to help developers get started on the Arm transition right away02:45PM EDT- Documentation, forums access, and a DTK dev box with an A12Z in a Mac Mini chassis02:45PM EDT- 16GB RAM, 512GB SSD02:45PM EDT- Developers can begin applying to the program today02:46PM EDT- Back to Tim02:46PM EDT- \"What's the timeline?\"02:47PM EDT- Shipping the first customer Mac with Apple silicon by the end of this year02:47PM EDT- Apple expects to transition their products over the next 2 years02:47PM EDT- Plenty of ongoing x86 Mac support to come, with some new Intel-based Macs yet to come02:48PM EDT- As for software, developer betas today. Public betas in July. Launching in the fall02:48PM EDT- And that's a wrap. Thanks for joining us. Now to dig more into the Arm announcement\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15870/the-apple-wwdc-2020-keynote-live-blog\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Mobile Flagship Phone Camera Overview 2020 H1: Still Picture Battle\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-06-18T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/15845/mobile-phone-camera-overview-2020-h1\n",
      "Content: These days whenever you select a flagship smartphone, you generally get more or less the same fundamental formula no matter the vendor you chose. It’s a glass slab with a screen, and more often than not even the internal hardware powering the phones isn’t all that different, with just a few exceptions. Whilst most vendors try to differentiate themselves in their designs and ergonomics – some with more success than others – the one aspect where smartphones can still be very different from each other is their cameras.This year we’ve seen smartphones with more variety than ever in terms of their camera setups. The last few years has seen an explosion of fast-paced innovation in the image capture abilities of smartphones, with vendors focusing on this last aspect of a phone where they can truly differentiate themselves from others, and try to one-up the competition.We’re halfway through 2020, and almost all vendors have released their primary flagship devices – many of which we still had yet to cover in full reviews. This was a perfect opportunity to put all of the new generation devices against each other and compare their cameras systems to really showcase just how different (or similar) they are to each other. Today’s article is a battle-royale for smartphone photography, providing an apples-to-apples comparison across the most important devices available today.The Phones - A Camera OverviewBefore we go into the picture reviews and analysis of the different scenarios, it’s good to have a refresher of what the different phones sport in terms of their hardware, and what makes them stand out from the rest of the crowd.Apple’s iPhone 11 Pro & iPhone SE (Initial Review,SE Review)Apple’s phones here represent the oldest devices in the round-up, being released last September. The iPhone 11 Pro was a major step-up for Apple in the camera department – both design-wise and hardware-wise. On the main camera sensor Apple still went with a regular 12MP Bayer sensor, employing 1.4µm pixels as its predecessors, but for the first time we see the use of dual-photodiodes which can serve as full sensor phase-detect auto-focus points as well as aid wider dynamic range exposures.The main camera is joined by a 12MP telephoto module, as well as for the first time for Apple a ultra-wide-angle module that had been the great rage in 2019. All in all, Apple’s hardware wasn’t all that innovative as there’s nothing here that we haven’t seen before. Where the iPhone does differ is in its image processing, as the new A13 chip here is able to employ a stronger ISP and greater computing performance, allowing new features such as night mode and deep fusion image merging for better details.In April this year, Apple also launched the iPhone SE. This budget iPhone uses the same camera hardware as on the iPhone 8, but takes advantage of some of the new image processing prowess of the A13 chip. It’s a simple single camera setup, so it’s a pretty straightforward design.Samsung's Galaxy S20+ and Galaxy S20 Ultra (Initial Review)Samsung’s Galaxy S20 series also had a huge year in 2020 in terms of the camera hardware. Starting off with the new Galaxy S20+, Samsung here employed an unusual but innovative setup, using a primary module with a 12MP sensor with 1.8µm pitch pixels, with 26mm equivalent focal length optics. There’s a second 28mm equivalent focal length module on the phone which is very unusual. The sensor here captures up to 64MP pictures and also serves as the video camera for 8K recording. In essence, the phone has no real optical telephoto module, but rather uses the higher resolution sensor to be able to crop in with greater detail. Both these sensors are of the newer generation 1/1.7” size which allow them to capture more light.The ultra-wide module also saw an upgrade to a bigger sensor and upgrades the pixel pitch to 1.4µm, however we lose resolution as this year it’s only 12MP.The Galaxy S20 Ultra’s camera setup is quite extravagant in its size. With a 108MP 1/1.33” sensor with 0.8µm pixels it’s amongst the biggest sensors out there. Samsung opted to go for a 3x3 Nona-Bayer colour filter setup which is unique, and means that the sensor captures 12MP pictures in every-day scenarios.The zoom module on the S20U is also huge: Samsung crammed in the IMX586 into a periscope telephoto module, resulting in a 4x optical zoom / 103mm equivalent focal length. Thanks to the 48MP sensor, this can be cropped a lot, achieving good quality up to around 10x zoom.Google's Pixel 4 (Initial Review)The other 2019 device in this list is Google’s Pixel 4 phone. Google’s forte is clearly on the software side, showcasing some excellent image processing. On the hardware side, things are a bit simpler. We’re seeing a 12MP main sensor that’s seemingly quite older, as well as telephoto lens that’s actually a 43mm equivalent with 1.6x zoom, compensated by the fact that the sensor is a 16MP unit. Such an approach allows for better intermediate zoom level quality. What’s really missing here is an ultra-wide-angle module, and alongside the new iPhone SE, it’s the only other phone in this comparison lacking such a camera view.OnePlus' 8 & 8 Pro (Announcement)OnePlus this year also had some big improvements in the camera department – at least the Pro version did. The OnePlus 8 Pro features amongst the first uses of the new Sony IMX689 sensor which is a 1/1.4” module featuring 48MP 1.12µm pixels in a quad-bayer colour filter layout (2.44µm 12MP binned). It’s amongst the more sensible choices and represents a compromise between the low-MP vs high-MP designs out there. There’s a 3x optical telephoto module coming at 8MP resolution, and the ultra-wide-angle actually adopts the IMX586 – last year’s popular sensor at 48MP (0.8µmnative / 1.6µm binned pixels).The regular OnePlus 8 sees this same sensor on its main camera – in essence here nothing changed compared to the OnePlus 7 Pro except that it’s lacking a dedicated telephoto camera, instead using the main camera sensor in its 48MP in a crop mode to get to 2x magnification pictures with little quality loss.OnePlus this year has seemingly worked a lot on their image processing, resulting in better quality than prior years.Huawei's Mate 30 Pro & P40 Pro (Initial Review,Announcement)Huawei’s efforts over the last years in the P-series can be strongly attributed to sparking the recent boom in smartphone camera advances, having been the first to popularise high-megapixel sensor usage as well as the first to come to market with computational photography modes such as Night Mode. The company has really been pushing it in terms of both hardware and software.The Mate 30 Pro was released late last year and uses a 40MP RYYB sensor – still a large 1/1.7” unit. There’s an 8MP 3x telephoto module for further reach, and the special thing about Huawei’s new phones is that the ultra-wide-angle is actually a 3:2 format sensor rather than the usual 4:3 aspect ratio, resulting in a different field of view than what you’re normally used to. This 40MP sensor is still very big and is the largest of this kind on the market.The newer P40 Pro updates the main camera sensor to a new 50MP 1/1.28” unit, making this the largest camera sensor out there in the market. Each pixel is natively 1.22µm in dimensions and Huawei is binning it to 2.44µm as it’s a quad-Bayer colour filter. The odd thing here is that the P40 Pro by default takes 27mm equivalent cropped wide-angle pictures even though the lens module is actually 23mm wide. Even weirder is that the phone scales this crop up back to 12MP. Something to keep in mind for the evaluation section.The ultra-wide-angle is the same as on the Mate 30 Pro, but the telephoto module is another periscope design, featuring 5x optical magnification with a 12MP RYYB sensor, making that also the first and only of its kind featuring such a colour filter array on a telephoto module.LG's V60 ThinQ (Announcement)LG’s V60 is another dual-camera setup phone like the Pixel 4, with the difference being that LG opted to skip the telephoto instead of their pioneering use of ultra-wide-angle modules.On the main camera, we’re seeing a new 64MP 1/1.7” sensor with 0.8µm pixels, binning down to 16MP for regular pictures. LG’s lack of a telephoto is compensated by the fact that the main sensor is of such a high resolution, and will actually capture a 16MP crop at 2x magnification while in the native resolution mode of the sensor, only losing a bit of dynamic range but still retaining near full resolution sharpness, making this in my view a quite well-working solution to the lack of a telephoto.The ultra-wide-angle is a 13MP unit with a 1.0µm pixel pitch sensor and a 117° field of view (15mm equivalent).Xiaomi's Mi 10 Pro (Announcement)Xiaomi was the first vendor to bring to market Samsung’s new 108MP HMX sensor. It’s of the same huge 1/1.3” size as that found in the S20 Ultra, but the difference here is that Xiaomi opts for a regular quad-Bayer colour filter array, meaning that the phone on this camera actually captures 27MP images natively, notably more than any other phone.What Xiaomi also does on the telephoto end is also extremely interesting. Instead of going with a periscope module, they’ve managed to use a regular module design to achieve a 5x optical zoom module thanks to a small 1.0µm 8MP sensor. Xiaomi also thought of the issue of the quality gap between the focal ranges of the modules and bridged this with a traditional 2x optical zoom module coming in at 12MP.The fourth camera is the ultra-wide-angle with a 20MP unit with 1.0µm pixels. Generally, Xiaomi’s approach to the camera setup seems extremely level-headed as it avoids many compromises we’ve seen from more ambitious designs such as in the S20 Ultra or other periscope camera designs.OPPO’s Reno3 Pro & Reno3 Pro 5GAlthough not OPPO’s flagship phones for 2020, the two units are interesting as they’re featuring similar camera setups configured by the same vendor, but coming in a MediaTek and Qualcomm SoC variants. The telephoto module and ultra-wides are the same on both, a 13MP f/2.4 2x optical with 1.0µm pixel pitches and 8MP f/2.2 13mm equivalent with 1.4µm pixel sensors.The main sensor differs between the units. The Pro 5G comes with a smaller 1/2.0” sensor with 0.8µm pixels at 48MP, whilst the European Reno 3 Pro comes with a 64MP 1/1.7” sensor with the same pixel pitch size. Both do quad-Bayer binning to respectively 12 and 16MP resolution shots.Reference Camera - Fujifilm X-T30Over the last few years in our camera reviews there had always been a lot of discussions about the colour reproduction of various smartphones – which one was accurate and which wasn't? As an addition to this year’s reviews I had started to include a mirrorless camera to act as a reference point for this.I’m using a Fujifilm X-T30 which uses a 26.1MP APS-C format sensor (3.77µm pixel pitch). The lens is a fairly standard and common 18-55mm f/2.8-4.The pictures were captured in RAW format and processed in Capture One for manual recovery of dynamic range and exposure to as accurate as I could based on the scenes. Colours remained unaltered.Goal of the ArticleThe goal of the evaluation is mainly surrounding the technical aspects that each smartphone vendor has adopted in their design. We’re looking to juxtapose the different camera sensor and optical technologies between the various vendors and try to come to some sort of verdict of the best implementations. We'll try to pin down the camera’s strengths and weaknesses, both from a hardware and software perspective.The evaluation here is just based on technical landscape still picture photography. Portrait or video evaluations aren’t in the scope of the piece (for various practical reasons).\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15845/mobile-phone-camera-overview-2020-h1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Lion Semi: How High-Efficiency ICs Enable Fast-Charging\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-06-08T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15834/lion-semi-how-highefficiency-ics-enable-fastcharging\n",
      "Content: The last few years have seen quite a large shift in the mobile market as smartphone vendors have engaged in a literal arms-race aiming for the fastest charging phones possible. In only a few years we’ve seen phones go from what used to be considered “fast charging” at rate of up to 18W to new advertised 65W rates. What a lot of consumers however often misunderstand, is that these new fast-charging systems aren’t primarily enabled by new battery technologies, but rather by new advances in charging systems that have become more and more efficient.A smartphone’s thermal envelope is really only about 4W, maybe 5W in the very biggest form-factor devices nowadays. This would the be power dissipation that the phone is able to handle before its temperature rises to undesirable figures, both for safe of use for the consumer, as well as safety for the internal components.This puts indirectly a fundamental physics limit to the charging speed of a phone; charging is always a lossy transfer of energy – be it on one side from the voltage conversion needed from the external charger to the internal battery charging voltage, or from losses from the internal resistance of the battery itself. At even a 99% charging efficiency, a 100W charging system would mean a 1W loss and heat dissipation in a phone. Given that a phone only has that fixed 4-5W thermal envelope – and you probably would still want to be able to actually use the phone while charging, it means that charging systems need to be as efficient as possible to get near that 100% mark to be able to reach higher wattage charging speeds.There are many different solutions to increasing charging efficiency, but today’s topic surrounds a younger start-up called Lion Semiconductor that specialises in a very different voltage conversion technology for power ICs, called switched-capacitor voltage converters. The San Francisco based start-up is seeing some increasing success in today’s mobile market where it enables vendors to achieve some of today’s fast charging phones.The key characteristic of a switched-cap converter as to opposed to more traditional buck-converter based ICs is that they are able to achieve higher conversion efficiencies with lower component complexity. A traditional buck converter as you’d find in any regular PMIC power rail on PC or mobile phones operates by pulse-wave modulation of an input voltage into an inductor that stores and “buffers” the energy into a lower “smoothed” output voltage. The problem with such designs is that one requires large PCB space as the magnetic components can be quite large in physical size. One can reduce the inductor size by increasing the PWM operating frequency, but this then increases the switching related power loss on the circuit. A typical buck converter nowadays ranges in the 90-94% efficiency, which isn’t enough for use-cases such as high-wattage fast charging.A switched-cap design makes away with the magnetic components and simply switches its input power into a small number of capacitors, vastly decreasing the power loss by several factors by up to a factor of 4x. The disadvantage of a switched-cap system is that it cannot operate as a regulator, meaning its voltage conversion is fixed at design, instead of the dynamic voltage that a buck converter can achieve. In the topic of a smartphone, we’re talking mostly about voltage divider switched capacitor designs with a common 2:1 conversion ratio.As noted, we’ve seen fast charging becoming a key marketing differentiation point for various smartphone vendors in the industry. Primarily we’ve seen a ton of push by Chinese-vendors that seemingly have been trying to one-up each other in terms of the achievable charging speeds on an almost bi-yearly rate. This aggressive push has been requiring charging technologies to innovate as they get nearer to the limits of what’s physically possible.Your typical wired input voltage more often than not was found to be around 5V – with some other fast charge system operating at other higher voltages such as 9V in order to circumvent the current limit of USB cables. These fixed voltage chargers would feed into the phone, go through a simple over-voltage protection circuit, and be converted from their input voltage to the lithium battery charge voltage (Usually around 4.4V) by a buck converter. The conversion efficiency here is usually not too great at about 92% efficiency, meaning that for a 18W system that would result in a 1.2W dissipation loss inside of the phone. The conversion loss would be even greater for wireless charging systems that operate at higher voltages.Newer generation phones have tried to solve this power conversion limitation by simply moving the main voltage regulation and conversion circuits to outside of the phone. USB PPS (programmable power supply) is the newest wide-spread standard that allows the actual chargers to regulate their output voltage in a fine-grained manner. In essence this eliminates the problem of limited power dissipation of a phone, but we need to add some complexity to the internal charging circuits of the phone. Because we’re still limited by the USB cable currents, to achieve higher power input the input voltage with a PPS or variable output voltage circuit is still bigger than the nominal voltage of the lithium battery, most of the time at a 2:1 ratio.Such an implementation that precedes the USB PD PPS spec is Oppo’s SuperVOOC or OnePlus’ Dash Charge systems which have chargers that track the battery’s current voltage, most often at a 2:1 doubled voltage ratio. For context, usually a discharged lithium battery has a voltage of around 3.8V while fully charged it measures in at 4.3-4.4V. The charger here tracks the increasing voltage curve during charging, for example from a starting point of 7.6V to a fully charged voltage of near 8.8V.It’s here that switched cap converters come into play, reducing that doubled up input voltage back to the regular 3.8-4.4V voltage that the battery requires. The gain here is that within the phone, instead of having a 92% conversion efficiency, we now have a 98% conversion efficiency.This gain can also be notable for newer wireless fast charging systems that operate at even higher input voltages such as 20V. Using a 4:2 converter phones are able to efficiently divide this operating voltage once more before another division reduces it to the battery operating voltage. In essence, a 30W wireless charging system her would only have 1.2W total loss in the phone.Source:TechInsights Xiaomi Mi 10 TeardownLion Semi has such a design with Xiaomi, being employed as the wireless charging conversion IC in newer phones such as the Mi 10 series.Some vendors have opted for even more exotic charging systems in order to achieve even higher charging speeds. 2S battery systems such as used by Oppo use two batteries connected in serial. What this allows is the elimination of any kind of voltage conversion within the phone during charging, and the charger IC here only controls the input current, achieving a 99% charging efficiency.This is how for example Oppo’s newest phones are able to achieve 60W charging power without overheating the phones. The problem with such a dual-battery solution is that while it solves the charging power problem, smartphone components and PMICs are designed to operate with regular lithium battery operating voltages, so there’s a need to convert the higher series voltage of the two batteries down to a regular voltage, re-introducing a 2% efficiency loss, this time not during charging, but during normal operation of the phone. In essence, if Oppo advertises 4000mAh in such a phone, it effectively only ever is able to use ~3920mAh (@98% efficiency) as the rest is never “usable” by the phone other than being lost in conversion. In essence, it’s a compromise between charging speed and battery capacity of the phone.Overall, fast charging is a convenient feature to add to a smartphone as it’s relatively low-complexity in terms of adding to a design, and it solves the problem of battery anxiety by being able to top-up your phone in a very short amount of time.Drawbacks continue to be that these new fast-charging systems are evolving at a much quicker rate than what battery technology is able to support, and will result in more severe capacity degradation over time. Common industry figures are quoted to be around 70% charge over 600 cycles with charging speeds that scale up to 2-3C.Lion Semiconductor, while being the smallest vendor on the field, certainly isn’t the only one providing newer high-efficiency switched-cap solutions to the market as we’re also seeing solutions from the big incumbent vendors such as Dialog. As consumers become more demanding of such fast-charging systems, we’re likely to see wider adoption of such technologies to improve the charging efficiency of consumer electronics beyond just smartphones.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15834/lion-semi-how-highefficiency-ics-enable-fastcharging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ISCA 2020: Evolution of the Samsung Exynos CPU Microarchitecture\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-06-03T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/15826/isca-2020-evolution-of-the-samsung-exynos-cpu-microarchitecture\n",
      "Content: ISCA, the International Symposium for Computer Architecture is an IEEE conference that usually we don’t tend to hear from all that often in the public. The main reason for this is that most sessions and papers tend to be more academically oriented, and thus generally quite a bit further away from the practice of what we see in real products. This year, the conference has changed its format in adding an industry track of sessions, with presentations and papers from various companies in the industry, covering actual commercial products out there in the wild.Amongst the sessions, Samsung’s SARC (Samsung Austin R&D Centre) CPU development team has presented a paper titled “Evolution of the Samsung Exynos CPU Architecture”, detailing the team’s efforts over its 8-year existence, and presented some key characteristics of its custom Arm CPU cores ranging from the Exynos M1, to the most recent Exynos M5 CPU as well as the unreleased M6 design.As a bit of background, Samsung’s SARC CPU team was established in 2011 to develop custom CPU cores that Samsung LSI would then deploy in its Exynos SoCs, ranging from the first-generationExynos 8890released in 2015 in the Galaxy S7, up till the most recentExynos 990with its M5 cores in theGalaxy S20. SARC had completed the M6 microarchitecture beforethe CPU team had gotten news of it being disbandedin October of 2019, effective last December.The ISCA paper is a result of Samsung’s willingness to publish some of the development team’s ideas that were considered worthy of preserving in the public, essentially representing a high-level burn-through of 8 years of development.From M1 to M6: A continuously morphing CPU µarchThe paper presents a gross overview table of the microarchitectural differences between Samsung’s custom CPU cores:The disclosure covers some of the well-known characteristics of the design as had been disclosed by Samsung in its initialM1 CPU microarchitecture deep dive at HotChips 2016, to the more recentM3 deep dive at HotChips 2018. It gives us an insight into the new M4 and M5 microarchitectures that we had measured inour S10andS20 reviews, as well as a glimpse of what the M6 would have been.The one key characteristic of Samsung’s design was over the years, it was based off the same blueprint RTL that was started off with the M1 core in 2011, with continuous improvements of the functional blocks of the cores over the years. The M3 had been a big change in the design, widening the core substantially in several aspects, such as going from a 4-wide design to a 6-wide mid-core.The new disclosures that weren’t public before regard the new M5 and M6 cores. For the M5, Samsung had made bigger changes to the cache hierarchy of the cores, such as replacing private L2 caches with a new bigger shared cache, as well as disclosing a change in the L3 structure from a 3-bank design to a 2-bank design with less latency.The unreleased M6 core that had been in development was seemingly to be a bigger jump in terms of the microarchitecture. The SARC team here had prepared large improvements, such as doubling the L1 instruction and data caches from 64KB to 128KB – a design choice that’s currently only been implemented before by Apple’s CPU cores starting with the A12.The L2 is said to have been doubled in its bandwidth capabilities to up to 64B/cycle, and also there would have been an increase in the L3 from 3 to 4MB.The M6 would have been an 8-wide decode core, which as far as we know would have been the widest commercial microarchitecture that we know of – at least on the decode side of things.Interestingly, even though the core would have been much wider, the integer execution units wouldn’t have changed all that much, just seeing one complex pipeline adding a second integer division capability, whilst the load/store pipelines would have remained the same as on the M5 with 1 load unit, 1 store unit, and one 1 load/store unit.On the floating-point/SIMD pipelines we would have seen an additional fourth unit with FMAC capabilities.The TLBs would have seen some large changes, such as the L1 DTLB being increased from 48 pages to 128 pages, and the main TLB doubling from 4K pages to 8K pages (32MB coverage).The M6 would also have ben the first time since the M3 that the out-of-order window of the core would have been increased, with larger integer and floating-point physical register files, and an increase in the ROB (Reorder buffer) from 228 to 256.One key weakness of the SARC cores seems to still have been present in the M5 and upcoming M6 core, and that’s its deeper pipelines stages resulting in a relatively expensive 16-cycle mispredict penalty, quite higher than Arm’s more recent designs which fall in at 11 cycles.The paper goes into more depth into the branch predictor design, showcasing the core’s Scaled Hashed Perceptron based design. The design had been improved continuously over the years and implementations, improving the branch accuracy and thus reducing the MPKI (mis-predicts per kilo-instructions) continuously.An interesting table that’s showcased is the amount of storage structures that the branch predictor takes up within the front-end, in KBytes:We’re not aware of any other vendor ever having disclosed such figures, so it’s interesting to put things into context of what a modern front-end has to house in terms of storage (and this is *just* the branch predictor).The paper goes onto further detail onto the cores prefetching methodologies, covering the introduction of a µOP cache in the M5 generation, as well as the team’s efforts into hardening the core against security vulnerabilities such as Spectre.Generational IPC Improvements - 20% per year - 2.71x in 6 yearsThe paper further describes efforts by the SARC team to improve memory latency over the generations. In the M4 core, the team had included a load-load cascade mechanism that reduced the effective L1 cycle latency from 4 cycles to 3 on subsequent loads. The M4 had also introduced a path bypass with a new interface from the CPU cores directly to the memory controllers, avoiding traffic through the interconnect, which explains some of the biggerlatency improvements that we’ve seen in the Exynos 9820. The M5 had introduced speculative cache lookup bypasses, issuing a request to both the interconnect and the cache tags simultaneously, possibly saving on latency in case of a cache miss as the memory request is already underway. The average load latency had been continuously improved over the generations, from 14.9 cycles on the M1 down to 8.3 cycles on the M6.In terms of IPC improvements, the SARC team had managed to get to an average of 20% annual improvements over the 8 years of development. The M3 had been in particular a big jump in IPC as seen in the graph. The M5 roughly correlates to what we’ve seen in our benchmarks, at around 15-17% improvement. IPC for the M6 is disclosed at having ended up at an average of 2.71 versus 1.06 for the M1, and the graph here generally seems to indicate a 20% improvement over the M5.During the Q&A of the session, the paper’s presenter, Brian Grayson, had answered questions about the program’s cancellation. He had disclosed that the team had always been on-target and on-schedule with performance and efficiency improvements with each generation. It was stated that the team’s biggest difficulty was in terms of being extremely careful with future design changes, as the team never had the resources to completely start from scratch or completely rewrite a block. It was said that with hindsight, the team would have done different choices in the past with of some of the design directions. This serial design methodology comes in contrast to Arm’s position, having multiple leapfrogging design centres and CPU teams, allowing them to do things such as ground-up re-designs, suchthe Cortex-A76.The team had plenty of ideas for improvements for upcoming cores such as the M7, but the decision to cancel the program was said to have come from very high up at Samsung. The SARC CPU cores were never really that competitive, suffering from diminished power efficiency, performance, and area usage compared to Arm’s designs. WithArm’s latest Cortex-X1 divulged last weekgoing for all-out performance, it looks to me that SARC’s M6 design would have had issues competing against it.The paper's authors are extremely thankful for Samsung’s graciousness in allowing the publication of the piece, and thank the SARC leadership for their management over the years on this “moonshot” CPU project. SARC currently still designs custom interconnects, memory controllers, as well as working on custom GPU architectures.Related Reading:Arm's New Cortex-A78 and Cortex-X1 Microarchitectures: An Efficiency and Performance DivergenceThe Samsung Galaxy S20+, S20 Ultra Exynos & Snapdragon Review: Megalomania DevicesThe Samsung Galaxy S10+ Snapdragon & Exynos Review: Almost Perfect, Yet So FlawedHot Chips 2018: Samsung’s Exynos-M3 CPU Architecture Deep DiveHot Chips 2016: Exynos M1 Architecture Disclosed\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15826/isca-2020-evolution-of-the-samsung-exynos-cpu-microarchitecture\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Russia’s Elbrus 8CB Microarchitecture: 8-core VLIW on TSMC 28nm\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-06-01T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/15823/russias-elbrus-8cb-microarchitecture-8core-vliw-on-tsmc-28nm\n",
      "Content: All of the world’s major superpowers have a vested interest in building their own custom silicon processors. The vital ingredient to this allows the superpower to wean itself off of US-based processors, guarantee there are no supplemental backdoors, and if needed add their own. As we have seen with China, custom chip designs, x86-based joint ventures, or Arm derivatives seem to be the order of the day. So in comes Russia, with its custom Elbrus VLIW design that seems to have its roots in SPARC.Russia has been creating processors called Elbrus for a number of years now. For those of us outside Russia, it has mostly been a big question mark as to what is actually under the hood – these chips are built for custom servers and office PCs, often at the direction of the Russian government and its requirements. We have had glimpses of the design, thanks todocuments from Russian supercomputing events, however these are a few years old now. If you are not in Russia, you are unlikely to ever get your hands on one at any rate. However, it recently came to our attention of a new programming guide listed online for the latest Elbrus-8CB processor designs.The latest Elbrus-8CB chip, as detailedin the new online programming guide published this week, built on TSMC’s 28nm, is a 333 mm2design featuring 8 cores at 1.5 GHz. Peak throughput according to the documents states 576 GFLOPs of single precision, with the chip offering four channels of DDR4-2400, good for 68.3 GB/s. The L1 and L2 caches are private, with a 64 kB L1-D cache, a 128 kB L1-I cache, and a 512 kB L2 cache. The L3 cache is shared between the cores, at 2 MB/core for a total of 16 MB. The processor also supports 4-way server multiprocessor combinations, although it does not say on what protocol or what bandwidth.It is a compiler focused design, much like some other complex chips, in that most of the optimizations happen at the compiler level. Based on compiler first designs in the past, that typically does not make for a successful product. Documents from 2015 state that a continuing goal of the Elbrus design is x86 and x86-64 binary translation with only a 20% overhead, allowing full support for x86 code as well as x86 operating systems, including Windows 7 (this may have been updated since 2015).The core has six execution ports, with many ports being multi-capable. For example, four of the ports can be load ports, and two of the ports can be store ports, but all of them can do integer operations and most can do floating point operations. Four of the ports can do comparison operations, and those four ports can also do vector compute.Elbrus 8CB CoreThis short news post is not meant to be a complete breakdown of the Elbrus capabilities – we have amusingly joked internally at what frequency a Cortex X1 with x86 translation would match the capabilities of the 8-core Elbrus, however users who want to get to grips with the design can open and read the documentation at the following address:http://ftp.altlinux.org/pub/people/mike/elbrus/docs/elbrus_prog/html/index.htmlThe bigger question is going to be how likely any of these state-funded processor development projects are going to succeed at scale. State-funded groups should, theoretically, be the best funded, however even with all the money in the world, engineers are still required to get things done. Even if there ends up being a new super-CPU for a given superpower, there will always be vested interests in an amount of security though obscurity, especially if the hardware is designed specifically to cater to state-secret levels of compute. There's also the added complication of the US government tightening its screws around TSMC and ASML to not accept orders from specific companies - any plans to expand those boundaries could occur, depending how good the products are or how threatened some nations involved feel.Source:Blu (Twitter)Related ReadingGuide to Effective Programming on the Elbrus Platform (2020-05-30)Elbrus, as presented at Russian Supercomputing Days 2015\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15823/russias-elbrus-8cb-microarchitecture-8core-vliw-on-tsmc-28nm\n",
      "Title: Arm Announces Ethos-N78 NPU: Bigger And More Efficient\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-05-27T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/15817/arm-announces-ethosn78-npu-bigger-and-more-efficient\n",
      "Content: Yesterday Arm released the newCortex-A78, Cortex-X1 CPUsand the newMali-G78 GPU. Alongside the new “key” IPs from the company, we also saw the reveal of the newest Ethos-N78 NPU, announcing Arm’s new second-generation design.Over the last few years we’ve seen a literal explosion of machine learning accelerators in the industry, with a literal wild west of different IP solutions out there. On the mobile front particularly there’s been a huge amount of different custom solutions developed in-house by SoC vendors, this includes designs such as from Qualcomm, HiSilicon, MediaTek and Samsung LSI. For vendors who do not have the design ability to deploy their own IP, there’s the possibility of licensing something from an IP vendor such as Arm.Arm’s “Ethos” machine learning IP is aimed at client-side inferencing workloads, originallydescribed as “Project Trillium”and the first implementationseeing life in the form of the Ethos-N77. It’s been a year since the release of the first generation, and Arm has been working hard on the next iteration of the architecture. Today, we’re covering the “Scylla” architecture that’s being used in the new Ethos-N78.From a very high-level view, what the N78 promises is a quite large boost both in performance and efficiency. The new design scales up much higher than the biggest N77 configuration, now being able to offer 2x the peak performance at up to 10TOPs of raw computational throughput.Arm has revamped the design of the NPU for better power efficiency, enabled through various new compression techniques as well as an improvement in external memory bandwidth per inference of up to 40%.Strong points of the N78 are the IP’s ability to scale performance across different configuration options. The IP is available at 4 different performance points, or better said at four different distinct engine configurations, from the smallest config at 1TOPs, to 2, 5 and finally a maximum of 10TOPs. This corresponds to MAC configurations of 512, 1024, 2048 and 4096 units for the totality of the design.The interesting aspect of scaling bigger is that the area efficiency of the IP actually scales better the bigger the implementation, due to probably the fact that the unique fixed shared function blocks area percentage shrinks with the more computation engines the design has.Architecturally, the biggest improvements of the new N78 were in the way it handles data around in the engines, enabling new compression methods for data that not only goes outside the NPU (DRAM bandwidth improvement), but also data movement within the NPU itself, improving efficiency for both performance and power.The new compression and data handling can significantly reduce the bandwidth of the system with an average 40% reduction across workloads – which is an extremely impressive figure to showcase between IP generations.Generational performance uplifts, thanks to the higher performance density and power efficiency are on average 25%, which along with the doubled peak performance configuration means that it has the potential to represent a large boost in end devices.It’s quite hard to analyse NPUs on how they perform in the competitive landscape – particularly here in Arm’s case given that we haven’t yet seen the first generation NPU designs in silicon. One interesting remark that Arm has made, is that in this space, software matters more than anything else, and a bad software stack can possibly ruin what otherwise would be a good hardware design. Arm mentioned they’ve seen vendors adopt their own Ethos IP and dropping competitor designs because of this – Arm says they invest a very large amount of resources into software in order to facilitate customers to actually properly make use of their hardware designs.Arm’s new Ethos-N78 has already been licensed out to customers and they’re taping in their designs with it, with likely the first products seeing the light of day in 2021 at the earliest.Related Reading:Arm Announces New Ethos-N57 and N37 NPUs, Mali-G57 Valhall GPU and Mali-D37 DPUARM Details \"Project Trillium\" Machine Learning Processor ArchitectureImagination Goes Further Down the AI Rabbit Hole, Unveils PowerVR Series3NX Neural Network AcceleratorCEVA Announces NeuPro-S Second-Generation NN IPCadence Announces Tensilica Vision Q7 DS\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15817/arm-announces-ethosn78-npu-bigger-and-more-efficient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm's New Cortex-A78 and Cortex-X1 Microarchitectures: An Efficiency and Performance Divergence\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-05-26T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15813/arm-cortex-a78-cortex-x1-cpu-ip-diverging\n",
      "Content: 2019 was a great year for Arm. On the mobile side of things one could say it was business as usual, as the company continued to see successes with its Cortex cores, particularlythe new Cortex-A77which we’ve now seen employed in flagship chipsetssuch as the Snapdragon 865. The bigger news for the company over the past year however hasn’t been in the mobile space, but rather in the server space, where one can todayrent Neoverse-N1 CPUssuch asAmazon’s impressive Graviton2 chip, with more vendors such asAmpere expected to releasetheir server products soon.While the Arm server space is truly taking off as we speak, aiming to compete against AMD and Intel, Arm hasn't reached the pinnacle of the mobile market – at least, not yet. Arm’s mobile Cortex cores have lived in the shadow of Apple’s custom CPU microarchitectures over the past several years, as Apple has seemingly always managed to beat Cortex designs by significant amounts. While there’s certainly technical reasons to the differences – it was also a lot due to business rationale on Arm’s side.Today for Arm’s 2020 TechDay announcements, the company is not just releasing a single new CPU microarchitecture, but two. The long-expected Cortex-A78 is indeed finally making an appearance, but Arm is also introducing its new Cortex-X1 CPU as the company’s new flagship performance design. The move is not only surprising, but marks an extremely important divergence in Arm’s business model and design methodology, finally addressing some of the company’s years-long product line compromises.The New Cortex-A78: Doubling Down on EfficiencyThe new Cortex-A78 isn’t exactly a big surprise – Arm hadfirst publicly divulged the Hercules codenameover two years ago when they had presented the company’s performance roadmap through 2020. Two years later, and here we are, with the Cortex-A78 representing the third iteration of Arm’s new Austin-family CPU microarchitecture,which had started from scratch with the Cortex-A76.The new Cortex-A78 pretty much continues Arm’s traditional design philosophy, that being that it’s built with a stringent focus on a balance between performance, power, and area (PPA). PPA is the name of the game for the wider industry, and here Arm is pretty much the leading player on the scene, having been able to provide extremely competitive performance at with low power consumption and small die areas. These design targets are the bread & butter of Arm as the company has an incredible range of customers who aim for very different product use-cases – some favoring performance while some other have cost as their top priority.All in all (we’ll get into the details later), the Cortex-A78 promises a 20% improvement in sustained performance under an identical power envelope. This figure is meant to be a product performance projection, combining the microarchitecture’s improvements as well as the upcoming 5nm node advancements. The IP should represent a pretty straightforward successor to the already big jump that were the A76 and A77.The New Cortex-X1: Breaking the Design Constraint ChainsArm’s existing business model was aimed at trying to create a CPU IP that covers the widest range of customer needs. This creates the problem that you cannot hyper-focus on any one area of the PPA triangle without making compromises in the other two. I mentioned that Arm’s CPU cores have for years lived in the shadow of Apple’s CPU cores, and whilst for sure, the Apple's cores were technical superior, one very large contributing factor in Arm's disadvantage was that the business side of Arm just couldn’t justify building a bigger microarchitecture.As the company is gaining more customers, and is ramping up R&D resources for designing higher performance cores (with the server space being a big driver), it seems that Arm has finally managed to get to a cross-over point in their design abilities. The company is now able to build and deliver more than a single microarchitecture per year. In a sense, we sort of saw the start of this last year withthe introduction of the Neoverse-N1 CPU, already having some more notable microarchitectural changes over its Cortex-A76 mobile sibling.Taking a quick look at the new Cortex-X1, we find the X1 higher up in Arm’s Greek pantheon family tree of CPU microarchitectures. Codenamed Hera, the design at least is named similarly to its Hercules sibling, denominating their close design relationship. The X1 is much alike the A78 in its fundamental design – in fact both CPUs were created by the same Austin CPU design team in tandem, but with the big difference that the X1 breaks the chains on its power and area constraints, focusing to get the very best performance with very little regard to the other two metrics of the PPA triangle.The Cortex-X1 was designed within the frame of a new program at Arm, which the company calls the “Cortex-X Custom Program”. The program is an evolution of what the company had previously already done with the “Built on Arm Cortex Technology” program released a few years ago. As a reminder, that license allowed customers to collaborate early in the design phase of a new microarchitecture, and request customizations to the configurations, such as a larger re-order buffer (ROB), differently tuned prefetchers, or interface customizations for better integrations into the SoC designs. Qualcomm was the predominant benefactor of this license, fully taking advantage of the core re-branding options.The new Cortex-X program is an evolution of the BoACT license, this time around making much more significant microarchitectural changes to the “base” design that is listed on Arm’s product roadmap. Here, Arm proclaims that it allows customers to customize and differentiate their products more; but the real gist of it is that the company now has the resources to finally do what some of its lead customers have been requesting for years.One thing to note, is that while Arm names the program the “Cortex-X Custom Program”, it’s not to be confused with actual custom microarchitectures by vendors with an architectural license. The custom refers to Arm’s customization of their roadmap CPU cores – the design is still very much built by Arm themselves and they deliver the IP. For now, the X1 IP will also be identical between all licensees, but the company doesn’t rule out vendor-specific changes the future iterations – if there’s interest.This time around Arm also maintains the marketing and branding over the core, meaning we’ll not be seeing the CPU under different names. All in all, the whole marketing disclosure around the design program is maybe a bit confusing – the simple matter of fact is that the X1 is simply another separate CPU IP offering by Arm, aimed at its leading partners, who are likely willing to pay more for more performance.At the end of the day, what we're getting are two different microarchitectures – both designed by the same team, and both sharing the same fundamental design blocks – but with the A78 focusing on maximizing the PPA metric and having a big focus on efficiency, while the new Cortex-X1 is able to maximize performance, even if that means compromising on higher power usage or a larger die area.It’s an incredible design philosophy change for Arm, as the company is no longer handicapped in the ultra-high-tier performance ring with the big players such as Apple, AMD, or Intel – all whilst still retaining their bread & butter design advantages for the more cost-oriented vendors out there who deliver hundreds of millions of devices.Let’s start by dissecting the microarchitecture changes of the new CPUs, starting off with the Cortex-A78…\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15813/arm-cortex-a78-cortex-x1-cpu-ip-diverging\n",
      "Title: Arm Announces The Mali-G78 GPU: Evolution to 24 Cores\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-05-26T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15816/arm-announces-the-malig78-evolution-to-24-cores\n",
      "Content: Today as part of Arm’s 2020 TechDay announcements, alongside the release of the brand-new Cortex-A78 and Cortex-X1 CPUs, Arm is also revealing its brand-new Mali-G78 and Mali-G68 GPU IPs.Last year, Arm had unveiled the new Mali-G77 which was the company’s newest GPU design based on a brand-new compute architecture called Valhall. The design promised major improvements for the company’s GPU IP, shedding some of the disadvantages of past iterations and adapting the architectures to more modern workloads. It was a big change in the design, with implementations seen in chips such as the Samsung Exynos 990 or the MediaTek Dimensity 1000.The new Mali-G78 in comparison is more of an iterative update to the microarchitecture, making some key improvements in the matter of scalability of the configuration as well as balance of the design for workloads, up to some more radical changes such as a complete redesign of its FMA units.On the scalability side, the new Mali-G78 now goes up to 24 cores in an implementation, which is a 50% increase in core count compared to the maximum MP16 configuration of the Mali-G77. To date, the biggest configuration we’ve seen in the wild of the G77 was the M11 setup of the Exynos 990, with MediaTek employing an MP9 setup.In a projected end-device solution comparison between 2020 and 2021 devices, Arm is projecting the new Mali-G78 to achieve 25% better performance, which includes both microarchitectural as well as process node improvements. That’s generally the reasonable target that vendors are able to achieve on newer generation IPs, but it’s also going to be strongly depending on the exact process node improvements that are projected here – as GPUs generally scale better with improves process density rather than just frequency and power improvements of the silicon.At an ISO-process node under similar implementation area conditions, the Mali-G78 is claimed to improve performance density by 15%. This is referring to the either performing 15% better at the same area, or shaving off 15% area for the same performance, given that this can be done linearly by just adjusting the amount of GPU cores implemented.Power efficiency sees a more meagre 10% improvement, which honestly isn’t too fantastic and not that big of a leap to the Mali-G77. ML performance is also said to be improved by 15% thanks to some new microarchitectural tweaks.Seemingly, the Mali-G78 doesn’t look like too much of an upgrade compared to the vast new redesign we saw last year with the G77 – and in a sense, that does seem somewhat reasonable. Still, the G78 does some interesting changes to its microarchitecture, let’s dwell a bit deeper into what’s changed…\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15816/arm-announces-the-malig78-evolution-to-24-cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Avantek's Arm Workstation: Ampere eMAG 8180 32-core Arm64 Review\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-05-22T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/15733/ampere-emag-system-a-32core-arm64-workstation\n",
      "Content: Arm desktop systems are quite a rarity. In fact, it’s quite an issue for the general Arm software ecosystem in terms of having appropriate hardware for developers to actually start working in earnest on more optimised Arm software.To date, the solution to this has mostly been using cloud instances of various Arm server hardware – it can be a legitimate option andnew powerful cloud instances such as Amazon’s Graviton2certainly offer the flexibility and performance you’d need to get things rolling.However, if you actually wanted a private local and physical system, you’d mostly be relegated to small low-performing single-board computers which most of the time had patchy software support. It’s only been in the last year or two where Arm-based laptops with Qualcomm Snapdragon chips have suddenly become a viable developer platform thanks to WSL on Windows.For somebody who wants a bit more power and in particular is looking to make use of peripherals – actively using large amounts of storage or PCIe connectivity, then there’s options such as Avantek’s eMag Workstation system.The system is an interesting mish-mash of desktop and server hardware, and at the centre of it all enabling is Ampere’s “Raptor” motherboard containing the eMAG 8180 32-core chip. This is a server development board that doesn’t really adhere to any standard form-factor standard, but Avantek was able to make it fit into BeQuiet tower chassis with some modifications.Ian had published a more in-depth visual inspection of the machine a few weeks ago, so I recommend reading that in terms of the analysis of what’s physically present in the machine and its quirks.Read:Arm Development For The Office: Unboxing an Ampere eMag WorkstationThe notable characteristics of the system is that in fact it’s a setup that was designed by a vendor that’s usually server oriented – this is Avantek’s first foray into a desktop system.As noted, because the motherboard isn’t adhering to an ATX standard, the biggest incompatibility lies on the part of the PCIe slots which don’t match up with the slots of the chassis. Avantek here had to resort to using a riser card and a custom backplate in order to fit the graphics card.The graphics card provided in our sample was a Radeon Pro WX5100 – a lower-end unit meant for workstations.The biggest advantage of the system which we’ll address in more detail in a bit, is the fact that this is an SBSA (Server Base System Architecture) compliant system, which means it’ll be compatible with “most” PCIe hardware out there. For example, I had no issues replacing the graphics card with an older Radeon HD 7950 I had lying around and the system booted with display output without any issues. This might sound extremely boring, and it is – but for the Arm ecosystem it’s been a decade long journey to reach this point.In terms of general form-factor, Avantek’s choice here to go with a desktop chassis works well. It’s a big motherboard so it does require a bigger case, allowing it for plenty of additional hardware inside.I think one negative on the system from a practical hardware perspective is Avantek’s server pedigree. The CPU cooler in particular is the type you’d find in a server system, and the fan choice isn’t something you’d see in any traditional desktop system as it's a more robust 90mm fan. Although the company has said that it tried to minimise the noise of the system by adjusting the fan curves as well as opting for a low acoustics chassis – it’s still subjectively loud for a desktop system. I measured around 42dBA at idle which is still a bit much - but that also depends on your typical expectations of a silent system.I hope Avantek would change in the future is employ a more consumer grade CPU cooler system and reduce the acoustics of the machine.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15733/ampere-emag-system-a-32core-arm64-workstation\n",
      "Title: The Intel Comet Lake Core i9-10900K, i7-10700K, i5-10600K CPU Review: Skylake We Go Again\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-05-20T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15785/the-intel-comet-lake-review-skylake-we-go-again\n",
      "Content: The first thing that comes to mind with Intel’s newest line of 10thGeneration desktop processors is one of ‘14nm Skylake, again?’. It is hard not to ignore the elephant in the room – these new processors are minor iterative updates on Intel’s 2015 processor line, moving up from four cores to ten cores and some extra frequency, some extra security measures, a modestly updated iGPU, but by and large it is still the same architecture. At a time when Intel has some strong competition, Comet Lake is the holding pattern until Intel can bring its newer architectures to the desktop market, but can it be competitive?Three weeks ago,Intel announced the Comet Lake 10thGeneration Core processor family line for desktops. From Celeron and Pentium all the way up to Core i9 there were 32 new processor models, representing a sizeable offering to the market. The key elements to this range of processors was the introduction of 10 cores for the Core i9 parts at the high-end – an increase of two cores over the last generation – and the introduction of Intel’s Thermal Velocity Boost for Core i9 that enables +100 MHz in the cooler thermal environments.The best processor from the range, the Core i9-10900K, promises 5.3 GHz peak turbo in optimal conditions for two preferred cores, or 4.9 GHz for all-core situations. Everything from Core i9, Core i7, Core i5, Core i3, and the Pentium Gold processors have hyperthreading, making the processor stack easier to understand for this generation. Compared to the previous generation, there are a lot of similar processor matchups, and except for the top 10-core parts, the offerings should move down one price bracket this time around.Intel has changed the socket for this generation, moving to an LGA1200 platform. This also means there are new motherboards, the Intel 400 series family, including the Z490 chipset which has 44+ entrants ranging from $150 all the way up to $1200. We have a very thorough analysis of every motherboard in ourZ490 motherboard overview.The Processor StackAs mentioned, there are 32 processors for the new Comet Lake 10thGeneration Core family. The Core i9/i7/i5/i3 parts will broadly fall into four categories:K = Overclockable with Integrated Graphics, 125 W TDPKF = Overclockable but no Integrated Graphics, 125 W TDPF = No Integrated Graphics, 65 WT = Low Power with Integrated Graphics, 35 WNo Suffix = Regular CPU with Integrated Graphics, 65 WIntel uses these divisions based on both customer demand but also its ability to separate the best quality silicon from its manufacturing. Silicon that can enable low-power operation becomes T processors, while silicon that can push the highest frequencies at reasonable voltages becomes the K silicon. Some silicon might not be up to par with the integrated graphics, and so these become F processors, and are generally cheaper than the non-F versions to the tune of $11-$25.Here is how these new processors stack-up.Intel 10th Gen Comet LakeCore i9 and Core i7AnandTechCoresBaseFreqTB22CTB2nTTB32CTVB2CTVBnTTDP(W)IGPPrice1kuCore i9i9-10900K10C/20T3.75.14.85.25.34.9125630$488i9-10900KF10C/20T3.75.14.85.25.34.9125-$472i9-1090010C/20T2.85.04.55.15.24.665630$439i9-10900F10C/20T2.85.04.55.15.24.665-$422i9-10900T10C/20T1.94.53.74.6--35630$439Core i7i7-10700K8C/16T3.85.04.75.1--125630$374i7-10700KF8C/16T3.85.04.75.1--125-$349i7-107008C/16T2.94.74.64.8--65630$323i7-10700F8C/16T2.94.74.64.8--65-$298i7-10700T8C/16T2.04.43.74.5--35630$325Core i5i5-10600K6/124.14.84.5---125630$262i5-10600KF6/124.14.84.5---125-$237i5-106006/123.34.84.4---65630$213i5-10600T6/122.44.03.7---35630$213i5-105006/123.14.54.2---65630$192i5-10500T6/122.33.83.5---35630$192i5-104006/122.94.34.0---65630$182i5-10400F6/122.94.34.0---65-$157i5-10400T6/122.03.63.2---35630$182Core i3i3-103204/83.84.64.4---65630$154i3-103004/83.74.44.2---65630$143i3-10300T4/83.03.93.6---35630$143i3-101004/83.64.34.1---65630$122i3-10100T4/83.03.83.5---35630$122Pentium GoldG66002/44.2-----58630$86G65002/44.1-----58630$75G6500T2/43.5-----35630$75G64002/44.0-----58610$64G6400T2/43.4-----35610$64CeleronG59202/23.5-----58610$52G59002/23.4-----58610$42G5900T2/23.2-----35610$42The Core i9 and Core i7 processors will support DDR4-2933, while everything else supports DDR4-2666. These processors are all PCIe 3.0, with sixteen lanes from the CPU available for add-in cards and direct connected storage. Intel likes to point out that they offer another 24 PCIe 3.0 lanes on the chipset, however the uplink to the processor is still a DMI/PCIe 3.0 x4 link.As far as we understand, Intel will be coming to market first with the K processors, and the other processors should be a quick follow-on. That being said, a large number of Intel’s Core 9thGen processor line have been difficult to obtain at retailas the company sees record demand for its server processors. As those command a higher operating margin, Intel would rather spend its manufacturing resources making those server processors instead, leading to shortages of the consumer mainline CPUs. Even as a primary reviewing technology media organization focusing on companies like Intel, Intel has not proactively sampled the media with many of the 9thGeneration parts - perhaps the lack of availability is one of those reasons. It will be interesting to see how many of the Intel 10thGen processors are made available to both reviewers and the public alike.For this review, we were able to obtain the 10-core Core i9-10900K, the 8-core Core i7-10700K, and the 6-core Core i5-10600K.Getting Complicated with TurboIn the charts above, we have multiple different levels of ‘turbo’ for every processor. Intel loves to talk turbo in the sense of offering performance, however it can get complicated about which levels of turbo apply at any given time. Turbo Boost 2, Turbo Boost Max 3.0, and Thermal Velocity Boost without context make very little sense to anyone not necessarily au fait with the world of computer processors. To break it down:Base Frequency: Minimum guaranteed frequency at any timeTurbo Boost 2.0: A potential upper limit frequency that all cores can achieve at any timeTurbo Boost Max 3.0: Also known as Favored Core, this is a peak frequency that two select cores can achieve at any timeThermal Velocity Boost: A new upper limit mode frequency where all cores can gain +100 MHz if the processor temperature is below a given limit, including favored cores in TBM3 modeFavored Core: Up to two cores per processor are selected as the cores that provide thebestvoltage-to-frequency-to-power response, making them the best candidates for additional turbo frequencyWith these definitions in mind, we can go through each of the different turbo modes into some level of specificity:Base Frequency: The guaranteed frequency when not at thermal limitsTurbo: A frequency noted when below turbo power limits and turbo power timeAll-Core Turbo: The frequency the processor should run when all cores are loaded during the specified turbo time and limitsTurbo Boost 2.0: The frequency every core can reach when run with a full load in isolation during turbo timeTurbo Boost Max 3.0: The frequency a favored core can reach when run with a full load in isolation during turbo timeThermal Velocity Boost: The +100 MHz boost given to a core when run with a full load and is below the specified temperature (70ºC for Comet Lake) during turbo timeIntel TVB All-Core: The frequency the processor should run when all cores are loaded during the specified turbo time and limits and is below the specified temperature (70ºC for Comet Lake) during turbo timeEven when speaking with a number of my industry peers, the way this has all been described makes it very complex and difficult to explain to each other sometimes, let alone non-technical users. It can be quite complex to explain to a friend why they are not seeing the maximum turbo frequency on the box for their system due to specified thermal windows that are not being achieved, or why the turbo might not last as long.For the case of the Core i9 parts, Intel’s Thermal Velocity Boost (TVB) limits for the i9-10900K are 5.3 GHz single core, 4.9 GHz all-core, and after the turbo budget is used, the CPU will operate somewhere above the base clock of 3.7 GHz. If the processor is above 70ºC, then TVB is disabled, and users will get 5.2 GHz on two favored cores (or 5.1 GHz for other cores), leading to 4.8 GHz all-core, until the turbo budget is used and then back to somewhere above the base clock of 3.7 GHz.With all these qualifiers, it gets very complicated to understand exactly what frequency you might get from a processor. In order to get every last MHz out of the silicon, these additional qualifiers mean that users will have to pay more attention to the thermal demands of the system, airflow, but also the motherboard.As explained in many of our other articles, motherboard manufacturers have the option to disregard Intel’s turbo limit recommendations completely. This cannot be overstated enough – at least one of my colleagues had issues with a motherboard implementing a different turbo profile than Intel’s suggested recommendations. This is because with an appropriately built motherboard, a manufacturer might enforce an all-core 5.1-5.3 GHz scenario with the i9-10900K, regardless of the temperature, for an unlimited time – if the user can cool it sufficiently. Intel states that the Core i9-10900K has a peak turbo power around 250 W, however motherboard manufacturers earlier this year told us they were building boards for 320-350 W turbo power to give additional thermal headroom or in the event that the 250 W suggestion is completely ignored. Choosing a motherboard just got more complex if a user wants the best out of their new Comet Lake processor.For example, here is an output from our y-Cruncher test, which uses an AVX2 optimized code path. We see that the Core i9-10900K boosts uses up to 254 W at peak moments, but through the whole test it uses 4.9 GHz for ~175 seconds. Intel's turbo has a recommended length of 56 seconds according to the specification sheets, and on our test system here, the motherboard manfuacturer is confident that its power delivery can support a longer-than-56 second turbo time. This is all above board according to Intel, as they recommend that motherboard vendors apply what they think is best for the product they have built. It only becomes out of specification if an overclock is applied - Intel does not consider this an overclock.Beyond the standard Core i9 parts, it’s worth pointing out the low power processors, such as the Core i9-10900T. This processor has a TDP of 35 W, and a base frequency of 1.9 GHz, but can turbo all cores up to 3.7 GHz. Here’s a reminder that the power consumed while in turbo mode can go above the TDP, into the turbo power state, which can be 250 W to 350 W. I’ve asked Intel for a sample of the processor, as this is going to be a key question for the chips that have a strikingly low TDP.It’s worth noting that only the Core i9 parts have Intel Thermal Velocity Boost. The Core i7 hardware and below only have Turbo Max 3.0 ‘favored core’ arrangements. We’ve clarified with Intel that the favored core drivers have been a part of Windows 10 since 1609, and have been mainlined into the Linux kernel since January 2017.With the F processors, the ones without integrated graphics, the price saving seems to be lower for Core i9 than for any other of Intel’s segments. The cost difference per-unit between the 10900K and 10900KF is only $16, whereas the 10700 and 10700F is $25.This ReviewFor this review, we managed to secure three processors for testing: the Core i9-10900K, the Core i7-10700K, and the Core i5-10600K. These three 125 W processors represent the overclocking parts from each of the main categories (there is no overclockable Core i3 this generation).We tested all three processors in theASRock Z490 PG Velocita, and the only serious issue experienced was an error completely on my part –I got fluff in the socketwhen changing processors. The ASRock board and all three CPUs cruised through our test-suites.All three CPUs are based on the 10 core silicon dies (more on the next page), which lead to some interesting core-to-core latencies which we’ll go into. Unlike some of Intel’s previous parts, we had no issues hitting the Thermal Velocity Boost on the Core i9-10900K, as show in the CPU-Z screenshot with 5.3 GHz being registered. It does look like that most motherboards are ignoring Intel’s TVB completely, and making those numbers the new Turbo Boost Max 3.0 – both our system and our colleagues at other Future publications saw similar with their motherboards tested.Elephant in the RoomAs mentioned, 10thGen Comet Lake is, by and large, the same CPU core design as6thGen Skylake from 2015. This is, for lack of a better explanation, Skylake++++ built on 14++. Aside from increasing the core count, the frequency, the memory support, some of the turbo responses, and enabling more voltage/frequency customization (more on the next page), there has been no significant increase in IPC from Intel all while AMD has gone from Excavator to Zen to Zen 2, with sizable IPC increases and efficiency improvements. With Intel late on 10nm, Comet Lake does feel like another hold-over until Intel can either get its 10nm process right for the desktop market or backport its newer architectures to 14nm; so Intel should be trying its best to avoid a sixth generation of the same core design after this. Comet Lake is still aiming to carve a spot in the market, with the main marketing materials from Intel promising the best gaming experience.Despite Intel telling us over the previous years that ‘mega-tasking’ is the new buzzword for demanding software running simultaneously on an enthusiast system, with Comet Lake the messaging is back to one purpose – offering the best single-threaded gaming experience. We know that AMD’s Zen 2 has as a slight 10-15% IPC advantage over 9thGen Coffee Lake, so it will be interesting to see if Intel’s 10% peak frequency advantage affords many benefits. We’ll be keeping an eye on that power consumption too, something that Intel users have chastised AMD hardware for in the past.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15785/the-intel-comet-lake-review-skylake-we-go-again\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD Ryzen 5 3600 Review: Why Is This Amazon's Best Selling CPU?\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-05-18T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15787/amd-ryzen-5-3600-review-amazons-best-selling-cpu\n",
      "Content: Every so often there comes a processor that captures the market. It ends up being that right combination of price, cores, frequency, performance, features and compatibility when added to the right sort of motherboard that makes it fly off the shelves. The main CPU this cycle seems to be the Ryzen 5 3600, offering six high-performance Zen 2 cores and 24 lanes of PCIe 4.0 for only $199. It currently sits at #1 on the Amazon best seller list, so we put one through the paces just to see if the hype was actually real.At $199, the AMD Ryzen 5 3600 has been one the cheapest way to get ahold ofAMD’s latest Zen2 microarchitecture. In our reviews of thelead generation Ryzen products, as well asZen2 on Threadripper,Zen2 in EPYC, andZen2 in Renoir, this microarchitecture is pushing new performance boundaries clock-for-clock against Intel’s other desktop offerings. In fact, until the latest launch of the Ryzen 3 line of processors, the Ryzen 5 was the cheapest Zen 2 processor on the market.(On 5/18, Amazon's price was down to $189. Newegg was $172, but sold out).CompetitionWith six cores and twelve threads, the comparative Intel options vary between something like the Core i7-9600KF with six cores and no hyperthreading, or to the i7-9700KF with eight cores and no hyperthreading. The downside is that both of these processors are more expensive: where the Ryzen 5 3600 is $199, the i5-9600KF is $263 and the i7-9700KF is $385. Frequencies between the three are competitive, however the AMD has a TDP of 65 W, compared to 95 W, and it comes with DDR4-3200 support with 24 lanes of PCIe 4.0, rather than DDR4-2666 and 16 lanes of PCIe 3.0.AMD Ryzen 5 3600 vs Overclockable Intel EquivalentsAMDRyzen 5 3600AnandTechIntel Corei5-9600KFIntel Corei7-9700KF$199 / $189Price$263$385Zen 2ArchitectureCoffee Lake-R(Skylake+++)Coffee Lake-R(Skylake+++)6C / 12TCores6C / 6T8C / 8T3600 MHzBase Freq3700 MHz3600 MHz4200 MHzTurbo Freq4600 MHz4900 MHz65 WTDP95 W95 W2 x DDR4-3200DDR42x DDR4-26662x DDR4-2666PCIe 4.0 x24PCIePCIe 3.0 x16PCIe 3.0 x16Just by going with these on-paper specifications, it’s not hard to see why the Ryzen 5 3600 has been so popular. Even at the $199 price point, the i5-9400F is a $182 processor with the same memory/PCIe downsides, as well as being lower in frequency, despite matching the power rating. The Ryzen 5 3600 is also an unlocked processor, for anyone that wants to overclock.Intelhas announced its newest 10thGeneration processor line, however the official launch date of the processors has not been officially announced yet. Out of the processor lineup however, the closest match would be the Core i5-10500.AMD Ryzen 5 3600 vs Intel 10th Genat ~$200AMDRyzen 5 3600AnandTechIntel Corei5-10500Intel Corei5-10600$199 / $189Price$192$213Zen 2ArchitectureComet Lake(Skylake++++)Comet Lake(Skylake++++)6 C / 12 TCores6 C / 12 T6 C / 12 T3600 MHzBase Freq3100 MHz3300 MHz4200 MHzTurbo Freq4500 MHz4800 MHz65 WTDP65 W65 W2x DDR4-3200DDR42x DDR4-26662x DDR4-2666PCIe 4.0 x24PCIePCIe 3.0 x16PCIe 3.0 x16YesOverclockableNoNoNoiGPUYesYesThis processor matches the six cores and twelve threads, is near in price, doesn’t quite match the base frequency but does exceed in the turbo. It is 65 W, the same as AMD, and on the plus side it does have integrated graphics. But again, it is only DDR4-2666 and only has 16 PCIe 3.0 lanes, compared to AMD’s DDR4-3200 and 24 PCIe 4.0 lanes.Not only this, but our recent trips to brick-and-mortar stores (before the lockdown) looking for Intel mid-range 9thprocessors have been relatively fruitless. Intel is still facing increased demand for its high-end silicon, and is still focusing on making those parts that command the highest margins, like the Xeons. We also understand that Intel might be staggering the exact release of some of this hardware, focusing on the 10thGen K processors first, so it might be a while before we see the mid-range CPUs at retail.The AMD Ryzen 3 3300X and 3100 CPU Review: A Budget Gaming BonanzaThe third angle in the competition for the Ryzen 5 3600 will be with AMD’s own hardware. Having recently launched the Ryzen 3 3300X for only $120, users will have to decide if the extra $80 is worth the two extra cores in the processor. The Ryzen 5 3600 may only be popular because of it being the cheapest Zen 2 processor on the market, and if that is the case then the Ryzen 3 3300X could easily fill that role (or the Ryzen 3 3100, at $99).We tested the Ryzen 3 3300X and Ryzen 3 3100 very recently, and that review is well worth a read.AMD vs AMDAMDRyzen 5 3600AnandTechAMDRyzen 3 3300X$199 / $189Price$120Zen 2MicroarchitectureZen 26 C / 12 TCores4 C / 8 T3600 MHzBase Freq3800 MHz4200 MHzTurbo Freq4300 MHz65 WTDP65 W2 x DDR4-3200DDR42 x DDR4-3200PCIe 4.0 x24PCIePCIe 4.0 x24This is one area where the Ryzen 5 3600 is in a bit of an awkward position, especially with therecent announcement relating to B550.The Ryzen 5 3600 is a popular mid-range processor, meaning that it should be paired with a good mid-range motherboard. For the longest time, that was the B450 motherboard line, with an expectation of a possible upgrade to Ryzen 4000 later this year or next year. Unfortuantely AMD has stated that it will be locking the possible CPUs on B450 to Ryzen 3000 and below, meaning that the highest processor that a B450 owner can use is the Ryzen 9 3950X.As is perhaps understandable, B450 owners with mid-range CPUs looking for an upgrade path are not too happy. With the announcement of B550 offering an upgrade path, there will be a lot of potential mid-range customers now waiting for the B550 motherboards to come to market.The AMD X570 Motherboard Overview: Over 35+ Motherboards AnalyzedFor those that have some money burning a hole in the pocket, X570 is always an option, with the cheapest boards available being around $150. We have performed a large round-up of all the X570 boards in the market, with specific one-off reviews for some of the more impressive models. I suspect however that potential Ryzen 5 3600 customers might be waiting for a good $120 B550 board, should one come to market.This ReviewAt the request of a number of our readers, we sourced the Ryzen 5 3600 to put it through its paces in our updated test suite. Based on the responses on social media, it looks like potential Ryzen 5 3600 customers are into gaming and/or workflow on reasonably priced systems, so we’ll tackle both areas.In our review, there are two key comparisons to look out for:Ryzen 5 3600 vs Ryzen 3 3300XRyzen 5 3600 vs Core i5-8400 / 9400Unfortunately we don’t have an i5-9400F for comparison, however the i5-8400 is basically the same chip by 100 MHz, with the same memory support and microarchitecture design. To make the graphs easier to understand, we've listed the results as 8400/9400. If we get a 9400 or 9400F in for testing, we will update the graphs as necessary.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15787/amd-ryzen-5-3600-review-amazons-best-selling-cpu\n",
      "Title: Honor Magicbook 14 Notebook Review: Where Style Paints a Picasso\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-05-15T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15786/honor-magicbook-14-notebook-review\n",
      "Content: There are three major selling factors when it comes to laptops: Specifications, Price, and Style. It is exceedingly rare to have a laptop hit all three, and if often takes a flagship device to score high marks on all three. The first two, specifications and price, often go hand in hand, but one aspect that often gets overlooked in the mid-range is the element of Style.The Honor Magicbook 14 notebook, which is the subject of this review, scores remarkably high marks for style. Honor, and its parent company Huawei, are primarily in the smartphone business when it comes to consumer products, but over the past couple of years, they have both made efforts to enter the notebook market. With that they bring the sense of style that has governed their smartphone designs to become the second biggest seller in the world. Regardless of what may be happening on a political level, Smartphones from Huawei and Honor are very well received for their ability to capture the eye in a way that other devices do not, and it is this style that Honor brings to its new Magicbook line.This covers not only the polished metal chassis, the easy-to-use keyboard and trackpad, the thin-bezel display, but the edges of our space grey Magicbook are lined with a stellar azure blue like I have never really seen on a notebook before. It is a marque which brings to mind the HP Spectre notebooks in black and gold – except where those devices are north of $1500 apiece, the Honor Magicbook is nearer $550 (currently £550 in the UK including sales tax). The Magicbook is an ultrathin device but keeps costs down by not going completely space-age in its material choice. The 14-inch form factor and the weight at almost exactly 3 lbs makes for a sleek lightweight travel machine.Under the hood is an AMD design, with Honor choosing the Picasso-based Ryzen 5 3500U quad-core processor in cTDP down mode. I can predict that the first comment from a number of our technical users is going to be that this processor is the previous generation, based on AMD’s Zen+ design, while the company has recently launched Renoir-based Zen2 into its portfolio for these sorts of devices. Given that this is Honor’s first notebook into the international market, and how the timeframe lined up, the company went down the safer route to begin with, in order to learn the AMD platform, and hopefully offer a Renoir-based design in the future.For specifications, the Honor Magicbook 14 has a 14-inch 1920x1080 LCD display, with ours peaking in brightness around 240 nits and while the initial calibration was not great, we did get a really nice calibration profile with our equipment. The display has a ~5mm bezel around the edge, reminiscent of other company’s ‘Infinity’ like displays. In order to enable a thin bezel at the top, the webcam has been moved to into the keyboard, similar to the Huawei Matebook notebooks. The display is not touchscreen, but it can rotate back to a 178-degree horizontal mode.In our unit we have the Ryzen 5 3500U processor, which as I mentioned is actually at a slightly lower power setting than the 15 W standard, likely to balance performance and battery life (more on those later). There are Ryzen 7 3700U models, however we understand that those are for China only. This system has 8 GB of DDR4-2400 in dual channel mode, and storage options are 256 GB for the Ryzen 5 and 512 GB for the Ryzen 7. The storage drives are actually very nice, with Samsung NVMe units inside the system – there’s no mechanical spinning rust here.The battery capacity is 56 Wh, which Honor rates for 9-10 hours in video/web/office type workloads at 150 nits (we got 7h at 200 nits in our web test). The battery supports 65 W fast charging with any suitable Huawei or Honor charging adaptor, with a claim of charging from 0 to 48-53% within 30 minutes. For a Zen+ based 14-inch notebook, those are some really nice numbers.The keyboard is an attempt to copy Apple’s chiclet design. I am not that much of a fan of these sorts of keyboards, due to the limited about of tactile feedback, but the keyboard on the Magicbook is one of the nicer ones I have tested in recent memory. Perhaps just because I am using the UK version, but I am glad I have a tall enter key! Unfortunately, there is not a right click key that I’m increasingly becoming accustomed to.The power button is located as a separate button to the right of the delete key, but it requires a proper press in order to activate. Inside this is a Goodix fingerprint sensor, likely identical to the ones that Honor uses on their smartphones – out of most of the fingerprint sensors I’ve used, I often find that the Honor/Huawei implementation of Goodix models to be some of the best and most accurate. The trackpad is super large below the keyboard, capable of ignoring false touches while typing. A number of notebooks in this price range often have dodgy trackpads, but this one is extremely accurate and responsive, and as much as I hate touch tapping, it takes a good amount on this trackpad so I’m very happy with it.For connectivity, the notebook has a full-sized HDMI port, a USB 2.0 port, a USB 3.0 port, a 3.5mm jack, and a USB-C/charge port. Nothing too extravagant here.Under the hood there is not too much to see. The 56 Wh battery takes up most of the space on the bottom of the device, sandwiched between the bottom facing speakers. The AMD Ryzen APU is located under a heatsink connected via a heatpipe to a thin fan blower, which exhausts the air out of the rear of the notebook towards the display hinge. The DRAM in this system isn’t replaceable, but the NVMe drive is.For wireless connectivity, we get a Realtek 8822CE 802.11ac (Wi-Fi 5) controller, which supports a 2x2 connection with MU-MIMO and Bluetooth 4.2. There is also an NFC antenna next to the trackpad, which is used for Honor’s Magic-link application to easily share photos and videos between a smartphone and a laptop.The model as tested in this review today iscurrently available at retail for £550 in the UK (including sales tax), which would make it around $560 in US prices pre-tax.As part of our testing, we have run a few benchmarks and tests on the next few pages.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15786/honor-magicbook-14-notebook-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The AMD Ryzen 3 3300X and 3100 CPU Review: A Budget Gaming Bonanza\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-05-07T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15774/the-amd-ryzen-3-3300x-and-3100-cpu-review\n",
      "Content: When AMD announced the new Ryzen 3 processors built on Zen 2, I was under the impression that these were essentially the reject parts from AMD’s successful Ryzen 3000 line. Inside is a single chiplet with only four cores active out of eight, pushing up to 4.3 GHz; but the kicker was the low price of $120 for the high frequency version, or $99 for a bit slower. AMD has sold quad-core CPUs at $99 for a while, but this is the new core and the new manufacturing process, so would this be any different? We put them up against a $350 quad core from three years ago. It seemed like a crazy idea at the time.AMD Ryzen 3000 CPUs: Chiplets Go MainstreamAfter successful launches of the Ryzen 9, Ryzen 7, and Ryzen 5 families of Zen 2 hardware, AMD has been sitting pretty at north of $200. At each price point, the company offered a compelling option against the competition, as we’ve noted in our reviews of hardware like the3950X,3900X, and3700X. Others, like the Ryzen 5 3600, are some of the best sellers in Amazon’s top list. As we notedin our recent CPU buyers’ guide, out of the top 10 spots on Amazon’s best seller list for CPUs, AMD has 8 of them, and the first 5.That’s all well and good for the higher end of the market, however in the sub-$200 category, this is where the volume often is. Intel has been neglecting this market of late, due to theabnormally high demand for Xeon siliconforcing manufacturing to spend more time on 28-core hardware than quad-core hardware. This leaves the door open for AMD, so it was always going to be interesting what the company did here. For a long time, AMD did nothing, pushing users to its Ryzen 2000 hardware or Ryzen 3000 APUs, namely because they were selling really well (the Ryzen 2600 / 1600 AF offers 6 cores for as low as $85).With the recent launch of AMD’slatest Ryzen Mobile generation APUs, based on Zen 2 and Vega, we were unsure whether AMD would fill this sub-$200 gap with desktop versions of those APUs, or offer lower binned Ryzen 5 3000 parts. After a very successful launch of Ryzen Mobile 4000, leadingto some stellar reviews, it was clear that the mobile silicon was commanding a strong premium in the market, and so we get Matisse based CPUs coming to the sub-$200 segment instead. With that, on April 21st,AMD announced its new Ryzen 3 3300X and Ryzen 3 3100 processors.AMD 'Matisse' Ryzen 3000 Series CPUsAnandTechCoresThreadsBaseFreqBoostFreqL2CacheL3CachePCIe4.0ChipletsIO+CPUTDPPrice(SEP)Ryzen 93950X16C32T3.54.78 MB64 MB16+4+41+2105W$749Ryzen 93900X12C24T3.84.66 MB64 MB16+4+41+2105W$499Ryzen 9390012C24T3.14.36 MB64 MB16+4+41+265WOEMRyzen 73800X8C16T3.94.54 MB32 MB16+4+41+1105W$399Ryzen 73700X8C16T3.64.44 MB32 MB16+4+41+165W$329Ryzen 53600X6C12T3.84.43 MB32 MB16+4+41+195W$249Ryzen 536006C12T3.64.23 MB32 MB16+4+41+165W$199Ryzen 53500X6C6T3.64.13 MB32 MB16+4+41+165WOEMRyzen 33300X4C8T3.84.32 MB16 MB16+4+41+165W$120Ryzen 331004C8T3.63.92 MB16 MB16+4+41+165W$99Filling the bottom at price points of $99 and $120 is very aggressive. Here is AMD’s latest generation Zen 2 hardware, on a 7nm TSMC high performance manufacturing node, bundled with a 14nm IO die from GlobalFoundries, packaged together with frequencies up to 4.3 GHz. At the time of the announcement, we noted that AMD is going to be competing with itself a lot here, for performance and price. Suddenly that $85 Ryzen 5 1600AF only looks appetizing if you want six Zen 1 cores – four Zen 2 cores at higher frequencies and higher IPCs for $99 on paper is probably the better deal.Both processors officially support DDR4-3200, and AMD is reiterating that DDR4-3600/3733 is a nice sweet spot for those purchasing faster memory. Both chips also have 24 PCIe 4.0 lanes from the chipset: 16 for PCIe, 4 for an M.2 drive, and 4 for the chipset. For X570 chipsets, these should be running in PCIe 4.0 mode – for B550 chipsets and others, these chipset lanes will run in PCIe 3.0 mode (see below).AMD sampled us both the Ryzen 3 3300X and the Ryzen 3 3100 for review. These only arrived recently, so we are still in the process of benchmarking the chips on a few benchmarks.Elephant in the Room: B550 MotherboardsOne of the key points for a cheaper build is often a cheaper motherboard. AMD and Intel both supply the market with mid-range and low-end chipsets, which motherboard manufacturers then use to build something more palatable in the $60 to $120 range. Technically AMD is also launching the B550 chipset today too, offering PCIe 4.0 from the CPU and PCIe 3.0 from the chipset, however news on these motherboards has been quite thin. We haven’t received one to test with these processors, which makes an X570 + Ryzen 3 review somewhat non-real world.AMD has provided us will a full list of motherboard compatibility charts for all of the AM4 processors aligned with all of the AM4 motherboards. Due to technical limitations around BIOS size (i.e. motherboard vendors using too small of BIOS chips), only various families of hardware are verified in different motherboards. Most motherboards will likely accept processors outside these designations, especially if the vendor has used a larger BIOS chip, however AMD is putting these guidelines in to make it easier to follow. So while AM4 is heralded as a platform that can support ‘A-Series to 16-cores’, and it does, but only across several boards - very few boards (if any) will support the full gamut of hardware.As for B550, the chipset looks very similar to B450 but with some upgrades. Rather than PCIe 2.0 support from the chipset, we get PCIe 3.0, with a PCIe 3.0 uplink to the processor. B550 motherboards will also be engineered to support PCIe 4.0 from the CPU, which means at least the first (and perhaps the second) PCIe slot will be PCIe 4.0 enabled, and there should also be an M.2 slot.These are the cheaper motherboards, so we’re not expecting any miracles here. B550 is designed to support 3000-series Matisse CPUs only, so AMD is suggesting that current APUs in the market (3200G/3400G) aren’t really suited for this board if they work at all.AMD claims there are over 60 B550 motherboards in development. Some of them look very flash, which makes me wonder if there won’t be $300 B550 models on the shelves. That’s a scary thought.A Word on CompetitionWith Intel staying on the Skylake microarchitecture for another generation in Comet Lake, and the competitiveness of Ryzen 3000 so far, I was keen to see if AMD is able to surpass Intel at the quad core level. Intel finally stopped giving us quad cores at the top Core i7 after Kaby Lake (7th gen Core), with the Core i7-7700 at 65 W and i7-7700K at 91 W. These were $350 parts in 2017, offering 4.2 GHz base frequency and 4.5 GHz turbo for the 7700K. That’s a frequency advantage over the 3300X, but the 3300X has a better IPC.AMD Ryzen 3 3300X: Zen 2, 4C/8T, 3.9-4.3 GHz, 65 W, DDR4-3200, 24x PCIe 4.0, $120Intel Core i7-7700K: Kaby Lake, 4C/8T, 4.2-4.5 GHz, 91 W, DDR4-2400, 16x PCIe 3.0, $350Can AMD’s $120 CPU in 2020 give the same performance as Intel’s 2017 flagship CPU at only a third of the cost???Sounds insane, doesn’t it?There’s a Difference between the 3300X and 3100Without probing any deeper, one might assume there’s little to separate the Ryzen 3 3300X and the Ryzen 3 3100 aside from a few hundred MHz and some cost. To our surprise, it goes deeper than that. Due to whatever binning is in place, AMD is using two different core configurations for these chips, despite both being quad core.Both Ryzen 3 processors have a single eight core chiplet, from which only four cores are active.On the Ryzen 3 3300X, all of those four cores come from the same quad-core CCX, providing a unified latency platform for the cores to use. It comes in a ‘4+0’ configuration, with one CCX fully active, and the other one disabled.On the Ryzen 3 3100, the four cores come from two different CCXes, which adds extra complexity to the latency structure. If a core in one CCX wants to communicate with the other CCX, it has to send a request out through the Infinity Fabric, which adds latency. This is called the ‘2+2’ configuration.Both designs are built with 16 MB of L3 cache, and with the 3300X that is all on one CCX, but split for the 3100.The performance penalty this incurs is difficult to predict. Because of the lower core count than the other Ryzen hardware, the effect of this split on the Ryzen 3 3100 is going to be more pronounced than others. On our part, we ran our core-to-core latency tests.For the Ryzen 3 3300X, as you can see, we have a unified latency topology.Whereas for the Ryzen 3 3100, as it is slower and has the dual CCX layout, this translates to a 5+ nanosecond addition to go within a CCX, but a 50+ nanosecond additional between CCXes.Aside from the frequency difference, this will be a driving factor in our review.For completeness, here's the Core i7-7700K, which is almost 33% faster on core-to-core transfers against core-to-core within a CCX.Read on to find out more.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15774/the-amd-ryzen-3-3300x-and-3100-cpu-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Intel Z490 Overview: 44+ Motherboards Examined\n",
      "Author: Gavin Bonshor\n",
      "Date Published: 2020-04-30T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/15723/the-intel-z490-motherboard-overview\n",
      "Content: After another long wait for a new Intel platform, Comet Lake and the 400 series finally descends. Here we get a new socket,support for up to 10-cores with its flagship Core i9-10900K/KF processors, and a few interesting adjustments on ethernet, Wi-Fi. Scrambling to be the latest and greatest, some motherboard vendors include support for PCIe 4.0 'for a future platform', which some have outright identified as Rocket Lake.We asked every motherboard vendor for specifications and details on their new product lines.This article is a work in progress, and although we have a lot of the information, new information is coming in at a rapid pace. We will endeavour to keep this Z490 Overview updated frequently with new boards, MSRP pricing, and new models that get announced.The Intel Z490 Chipset: New Socket LGA1200Due to the new increased power requirements of the top-end 10 core Intel processors, Intel has changed the socket once again, moving from the LGA115x family onto LGA1200. The physical dimensions of the CPU package have not changed, and all LGA115x coolers will work in the new sockets, but it does mean that these new processors won't work in old motherboards. Instead, we get a new family of 400 series, with the flagship Z490 being at the forefront of all the motherboard manufacturers' marketing materials/From Intel, the key difference for Z490 comes in networking. Integrated into the Z490 chipset is an Intel Wi-Fi 6 CNVi which allows motherboard vendors to integrate its AX200 wireless solutions directly from the chipset with a CRF module. On Z390, this was limited to 802.11ac / Wi-Fi 5, but Intel has stepped up its game with Wi-Fi 6. Motherboard vendors still need to purchase the CRF. In a similar vein, the chipset now supports a 2.5 gigabit ethernet connection through single PCIe 3.0 x1 lane, however this requires purchase of an Intel I225 network controller. Other controllers require using a PCIe lane.Intel Z390, B460, Z370 and Z270 Chipset ComparisonFeatureZ490B460Z390Z370Max PCH PCIe 3.0 Lanes24242424Max USB 3.1 (Gen2/Gen1)6/10?6/100/10Total USB??1414Max SATA Ports6666PCIe Configx16x8/x8x8/x8/+4x16x8/x8x8/x8/+4x16x8/x8x8/x4/+4x16x8/x8x8/x4/+4Memory Channels (Dual)2/22/22/22/2Intel Optane Memory SupportY?YYIntel Rapid Storage Technology (RST)Y?YYMax Rapid Storage Technology Ports3?33Integrated WiFi MACWi-Fi 6Wi-Fi 6Wi-Fi 5NIntel Smart SoundY?YYIntegrated SDXC (SDA 3.0) Support??YNDMI3.03.03.03.0Overclocking SupportYNYYIntel vPro??NNMax HSIO Lanes30?3030Intel Smart Sound??YYME Firmware14141211The Z490 chipset and Intel 10th Generation Comet Lake desktop processors have a total combined PCIe count of 40, which is 16 from the CPU and 24 from the chipset. What's interesting is how similar the Z490 and Z390 chipsets are in terms of specifications, which adds the question of why Intel has opted for a new socket, on what is effectively a refreshof its 14 nm process node.Intel retains the support of its Optane memory modules and uses dual-channel memory, up to DDR4-2933 with Core i9 and Core i7 parts, or DDR4-2666 with all other processors. The Z490 chipset also retains the same 30 HSIO used for any PCIe add-in card or controller, such as USB controllers, ethernet ports, sound cards, RAID cards, etc. Motherboard vendors use controllers such as theASMedia ASM3242chip to offer USB 3.2 G2 and 20 Gbps connectivity. This effectively allows vendors to implement fast Type-C connectivity.The Z490 chipset supports overclocking with Intel's unlocked processors, which was expected. While it is not recommended to overclock the clock generator (BCLK) as it's tied into multiple areas where stability is a large factor, Intel has enabled support for DMI and PEG overclocking which has now been separated out to have no effect on SATA ports and such.Z490 and LGA1200: Motherboards Built with PCIe 4.0?After looking through the specification sheets of all the Z490 options, there was one obvious sticking point to all the press materials. The motherboard manufacturers all had to build their systems for processors that didn't exist yet. Those that mention they contain PCIe 4.0 also mention Rocket Lake processors, which are what we assume to be Intel's next-generation (11th Gen?) hardware. These processors will also have the same LGA1200 socket as these motherboards but clearly will have access to some form of PCIe 4.0 compatibility. This means that the motherboards have to be built with Rocket Lake in mind. That being said, we assume that Rocket Lake hasn't even been built yet - or at least, even the early stage silicon is probably not ready. This means that the motherboard vendors have to work on specifications and standards provided by Intel, some of which could possibly change by release, but also some of these specifications could drastically vary from Comet Lake (10th Gen) to Rocket Lake (11th Gen?).We already know that PCIe 4.0 is a differentiating factor, but what about the number of PCIe lanes? Comet Lake only supports 16 lanes from the processor, and so most of these Z490 boards are built with that in mind. But if Rocket Lake supports more, say 20 PCIe lanes from the processor, then that leaves four lanes to add an M.2. In order to support this M.2 slot from the CPU on Z490, the motherboard vendors have to put in the required switches such that the slot works on both Comet Lake in PCIe 3.0 mode and Rocket Lake in PCIe 4.0 mode. Either that or disable the slot completely for Comet Lake, because it would end up reducing the main PCIe slot to an x8 due to bifurcation.This is but one example, but depending on memory support, power support, graphics support - all of these mean that motherboard manufacturers can take Z490 in one of two ways. The one that most vendors seem to be doing is to make their boards hybrids - suitable for both Comet and Rocket, but not really mastering one. For users intending to upgrade mid-cycle without a motherboard change, these hybrid designs are probably best. The second option is to make specific boards for the specific chips, despite technically supporting both: making the Z490 the best board it can be for Comet Lake, and then some future board (Z590?) being the best board for Rocket Lake. Personally, I prefer the latter, because I'd like the best out of my processor. However, prices of the best motherboards are matching (or even surpassing) that of the processor, which makes the quandary a little more complex than on first glance.The Current Z490 Product Stack: Over 40+ New Motherboards UnveiledWith the release of Intel's 10th generation Comet Lake CPU's offering multiple processors ranging from dual-core Celerons to the latest 10-core i9-10900K SKUs, the demand on motherboard vendors to improve on existing designs is paramount. The onus is on motherboard vendors to deliver to its target markets, whether it's gaming which Intel claims the Core i9-10900K and i9-10900KF to be the best gaming processors ever, but content creation is bigger than it has ever been too. There are seven motherboards with Thunderbolt 3, for example.Below is the current Z490 product stack which has been announced for launch, with some models not expected to hit shelves until later on. The EVGA Dark series makes a reappearance, while MSI has its MEG, MPG and MAG stack. Also within the deep Z490 product stack are GIGABYTE Aorus and its new redesigned Vision D, or previously known as the Designare. ASUS is now on the Maximus XII range with its fabled Extreme and Formula models, with an Apex in for overclockers, as well as its Strix, Prime series, and a fresh ProArt Z490-Creator 10G model too. There will likely be even more boards announced over the coming months, with some surprises we can't discuss, all looking to make Intel's seemingly last PCIe 3.0 desktop Z series chipset a big hit with consumers.ASRock:Inside of the big four motherboard vendors, ASRock has the smallest product stack at launch, with more models expected over the coming months. Included in its stack is the now highly recognisable Taichi which has a pretty strong feature set and more recently announced, the flagship and water-cooled Z490 Aqua.For gamers, the Z490 PG Velocita makes its debut with a unique naming scheme, hopefully looking to gain some notoriety. Also included is the ever-popular mini-ITX Z490 Phantom Gaming ITX/TB3 with Thunderbolt 3 rear panel connectivity one of its main features. A pair more for entry-level users includes the Z490 Steel Legend and Z490 Extreme4, which is one of its longest-standing ranges, stretching back many years.ASRock Z490 Motherboard Product StackModelSizePriceASRock Z490 AquaEATX$1100ASRock Z490 PG VelocitaATX$260ASRock Z490 TaichiATX$370ASRock Z490 Phantom Gaming-ITX/TB3mITX$280ASRock Z490 Steel LegendATX$185ASRock Z490 Extreme4ATX$195ASRock Z490 Phantom Gaming 4/2.5GATX$160ASRock Z490 Phantom Gaming 4/acATX-ASRock Z490 Phantom Gaming 4/axATX-ASRock Z490 Phantom Gaming 4ATX$150ASRock Z490 Pro4ATX$170ASRock Z490M Pro4mATX$150ASRock Z490M-ITX/acmITX$160ASRock Z490 Phantom Gaming 4SRATX-The ASRock Z490 Phantom Gaming 4 series of models offer a variety of options with different networking capability and targets the entry-level point. The base Phantom Gaming 4 model has a cost of $150, which is ASRock's cheapest Z490 model at present, while the 2.5G variant can be purchased for just $10 more coming in at $160.ASUS:ASUS as always has a huge product stack available for Z490, with the high-end ROG Maximus XII series with the Extreme leading the charge, to the Formula for water coolers and the Apex primarily geared up for extreme overclocking. The ROG Maximus XII Hero Wi-Fi acts as a bridge including the majority of the Maximus XII's core feature set, but with a more affordable controller set.The gaming-focused Strix range has many alphabet models including the Z490-E, Z490-F, Z490-G, etc., while the Prime series offers a variety of models at different price points for users that want a more neutral look. Also included in the product stack is the content creator focused ProArt Z490-Creator 10G which looks goodASUS Z490 Motherboard Product StackModelSizePriceASUS ROG Maximus XII Extreme Glacial?-ASUS ROG Maximus XII ExtremeEATX$750ASUS ROG Maximus XII FormulaATX$500ASUS ROG Maximus XII ApexATX-ASUS ROG Maximus VII Hero WiFiATX$399ASUS ROG Strix Z490-E GamingATX$300ASUS ROG Strix Z490-F GamingATX$269ASUS ROG Strix Z490-G GamingmATX-ASUS ROG Strix Z490-G Wi-Fi GamingmATX-ASUS ROG Strix Z490-A GamingATX-ASUS ROG Strix Z490-I GamingmITX$300ASUS TUF Z490-PlusATX-ASUS TUF Z490-Plus WiFiATX$200ASUS Prime Z490-AATX$230ASUS Prime Z490-PATX$160ASUS Prime Z490M-PlusmATX$150ASUS ProArt Z490-Creator 10GATX-Biostar:Biostar has just three models at launch, with two ATX sized models via the Z490GTA EVO, and Z490GTA, and a mini-ITX model, the Z490GTN.Biostar Z490 Motherboard Product StackModelSizePriceBiostar Racing Z490GTA EvoATX$239Biostar Racing Z490GTAATX$209Biostar Racing Z490GTNmITX$199EVGA:Normally bringing its models later on in the product cycle, EVGA has announced models for Intel's Z490, the EVGA Z490 Dark which is heavily stacked on overclocking features. At the same time, the EVGA Z490 FTW WiFi offers users a solid-option for the mid-range.EVGA Z490 Motherboard Product StackModelSizePriceEVGA Z490 DarkEATX-EVGA Z490 FTW WiFiATX-GIGABYTE:GIGABYTE has opted for a mixture of models in its product stack with a wave of Aorus branded boards aimed at gamers and enthusiasts, with some new additions and re-works to previous models on other chipsets. In a move to shorten some of its model names for Z490 (on the most part), we see a couple of models with subtler aesthetics including the new Z490 Vision D and Z490 Vision G. The GIGABYTE Z490 Vision series includes a couple of models, with the Z490 Vision D and Vision G seen as a direct replacement to the Designare series; the Vision G has with a clean aluminium aesthetic.Other models making a return include the Aorus Xtreme, with the Xtreme WaterForce model decked with a custom water block, and an eye-watering price to go with it. Other regular Aorus mainstays in the stack include the Aorus Ultra, Aorus Master, and Aorus Elite. For small form factor aficionados, there's also the mini-ITX Z490I Aorus Ultra, with the micro-ATX Z490M Gaming X expected shortly.GIGABYTE Z490 Motherboard Product StackModelSizePriceGIGABYTE Z490 Aorus Xtreme WaterForceEATX$1299GIGABYTE Z490 Aorus XtremeEATX$799GIGABYTE Z490 Aorus MasterATX$389GIGABYTE Z490 Aorus UltraATX$299GIGABYTE Z490 I Aorus UltramITX$269GIGABYTE Z490 Aorus Pro AXATX$269GIGABYTE Z490 Aorus Elite ACATX$219GIGABYTE Z490 Gaming XATX$200GIGABYTE Z490M Gaming XmATX-GIGABYTE Z490 Vision DATX$299GIGABYTE Z490 Vision GATX$199MSI:The MSI Z490 product stack consists of many previously seen before models including its flagship EATX sized MEG Z490 Godlike, with the Z490 Ace, and the more recent Unify range. MSI's only mini-ITX model comes under the MEG enthusiast gaming banner via the Z490I Unify, while the MPG range offers a good range of features for the mid-range.Towards the bottom of the stack is the MAG Z490 Tomahawk, which is usually reserved for the budget B chipsets, and the MSI Z490-A Pro caters to entry-level users. It's worth noting that from top to bottom, every MSI Z490 motherboard has a Realtek RTL8125G 2.5 G Ethernet port or better.MSI Z490 Motherboard Product StackModelSizePriceMSI MEG Z490 GodlikeEATX$750MSI MEG Z490 AceATX$400MSI MEG Z490 UnifyATX-MSI MEG Z490I UnifymITX-MSI MPG Z490 Gaming Carbon WIFIATX$270MSI MPG Z490 Gaming Edge WIFIATX$200MSI MPG Z490M Gaming Edge WIFImATX-MSI MAG Z490 TomahawkATX$190MSI Z490-A ProATX$160Supermicro:More focused on its server-grade and workstation motherboards, Supermicro has just two models announced for Z490, the C9Z490-PGW and C9Z490-PG. The only difference between the two is a wireless interface, while both these models are the only Z490 models to leverage a PLX chip for PCIe lane bifurcation.Supermicro Z490 Motherboard Product StackModelSizePriceSupermicro C9Z490-PGWATX$395Supermicro C9Z490-PGATX$375On the next page is a summary of each board's power delivery system, with each subsequent page containing a brief analysis/rundown of all the individual Z490 boards.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15723/the-intel-z490-motherboard-overview\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Development For The Office: Unboxing an Ampere eMag Workstation\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-04-22T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15737/arm-development-for-the-office-unboxing-an-ampere-emag-workstation\n",
      "Content: One of the key elements I’ve always found frustrating with basic software development is that it can often be quite difficult to actually get the hardware in hand you want to optimize for, and get a physical interaction that isn’t delayed by networking or logging in or anything else. Having a development platform on the desk guarantees that direct access, and for the non-x86 vendors, I’ve been asking for these for some time. Thankfullywe’re now starting to see some appear, and Avantek, one of the Arm server retailers, have built an Ampere eMag workstation out of a server board, with some interesting trickery to get it to fit. They sent us one to have a look at.While Andrei is testing the system for our full review, I wanted to take some time to actually have a physical look at what one of the first Arm server workstations looks like. This system isn’t built by Ampere, but by Avantek, who takes one of the Ampere eMag motherboards and places into a consumer based PC chassis from Be Quiet, then modifies the chassis to fit the server-sized motherboard. This involves customization, given that the server motherboard does not have the standard E-ATX holes or PCIe spacings for the rear panel. This particular chassis has the option of a transparent side panel and LEDs – Avantek says that despite this market not being the typical recipient of these more consumer aesthetics, they had the demand!Inside the system is a 32-core Ampere eMag server, with 256 GB of eight-channel DDR-2666 memory, a 500GB WD Black SN750 NVMe SSD, a 960 GB Micron 5300 Pro SATA SSD in the rear, a Corsair VS 650W power supply, and an AMD Radeon Pro WX 5100 graphics accelerator, connected through a PCIe riser cable to be vertical. This is due to some awkward placement of the PCIe slots, as we’ll see in a bit.The power delivery for the 125 W processor is through a 5-phase design, using what looks like standard copper core chokes. Nonetheless, these are rated for a server environment.The CPU cooler looks very standard for a tower-style PC, with what looks like 5 double-sized heatpipes and a strong fan with extra baffles to direct airflow.Back to the memory, and we have eight 32 GB DDR4-2666 modules of Samsung’s RDIMMs.So a word on the cabling. Behind the GPU there is a USB 3.0 connector for the front panel, however due to the placement Avantek are using a right angled adaptor here, with the front panel cable eventually going behind the motherboard. The SATA ports to the left are out of the way, but there’s that big 24-pin power cable going right up through the front of the motherboard, rather than behind it. This is because the Corsair VS 650W power supply is a fully wired model, with fixed lengths. This sort of cabling would be standard for a server build, however a modular version might be a bit tidier and also offer the potential for custom cabling should lengths not fit. That being said, the 650W is an 80 PLUS ‘White’ model and easily enough for the 125 W processor and a 75W max graphics card. Should users decide to specify a more powerful GPU, then this power supply will handle it easily.For the PCIe slots, you will notice that we have two PCIe 3.0 x16 slots right next to each other, next to an OCP slot. Again, in a server chassis, this can be very common, given that add-in devices are typically given riser cables. In order to make this motherboard fit in the chassis, Avantek used a riser cable to the GPU mount with some modifications.Here we see the Phanteks riser cable with the AMD GPU. The customization of the chassis, as shown on the right, happens on the back panel, given that this chassis wasn’t designed for a vertical GPU. There are chassis in the market that have vertical setups, however Avantek couldn't find the right one that could also fit this motherboard well enough. They're more than happy to take on suggests for the next generation, but they also have to balance sound levels too.Avantek have cut away most of the horizontal GPU mounts here to put the graphics card in on a custom vertical mount. At the rear of the chassis here we also see the single 120mm fan, and the IO panel, consisting of an Intel I210-AT gigabit Ethernet port, two USB 2.0 ports, a D-Sub output from the BMC, and a COM port. There’s another Ethernet port, again for the BMC.Here’s conformation of that I210-AT controller.Here’s the BMC, a very common ASPEED AST2500 chip paired with some Micron memory. This enables the 2D interface over the D-Sub connector, or system monitoring and control through the Ethernet port.On the front of the chassis, there are no extra ODDs in the bays, but we get two front fans.Gallery:Ampere eMag Workstation by AvantekWe're going to take the CPU cooler off for a photo when our testing is complete, just in case (!). But for those interested, here's the lscpu:This Ampere eMag Arm-based system is unique to Avantek, and as we’ve covered before at AnandTech, andstarts at ~$2795 for the base model, with 8 GB of DRAM and a 240 GB SSD. The workstation is only offered with a single CPU SKU, the eMAG 8180. This isn’t to be confused with Intel’s 8180: this one has more cores! The eMAG 8180 is a 32-core design running at 2.8 GHz with a turbo up to 3.3 GHz, with a TDP of 125 W. This is a first generation eMAG, which uses the old AppliedMicro Skylark microarchitecture, a custom design of Arm v8 with 32 MB of L3, 42 PCIe lanes, and eight memory channels. Avantek offers the system with three optional graphics cards: AMD FirePro W2100, a Radeon Pro WX 5100, and the NVIDIA Quadro GV100. OS options are variants of Linux: Ubuntu, CentOS, SUSE SLES, and openSUSEAs mentioned, we’re planning a full review of the eMag processor shortly as a development system. Technically Ampere has already announced its next generation hardware,the 80-core Altra based on Arm N1 cores, for later this year, however the eMag has been around for a while and it is nice to get numbers to compare it to, especially given those that have deployed eMag will likely be retaining that hardware for several years.Avantek are looking to build an Altra based workstation model as well, should demand be sufficient. But I'd also like to see ThunderX2/X3 workstation systems, Phytium development systems, Graviton2 development systems, and when it comes around to it, Nuvia development systems.For those users that develop hardware-specific software, do you prefer local systems to work on, or cloud/non-local resources? What do you want to see in an upcoming workstation? Let us know in the comments.Related ReadingNext Generation Arm Server: Ampere’s Altra 80-core N1 SoC for Hyperscalers against Rome and Xeon80-Core N1 Next-Gen Ampere, ‘QuickSilver’: The Anti-Graviton2Arm Server CPUs: You Can Now Buy Ampere’s eMAG in a Workstationhttps://store.avantek.co.uk/ampere-emag-64bit-arm-workstation.htmlAmpere Computing: Arm is Now an Investor\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15737/arm-development-for-the-office-unboxing-an-ampere-emag-workstation\n",
      "Title: The ASRock Rack EPYCD8-2T Motherboard Review: From Naples to Rome\n",
      "Author: Gavin Bonshor\n",
      "Date Published: 2020-04-20T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/14171/the-asrock-rack-epycd8-2t-motherboard-review\n",
      "Content: It's no secret that AMD is looking to carve out a bigger share in the server market with their enterprise EPYC processors, and much fanfare has been made about the high core-count offered for the price when compared to Intel's Xeon range of processors. The ASRock RackEPYCD8-2T looks to utilize all of the processing power offered by AMD's EYPC, and the EPYCD8-2T has a professional-centric feature set built into its ATX design. We also have eight memory slots, up to nine SATA ports, has two OCuLink to U.2 slots, dual 10 G Ethernet, and seven PCIe 3.0 slots. This model also supports both AMD's EPYC 7001 Naples and 7002 Rome processors (Rome via an update).ASRock Rack EPYCD8-2T OverviewASRock Rack is the enterprise arm of ASRock, and caters to the workstation, server and data center market. For the longest time, ASRock Rack catered mainly to Intel's offerings, including Intel Xeon and the large Atom designs. Now the company has a small but expanding team focusing on the EPYC side of the market, and the ASRock Rack EPYCD8-2T is an ATX sized solution which is compatible with both Naples and Rome. Today we will be focusing on the EPYCD8-2T and its server and workstation feature set.The ASRock Rack EPYCD8-2T is an ATX sized single LGA 4094 socket option designed for AMD's EPYC processors. The board has an all-green PCB and has a transposed CPU socket designed for more efficient airflow when installed into a 1U or related chassis type. Memory support stretches across eight slots with support for RDIMMS up to 32 GB, and LRDIMMs up to 128 GB per slot. This means that the EPYCD8-2T can house up to 1 TB of DDR4 operating in eight-channel. This model has supports DDR4-3200/2933/2666/2400, both in the RDIMM and LRDIMM variety.Providing BMC maintenance functions is the stalwartAspeed AST2500 management controllerwhich allows users to remotely manage the system. The networking is taken care of by an Intel X550-AT2 Ethernet controller which provides dual 10 G Ethernet on the rear panel. A separate Realtek RTL8211E acting as a dedicated IPMI Ethernet port, with a D-sub 2D video output powered by the Aspeed AST2500.On the PCIe front, the ASRock Rack EPYCD8-2T has plenty of expansion slot support to make the most of the 128 lanes from the CPU. which include four full-length and three half-length (but open-ended) PCIe 3.0 slots. These slots operate at x16/x8/x16/x8/x16/x8 which makes for a total of 88 PCIe 3.0 lanes dedicated to graphics and expansion support.For the storage, ASRock Rack includes two mini SAS HD connectors which each offers the capability to install up to four SATA devices, with up to eight in total. A separate SATA DOM port allows for another SATA device to be installed bringing the boards total of SATA capability up to nine. A total of two PCIe 3.0 x4 M.2 slots are located vertically below the right-hand side bank of memory slots, with two OCuLink ports just to the right of the DRAM slots for U.2 devices.ASRock Rack EPYCD8-2T Block DiagramThe ASRock EPYCD8-2T was originally built forNaples (7001), but Rome(7002) is supported by updating the firmware to v2.30. It is worth noting that this update requires a 32 MB BIOS chip - some of the early units (like ours) only have a 16 MB chip.The performance of the ASRock Rack EPYCD8-2T is competitive with other models we have tested, including theGIGABYTE MZ31-AR0. In comparison to the GIGABYTE model, the EPYCD8-2T shows much better power efficiency with a strong showing in our long idle power testing, as well as at full-load with our AMD EPYC 7351P processor. Server and workstation motherboards tend to take longer to boot up into Windows due to controller and BMC initialization during POST, but our POST time testing shows the EPYCD8-2T to POST in just over 50 seconds, with a slightly quicker POST time of 45 seconds with non-essentially controllers disabled. The ASRock Rack EPYCD8-2T is the only model I've personally tested on any platform to be under 50 µs in our DPC Latency testing, making this a solid option for users building an audio-focused workstation.The ASRock Rack EPYCD8-2T currently retails for $498 at Newegg and represents just handful of single-socket LGA4094 models at the sub $500 price point. Included in that list is the similar, but cheaper ASRock Rack EPYCD8 ($460) which is essentially the same board, but without dual 10 G Ethernet. Other models at a similar price point include the Supermicro MBD-H11SSL-NC ($470) with dual 1 G Ethernet and fewer SATA, as well as the ASUS KNPA-U16 ($462) which has superior storage and better memory support but opts for two 1GbE too. The distinguishing factor in specifications for the EPYCD8-2T that we're reviewing today is the Intel X550 Dual 10 gigabit Ethernet controller, and seven PCIe 3.0 slots which is impressive on an ATX sized model.Read on for more extended analysis.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/14171/the-asrock-rack-epycd8-2t-motherboard-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD’s Mobile Revival: Redefining the Notebook Business with the Ryzen 9 4900HS (A Review)\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-04-09T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15708/amds-mobile-revival-redefining-the-notebook-business-with-the-ryzen-9-4900hs-a-review\n",
      "Content: At every turn in the story of AMD’s notebook portfolio, we’ve been there to document the highs and lows. Five years ago, AMD was definitely suffering from a combination of a poor platform, and poor notebook designs tailored for the budget end of the market. Last year, AMD scored a design win in the Microsoft Surface, and now 2020 is set to be another significant step back into this market, with the new Ryzen Mobile 4000 series. Touting over 100+ design wins this year for the new 7nm processor line, we have the first of the halo products in for review: the ASUS Zephyrus G14, with an 8-core Ryzen 9 4900HS under the hood. We’re comparing it to an equivalent Razer Blade 15-inch, and it is very clear that AMD can take the lead in a lot of tests, and be very competitive in others.The Notebook Market and Ryzen Mobile 4000One of the strongest elements to PC market growth in recent years is the notebook market. Users have been updating their mobile PC more frequently than their desktop, especially when new form factors offer more performance in thinner and lighter designs, with new features such as faster Wi-Fi, high resolution displays, and high capacity fast storage. All of which, in turn, has lead to a push for a quicker update cycle.These new form factor designs, like thin and lights, or 2-in-1s, are driven by high performance components that are able to run efficiently across a wide spectrum of performance levels, to deliver throughput for gaming and work when needed, or to power down to conserve power when on the road or in an airplane. The cost of these new form factor devices have come down to something more palatable for the average user, but for a good number of years, AMD’s hardware wasn’t even in this market.Dr. Lisa Su at an AMD Ryzen Mobile PresentationFor 2020, AMD is expecting to be at the forefront of notebook design wins, due to two main features of the new Ryzen Mobile 4000 strategy: high performance components, and co-design with OEMs. When the OEMs start spending more money on designing higher profile systems for a specific processor, like the ASUS Zephyrus G14 with a Ryzen 9 that we have in for testing, it is clear that the hardware underneath should offer something that the market wants.In total there are eleven of AMD’s new ‘Renoir’ Ryzen Mobile 4000 CPUs, split across the 15 W and 45 W markets. The top CPUs in each offer up to eight Zen 2 cores, Vega 8 integrated graphics, and the main differences between the two sets is going to be the base frequencies.AMD Ryzen Mobile 4000 APUsAnandTechCoresThreadsBaseFreqTurboFreqL2L3GPU CUsGPU FreqTDPH-SeriesRyzen 9 4900H8 / 163.3 GHz4.4 GHz4 MB8 MB8 / 1750 MHz45 WRyzen 9 4900HS8 / 163.0 GHz4.3 GHz4 MB8 MB8 / 1750 MHz35 WRyzen 7 4800H8 / 162.9 GHz4.2 GHz4 MB8 MB7 / 1600 MHz45 WRyzen 7 4800HS8 / 162.9 GHz4.2 GHz4 MB8 MB7 / 1600 MHz35 WRyzen 5 4600H6 / 123.0 GHz4.0 GHz3 MB8 MB6 / 1500 MHz45 WRyzen 5 4600HS6 / 123.0 GHz4.0 GHz3 MB8 MB6 / 1500 MHz35 WU-SeriesRyzen 7 4800U8 / 161.8 GHz4.2 GHz4 MB8 MB8 / 1750 MHz15 WRyzen 7 4700U8 / 82.0 GHz4.1 GHz4 MB8 MB7 / 1600 MHz15 WRyzen 5 4600U6 / 122.1 GHz4.0 GHz3 MB8 MB6 / 1500 MHz15 WRyzen 5 4500U6 / 62.3 GHz4.0 GHz3 MB8 MB6 / 1500 MHz15 WRyzen 3 4300U4 / 42.7 GHz3.7 GHz2 MB4 MB5 / 1400 MHz15 WAll the 15 W CPUs are commonly referred to as the ‘U-Series’, while the 35-45 processors are known as ‘H-series’. We may use these terms interchangeably.The Ryzen 7 15 W processors offer eight cores in that tiny thermal envelope. This means at full use, each core will only have access to under 2 W of power, and the system is still expected to be north of 2.2 GHz. We’ve seen the desktop Ryzen processors hit 3.0 GHz at under 3 W each, and these mobile parts are likely to be the best bins for power efficiency.The 45 W processors are mainly aimed at higher throughput systems, and the first notebooks with this hardware will be paired with discrete graphics, providing systems totaling 100 W or more. For each H processor there is a corresponding HS processor, offering similar or almost similar specifications to the H processor, but at 10 W less. As mentioned above, these CPUs are ‘S’pecial in that OEMs have to work with AMD and meet specific criteria in the hardware design to be given the HS models. ASUS has an exclusive through Q2 and Q3 of 2020 on these with the Zephyrus G14, however we expect more models to come for the Christmas system. These HS systems will be part of AMD’s Continuous Validation Labs project, with a lab in Austin and a lab in Shanghai, that pre-tests any driver or software updates for compatibility before they are distributed, in order to maintain device performance.AMD didn't just magically get here. There were a number of tough years in the last decade on its notebook platform.2016: A Historic Low for AMD in NotebooksBack in 2016, we reviewed five laptops concurrently, all featuring AMD’s latest mobile platform at the time, Carrizo. These systems were built by AMD’s key OEM partners at the time, such as HP, Lenovo, and Toshiba, and were aimed at the $500 to $900 market. At the time, AMD was struggling with a product that was not that good, and although was better than the previous generation, it still struggled to be competitive.One thing that shot AMD in the foot was that AMD unified the design between its dual channel memory regular Carrizo parts and the single channel low cost Carrizo-L parts, which allowed OEMs to build regular systems with only a single memory channel to save costs. OEMs knew this crippled performance, but in enabled the headline processors in cheaper devices. These devices also ended up with low quality displays, mechanical hard drives, and were big and bulky because the user with a low budget could only afford this level of design. It ended up being a vicious circle of negative feedback – we covered the full story in a 24 page deep dive whichyou can read here.A slide from AMD in 2015, Showing the Target MarketsAs part of that analysis, we provided a number of potential solutions to the problem, including the fact that AMD should design its platforms for different segments and define its own market, rather than coalescing them all into one. We also also suggested AMD should take a leap and creating proper $1500 flagship reference systems for its OEM partners to provide a basis on which higher-end designs could be built. I also suggested that the OEMs also not be cheap and look at how $10 more on SATA storage can really bump the user experience.An overriding solution to this issue was that AMD should build a notebook processor that is not only competitive, but also aims to beat the competition. At the time of Carrizo, we were still wondering what AMD had in its sleeve – the company had started talking about Zen and returning to the high performance market and we heard promises of it also coming to the notebook form factor. The company received a lot of praise with its first generation Zen desktop product, which increased as we saw Zen 2 being launched on TSMC’s leading 7nm process node. The mobile chips by contrast have been the last of each generation to show, given that the desktop and server products take advantage of multiple chiplet designs, leveraging benefits such as increased yield and frequency binning with reduced costs, while the mobile processors are still monolithic.The first new mobile APUs, known as Raven Ridge and Picasso, combined Zen cores with Vega graphics, on a 14nm/12nm process, and targeted the 15 W notebook market. These achieved a variety of successes, by virtue of bringing performance back to a more palatable level, and AMD’s partners using the hardware in some key important designs, such as the Lenovo Thinkpad. The Thinkpad is one of the most important wins here, because AMD has always had a lot of success in the commercial market – this is where a company might purchase 2500 laptops for their employees to work on but they also require extra layers of management and administration to get working within the corporate environment. Despite the successes, Raven Ridge and Picasso still had two key disadvantages compared to Intel’s equivalent hardware – raw performance and battery life. This was shown in one of the latest products to appear on Picasso, whereby the Microsoft Surface 3 was available in AMD and Intel formats with an identical chassis and battery size.OurMicrosoft Surface 3 Review, comparing AMD’s Picasso-based Ryzen 7 against Intel’s 10nm-based Core i7, has been one of the best A vs. B notebook comparisons in recent memory. For the 16 GB / 512 GB variants, Intel commanded a $100 premium but offered Windows 10 Pro, Wi-Fi 6 and LPDDR4X-3733 memory, compared to Windows 10 Home, Wi-Fi 5 and DDR4-2400. The overall conclusion was firmly on the side of Intel, in CPU performance, power efficiency, and battery life. But ultimately here was AMD’s big design win, a premium notebook model.In For The Win: 2020This brings us onto today’s new hardware. AMD’s has teased the ‘Renoir’ platform since CES at the beginning of the year, combining its new Zen 2 cores with updated Vega graphics on TSMC’s leading edge 7nm process. What surprised us for the original announcement was the depths to which AMD wanted to push a lead: up to eight cores in a 15 W design, at competitive frequencies as well. The only way for Intel to put over four cores in a laptop processor is to go up to the 45 W bracket. AMD has been presenting us with some big benchmark gains over Intel, along with a 2x increase in efficiency generation on generation and new power management technology that will remove issues that plagued the battery life on previous iterations of designs using AMD processors.At CES, AMD boasted that the company would have a dozen Ryzen Mobile 4000 ‘Renoir’ systems on shelves within the quarter, and over 100 designs by the end of 2020. Recent world events have perhaps elongated those time spans a bit, but we did get to see what these Ryzen Mobile 4000 laptops would look like. AMD put its cards on the table and was very clear that its notebook partners were now fully onboard the Ryzen Mobile train – after successfully delivering Raven Ridge and Picasso on a regular cadence, with a lot better performance and a rise in demand, OEMs were more amenable to AMD’s roadmaps and what the performance claims are, enough to put significant resources into developing top-line halo hardware. If Microsoft were confident enough to put a Picasso in the Surface, then other OEMs should fall in line. And at CES in January, they all did, and the OEM partners were keen to show off the new AMD systems.One of the highlights of CES was the ASUS Zephyrus G14. This device, even without it being on sale, won numerous awards from the media for exhibiting what AMD’s new platform can do. In a 14-inch chassis, the hardware combined a top line Ryzen 9 4900HS processor with 8 cores, an NVIDIA RTX 2060 with Max-Q graphics card, Wi-Fi 6, NVMe storage, and a 1080p 120 Hz IPS display with FreeSync support, all within a small form factor. AMD was keen to point out that users would need to invest in a bulkier 15-inch notebook from Intel to get this performance, while ASUS focused on the new ‘HS’ processor model, which essentially meant that the company worked with AMD to design the device but it also conforms to a number of AMD’s standards and will be part of an AMD standards program. Not only that, but ASUS has an exclusive on the HS processors for six months. The benefit of the HS is a 35W power envelope, with a similar frequency to the 45 W hardware, but at a lower power. It also has 6536 holes on the top cover, which will have adjustable LEDs on which to run animations or logos.This is also the system that AMD was able to sample for our first review. We’re going to compare it to a very similar system from Intel, a Razer Blade 15-inch with a Core i7-9750H and RTX 2060.Dell G5 15 SE with 45 W Ryzen-HOther devices shown at CES include the ASUS TUF laptops, which were more of a 15-inch style of gaming notebook, then we have also heard about the Lenovo Yoga Slim 7, which uses the top line Ryzen U-series 15 W processor in an ultra-portable like design. There is also the Dell G5 15 SE, which looks like a workstation-class system which is paired with a Radeon RX 5000M series graphics card, as well as the Acer Swift 3, which is another 15 W ultra-portable like design. You can see the range of laptops that were part of the launch cycle in our coverage here:A Quick Overview of Ryzen Mobile 4000 Laptops From Acer, ASUS, Dell, & MSIIn our review today, we are testing the Ryzen 9 4900 HS, inside the ASUS Zephyrus G14 that AMD has supplied for review. This unit is a 14-inch device with a Pantone-Calibrated 1080p 120 Hz display with Freesync, featuring an RTX 2060 with Max-Q discrete graphics card, 16 GB of DDR4-3200 memory, a 1TB Intel 660p NVMe SSD, Intel Wi-Fi 6 connectivity, and a 78 Wh battery.The ASUS Zephyrus G14 as tested is set for $1449. There is a 4K version with Ryzen 7 for £1600.For comparison, we chose one of the most successful Intel comparison units, the Razer Blade 15-inch. This device features a 45 W Core i7-9750H, with the full RTX 2060, 16 GB of DDR4-2666, a Liteon 512 GB NVMe SSD, a 15-inch 1080p 144 Hz G-Sync display, Thunderbolt 3, and an 80 Wh battery.This Razer Blade 15-inchis currently at retail for $1679.These two systems are similarly matched, with a few key differences. Where the Razer Intel has six cores, the ASUS AMD has eight, but the Razer Intel has a higher TDP CPU and graphics card. The ASUS AMD has faster memory and a bigger SSD, but the Razer Intel has a higher refresh rate display. Both are designed as systems that can provide power when working or gaming, with the AMD touting that Intel can’t provide this level of performance in the 14-inch form factor of the ASUS Zephyrus G14.This review is going to look at the Ryzen 9 4900 HS processor, how it performs in different workloads and with different memory configurations, as well as an analysis at the new AMD halo system.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15708/amds-mobile-revival-redefining-the-notebook-business-with-the-ryzen-9-4900hs-a-review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Samsung Galaxy S20+, S20 Ultra Exynos & Snapdragon Review: Megalomania Devices\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-04-03T13:30:00Z\n",
      "URL: https://www.anandtech.com/show/15603/the-samsung-galaxy-s20-s20-ultra-exynos-snapdragon-review-megalomania-devices\n",
      "Content: It’s been a long couple of weeks, but the wait is now finally over. Today we’re ready to go on a deep dive into Samsung’s most important phones of 2020; the new Galaxy S20 series represents a huge jump for the Korean company, and also for the wider smartphone industry. The new devices have a lot of brand-new features premiering for the first time in mainstream flagship devices, and some cutting-edge capabilities that are outright new to the industry as a whole.The S20 series are probably best defined by their picture capturing capabilities, offering a slew of new camera hardware that represents Samsung’s most ambitious smartphone camera update ever. From a “periscope” design telephoto lens with 4x optical magnification and up to a quoted 100x digital magnification, to a new and humongous 108MP main camera sensor with a brand-new pixel array setup, the new Galaxy S20 Ultra is definitely an exotic device when it comes to its photography features. The new Galaxy S20+ also sees some massive new upgrades, ranging from a new, larger main camera sensor, to the innovative use of a 64MP wide-angle module that allows for high magnification hybrid crop-zooming. Overall it too is a big step-up in the camera department and certainly shouldn’t be overshadowed by its Ultra sibling. The phones are not only the first smartphones able to capture 8K video – but they’re also amongst the first consumer grade hardware out on the market with the capability, which is certainly an eye-catching feature.The new S20 series are also among the first devices to come with the latest generation of processors on the market, pioneering the usage of the new Snapdragon 865 as well as the new Exynos 990 SoCs. In recent years, it’s always been a contentious topic for Samsung’s flagship phones as the company continues to dual-source the SoCs powering its devices – with some years the differences between the two variants being larger than one would hope for. We have both chipset variants of the Galaxy S20 Ultra as well as an Exynos variant of the S20+ for today’s review, and we’ll be uncovering all the differences between the models.Let’s go over the specifications and the designs in more detail:Samsung Galaxy S20 SeriesGalaxy S20Galaxy S20+Galaxy S20 UltraSoC(North America, China, Korea, Japan)Qualcomm Snapdragon 8651x Cortex-A77 @ 2.84GHz3x Cortex-A77 @ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 640 @ 587MHz(Europe & Rest of World)Samsung Exynos 9902x Exynos M5 @ 2.73GHz2x Cortex-A76 @ 2.50GHz4x Cortex-A55 @ 2.00GHzMali G77MP11 @ 800 MHzDisplay6.2-inch3200 x 1440 (20:9)6.7-inch3200 x 1440 (20:9)6.9-inch3200 x 1440 (20:9)SAMOLEDHDR10+1200nits peak brightness120Hz Refresh Rate(@FHD+ software rendering)Dimensions151.7 x 69.1 x 7.9 mm163 grams161.9 x 73.7 x 7.8 mm187 grams166.9 x 76.0 x 8.8 mm220 gramsRAM8 GB (LTE)12 GB (5G)8 GB (LTE)12 GB (5G)12 /16GBNANDStorage5G = 128 GBLTE = 128 GB + mSD5G = 128-512 GBLTE = 128 GB + mSD16 GB model = 512 GB12 GB = 128 or 256 GB+ microSDBattery4000mAh(15.4Wh) typ.3880mAh (14.93Wh) rated4500mAh(17.32Wh) typ.4370mAh (16.82Wh) rated5000mAh(19.25Wh) typ.4855mAh (18.69Wh) rated15W Wireless Charging25WFast Charging45WSuper Fast ChargingFront Camera10MP4K video recordingF/2.2, 80-degree40MP4K video recordingF/2.2, 80-degreePrimary Rear Camera79° Wide Angle12MP 1.8µm Dual Pixel PDAF79° Wide Angle108MP 0.8µm DP-PDAF3x3 Pixel Binning to 12MP8K24 Video Recordingfixed f/1.8 opticsOIS, auto HDR, LED flash4K60, 1080p240, 720p960 high-speed recordingSecondaryRear Camera76° Wide Angle(Cropping / digital zooming telephoto)64MP0.8µmF/2.0 optics, OIS8K24 Video Recording24° Telephoto(5x optical magnification)48MP0.8µm2x2 Pixel Binning to 12MPF/3.5 prism optics, OISTertiaryRear Camera120° Ultra-Wide Angle12MP 1.4µm f/2.2ExtraCamera-Time of Flight (ToF) 3D Sensor4G / 5GModemSnapdragon 5G- Snapdragon Modem X55 (Discrete)(LTE Category 24/22)DL = 2500 Mbps - 7x20MHz CA, 1024-QAMUL = 316 Mbps 3x20MHz CA, 256-QAM(5G NR Sub-6 + mmWave*)DL = 7000 MbpsUL = 3000 Mbps*Depending on region and modelExynos 5G- Exynos Modem 5123 (Discrete)(LTE Category 24/22)DL = 3000 Mbps 8x20MHz CA 1024-QAMUL = 422 Mbps ?x20MHz CA, 256-QAM(5G NR Sub-6)DL = 5100 MbpsExynos 4G- Exynos Modem 5213 (Discrete)(LTE Category 24/22)DL = 3000 Mbps 8x20MHz CA 1024-QAMUL = 422 Mbps ?x20MHz CA, 256-QAMSIM SizeNanoSIM + eSIMWireless802.11a/b/g/n/ac/ax2x2 MU-MIMO,BT 5.0 LE, NFC, GPS/Glonass/Galileo/BDSConnectivityUSB Type-Cno 3.5mm headsetSpecial FeaturesUnder-screen ultrasonic fingerprint sensor(Qualcomm QC 2.0, Adaptive Fast Charging, USB-PD),reverse wireless charging (WPC & PMA),IP68 water resistanceLaunch OSAndroid 10 with Samsung OneUI 2.0Launch Prices128GB 5G:$999 / 999€ /£899128GB 5G:$1199 / 1099€ /£999512GB 5G:$1299 / 1249€/n/a £128GB 5G:$1399 / 1349€ /£1199512GB 5G:$1499 / 1549€/£13994Gvariants for100€ / 100£ cheaperavailable onlyin certain markets5G onlyHardware-wise, the new S20 series essentially checks all the boxes that you’d expect (or could ask for) in a 2020 phone. The new Snapdragon 865 and Exynos 990 both bring lots of performance to the table, and probably the most talked about aspect of the new generation is their ability to support new 5G networks. In most developed countries with early 5G deployments, the S20 series are indeed positioned as 5G devices – and technically their naming scheme contains the 5G moniker, such as the “Galaxy S20+ 5G” or the “Galaxy S20 Ultra 5G”, including our review devices today.Samsung however still offers 4G variants of the phones in some countries where 5G rollout is slow – these still feature the newest SoCs and their 5G capable modems, however they lack the corresponding 5G RF hardware needed to enable that radio connectivity. The silver lining here is that these 4G models do come at a cheaper price than the 5G variants – essentially matching the pricing of the S10 series in their respective configurations.The US in particular gets the most capable and connectivity-rich models of the S20 series; it’s currently the only market where the new phones will be launching with mmWave capabilities – at least at this point in time. It’s to be noted, however, that mmWave connectivity is currently only available on the S20+ and S20 Ultra, as Samsung’s been quoted to say that the smaller S20 didn’t have sufficient internal space to house the new mmWave modules from Qualcomm. A special variant of the S20 with mmWave is said to follow up on Verizon in a few months.The spec list is long and complex, but a few highlights are that the new phones now come with 12GB of RAM for the 5G models, with that going up to a massive 16GB for the 512GB Galaxy S20 Ultra. 128GB remains the minimum storage configuration, and the phones come with UFS 3.0-type storage chips, promising top of the line performance. The microSD slot also survives – which might be a boon for those wanting to record 8K video, because at 1GB/minute, it’s a storage killer.At the front of the new devices, we see a new design language dictating the new form-factor and aesthetics of the series. Centre-stage we find Samsung’s newest generation AMOLED screens. Compared to the S10 series, things have been elongated to a new, taller 20:9 aspect ratio, with the phones now all seeing a notable lengthening of their dimensions by a couple of millimeters.The display resolution is still 1440p – 3040x1440 to be exact – andunlike the Note10 series, the smaller S20 doesn’t see a downgrade to a 1080p panel, to which I've breathed a sigh of relief. What does make the S20 series' screen super special though is their support of a high 120Hz refresh rate. Samsung didn’t just aim to match the 90Hz capabilities of the 2019 competitors, but to one-up them. It’s an amazing feature that really stands out for the S20s, giving one a sense of fluidity and smoothness that usually reserved just for special gaming devices.Samsung has also redesigned their hole-punch camera. The new design language had already been introduced in the Note10 series, but the S20 phones further reduce the size of the camera cut-out. Comparing the S20+ to last year’s S10+, the difference is quite striking. The reduced footprint also reduces the thickness of the notification bar, which had been quite thick on the S10 series, further expanding the usable screen estate of the phones. I think, barring an actual see-through-screen front camera design, it’s as sleek a design as we’re going to get until that technology is ready for prime-time.Part of the screen’s design, but also of the wider language of the phone, is the reduced curvature of the display. Curved screens have been a main-stay for Samsung flagship phones since the Galaxy S8, but over the years the company has refined the designs for better usability. This year, the S20 series sees the largest regression of the display curvature to date, with a much-reduced radius that doesn’t go nearly as far to the sides of the phones as its predecessors. What you end up with is the flattest screen from a Galaxy S phone in recent years, without actually going fully flat.While the front curvature has been reduced, the back curvature has been expanded. In this respect the phone has more in common with the Galaxy S10 5G than the regular S10 series, as it adopts the same, much narrower side-frame aesthetic. The rest of the side of the phone is covered by the curved back glass panel – and I have to say that this is probably one of my favorite design features of the S20+, as it lends it some incredible ergonomics that allow it an in-hand feel that’s much narrower than what you’d expect from the phone. It’s only 0.4mm narrower than the S10+, but it just feels that much better in hand thanks to the new curves.It’s to be noted that the curved back glass sides aren’t just done for aesthetics, but also serve as a technical enabler for the mmWave modules which sit on the inside of the phone (two modules facing the lateral sides, one facing the back). Had the metal frame been wider, as in classical phone designs, these would have a harder time transmitting and receiving such high-frequency signals in an optimal manner.On the back of the phone there’s obviously the new camera designs – we’ll go into the technical details of the new cameras in a dedicated page later in the article. I’ll also talk about the S20 Ultra more extensively in just a bit, but first I wanted to give my opinion on the S20+.Having used the phone for a couple of weeks now, I’ve gotten used to the new camera position. It’s a departure from the classical center-camera positioning we’ve been used to in Galaxy S phones ever since the first model 10 years ago, but it’s become a technical necessity given the more complex camera systems out there and the better internal component space management it allows. It’s a bigger camera bump as that of the S10 series – and yes, it now causes the phone to no longer be stable on a flat surface, with it now wobbling when pressing the left side of the screen. Other phones out there have had this characteristic for years (Looking at you, iPhones), so it’s just something that one has to accept and live with.A few other details of the S20 series' design include changes in their finish. I do like that we once again have a black variant that’s actually fully black, including the metal frame. One aspect that I think Samsung missed the mark on was that they did not adopt a matte / frosted back glass option. Such designs have been slowly introduced by vendors since 2018, and last year was most notably made mainstream by Apple’s iPhone 11 Pro series. The Galaxy S20s still being the same glossy finger-print magnets in contrast feels a bit dated.Another change is in the audio department. The new center front-facing camera design means that the earpiece speaker has to undergo a bit of an internal redesign. The S20 here is a ton louder than its predecessor, to the point that I find that the earpiece speaker is now louder than the main speaker. We’ll be going over the audio quality changes later on in the article, but it’s probably one of the more striking differences you’ll notice compared to the S10 series. The main bottom speaker remains similar to the S10.And course, the S20 series no longer come with 3.5mm headphone jacks. I think I’ve riled on the topic enough over the years, but least to say I’m very disappointed by Samsung for this anti-consumer choice. Sony notably tracked back on their decision to deprecate the headphone jack,bringing it back on the new Xperia 1 II– so maybe there’s some hope Samsung might do the same, as long as there’s sufficient negative feedback from users.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15603/the-samsung-galaxy-s20-s20-ultra-exynos-snapdragon-review-megalomania-devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Intel’s Cooper Lake Plans: The Chip That Wasn’t Meant to Exist, Fades Away\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-03-17T20:00:00Z\n",
      "URL: https://www.anandtech.com/show/15631/intels-cooper-lake-plans-the-chip-that-wasnt-meant-to-exist-dies-for-you\n",
      "Content: Following an exclusive report fromSemiAccurate, and confirmed by Intel throughServeTheHome, the news on the wire is that Intel is set to can wide-spread general availability to its Cooper Lake line of 14nm Xeon Scalable processors. The company is set to only make the hardware available for priority scale-out customers who have already designed quad-socket and eight-socket platforms around the hardware. This is a sizeable blow to Intel’s enterprise plans, putting the weight of Intel’s future x86 enterprise CPU business solely on the shoulders of its 10nm Ice Lake Xeon future, which has already seen significant multi-quarter delays from its initial release schedule.Intel’s Roadmaps Gone AwryIn August 2018, Intel held aData-Centric Innovation Summit, where the company laid out its plans for the Xeon CPU roadmap. Fresh off the recent hires ofRaja KoduriandJim Kellerover the previous months, the company was keen to put a focus on Intel as being more focused on the ‘data-centric’ part of the business rather than just being ‘PC-centric’. This meant going after more than just the CPU market, but also IoT, networking, FPGA, AI, and general ‘workload-optimized’ solutions. Even with all the messaging, it was clear that Intel’s high market share in the traditional x86 server business was a key part of their current revenue stream, and the company spent a lot of time talking about the CPU roadmap.At the time, the event was the first year anniversary of Skylake Xeon, which launched in mid-2017 (the consumer parts in Q3 2015). The roadmap, as laid out at this event was to launch Cascade Lake 2ndGeneration Xeon Scalable by the end of Q4 2018 on 14nm, Cooper Lake 3rdGen Xeon Scalable in 2019 on 14nm, and Ice Lake 4thGen Xeon Scalable in 2020 on 10nm.Cascade Lake eventually hit the shelves in April 2019, and took a while to filter down to the rest of the market.Cooper Lake, on the other hand, was added to the roadmap rather late in the product cycle for 14nm. With Intel’s known 10nm delays, the company decided to add in another range of products between Cascade Lake and Ice Lake, with the new key feature between the two being support for bfloat16 instructions inside the AVX-512 vector units.Why is BF16 Important?Thebfloat16 standardis a targeted way of representing numbers that give the range of a full 32-bit number, but in the data size of a 16-bit number, keeping the accuracy close to zero but being a bit more loose with the accuracy near the limits of the standard. The bfloat16 standard has a lot of uses inside machine learning algorithms, by offering better accuracy of values inside the algorithm while affording double the data in any given dataset (or doubling the speed in those calculation sections).A standard float has the bits split into the sign, the exponent, and the fraction. This is given as: * 1 + * 2For a standard IEEE754 compliant number, the standard for computing, there is one bit for the sign, five bits for the exponent, and 10 bits for the fraction. The idea is that this gives a good mix of precision for fractional numbers but also offer numbers large enough to work with.What bfloat16 does is use one bit for the sign, eight bits for the exponent, and 7 bits for the fraction. This data type is meant to give 32-bit style ranges, but with reduced accuracy in the fraction. As machine learning is resilient to this type of precision, where machine learning would have used a 32-bit float, they can now use a 16-bit bfloat16.These can be represented as:Data Type RepresentationsTypeBitsExponentFractionPrecisionRangeSpeedfloat3232823HighHighSlowfloat1616510LowLow2x Fastbfloat161687LowerHigh2x FastBF16:FP32Images fromWikipediaAs far as we understand, there was one big customer for bfloat16: Facebook.Cooper Lake Set For PrimeTime, Or NotAside from bfloat16 support, we were told that Cooper Lake was to share a socket with Ice Lake, which should mean eight-channel memory support based on data provided by Intel’s partners.In August 2019, Facebook detailed its Zion Unified Training Platform (ZION) in use inside its datacenters. The platform was designed to deal with machine learning training algorithms with sparse datasets, and in the presentation the company said that it expects that data to grow 3x year on year, and as such they were building specific training systems to meet these requirements. Part of that angle was software and hardware co-design, such that the hardware was useable as fast as possible but also the software was able to do what was required in order to achieve the results.In order to achieve some of this, Facebook stated during its presentation that it was using a unified bfloat16 format across both CPUs and accelerators in order to take advantage of both high capacity DDR but also high-bandwidth HBM. By keeping all the data in the same format as it is transferred between the CPU fabric and the accelerator fabric, it saved power and time rather than managing the conversions. It also allows the software stack to not have to worry whether it is working on a CPU or an accelerator, regardless of what number standard the training was actually being done in.Note, when this presentation was made, in August 2019, there was no CPU on the market that supported bfloat16. In the Q&A session, I asked a couple of questions: firstly, to clarify that the processorhasto have BF16 support (answer: that is how the specification is defined), and secondly which CPU was in use. Misha Smelyanskiy from Facebook, presenting the topic, stated that he could probably mention which CPU they were using, but to be on the safe side, he wasn’t going to mention, but he did at least point out that Intel had made an announcement about supporting the standard (that’s code for using Intel, given how close the two companies work together).You can see the video of that exchange here:The scope of the Zion platform is built around dual socket blades acting in a glueless-logic fashion as an eight-socket system. A 4U accelerator system would be attached to these sets of blades, creating a ZION in about 8U of server space. Each ZION would be connected in a hypercube mesh to other ZIONs.It’s important to note that the Facebook presentation was talking about systems they were already working on in their datacenters. They already had Cooper Lake silicon up and running. Given that we expect Cooper to be very similar to Cascade, this wasn’t a surprise – supporting bfloat16 in current hardware should only require a small hardware change and a firmware change at most.Normally Intel works with its lead partners on upcoming silicon. From our best estimates, there are various timeframes on how these relationships work. If we consider the official launch date as when we consider the chips on ‘general availability’, then priority partners are likely to start getting silicon up to 12 months in advance of that date.This 12-month-before-launch silicon is typically early engineering sample (ES) silicon, with the potential to have bugs and likely only working at around 1 GHz – or might not have the memory controllers working properly. With this hardware, the priority partner can start to plan their systems and build their software to be optimized for the hardware coming later. Sometimes these CPUs make their way onto eBay and such, which is why we sometimes see big CPUs at low frequencies being sold off cheap.Around six months before launch, the priority partners are likely to have qualification sample (QS) units, which are essentially near-final to launch. A lot of these QS units actually go into deployed systems with those priority partners, especially on internal projects that aren’t public facing. This means that partners like Google, Facebook, Microsoft, Tencent and others could be using +1 generation hardware on their back-end services and the public (even the tech public) don’t even know about it.In this instance, Facebook has had Cooper Lake silicon in-house for a long time. They already had it in August 2019, and given that Intel originally said that Cooper Lake was set for a 2019 launch, we expect the silicon in hand at Facebook to be near-final QS silicon, and the launch was only a short way away. Intel had statedas far back as May 2019that Cooper Lake (and Ice Lake) were already sampling with customers.2019 ended and Cooper Lake was nowhere in sight. At Intel’s 2019 investor meeting, Cooper Lake had shifted into a 2020 time slot. This is despite Intel promising a cadence speedup between new platforms.Cooper Lake Canned (Well, It Is For You and Me)Today’s news is that Intel is pulling the plug on Cooper Lake. That’s despite being a product that was ES sampling as early as 18 months ago, potentially QS sampling 12 months ago, and should be out already. If you wanted a single socket or a dual socket Cooper Lake server, then bad luck – Intel is set to only sample Cooper Lake to key customers (Facebook) who are driving quad-socket and eight-socket systems.As reported atServeTheHome, Intel gave the following guideance. We’ve split it into several segments to discuss what is being said.Given the continued success of our recent expansion of 2nd Gen Xeon Scalable products, in addition to customer demand for our upcoming 10nm Ice Lake processors, we have decided to narrow the delivery of our Cooper Lake products that best meets our market demand.Intel’s upcoming Cooper Lake processors will be supported on the Cedar Island platform, which supports standard and custom configurations that scale up to 8 sockets.Customers, including some of the largest AI innovators today, are uniquely interested in Cooper Lake’s enhanced DL Boost technology including the industry’s first inclusion of bfloat16 instruction processing support. We expect strong demand for the technology and processing capability with certain customer segments and AI usages in the marketplace that support deep learning for training and inference use cases.We continue to expect delivery of Cooper Lake starting in the first half of 2020.This is the meat of the announcement – it essentially reads that if you’re a key customer for Cooper Lake already, then you’ll get it by the end of Q2 this year. These key customers are obviously big players and will want thousands of systems, which is where Intel sees ‘strong demand’.Intel constantly evaluates our product roadmaps to ensure we are positioned to deliver the best silicon portfolio for data center platforms.Intel’s upcoming 10nm Ice Lake processors will be introduced on the upcoming Whitley platform.Intel remains on track for delivery of 10nm Ice Lake CPUs later this year.This is Intel’s ‘we have the right to adjust our roadmaps as we see fit’ clause. If I was reading between the lines, there might be an upside to these three statements – Intel might be more confident in Ice Lake than most people expect. Either that, or Intel is set to put all of its enterprise CPU eggs into a single 10nm basket.With Intel narrowing the scope of Cooper Lake to key customers, I highly doubt that we’re going to get samples for review.Personal Thoughts on Cooper LakePersonally, I’m of the opinion that Cooper Lake wasn’t actually meant to exist, at least not as a named part. We already suspect that Cooper Lake was added into the roadmap to ease the transition between Cascade Lake and Ice Lake, and that made it feel like a bit of a knee-jerk reaction to Intel’s woes. But we know that Intel already does a number of custom silicon designs for its partners, or silicon with custom firmware to get access to parts of the die that others don’t. However, I’m not entirely sure that it was intended to be a whole family of products. Intel could have happily supplied Cascade Lake silicon to Facebook (and others) that wanted BF16, and it was just enabled through firmware. No-one outside of those companies would have known about it, and we’d all be on Ice Lake by now (or Facebook would have been on BF16 versions of Ice Lake, which could potentially exist).But because of the delay, this custom edition CPU became a platform all on its own. Intel had to gear up a design for mass market, including adjusting the memory channels and the socket. It was no longer just a Cascade-Plus part, but a whole new platform. That required Intel’s partners to redesign their stack to compensate (if they didn’t know already).But now Intel has cut the high-volume parts of the Cooper Lake story away, leaving only the core for its key partners. Which again, brings me back to my original theory – these Cooper Lake parts were only meant to be a Cascade-Plus type design for key customers. Similar to how process node advancements in-between nodes weren’t publicised (e.g. several improvements on 45nm before 32nm to get better voltage/frequency), but nowadays they are, Cooper Lake kind of feels like that. Except now it’s going back into the shadows. Who knows how many other custom variants of Intel’s CPUs with different instructions, or different QoS policies, or different RAS options, exist. Intel doesn’t talk about them, unless it needs to.Focusing on Ice Lake 2020 #IceLake2020If we consider Cooper Lake done and dusted for the mass market, the attention turns to Ice Lake, and Intel’s ability to execute on 10nm. Intel technically showed off what it called an Ice Lake CPU in December 2018 at its Architecture Day, but as we’ve seen in the mobile and desktop space, 10nm is currently having a hard time.Sailesh Kottapalli of Intel, showing an Ice Lake Xeon CPU, which was set to share a socket with Cooper LakeThere have repeated reports about Intel’s Ice Lake Xeon delays, some as recent as December 2019, saying that the platform has been delayed more and more, putting into doubt as to whether Intel can get general availability for Ice Lake Xeon inside 2020. There are also discussions about core counts, frequencies, power, and whether Intel will have to move to a dual-die strategy for Ice Lake in order to maintain core count pace with otherx86andArmcompetition, who are hitting 64 cores per socket.Intel’s CFO GeorgeDavis has already stated Intel’s position on its 10nm portfolio, about how the financial bottom line of the platform is likely to be lower than 14nm and 22nm, and how the company is looking to its 7nm process to regain parity with the competition. If there are subsequent delays to Ice Lake, or the platform is going to look substantially different compared to the monolithic designs we’ve all been expecting to this point, then Intel is going to take hit to both adoption of the new platform and its bottom line.Customers moving from old Broadwell Xeon (Q1 2016) systems or Skylake Xeon (Q3 2017), who normally have a 3-5 year update cycle, are desperately looking at something to update to.Sources:SemiAccurate,ServeTheHome\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15631/intels-cooper-lake-plans-the-chip-that-wasnt-meant-to-exist-dies-for-you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD Details Renoir: The Ryzen Mobile 4000 Series 7nm APU Uncovered\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-03-16T15:00:00Z\n",
      "URL: https://www.anandtech.com/show/15624/amd-details-renoir-the-ryzen-mobile-4000-series-7nm-apu-uncovered\n",
      "Content: The notebook market has not been kind to AMD over the last decade – for a long, long time the company was only ever seen as the discount option for those on a strict budget. It didn’t help that OEMs only saw AMD in that light, fitting bulky units with sub-standard displays and storage options meant that even retailers were only presenting AMD as something for the budget conscious.All that seems set to change. Fast forward to 2020, and notebook users are eagerly awaiting the arrival of products based on AMD’s latest Ryzen Mobile 4000 series processors, which combine up to eight Zen 2 cores and upgraded Vega graphics into a small CPU for the notebook market. AMD has already made waves with its Zen 2 cores in the desktop and enterprise space, and the company has already announced it plans to put eight of those cores, along with a significantly upgraded graphics design, into a processor that has a thermal design point of 15 W. These 15 W parts are designed for ultraportable notebooks, and AMD has a number of design wins lined up to show just how good an AMD system can be.The same silicon will also go into 45 W-class style notebooks, with a higher base frequency. These parts are geared more towards discrete graphics options, for gaming notebooks or more powerful business designs. The gaming market (at 45 W), the commercial market (15W to 45W) and the ultraportable market (15 W) are where AMD is hoping to strike hardest with the new hardware.Since earlier this year in January, at the annual CES trade show, we saw a number of early designs based on the new Ryzen Mobile 4000 family. These included TUF laptops from ASUS, the Lenovo Yoga Slim 7, Lenovo Thinkpads using Ryzen Mobile 4000 Pro, Dell’s G5 15 SE, the Acer Swift 3, and the ASUS Zephyrus G14 to name but a few. All of these are key design wins for different segments of the market, and the two that AMD seem to be pushing most are the Zephyrus and the Yoga Slim 7.The rear panel of the ASUS Zephyrus G14 with its LED rearThe ASUS Zephyrus G14 is set to be the only 14-inch laptop on the market that has both a H-series processor, a 1080p 120Hz panel, and an RTX 2060 discrete graphics card solution in that form factor. The aim here is to have something both portable and high performance, with within the right thermal envelope, for gamers and users who need something with a bit more oomph while on the road, such as video editors who need up to 32 GB of DDR4 inside. There’s an added rear panel effect with moveable LEDs, just for a DJ to show off or to show a logo. The Zephyrus G14 will also be the first design with a HS-series processor, which we’ll cover in a bit.The second key system AMD is promoting is an ultraportable, the Lenovo Yoga Slim 7. It comes with the highest grade Ryzen Mobile 4000 15 W U-series processor, the Ryzen 7 4800U, and is designed to turbo up to 25 W when needed based on the design of the chassis. Paired with Wi-Fi 6, a FreeSync display, and LPDDR4X, this is the system that AMD is using for all their battery life performance demonstrations.AMD is working with Lenovo to source these units for press sampling, which should have been for today, however due to the world situation the shipment of these have been delayed, and users will start to see reviews from next month, even though they might be available in China before then.The Processor OfferingsAs with Intel’s mobile processors, AMD’s latest lines fall into two categories. For the ultraportable and low end gaming market, we have 15 W parts called ‘U-Series’. For the gaming market where discrete GPUs are used, there are 45 W parts called ‘H-Series’. The commercial market will take from both sets, and later in the year we might see mini-PC manufacturers (like Zotac, perhaps) use one or the other to bolster their portfolio.Not previously announced until today is the AMD Ryzen 9 4900H family, the new halo Ryzen Mobile 4000 hardware. These are AMD’s first processors with the Ryzen 9 designation, and we have a specific news story about themhere.AMD Ryzen Mobile 4000 APUsAnandTechCoresThreadsBaseFreqTurboFreqL2L3GPU CUsGPU FreqTDPH-SeriesRyzen 9 4900H8 / 163.3 GHz4.4 GHz4 MB8 MB8 / 1750 MHz45 WRyzen 9 4900HS8 / 163.0 GHz4.3 GHz4 MB8 MB8 / 1750 MHz35 WRyzen 7 4800H8 / 162.9 GHz4.2 GHz4 MB8 MB7 / 1600 MHz45 WRyzen 7 4800HS8 / 162.9 GHz4.2 GHz4 MB8 MB7 / 1600 MHz35 WRyzen 5 4600H6 / 123.0 GHz4.0 GHz3 MB8 MB6 / 1500 MHz45 WRyzen 5 4600HS6 / 123.0 GHz4.0 GHz3 MB8 MB6 / 1500 MHz35 WThe H series processors are split into H and HS parts. For all except the Ryzen 9, the specifications between the two match, aside from the TDP, which is 45 W for the H and 35 W for the HS, but both of them are considered ‘H-Series class’ processors. Technically the H series can be de-rated to run at 35 W, however to get the S in the name requires collaboration with AMD, which we’ll get into later.AMD Ryzen Mobile 4000 APUsAnandTechCoresThreadsBaseFreqTurboFreqL2L3GPU CUsGPU FreqTDPU-SeriesRyzen 7 4800U8 / 161.8 GHz4.2 GHz4 MB8 MB8 / 1750 MHz15 WRyzen 7 4700U8 / 82.0 GHz4.1 GHz4 MB8 MB7 / 1600 MHz15 WRyzen 5 4600U6 / 122.1 GHz4.0 GHz3 MB8 MB6 / 1500 MHz15 WRyzen 5 4500U6 / 62.3 GHz4.0 GHz3 MB8 MB6 / 1500 MHz15 WRyzen 3 4300U4 / 42.7 GHz3.7 GHz2 MB4 MB5 / 1400 MHz15 WThe U-series parts, by the nature of the lower TDP, ultimately have a lower base frequency than the others. These CPUs also tend to rely more on the integrated graphics, which means that the power budget is often split between the CPU and GPU. AMD is also going for an interesting mix here of parts with-and-without simultaneous multithreading. The bottom processor, the Ryzen 3 4300U, even has half of its L3 cache disabled.All of these CPUs support DDR4-3200 (up to 64 GB, 51.2 GB/s) and LPDDR4X-4266 (up to 32 GB, 68.3 GB/s), and it will be up to the OEM which one to use: LPDDR4X should offer better idle battery life and peak performance, but DDR4 offers more capacity. It is likely that we’ll see the ultraportable market use LPDDR4X, while the more gaming and workstation class systems will use DDR4.All of the CPUs are PCIe 3.0 only, rather than PCIe 4.0 like the desktop parts. This is primarily due to power – the double bandwidth of PCIe 4.0 requires more power, and given that storage or graphics rarely need peak speeds, AMD felt the product portfolio would prefer battery life in this regard. Each chip has sixteen PCIe 3.0 lanes, split such that x8 is available for a graphics card, and two x4 links for storage. There are separate PCIe lanes for other modules such as Wi-Fi 6 or mobile network access (4G/5G).Display support for the CPUs allows for two 4K monitors through DisplayPort over Type-C, an additional 4K monitor if Thunderbolt is used, and a fourth monitor if USB 4.0 used. AMD has designed Renoir to not need additional chips to detect which way a Type-C is connected – that is all handled on die. With the display and USB support, the processor allows for concurrent USB 3.2 and DisplayPort use, with the peak DP v1.4 8.1G HBR3 standard in play using display stream compression (DSC).Silicon DetailsAMD surprised us by offering some details on the silicon here. The APU was manufactured on TSMC’s N7 process (7nm DUV), using a 13-layer metal stack. The whole die is 9.8 billion transistors. In January, we calculated through photography the die size to be about 150-151 mm2. AMD is stating that it is 156 mm2, which given previous measurements, probably doesn’t include scribe lines.While we don’t have performance numbers for Renoir today, due to world events, we do have some deeper details into the platform that have not been disclosed before. These cover CPU and GPU improvements, significant changes to power management, Infinity Fabric, and how AMD is taking better control of thermals, performance, and battery life this type around.AMD has stated that they expect to see 100+ designs using Renoir this year, with a number of those being key design wins that the company has not had in recent memory. Considering where the company wasonly four years ago, surrounded in a vicious negative feedback loop, this is a significant upswing in OEM participation, putting AMD in premium designs. Ultimately it’s the consumer who wins, as we should now see some serious competition in the notebook market.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15624/amd-details-renoir-the-ryzen-mobile-4000-series-7nm-apu-uncovered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Marvell Announces ThunderX3: 96 Cores & 384 Thread 3rd Gen Arm Server Processor\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-03-16T12:30:00Z\n",
      "URL: https://www.anandtech.com/show/15621/marvell-announces-thunderx3-96-cores-384-thread-3rd-gen-arm-server-processor\n",
      "Content: The Arm server ecosystem is well alive and thriving, finally getting into serious motion after several years of false-start attempts. Among the original pioneers in this space was Cavium, which went on to be acquired by Marvell in 2018. Among the company’s server CPU products is the ThunderX line; while thefirst generation ThunderXleft quite a lot to be desired, theThunderX2 was the first Arm server silicon that we deemed viable and competitiveagainst Intel and AMD products. Since then, the ecosystem has accelerated quite a lot, and only last week we saw howimpressive the new Amazon Graviton2 with the N1 chips ended up. Marvell didn’t stop at the ThunderX2, and had big ambitions for its newly acquired CPU division, and today is announcing the new ThunderX3.The ThunderX3 is a continuation and successor to then-Cavium’s custom microarchitecture found in the TX2, adopting a lot of the key characteristics, most notably the capability of 4-way SMT. Adopting a new microarchitecture with higher IPC capabilities, the new TX3 also ups the clock frequencies, and now hosts up to a whopping 96 CPU cores, allowing the chip to scale up to 384 threads in a single socket.Marvell sees the ecosystem shifting in terms of workloads as more and more applications are shifting to the cloud, and applications are changing in their nature, with more customers employing their own custom software stacks and scaling out these applications. This means that workloads aren’t necessarily focused just on single-threaded performance, but rather on the total throughput available in the system, at which point power efficiency also comes into play.Like many other Arm vendors, Marvell sees a window of opportunity in the lack of execution of the x86 incumbents, very much calling out Intel’s stumbling in process leadership over the past few years, and in general x86 designs being higher power. Marvell describes that part of the problem is that the current systems by the x86 players were designed with a wide range of deployment targets ranging from consumer client devices to the actual server machines, never actually achieving the best results in either workloads. In contrast, the ThunderX line-up is reportedly designed specifically with server workloads in mind, being able to achieve higher power efficiency and thus also achieving higher total throughput in a system.We’ve known that ThunderX3 has been coming for quite a while now, admittedly expecting it towards the latter half of 2019. We don’t know the behind-the-scenes timeline, but now Marvell is finally ready to talk about the new chip. Marvell’s CPU roadmap is on a 2-year cadence, and the chip company here explains that this is a practical timeline, allowing customers time to actually adopt a generation and get good return on investment on the platform before possibly switching over to the next one. Of course, this also gives the design team more time to bring to market larger performance jumps once the new generations are ready.The ThunderX3 - 96 Cores and 384 Threads in Arm v8.3+So, what is the new ThunderX3? It’s a ambitious design hosting up to 96 Arm v8.3+ custom cores running at up to frequencies of up to 3GHz all-core, at TDPs ranging from 100 to 240W depending on the SKU.Marvell isn’t quite ready to go into much details of the new CPU microarchitecture just yet, saying that they’ll divulge a deeper disclosure of the TX3 cores later in the year (They’re aiming for Hotchips), but they do say that one key characteristic is that it now features 4 128-bit SIMD execution units, matching the vector execution throughput of AMD and Intel’s cores. When fully using these units, clock frequencies for all-core drop between 2.2 and 2.6GHz, limited by the thermal and power headroom available to the chip.Having SMT4, the 96-core SKU is able to scale up to 384 threads in a socket, which is by far the highest thread count of any current and upcoming server CPU in the market, a big differentiating factor for the ThunderX3.Marvell doesn’t go into details of the topology of the chip or its packaging technology, only alluding that it’ll have monolithic latencies between the CPU cores. The design comes in either 1 or 2 socket configurations, and the inter-socket communication uses CCPI (Cavium Cache Coherent Interconnect) in its 3rdgeneration, with 24 lanes at 28Gbit/s each, between the two sockets.External connectivity is handled by 64 lanes of PCIe 4.0 with 16 controllers per socket, meaning up to 16 4x devices, with the choice of multiplexing them for higher bandwidth connectivity for 8x or 16x devices.Memory capabilities of the chip is in line with current generation standards, featuring 8 DDR4-3200 memory controllers.Marvell plans several SKUs, scaling the core count and memory controllers, in TDP targets ranging from 100W to 240W. These will all be based on the same silicon design, and binning the chips.Large Generational Performance ImprovementsIn a comparison to the previous generation ThunderX2, the TX3 lists some impressive performance increases. IPC is said to have increased by a minimum of 25% in workloads, with total single-threaded performance going up to at least 60% when combined with the clock frequency increases. If we use the TX2 figures we have at hand, this would mean the new chip would land slightly ahead of Neoverse-N1 systemssuch as the Graviton2, and match more aggressively clocked designs such asthe Ampere Altra.Socket-level integer performance has at least increased by 3-fold, both thanks to the more capable cores as well as their vastly increased core number to up to 96 cores. Because the new CPU has now more SIMD execution units, floating point performance is even higher, increasing to up to 5x.Because the chip comes with SMT4 and it’s been designed with cloud workloads, it is able to extract more throughput out of the silicon compared to other non-SMT or SMT2 designs. Cloud workloads here essentially means data-plane bound workloads in which the CPU has to wait on data from a more distant source, and SMT helps in such designs in that the idle execution clocks between data accesses is simply filled by a different thread, doing long latency accesses itself.ThunderX3 Performance Claims Against the CompetitionUsing this advantage, the ThunderX3 is said to have significant throughput advantages compared to the incumbent x86 players, vastly exceeding the performance of anything that Intel has currently to offer, and also beating AMD’s Rome systems in extremely data-plane bound workloads thanks to the SMT4 and higher core counts.More execution and compute bound workloads will see the least advantages here, as the SMT4 advantages greatly diminishes.Yet for HPC and in particular floating-point workloads, the ThunderX3 is said to also be able to showcase its strengths thanks to the increased SIMD units as well as the overall power efficiency of the system, allowing for significant higher performance in such calculations. Memory bandwidth is also higher than a comparative AMD Rome based system because the lower latencies the TX3 is able to achieve. It’s to be noted that the ThunderX3 will be coming to market later in the year, by which time they’ll have to compete with AMD’s newer Milan server CPU.Marvell says that Arm in the cloud is gaining a lot of traction, and the company is already the market leader in terms of deployments of its ThunderX2 system among companies and hyperscalers (Microsoft Azure currently being the one publicly disclosed, but it’s said that there are more). I don’t really know if having a extremely high number of virtual machines being hosted on a single chip is actually an advantage (because of SMT4, per-VM performance might be quite bad), but Marvell does state that they’d be the leader in this metric with the ThunderX3, thanks to be able to host up to 384 threads.Finally, the company claims a 30% perf/W advantage over AMD’s Rome platform across an average of different workloads, thanks to the more targeted microarchitecture design. The more interesting comparison here would have been a showcase or estimate of how the ThunderX3 would fare against Neoverse-N1 systems such as the Graviton2 or the Altra, as undoubtedly the latter system would pose the closest competitor to the new Marvell offering. Given that the Altra isn’t available yet, we don’t know for sure how the systems will compete against each other, but I do suspect that the ThunderX3 to do better in at least FP workloads, and of course it has an indisputable advantage in data-plane workloads thanks to the SMT4 capability.More Information at Hotchips 2020Marvell hasn’t yet disclosed much about the cache configuration or any other specifics of the system, for example what kind of interconnect the cores will be using or what kind of CPU topology they will be arranged in. The ThunderX3’s success seemingly will depend on how it’s able to scale performance across all of its 96 cores and the 384 threads – but at least as an initial impression, it seems that it might do quite well.Today is just the initial announcement of the TX3, and Marvell will be revealing more details and information about the new CPU and the product line-up over the following months till the eventual availability later in the year.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15621/marvell-announces-thunderx3-96-cores-384-thread-3rd-gen-arm-server-processor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Ultimate Hacking Keyboard Review: A Truly Unique, Truly Expensive Keyboard for Pros\n",
      "Author: E. Fylladitakis\n",
      "Date Published: 2020-03-12T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/15612/the-ultimate-hacking-keyboard-review-truly-unique-for-pros\n",
      "Content: The explosive growth of the mechanical keyboard market over the past several years has led to an overly saturated market, with hundreds of products covering every desire and niche. Competition is certainly a good thing from a consumer’s point of view, but there is relatively little room to differentiate on features when it comes to keyboards, forcing most companies to focus their development and design efforts almost entirely on aesthetics.Today we are taking a look at a product that (thankfully) breaks away from the norm: theUltimate Hacking Keyboard. The company, Ultimate Gadget Laboratories, is currently exclusively focused on its development and marketing. It is a 60% mechanical keyboard designed with productivity in mind, implementing unique features and is made in Hungary – but also comes with an exorbitant price tag.Diving right into matters, we received the Ultimate Hacking Keyboard inside an aesthetically simple cardboard box, with little artwork printed on it, yet very well-designed and practical. The keyboard and its bundle are neatly packed inside.Inside the box, we found a basic user’s manual, one plastic keycap puller, a bag of rubber o-rings, and the keyboard’s USB cable. The o-rings are optional and can be purchased alongside the keyboard. They are meant to reduce the keystroke noise coming from the keycap bottoming down on the body of the switch but they will also reduce the effective key travel.Dissecting the Ultimate Hacking KeyboardThe Ultimate Hacking Keyboard definitely is unlike any mechanical keyboard that we have reviewed to date. It is based on a 60% layout, making it the most compact mechanical keyboard that has ever come through our labs. The aesthetic design is simplistic and serious, as anyone would expect from a keyboard targeting very specific segments of IT professionals.The 60% layout means that the keyboard is missing more than just its Numpad, as tenkeyless designs do. There are no arrow/control keys or a function key row at all. The standard function keys (F1-F12), control keys (Home, End, etc.), arrow keys, and more can be accessed by holding down the Fn key. It will be easy for an expert who has been using such a layout for a while to perform these functions but it will definitely be confusing for amateurs and users who switch between more than one keyboard every day.The keycaps of the Ultimate Hacking Keyboard are simple ABS plastic, with the company offering more durable PBT keycaps as an extra. The main keys have their symbols printed right at the center of the keycap, while keys with extra characters have their primary and secondary characters printed towards the bottom and the top of the keycap respectively. Most keys also have a third character/function printed to their front side, which indicates the default function of the key while the Fn key is being held down.The Ultimate Hacking Keyboard also has two more keys meant for keystroke combinations – the Mod and Mouse keys. The Mod key is to the left of the Space Bar and the Mouse key is where the Caps Lock key is by default. This means that every layout of the keyboard can simultaneously have four different layers – the default layer, the Fn layer, the Mod layer, and the Mouse layer. Using the software, users can also choose for the layers to be active only while the appropriate key is being held pressed, or for a key to permanently toggle the layout to that layer. A seasoned expert can program more functions into a single profile than a regular person can possibly remember.The 60% layout of the Ultimate hacking keyboard does not follow any established ISO/ANSI standards. The bottom row will appear to be entirely chaotic for most users, with the Space Bar key being smaller than the Ctrl key. There are two buttons below the Space Bar and Mod keys, hidden within the design of the keyboard’s frame. A sizable three-character display can be found at the top left part of the keyboard, next to the subtle indicator LEDs.Perhaps the basic feature of the Ultimate Hacking Keyboard is that it can be split in two. The stock cable provided allows for the two halves to be placed up to 30 cm apart, with the company providing instructions on how to make your own cable as well. The keyboard is designed so as to take expansion modules (trackball, touchpad, extra keys, etc.) but none of these modules were available to us at the time of this review.The company provided us with a set of wrist rests for the Ultimate Hacking Keyboard, which are made of real wood. They are not very soft but certainly feel very classy. These also add the options of tenting and negatively tilting to the keyboard, which can only be positively tilted without the wrist rests attached.Removing the keycaps of the Ultimate Hacking Keyboard reveals mechanical switches made by Kailh, a huge surprise for a keyboard of this price range. They do offer the keyboard with original Cherry MX switches but only the Green and White variants are available, at no extra cost. Choosing any of the “standard” switches, Red, Blue, Brown, or Black, Kailh switches are the only available option. It is possible that the manufacturer cannot source original Cherry MX switches for some reason and is forced to go with Kailh switches instead.Removing the plastic frame of the keyboard reveals two PCBs with Kailh mechanical switches attached and shiny steel plates providing mechanical support. It is a shame that the keyboard has no backlighting, as we suspect that the polished metal would make it look great at low intensity levels. The assembly job is excellent but the metal is not too sturdy on its own, relying on the thick plastic frame for mechanical strength.The heart of the Ultimate Hacking Keyboard is an NXP K22 series microcontroller (MK22FN512). It is a very powerful microcontroller with an ARM-based 120 MHz processor, 128KB of RAM, and 512KB Flash memory. Such a microcontroller is a little bit of an overkill for a keyboard without fancy backlighting to worry about and highly advanced Macro commands – then again, a little bit of processing power overkill never hurt anyone.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15612/the-ultimate-hacking-keyboard-review-truly-unique-for-pros\n",
      "Title: Load Value Injection: A New Intel Attack Bypasses SGX with Significant Performance Mitigation Concerns\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-03-11T12:00:00Z\n",
      "URL: https://www.anandtech.com/show/15608/load-value-injection-a-new-intel-attack-bypasses-sgx-with-significant-performance-mitigation-concerns\n",
      "Content: Microarchitectural attacks have been all the rage. For the past two years, we’ve seen attacks like Meltdown, Spectre, Foreshadow/L1TF, Zombieload, and variants all discuss different ways to probe or leak data from a victim to a host. A new attack, published on March 10thby the same research teams that found the previous exploits, turns this principle on its head, and allows an attacker to inject their own values into the victim’s code. The data injection can either be instructions or memory addresses, allowing the attacker to obtain data from the victim. This data injection bypasses even stringent security enclave environments, such as Intel’s Software Guard Extensions (SGX), and the attackers claim that successful mitigation may result in a slowdown of 2x to 19x for any SGX code.The High Level OverviewThe attack is formally known as LVI, short for ‘Load Value Injection’, and has the MITRE referenceCVE-2020-0551. The official website for the attack ishttps://lviattack.eu/. The attack was discovered on April 4th2019 and reported to Intel, and disclosed publicly on March 10th2020. A second group discovered and produced a proof-of-concept for one LVI attack variant in February 2020.Currently Intel has plans to provide mitigations for SGX-class systems, however non-SGX environments (such as VMs or containers that aren’t programmed with SGX) will remain vulnerable. The researchers state that ‘in principle any processor that is vulnerable to Meltdown-type data leakage would also be vulnerable to LVI-style data injection’. The researchers focus was primarily on breaking Intel SGX protections, andproof of concept code is available. Additional funding for the project was provided by ‘generous gifts from Intel, as well as gifts from ARM and AMD’ – one of the researchers involved has stated on social media that some of his research students are at least part-funded by Intel.Intel was involved in the disclosure, and has asecurity advisoryavailable, listing the issue as a 5.6 MEDIUM on the severity scale. Intel also listsall the processors affected, including Atom, Core and Xeon, which goes as far back as Silvermont, Sandy Bridge, and even includes the newest processors, such as Ice Lake (10thGen)* and the Tremont Atom core, which isn’t in the market yet.*The LVI website says that Ice Lake isn’t vulnerable, however Intel’s guidelines says it is.*Update: Intel has now updated its documents to say both Ice Lake and Tremont are not affected.All told, LVI's moderate CVE score is the same as the scores assigned to Meltdown and Spectre back in 2018. This reflects the fact that LVI has a similar risk scope as those earlier exploits, which is to say data disclosure. Though in practice, LVI is perhaps even more niche. Whereas Meltdown and Spectre were moderately complex attacks that could be used against any and all \"secure\" programs, Intel and the researchers behind LVI are largely painting it as a theoretical attack, primarily useful against SGX in particular.The practical security aspects are a mixed bag, then. For consumer systems, at least, SGX is rarely used outside of DRM uses (e.g. 4K Netflix), which isn't likely to upend too much. None the less, the researchers behind LVI have toldZDNet that the attack could theoretically be delivered via JavaScript, so it could potentially be delivered in a drive-by fashion, as opposed to requiring some kind of local code execution. The upshot, at least, is that LVI is already thought to be very hard to pull off, and JavaScript certainly wouldn't make that any easier.As for enterprise and business users, the potential risk is greater due to both the more widespread use of SGX there, and the use of shared systems (virtualization). Ultimately such concerns are going to be on a per-application/per-environment basis, but in the case of shared systems in particular, the biggest risk is leaking information from another VM, or from a higher privileged user. Enterprises, in turn, are perhaps the best equipped to deal with the threat of LVI, but it comes after Meltdown and Spectre already upended things and hurt system performance.The AttackLoad Value Injection is a four stage process:The attacker fills a microarchitectural buffer with a valueThis induces a fault or assisted load within the victim’s software (by redirecting the dataflow)The attacker’s value invokes a ‘code gadget’, allowing attacker instructions to be runThe attacker hides traces of the attack to stop the processor detecting itThe other recent microarchitectural exploits, such as Spectre, Meltdown, L1TF, Zombieload and such, are all related to data leaks. They rely on data to be leaked or extracted from various buffers that are ‘all-access’ from the microarchitectural standpoint. LVI is different, in that it’s more of a direct ‘attack’ on the system in order to extract that data. While it means the attacker has to clean up after themselves, as a result of what the attack can do, it means it can be more dangerous than other previous exploits. The difference in the exploit means that current mitigations don’t work here, and this exploit according to the research essentially states that Intel’s secure enclave architecture requires significant changes in order to be useful again.The focus of the attack has been on Intel’s secure enclave strategy, known as SGX, due to the nature of the technology. As reported byThe Register, it is in fact the nature of SGX that actually assists the attack – SGX creates page faults for memory loads by altering non-secure buffer page tables (point 2 above).Intel’s Own AnalysisIntel’s own deep diveinto the problem explains that:‘If an adversary can cause a specified victim load to fault, assist, or abort, the adversary may be able to select the data to have forwarded to dependent operations by the faulting/assisting/aborting load.For certain code sequences, those dependent operations may create a covert channel with data of interest to the adversary. The adversary may then be able to infer the data's value through analyzing the covert channel.’Intel goes on to say that in a fully trusted environment, this shouldn’t be an issue:‘Due to the numerous, complex requirements that must be satisfied to implement the LVI method successfully, LVI is not a practical exploit in real-world environments where the OS and VMM are trusted.’But then states that the fact that its own SGX solution is the vector for the attack, these requirements aren’t as strict.‘Because of Intel SGX's strong adversary model, attacks on Intel SGX enclaves loosen some of these requirements. Notably, the strong adversary model of Intel SGX assumes that the OS or VMM may be malicious, and therefore the adversary may manipulate the victim enclave's page tables to cause arbitrary enclave loads to fault or assist.’Then to state the obvious, Intel has a line for the ‘if you’re not doing anything wrong, it’s not a problem’ defense.‘Where the OS and VMM are not malicious, LVI attacks are significantly more difficult to perform, even against Intel SGX enclaves.’As a poignant ending, Intel’s official line is that this issue is not much of a concern for non-SGX environments where the OS and VMM are trusted. The researchers agree – while LVI is particularly severe for SGX, they believe it is more difficult to mount the attack in a non-SGX setting. That means that processors from other companies are less vulnerable to this style of attack however, those that are susceptible to Meltdown might be able to be compromised.The Fix, and the CostBoth Intel and the researchers have provided the same potential solution to the LVI class of attacks. The fix isn’t being planned at the microcode level, but at the code level, with compilers and SDK updates. The way to get around this issue is to essentially make instructions serialized through the processor, ensuring a very specific order of control.Now remember that a lot of modern day processor performance relies on several things, such as the ability to rearrange micro-ops inside a core (out-of-order), and run multiple micro-ops in a single cycle (instructions per cycle). What these fixes do is essentially eliminate both of these when potentially attackable instructions are in flight.For those that aren’t programmers, there exists a term in programming called a ‘fence’. A broad definition of a fence is to essentially make sure a program (typically a program running across several cores) stop at a particular point, and check to make sure everything is ok.So, for example, imagine you have one core doing an addition, and another core doing a division at the same time. Now addition is a lot quicker than division, and therefore if there are a lot of parallel calculations to do, you might be able to fire off 4-10 additions in the time it takes to do a single division. However, if there is the potential for the additions or divisions to interact on the same place in memory, you might need a fence after a single addition+division, to make sure that there’s no conflict.In a personal capacity, when I wrote compute programs for GPUs, I had to use fences when I moved from a parallel portion of my code to a serial portion of my code, and the fence made sure that everything I needed for the serial portion of my code, computed from the parallel portion, had been completed before moving on.So the solution to LVI is to add these fences into the code – specifically after every memory load. This means that the system/program has to wait until every memory load is complete, essentially stalling the core for 100 nanoseconds or more. There is a knock on effect in that when a function returns a value, there are various ways for the ‘return’ to be made, and some of those are no longer viable with the new LVI attacks.The researchers are quite clear in how this fix is expected to hurt performance – depending on the applications and various optimizations, we’re likely to see slowdowns from 2x to 19x. The researchers examined compiler variants on an i7-6700K for OpenSSL and an i9-9900K for SPEC2017.Intel has not commented on potential performance reductions.For those that could be affected, Intel gives the following advice for SGX system users:Ensure the latest Intel SGX PSW 2.7.100.2 or above forWindowsand 2.9.100.2 or above forLinuxis installedAnd for SGX Application Providers:Reviewthe technical details.Intel is releasing an SGX SDK update to assist the SGX application provider in updating their enclave code. To apply the mitigation, SDK version 2.7.100.2 or above forWindowsand 2.9.100.2 or above forLinuxshould be used.Increase the Security Version Number (ISVSVN) of the enclave application to reflect that these modifications are in place.For solutions that utilize Remote Attestation, refer to theIntel SGX Attestation Technical Detailsto determine if you need to implement changes to your SGX application for the purpose of SGX attestation.Final WordsFrom the researchers, they toldThe Registerthat:\"We believe that none of the ingredients for LVI are exclusive to Intel processors. However, LVI turns out to be most practically exploitable on Intel processors … certain design decisions that are specific to the Intel SGX architecture (i.e. untrusted page tables). We consider non-SGX LVI attacks [such as those on AMD, Arm and others] of mainly academic interest and we agree with Intel's current assessment to not deploy extra mitigations for non-SGX environments, but we encourage future research to further investigate LVI in non-SGX environments,\"In the same light, all major chip architecture companies seem to have been told of the findings in advance, as well as Microsoft should parts of the Windows kernel need adjustment.Technically, there are several variants of LVI, depending on the types of data and attack:All can be found on theLVI website.Overall, Intel has had a rough ride with its SGX platform. It had a complicated launch with Skylake, not being enabled on the first batches of processors then being enabled in later batches, and then SGX has been the focus of a number of these recent attacks on processors. The need for a modern core, especially one involved in everything from IoT all the way up to the cloud and Enterprise, to have an equivalent of a safe enclave architecture is paramount, and up until this point it has been added to certain processors, rather than necessarily being built from the ground up with it in mind – we can see that with Ice Lake and Tremont still affected. The attack surface of Intel’s SGX solution, compared to those from AMD or Apple, has grown in recent months due to these new attacks based on a microarchitectural level, and the only way around them is to invoke performance limiting restrictions on code development. Some paradigm has to change.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15608/load-value-injection-a-new-intel-attack-bypasses-sgx-with-significant-performance-mitigation-concerns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Amazon's Arm-based Graviton2 Against AMD and Intel: Comparing Cloud Compute\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-03-10T12:30:00Z\n",
      "URL: https://www.anandtech.com/show/15578/cloud-clash-amazon-graviton2-arm-against-intel-and-amd\n",
      "Content: It’s been a year and a half since Amazon released their first-generation Graviton Arm-based processor core, publicly available in AWS EC2 as the so-called 'A1' instances. While the processor didn’t impress all too much in terms of its performance, it was a signal and first step of what’s to come over the next few years.This year, Amazon is doubling down on its silicon efforts, havingannounced the new Graviton2 processorlast December, and planning public availability on EC2 in the next few months. The latest generation implements Arm’s new Neoverse N1 CPU microarchitecture and mesh interconnect, a combined infrastructure orientedplatform that we had detailed a little over a year ago. The platform is a massive jump over previous Arm-based server attempts, and Amazon is aiming for nothing less than a leading competitive position.Amazon’s endeavours in designing a custom SoC for its cloud services started back in 2015, when the company acquired Isarel-based Annapurna Labs. Annapurna had previously worked on networking-focused Arm SoCs, mostly usedin products such as NAS devices. Under Amazon, the team had been tasked with creating a custom Arm server-grade chip, and the new Graviton2 is the first serious attempt at disrupting the space.So, what is the Graviton2? It’s a 64-core monolithic server chip design, using Arm’s new Neoverse N1 cores (Microarchitectural derivatives of themobile Cortex-A76 cores) as well as Arm’sCMN-600 mesh interconnect. It’s a pretty straightforward design that is essentially almost identical to Arm’s 64-core reference N1 platform that the company had presented back a year ago. Amazon did diverge a little bit, for example the Graviton2’s CPU cores are clocked in at a bit lower 2.5GHz as well as including only 32MB instead of 64MB of L3 cache into the mesh interconnect. The system is backed by 8-channel DDR-3200 memory controllers, and the SoC supports 64 PCIe4 lanes for I/O. It’s a relatively textbook design implementation of the N1 platform, manufactured on TSMC’s 7nm process node.The Graviton2’s potential is of course enabled by the new N1 cores. We’ve already seen the Cortex-A76perform fantasticallyin last year’s mobile SoCs, and the N1 microarchitecture is expected to bring even better performance and server-grade features, all whilst retaining the power efficiency that’s made Arm so successful in the mobile space. The N1 cores remain very lean and efficient, at a projected ~1.4mm² for a 1MB L2 cache implementation such as on the Graviton2, and sporting excellent power efficiency at around ~1W per core at the 2.5GHz frequency at which Amazon’s new chip arrives at.Total power consumption of the SoC is something that Amazon wasn’t too willing to disclose in the context of our article – the company is still holding some aspects of the design close to its chest even though we were able to test the new chipset in the cloud. Given the chip’s more conservative clock rate, Arm’s projected figure of around 105W for a 64-core 2.6GHz implementation, and Ampere’s recent disclosure of their 80-core 3GHz N1 server chip coming in at 210W, we estimate that the Graviton2 must come in around anywhere between 80W as a low estimate to around 110W for a pessimistic projection.Testing In The Cloud With EC2Given that Amazon’s Graviton2 is a vertically integrated product specifically designed for Amazon’s needs, it makes sense that we test the new chipset in its intended environment (Besides the fact that it’s not available in any other way!). For the last couple of weeks, we’ve had preview access for Amazon Web Services (AWS) Elastic Compute Cloud (EC2) new Graviton2 based “m6g” instances.For readers unfamiliar with cloud computing, essentially this means we’ve been deploying virtual machines in Amazon’s datacentres, a service for which Amazon has become famous for and which now represents a major share of the company’s revenues, powering some of the biggest internet services on the market.An important metric determining the capabilities of such instances is their type (essentially dictating what CPU architecture and microarchitecture powers the underlying hardware) and possible subtype; in Amazon’s case this refers to variations of platforms that are designed for specialised use-cases, such as having better compute capabilities or having higher memory capacity capabilities.For today’s testing we had access to the “m6g” instances which are designed for general purpose workloads. The “6” in the nomenclature designates Amazon’s 6thgeneration hardware in EC2, with the Graviton2 currently being the only platform holding this designation.Instance Throughput Is Defined in vCPUsBeyond the instance type, the most important other metric that defined an instance’s capabilities is its vCPU count. “Virtual CPUs” essentially means your logical CPU cores that’s available to the virtual machine. Amazon offers instances ranging from 1 vCPU to up to 128, with the most common across the most popular platforms coming in sizes of 2, 4, 8, 16, 32, 48, 64, and 96.The Graviton2 being a single-socket 64-core platform without SMT means that the maximum available vCPU instance size is 64.However, what this also means, is that we’re quite in a bit of an apples-and-oranges conundrum of a comparison when talking about platforms which do come with SMT. When talking about 64 vCPU instances (“16xlarge” in EC2 lingo), this means that for a Graviton2 instance we’re getting 64 physical cores, while for an AMD or Intel system, we’d be only getting 32 physical cores with SMT. I’m sure there will be readers who will be considering such a comparison “unfair”, however it’s also the positioning that Amazon is out to make in terms of delivered throughput, and most importantly, the equivalent pricing between the different instance types.Today’s CompetitionToday’s article will focus around two main competitors to the Graviton2: AMD EPYC 7571 (Zen1) powered m5a instances, and Intel Xeon Platinum 8259CL (Cascade Lake) powered m5n instances. At the moment of writing, these are the most powerful instances available from the two x86 incumbents, and should provide the most interesting comparison data.It’s to be noted that we would have loved to be able to include AMD EPYC2 Rome based (c5a/c5ad) instances in this comparison; Amazon had announced they had been working on such deploymentslast November, but alas the company wasn’t willing to share with us preview access (One reason given was the Rome C-type instances weren’t a good comparison to the Graviton2’s M-type instance, although this really doesn’t make any technical sense). As these instances are getting closer to preview availability, we’ll be working on a separate article to add that important piece of the puzzle of the competitive landscape.Tested 16xlarge EC2 Instancesm6gm5am5nCPU PlatformGraviton2EPYC 7571Xeon Platinum 8259CLvCPUs64Cores Per Socket643224(16 instantiated)SMT-2-way2-wayCPU Sockets112Frequencies2.5GHz2.5-2.9GHz2.9-3.2GHzArchitectureArm v8.2x86-64 + AVX2x86-64 + AVX512µarchitectureNeoverse N1ZenCascade LakeL1I Cache64KB64KB32KBL1D Cache64KB32KB32KBL2 Cache1MB512KB1MBL3 Cache32MB shared8MB sharedper 4-core CCX35.75MB sharedper socketMemory Channels8x DDR4-32008x DDR-2666(2x per NUMA-node)6x DDR4-2933per socketNUMA Nodes142DRAM256GBTDPEstimated80-110W?180W210Wper socketPrice$2.464 / hour$2.752 / hour$3.808 / hourComparing the Graviton2 m6g instances against the AMD m5a and Intel m5n instances, we’re seeing a few differences in the hardware capabilities that power the VMs. Again, the most notorious difference is the fact that the Graviton2 comes with physical core counts matching the deployed vCPU number, whilst the competition counts SMT logical cores as vCPUs as well.Other aspects when talking about higher-vCPU count instances is the fact that you can receive a VM that spans across several sockets. AMD’s m5a.16xlarge here is still able to deploy the VM on a single socket thanks to the EPYC 7571’s 32 cores, however Intel’s Xeon system here employs two sockets as currently there’s no deployed Intel hardware in EC2 which can offer the required vCPU count in a single socket.Both the EPYC 7571 and the Xeon Platinum 8259CL are parts which aren’t publicly available or even listed on either company’s SKU list, so these are custom parts for the likes of Amazon for datacentre deployments.The AMD part is a 32-core Zen1 based single-socket solution (at least for the 16xlarge instances in our testing) clocking in at 2.5 GHz all-cores to up to 2.9GHz in lightly threaded scenarios. The peculiarity of this system is that it’s somewhat limited by AMD’s quad-chip MCM system which has four NUMA nodes (one per chip and 2-channel memory controller), a characteristic that’s been eliminated inthe newer EPYC2 Zen2based systems. We don’t have concrete confirmation on the data, but we suspect this is a 180W part based on the SKU number.Intel’s Xeon Platinum 8259CL is based on thenewer Cascade Lake generation CPU cores. This particular part is also specific to Amazon, and consists of 24 enabled cores per socket. To reach the 16xlarge 64 vCPU count, EC2 provides us a dual-socket system with 16 out of the 24 cores instantiated on each socket. Again, we have no confirmation on the matter, but these parts should be rated at 210W per socket, or 420W total. We do have to remind ourselves that we’re only ever using 66% of the system’s cores in our instance, although we do have access to the full memory bandwidth and caches of the system.The cache configuration in particular is interesting here as things differ quite a bit between platforms. The private caches of the actual CPUs themselves are relatively self-explanatory, and the Graviton2 here does provide the highest capacity of cache out of the trio, but is otherwise equal to the Xeon platform. If we were to divide the available cache on a per-thread basis, the Graviton2 leads the set at 1.5MB, ahead of the EPYC’s 1.25MB and the Xeon’s 1.05MB. The Graviton2 and Xeon systems have the distinct advantage that their last level caches are shared across the whole socket, while AMD’s L3 is shared only amongst 4-core CCX modules.The NUMA discrepancies between the systems aren’t that important in parallel processing workloads with actual multiple processes, but it will have an impact on multi-threaded as well as single-threaded performance, and the Graviton2’s unified memory architecture will have an important advantage in a few scenarios.Finally, there’s quite a difference in the pricing between the instances. At $2.46 per hour, the Graviton2 system edges out the AMD system in price, and is massively cheaper than the $3.80 per hour cost of the Xeon based instance. Although when talking about pricing, we do have to remember that the actual value delivered will also wildly depend on the performance and throughput of the systems, which we’ll be covering in more detail later in the article.We thank Amazon for providing us with preview access to the m6g Graviton2 instances. Aside from giving us access, Amazon nor any other of the mentioned companies have had influence in our testing methodology, and we paid for our EC2 instance testing time ourselves.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15578/cloud-clash-amazon-graviton2-arm-against-intel-and-amd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Next Generation Arm Server: Ampere’s Altra 80-core N1 SoC for Hyperscalers against Rome and Xeon\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-03-03T16:30:00Z\n",
      "URL: https://www.anandtech.com/show/15575/amperes-altra-80-core-n1-soc-for-hyperscalers-against-rome-and-xeon\n",
      "Content: Several years ago, at a local event detailing a new Arm microarchitecture core, I recall a conversation I had with a number of executives at the time: the goal was to get Arm into 25% of servers by 2020. A lofty goal, which hasn’t quite been reached, however after the initial run of Arm-based server designs the market is starting to hit its stride with Arm’s N1 core for data-centers getting its first outings. Out of those announcing an N1-based SoC, Ampere is leading the pack with a new 80-core design aimed at cloud providers and hyperscalers. The new Altra family of products is aiming to offer competitive performance, performance per watt, and scalability up to 210 W and tons of IO, for any enterprise positioning.The Arm Server Market: 2010-2019 (Abridged)We’ve seen companies such as Broadcom/Cavium/Marvell,Calxeda,Huawei,Fujitsu,Phytium,Annapurna/Amazon,AppliedMicro/Ampere, and even AMD put Arm-based IP into silicon and subsequently into the server market. Up until recently, most designs have been fairly lackluster – with companies either developing their own core on an Arm architecture license and not getting a performance lift, or using the standard Arm cores and not finding the right mix of performance, power, and software uptake needed to drive home the design. As a result, we’ve seen multiple companies fall by the wayside, be acquired, or limit their activities to specific customers and keep very hush-hush.First Generation Ampere eMAG, Built on Applied Micro designsA big example of the ‘be acquired’ type of company was Annapurna, whom Amazon acquired and eventually released itsGraviton2 processorin recent months. This chip has 64 cores based on Arm’s N1 design, which is the leading microarchitecture layout for Arm server chips at this point. To that end, Ampere (who originally purchased Applied Micro) is now set to release its second generation product, with 80 of the N1-based cores, and it now has a name: Altra.Ampere AltraAmpere has already given a number of details away about Altra inan announcement late last year, however this time around we have concrete details and the company has performance projections. On the back of its first generation eMAG product, Ampere is looking to offer better-than-Graviton2 performance to any cloud provider or hyperscaler who isn’t called Amazon, given that Graviton2 is built by Amazon and only available to Amazon. In that regard, Ampere has taking Arm’s full recommendations for its N1 design, building a chip with the most number of cores that N1 is designed to support.As with other N1-based products, Altra will be single threaded, ensuring that each thread has its own core, its own resources, and removing any potential core-sharing thread security issues that have occurred recently. The Altra SoC is built with containers in mind, ensuring high-levels of quality of service with multiple customers on the same chip, and additional RAS features to ensure consistent performance.The N1 core isby design what we’ve coveredwhen Arm detailed the microarchitecture design last year. There is a 4-cycle 64 KB L1I/L1D caches per core, along with a 9-11 cycle 1 MB of private L2 per core. This is partnered with 32 MB of system wide LLC distributed through the SoC mesh, and all these caches are ECC with SECDED operation. It’s worth noting that 32 MB across 80 cores is less per core than Amazon’s Graviton2, which has 32 MB for 64 cores. 32 MB is actually half of what Arm recommends, as in Arm’s presentation it stated that it would expect a 64-core design to have 64 MB.On top of the 80 cores, the SoC will also have eight DDR4-3200 memory channels with ECC support, up to 4 TB per socket. There are also 128 PCIe 4.0 lanes, with which the CPU can use 32 of them to hook up to another CPU for dual socket operation. The dual socket system can then have a total of 192 PCIe 4.0 lanes between it, as well as support for up to 8 TB of memory. We are told that it’s actually the CCIX protocol that runs over these PCIe lanes, which means 25 GB/s per x16 linkup. That’s good for 50 GB/s in each direction.Each of the PCIe lanes can be bifurcated down to x8/x4/x2, and every different variant of the Altra SoC will only be segmented on core count and frequency: all CPUs will have 4 TB support and 128 lanes of PCIe 4.0. Each CPU can also support up to four CCIX-based accelerators.Altra is built on TSMC’s 7nm, and while is technically an Arm v8.2 design, it does borrow a couple of features from 8.3 and 8.5, namely hardware based mitigations for side channel attacks and a couple of other small micro-architectural features.Each of the 80 cores is designed to run at 3.0 GHz all-core, and Ampere was consistent in its messaging in that the top SKU is designed to run at 3.0 GHz at all times, even when both 128-bit SIMD units per core are being used (thus an unlimited turbo at 3.0 GHz). The CPU range will vary from 45W to 210W, and vary in core count - we suspect these SKUs will be derived from the single silicon design, and it will depend on demand as well as binning as to what comes out of the fabs. Exact SKUs are going to be announced later this year.Also on security, Ampere was keen to point out that its new SoC will have two control processors: an SM Pro and a PM Pro. These allow for server manageability, up to SBSA Level 4, as well as Secure Boot, RAS error reporting, and advanced power management/temperature control.Ampere will be launching with two reference designs for Altra, one in single socket called Mt. Snow, and one in dual socket called Mt. Jade. Each design will be available in 1U and 2U form factors, with PCIe 4.0 and CCIX attach, and up to 16 memory modules per socket. We know that the partner for the single socket is the GIGABYTE Server team, however the dual socket partner has not be announced yet. We have been told that the CPUs are socketed, which makes mass scale production and testing (at least on our side) a little easier.Projected PerformanceAmpere has some performance numbers, which as always we take with a grain of salt. These include 2.23x the performance on SPEC2017_int rate over a single 28-core Intel Xeon Platinum 8280, and 1.04x over a single 64-core AMD EPYC 7742. This is obviously extended into a number of claims about improved TCO. Ampere didn’t provide similar numbers for SPEC2017_fp, because the company states that the SoC has been developed with INT workloads in mind. Exact power/performance numbers were not given, but based purely on TDP, which is somewhat of an unreliable metric at times. We’ll wait to run our own numbers in due course.Developing a Roadmap: 2021, 2022One of the key questions going into our briefings with Ampere is how closely they are working with Arm on the next generation enterprise server core designs for upcoming SoCs. They weren’t keen to position themselves as Arm’s key partner in this venture (which might be Amazon, given they were first), but did state that there is a lot of collaboration and feedback that goes into the future designs. As a result, Ampere is able to formally declare a long-term roadmap for its product portfolio.In this instance, Ampere is stating that today it has the 80-core Altra design on 7nm. In 2021, it will launch its Mystique product, which is currently in development (and when asked, Ampere told us will share the same socket as Altra). In 2022, Ampere will launch Siryn, and at this time the product has been defined and requires development.Having a sustained product cadence has been critical to a number of processor designs in the last couple of decades – it tells potential ODM partners and customers that the company is in for the long haul, and committed to future developments with targets to meet. Obviously with Ampere tying itself to Arm’s roadmap helps in those product definition stages. It’s a feature that has crippled previous Arm designs from coming to market – without a clear roadmap, customers are unwilling to invest in a one-generation wonder and provide long term support for it. There’s always the issue as to whether any investment funding might run out, so Ampere’s goal here with Altra is to be the obvious answer to Graviton2 for the other hyperscalers. With that large market on offer, the goal is to be profitable and self-sustaining as quickly as possible, which then in turn gives potential customers even more confidence.Next Stage for AltraAt this point, Ampere has stated to us that Altra is currently sampling with its key customers who are looking to deploy the hardware. From previous experience, the key customers who are involved early tend to get priority for deployment, and in that respect Ampere has stated that an official SKU list will come to market mid-year, along with pricing, and with official SPEC submissions. Hopefully at that time we will also get instance pricing from the companies intending to deploy the new chip.We’re currently in talks with Ampere in order to obtain Altra for in-house testing when they feel it is ready. We have a version of Ampere’sprevious generation eMAG workstationthat just arrived in the labs, which should help us provide a good base-line from the previous design to the new one. Stay tuned for our coverage of eMAG and Altra!Related Reading80-Core N1 Next-Gen Ampere, ‘QuickSilver’: The Anti-Graviton2Arm Server CPUs: You Can Now Buy Ampere’s eMAG in a WorkstationAmpere Computing: Arm is Now an InvestorAmpere eMAG in the Cloud: 32 Arm Core Instance for $1/hrGallery:Ampere Altra\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15575/amperes-altra-80-core-n1-soc-for-hyperscalers-against-rome-and-xeon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Huawei Announces New MatePad Pro 5G High-End Tablet\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-02-24T13:30:00Z\n",
      "URL: https://www.anandtech.com/show/15522/huawei-announces-new-matepad-pro-5g-highend-tablet\n",
      "Content: Today Huawei is announcing the launch of its newest high-end tablet: The MatePad Pro 5G. The device isn’t exactly a new product as the company hadlaunched the 4G version in China back in November, but it is now also launching in the western markets as 5G variant and we’ve had some hands-on experience with the new unit.The Android tablet space has been relatively lacklustre over the last several years. The two only vendors who really are still trying to bring out new premium models are Samsung and Huawei, and Samsung has the tendency to skimp out on the internal hardware of its tablets, most of the time using last generation SoCs rather than employing the latest and greatest.Huawei’s approach with the new MatePad Pro here is different, as the company is opting for the best hardware the company can integrate. As such, the new MatePad Pro 5G comes with the company’s new Kirin 990 5G chipset, which isn’t just only an upgrade over the 4G variant of the Chinese models, but should also represent a significant lead over other high-end tablets in the market.Huawei MatePad Pro 5GSpecificationsSoCHiSilicon Kirin 990 5G2x Cortex-A76 @ 2.86 GHz2x Cortex-A76 @ 2.36 GHz4x Cortex-A55 @ 1.95 GHzGraphicsArm Mali-G76MP12 @ 600MHzDisplay10.8-inchsAMOLED2560×1600Storage128 GB256 GB512 GBNano Memory card slotMemory6 GB or 8 GB of RAMWirelessWi-Fi 802.11 a/b/g/n/ac 2.4G+5GHz, MU-MIMO, Wi-Fi Direct,Bluetooth v5.1GPSA-GPS, Glonass, Beidou, Galileo, QZSSConnectivityUSB 3.1 Type-C for data and chargingCameraRear Camera: 13 MPFront Camera: 8 MPVideoRecording: UHD 4K (3840×2160) @ 30 fpsPlayback: ?Audio4 × SpeakersUSB-C headsetSensorsAccelerometer, Gyro Sensor, Proximity Sensor, CompassBattery7250 mAhDimensions246 × 159 × 7.2 mm460 gramsColorWhite, Black, Green and OrangeOSAOSP 10 with EMUI 10Price6GB+128GB Wifi:549€8GB+256GB Wifi:649€8GB+256GB Wifi + Pencil + Leather:749€6GB+128GB LTE:599€8GB+256GB LTE:699€8GB+256GB 5G:799€8GB+512GB 5G:949€AccessoriesMatePad Sleeve: 39€Smart Keyboard: 129€M-Pencil: 99€Folio Cover: 39€As noted, the MatePad Pro 5G comes with the new Kirin 990 5G chipset which gives it sufficient oomph to power even the latest applications with ease.We have yet to get the full specifications of the western device configurations, but if the Chinese variant of the MatePad Pro are any indicator of what to expect, then we should see 6 or 8GB RAM configurations along with options of 128, 256 and 512GB storage.The MatePad Pro is defined by a 10.8” AMOLED screen coming in with a resolution of 2560 x 1600. In our hands-on with the device, we noted excellent picture quality, and the viewing experience on the tablet is really defined by the very narrow bezels, currently the smallest of any tablet on the market coming in at 4.9mm.Huawei is launching the MatePad Pro with two main accessories: The keyboard case, as well as the pencil.The keyboard case is a relatively straightforward accessory and allows you for fast typing on the tablet. The case is fully magnetic in its design and latches onto the back of the MatePad Pro. The keyboard case has the ability to rest the tablet in a vertical position at two fixed angles. Key input was relatively ok on the unit, however I’m not too convinced of such form-factors given that there’s no proper way to rest your palms while typing so I don’t think it’s fit for a proper productivity environment,such as say Microsoft’s Surface keyboards.There’s also a pencil accessory which pretty much works as you’d expect. The M-Pencil charges wirelessly and is attached magnetically to the top side of the MatePad Pro, with total battery life of up to 10h and charging times of 1h to full capacity, or 30s charging for 10 minutes of usage. Huawei advertises very low latency input lag of as low as 20ms and pressure sensitivity of up to 4096 levels.Both the keyboard case and the M-Pencil are separately bought accessories for the MatePad Pro.The device comes with a quad speaker setup, which is again something quite rare in the Android tablet market.I quite really enjoyed the ergonomics of the MatePad Pro and I think it’s amongst the best tablet designs on the market right now. The rounded corners were very comfortable to hold, and Huawei says it has improved its software touch rejection to avoid accidental touches on the new narrow bezel design.In terms of cameras, we see a front-facing 8MP unit integrated into the screen via a hole-punch design, while the rear unit is a 13MP module integrated into a more classical design alongside a microphone and flash.The MatePad Pro 5G looks to have excellent hardware specifications, but of course the device’s success isn’t only about hardware, but also software. Huawei’s ban on using Android and Google services still is a major handicap for the company’s devices in western markets, and will undoubtedly make this a much less attractive purchase. Huawei is investing a lot of money and resources into bringing the alternative HMS (Huawei Mobile Services) to market, and they claim we’ll be seeing the first products this year, and products such as the MatePad Pro will be receiving updates as well.WiFi and LTE versions of the MatePad Pro start at 549 and 599€ and will be available in April, while the 5G variant starts at 799€ with availability depending on markets.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15522/huawei-announces-new-matepad-pro-5g-highend-tablet\n",
      "Title: Samsung Announces The Galaxy S20, S20+ and S20 Ultra: 120Hz, 5G, Huge Batteries, Crazy Cameras and $$$\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-02-11T19:00:00Z\n",
      "URL: https://www.anandtech.com/show/15482/samsung-announces-galaxy-s20-s20-and-s20-ultra\n",
      "Content: The year is 2020. The 20’s are back (we’ll see if they’ll be roaring or not), and not only is it a new decade, but it’s also time for a new enumeration scheme for Samsung’s newest Galaxy S devices; Today Samsung is announcing the new Galaxy S20, S20+ and S20 Ultra flagship phones. The new phones have some similarities in the external design to last year’s S10 line-up and a clearly a successor – but in terms of specifications and hardware capabilities, it’s one of Samsung’s largest ever updates.The new trio of devices shake things up again compared to what we’ve seen in last year, and there’s a different positioning at the low- and high-end of the series. The S20 and S20+ are relatively straightforward successors to the S10 and S10+, however the lower end S10e sees no continuation, with Samsung instead opting for a super-high-end premium model in the form of the S20 Ultra – a literal behemoth of a phone.S20 UltraThe hardware updates this year are massive: This is Samsung’s first year where 5G connectivity comes as a standard feature for all models of the line-up (There are still some 4G variants depending on markets), featuring the newest Snapdragon 865 and Exynos 990 SoCs and their corresponding new generation modems. The phones now come with a 120Hz refresh rate screen all while retaining 1440p resolution on all models, and we’ve seen some substantial upgrades in terms of battery capacities to be able to power the new SoCs, 5G connectivity, and new display.The biggest upgrade and key focus point of the new S20 series however is the new camera configurations and setups. This is Samsung’s first time in several years that the company has opted to go for bigger camera sensors, and while there’s still “only” three cameras for the models in the line-up, they are completely revamped in their designs and have very little to do with their past brethren. The new S20 Ultra even goes a step further in terms of differentiation and now comes with a gargantuan 108MP main camera sensor as well with what Samsung describes a “100x Super Zoom” telephoto module that’s enabled by a prism optics right-angle mounted sensor setup. The camera improvements are numerous, and we’ll go into detail of each of them in just a bit.Samsung Galaxy S20 SeriesGalaxy S20(5G & 4G)Galaxy S20+(5G & 4G)Galaxy S20 Ultra5GSoC(North America, China, Japan)Qualcomm Snapdragon 8651x Cortex-A77 @ 2.84GHz3x Cortex-A77 @ 2.42GHz4x Cortex-A55 @ 1.80GHzAdreno 640 @ 587MHz(Europe & Rest of World)Samsung Exynos 9902x Exynos M5 @ 2.73GHz2x Cortex-A76 @ 2.50GHz4x Cortex-A55 @ 2.00GHzMali G77MP11 @ ? MHzDisplay6.2-inch3200 x 1440 (20:9)6.7-inch3200 x 1440 (20:9)6.9-inch3200 x 1440 (20:9)SAMOLEDHDR10+1200nits peak brightness120Hz Refresh Rate(@FHD+ software rendering)Dimensions151.7 x 69.1 x 7.9 mm163 grams161.9 x 73.7 x 7.8 mm187 grams166.9 x 76.0 x 8.8 mm220 gramsRAM8 GB (LTE)12 GB (5G)8 GB (LTE)12 GB (5G)12 /16GBNANDStorage5G = 128 GBLTE = 128 GB + mSD5G = 128-512 GBLTE = 128 GB + mSD16 GB model = 512 GB12 GB = 256 or 512 GB+ microSDBattery4000mAh(15.4Wh) typ.3880mAh (14.93Wh) rated4500mAh(17.32Wh) typ.4370mAh (16.82Wh) rated5000mAh(19.25Wh) typ.4855mAh (18.69Wh) rated15W Wireless Charging25WFast Charging45WSuper Fast ChargingFront Camera10MP4K video recordingF/2.2, 80-degree40MP4K video recordingF/2.2, 80-degreePrimary Rear Camera79° Wide Angle12MP 1.8µm Dual Pixel PDAF79° Wide Angle108MP 0.8µm DP-PDAF3x3 Pixel Binning to 12MP8K24 Video Recordingfixed f/1.8 opticsOIS, auto HDR, LED flash4K60, 1080p240, 720p960 high-speed recordingSecondaryRear Camera76° Wide Angle(Cropping / digital zooming telephoto)64MP 0.8µmF/2.0 optics, OIS8K24 Video Recording24° Telephoto(5x optical magnification)48MP 0.8µmF/3.5 prism optics, OISTertiaryRear Camera120° Ultra-Wide Angle12MP 1.4µm f/2.2ExtraCamera-Time of Flight (ToF) 3D Sensor4G / 5GModemSnapdragon 5G- Snapdragon Modem X55 (Discrete)(LTE Category 24/22)DL = 2500 Mbps - 7x20MHz CA, 1024-QAMUL = 316 Mbps 3x20MHz CA, 256-QAM(5G NR Sub-6 + mmWave*)DL = 7000 MbpsUL = 3000 Mbps*Depending on region and modelExynos 5G- Exynos Modem 5123 (Discrete)(LTE Category 24/22)DL = 3000 Mbps 8x20MHz CA 1024-QAMUL = 422 Mbps ?x20MHz CA, 256-QAM(5G NR Sub-6)DL = 5100 MbpsExynos 4G- ???? (Discrete)LTE Category 20/13DL = 2000 Mbps, 7x20MHz CAUL = 200 MbpsSIM SizeNanoSIM + eSIMWireless802.11a/b/g/n/ac/ax2x2 MU-MIMO,BT 5.0 LE, NFC, GPS/Glonass/Galileo/BDSConnectivityUSB Type-Cno 3.5mm headsetSpecial FeaturesUnder-screen ultrasonic fingerprint sensor(Qualcomm QC 2.0, Adaptive Fast Charging, USB-PD),reverse wireless charging (WPC & PMA),IP68 water resistanceLaunch OSAndroid 10 with Samsung OneUI 2.0Launch Prices128GB 5G:$999 / 999€ /£899128GB 5G:$1199 / 1099€ /£999512GB 5G:$1299 / 1249€/n/a £128GB 5G:$1399 / 1349€ /£1199512GB 5G:$1499 / 1549€/£13994Gvariants for100€ / 100£ cheaperavailable onlyin certain markets5G onlySnapdragon 865 & Exynos 990 - Next Gen-SoCs as The Brain of the PhonesAt the heart of the S20 series we find Qualcomm’s new Snapdragon 865 SoC as well as Samsung’s Exynos 990 chip. The Snapdragon 865 is a straightforward follow-up to the Snapdragon 855 that powered the vast majority of mobile devices in 2019 in an excellent manner. The new chip comes with a slightly updated manufacturing process and sports Arm’s newest Cortex A77 CPU cores which bring a 25% performance through microarchitectural updates, and an updated GPU sporting a 20% performance uplift. We’ve covered the Snapdragon 865 more extensively in our performance preview piece, and we’re sure the Galaxy S20 models using this chipset variant will give excellent experiences for 2020.S20+European costumers (and maybe others?) will see the new Exynos 990 chip being used. We don’t know too much about the chip’s performance, but it sports 2x new Samsung M5 CPU cores at 2.73GHz alongside 2x Cortex A76 cores at 2.5GHz and 4x Cortex A55 cores at 2GHz. On the GPU side, we see the brand-new Arm Mali-G77 in a 11-core configuration at yet undisclosed clocks. Samsung had claimed the CPU performance is 20% higher than that of the Exynos 9820, and first leaked benchmarks does put it roughly around the same level as the Cortex A77 cores of the Snapdragon 865 – of course this is all to be confirmed once we get our hands on the chip. There’s a more generational gap in terms of process node here between the S10 and S20 as we see the transition from 8nm to 7nm EUV – it would be more similar to the Exynos 9825 of the Note10, it will be interesting how power efficiency ends up at this year and how both Exynos and Snapdragon versions of the Galaxy S20 will fare against each other.Both chips bring with them massive upgrades in their imagine signal processors as well as their neural processing units, significantly increasing the processing power that’s required to operate the S20’s new complex camera systems.5G Becomes Mainstream - At Least For Some Markets And VariantsCommon between both chipset variants is the introduction of 5G connectivity that’s enabled by new discrete modems on both platforms. Qualcomm says that the Snapdragon 865 exclusively comes with the new X55 5G modem and it’s likely we’ll be seeing this being deployed in the majority of leading 5G markets such as the US and China, maybe even South Korea. The Exynos models however also have 5G connectivity thanks to the new Exynos Modem 5123 and these variants will be prevalent in European markets.At this moment it’s still unclear what S.LSI’s situation is on mmWave–we know that the company has solutions that they claim as being planned for adoption in 2020, but until Samsung confirms S20 variants with the functionality it’s something we currently can only speculate on. The vast majority of markets haven’t even begun licensing out mmWave spectrum, much less deploy it. At the time of writing, it’s also unclear whether all Snapdragon variants of the S20 will feature mmWave modules, or if we’ll be seeing a segmentation between sub-6 only models and sub-6+mmWave models depending on country or region. One thing that is clear is that Samsung is not completely abandoning 4G only variants of the S20 series, as these will still be launched in markets lagging in 5G adoption. These variants seemingly will feature the Exynos chip, but we’re unclear whether it’ll come with a different modem, or if it’ll simply just lack the corresponding 5G RF system that’s required for connectivity.Iterative Redesigns For S20 & S20+, New Behemoth S20 Ultra Form-FactorIn terms of design, the S20 and S20+ are iterative to their predecessors, with Samsung now tweaking the bezel situation as well as elongating the form-factor. All the new phones come with a 3200 x 1440 resolution display which results in a 20:9 aspect ratio, slightly longer than the 19:9 aspect ratio of the S10 series. The new 6.2” and 6.7” screen diagonals of the S20 and S20+ shouldn’t scare you off in thinking that the phones have majorly changed their form-factor, in fact, the width of the new models is actually narrower than the S10 and S10+ - it’s only in the length that the new phones have grown in size, meaning it should largely still have similar ergonomics. Thickness is the same as well, but the phones are a little bit heavier by 6 and 13 grams respectively, while still being relatively reasonable on the scale at 163g and 187g.As mentioned, the new screen is elongated in terms of its form-factor, but it’s not the only thing that has changed, as Samsung is using a brand-new panel technology allowing for higher efficiency as well as native 10-bit colour displaying. The biggest feature though is the new screen’s ability to work at a 120Hz refresh rate, doubling up on fluidity compared to the regular 60Hz. We hadn’t been briefed by Samsung as to how this works exactly, but pre-release reports had shown that the 120Hz mode is only possible when the phone is rendering in 1080p/FHD+ mode, and doesn’t have the ability to use 1440p at 120Hz. Whilst we have yet to confirm this, this would be a rather big compromise in terms of experience. 1080p rendering on a 1440p panel still look relatively good, but it’s not as sharp as native 1440p rendering. It’s likely that Samsung has had to opt for this route for power efficiency reasons – well have to dwell more into the topic once we get our hands on a device.The biggest visual change at the front of the phone is Samsung’s new screen and punch-hole design. Compared to the S10 series, Samsung has moved the front-camera punch-hole to the middle of the screen, similar to the layout of the Note10 series. While Samsung had already been able to reduce the size of the punch-hole in the Note10, it’s even smaller now on the S20 series, allowing for a much smaller notification bar at the top of the screen as the much smaller cut-out now wastes less space compared to previous iterations. Because it’s only a single camera at the front for all models, much like on the Note10 series, we see discontinuation of dual-camera “pill” setup found on the S10+ and S10 5G.Continuing on the differences in design, the bezels and frame of the phone is more similar to that of the S10 5G than the S10 series. We now see a much thinner metal frame on the sides of the phone as the back glass is curving more significantly to the sides. In teardowns of the S10 5G we’ve seen that Samsung did this in order to house the mmWave modules on the sides without them being obstructed by the metal frame, so I’m expecting a similar internal design on the S20’s.Galaxy S20Another major redesign this year is that Samsung has moved all buttons to the right side of the phone. The company had already done this move on the Note10 series – only there we had them all on the left side. The right sight probably makes more sense for the majority of users. Similarly to the Note10, we see the disappearance of the dedicated Bixby assistant button which now is integrated into the power button (officially the “side button” now) of the S20 in its functionality by double tapping or holding it pressed.The Galaxy S20 Ultra is a different phone compared to what we’ve seen in the past. In terms of footprint, it’s larger than the S20+ by 2.3mm in width and 5mm in height – it doesn’t sound too much but it’s larger than the Note10+. It’s a bit narrower than the S20 5G so that should make it more reasonable to handle, but still quite taller. What puts the S20 Ultra in a class of its own is that it’s 8.8mm thick and weighs in at 220g, making it quite a bit heavier than the S10 5G which came in at 198g last year. It’s to be noted that while this is quite heavy for a Samsung device and a lot more than users would be used to, it’s still 6g lighter than the 226g iPhone 11 Pro Max.The weight increases for the line-up should be related to the increase battery capacities of the phones. The S20 comes with a 4000mAh unit, the S20+ falls in at 4500mAh, and the S20 Ultra features a large 5000mAh battery. All figures are typical capacities, rated capacities should be slightly smaller. The new capacities represent 17.6% and 9% increases over the S10 and S10+ models, which means that the new smaller model will be seeing a larger battery life increase between generations, which should make users who prefer the smaller models very happy.Cameras: 12MP, 48MP, 64MP, 108MP Across Modules And DevicesAs mentioned, the biggest features of the S20 series are its cameras, and it’s the biggest upgrade we’ve ever seen in a Samsung device, revamping all sensors and modules across the range. Starting off with the Galaxy S20 and S20+, both of while share the same triple-camera setup:The main camera sensor on both phones remains a 12MP unit in terms of resolution, however Samsung is now using a new generation sensor that is physically larger in size. The pixel pitch has grown from 1.4µm to 1.8µm, meaning the new sensor has a 65% larger area and corresponding better light capture ability. I’m actually expecting the sensor to showcase a lot better electrical characteristics – although it hadn’t be discussed by S.LSI yet, the new generation of sensors for the first time employ dual gain converters to extend the full-well-capacity of each pixel, in theory doubling the dynamic range of the sensor. We don’t have confirmation if the S20 uses these sensors, but given Samsung’s showcase of such 64MP and 108MP sensors at IEDM last year, I feel it’s very likely does sport the new features in the new sensors.In terms of optics, it looks like Samsung has abandoned its dual-aperture system in favour of a fixed f/1.8 optics system. It’s likely that because the new sensors are much larger in size, that the company required more Z-height in the optics system for the larger lenses, and the dual-aperture was a victim and compromise versus increasing the size of the camera bump at the back of the phones (Which by the way, did have to increase this generation). The module has OIS as expected.S20 CameraS20+ CameraThe most interesting camera of the whole S20 series for me isn’t actually the 108MP module of the S20 Ultra which we’ll cover in a bit, but rather the 64MP second unit of the regular S20 phones. The important aspect here is that this isn’t a telephoto unit in terms of its optics, as it still has a 76° wide field of view which is only ever so slightly narrower than the primary camera’s 79° FoV. Of course, because this unit has quadruple the resolution, at 12MP resulting images the module could in theory crop to a 4x magnification compared to the main sensor. I think another reason for Samsung choosing to go this route is that it would be possible to record 8K video with this sensor, as the 12MP main unit doesn’t have sufficient resolution to achieve this.The possibilities and flexibility that Samsung has on this module and sensor configuration is actually quite large, I’m wondering if the phone would be able to capture native 64MP wide-angle or even 16MP wide-angle images with the module. The pixel pitch here is very small at 0.8µm and matches the description of the sensors that Samsung had disclosed at IEDM, so again I’m expecting some quite good light capture ability even though it has a small pixel pitch. The optics are f/2.0 and the module supports OIS.Finally, the third module is the ultra-wide-angle unit. Samsung here slightly reduced the FoV from 123° to 120°, but more importantly has switched from a 16MP 1.0µm sensor to a 12MP 1.4µm sensor which would be 47% larger in terms of area. This means it has reduced resolution, but has double the light capture per pixel, which should result in actually higher effective spatial resolution in the majority of situations, as well as much improved low-light capture ability, a scenario at which the UWA units traditionally have suffered in. The optics sport a f/2.2 aperture and there’s no OIS.We’ve covered the S20 and S20+ cameras, but likely the most noise and discussions will be on the topic of the S20 Ultra’s camera setup.S20 Ultra CameraHere Samsung has opted to go with the new huge 1/1.3” sensors at 108MP resolution as the primary camera. This unit actually kills two birds with one stone as it effectively covers the same use-cases that the 12MP and 64MP sensor covers on the regular S20’s. The pixel pitch again is tiny at only 0.8µm, but Samsung is able to do up to 3x3 pixel binning, capturing 12MP images with the 108MP sensor, allowing for effective 2.4µm pixel pitches in terms of light capture ability, for what would be 3x the amount of light capture per pixel. We weren’t pre-briefed by Samsung ahead of the launch, but there’s also the possibility it would support 2x2 binning, depending on how Samsung has arranged the colour filters of the sensor. Keeping 12MP files also allows for tremendous digital zooming via cropping. The unit features the same f/1.8 fixed aperture setup as well as OIS functionality.The telephoto module on the S20 Ultra is a real telephoto unit, and Samsung for the first time introduces a right-angled telescopic sensor setup that sits perpendicular to the phone, and looks out via a prism mirror. The amazing thing here is that Samsung was able to cram in a 48MP 0.8µm sensor in this setup, while all other vendors out there had maxed out with small 8MP sensors. The combination of a 5x optical magnification with the high-resolution sensor means that Samsung can zoom/crop in at up to 10x while retaining 12MP full resolution. Samsung advertises machine learning aided digital zooming in the same style and functionality as Google’s super res zoom to achieve a maximum of 100x zooming, with some extremely impressive results.We find the same UWA module on the S20 Ultra as on the S20 and S20+. The S20+ and S20 Ultra in addition feature a time of flight (ToF) sensor for 3D depth sensing, the regular S20 lacks this sensor / module.Impressive Overall PackagesOverall, the S20 series look like extremely solid upgrades in terms of their hardware and what they bring to the table. Samsung has focused on every aspect of the “core” features of a phone, delivering as they say “awesome screen, awesome camera, long lasting battery life” – well at least on paper, we shall see if they deliver on it. For me the S20 series feel like extremely major upgrades and probably Samsung’s most ambitious flagships of the last few years. I’m very excited to get my hands on an S20+ and S20 Ultra and see how the new camera setups perform, as I feel they’re amongst the most innovative designs over the last few years and this is definitely Samsung taking a page out of Huawei’s book and improving upon the recipe.Gallery:Samsung Galaxy S20Gallery:Samsung Galaxy S20 PlusGallery:Samsung Galaxy S20 UltraTheGalaxy S20, S20+, S20 Ultra are starting at $999, $1199 and $1399in the US and come only in 5G variants. The S20 only comes in a 128GB configuration. The S20+ and S20 Ultra 512GB variants oddly enough only cost an extra $100.In Europe the S20 and S20+ will be available in both 4G and 5G variants, depending on local storage and colour options in terms of the exact model availabilities. The4G S20 and S20+ start at 899€ and 999€which is the same price as the S10 and S10+ last year (!!). 5G variants cost an extra 100€.The S20 Ultra is 5G only and starts at 1349€. The S20+ and S20 Ultra 512GB version come at an extra 150€ and 200€ respectively, Samsung's 128 to 512GB pricing is a bit all over the place.Availability starts March 6th.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15482/samsung-announces-galaxy-s20-s20-and-s20-ultra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AMD at CES 2020: Q&A with Dr. Lisa Su\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2020-01-07T17:15:00Z\n",
      "URL: https://www.anandtech.com/show/15344/amd-at-ces-2020-qa-with-dr-lisa-su\n",
      "Content: This week AMD took the wraps off of its latest generation of mobile processors. The company is being aggressive, offering up to eight cores for both the traditional ultra-portable notebook as well as the higher-performance content creator and prosumer notebook. The move to 7nm, as well as design efficiency improvements, have been quite aggressive, with the new hardware claiming double the performance per watt, 20% lower SoC power, and 80% quicker adjustments from low-to-high power inside the chip. This has enabled some key design wins, as seen at CES this week, which are set to come out over the next year.As part of the CES announcements, AMD invited a small number of press to talk to the CEO Dr. Lisa Su about the announcements, 7nm, mobile, and for any other lingering questions about AMD’s recent position in technology news. Here was our chance to tackle AMD’s stance about what it thinks about the future of the mobile space, and where these new products fit into that ecosystem where the company has not traditionally performed so well. With the opportunity at hand, Dr. Su was very keen to discuss how AMD has worked with its partners to enable what it sees as high-performance hardware that users can tangibly feel offer them a differentiated experience.The format was for a general roundtable Q&A, and so the transcription below has been adjusted so questions that align are listed consecutively.Renoir Design TimelineDr. Ian Cutress, AnandTech: These chips would have been in the design phase three years ago - has the laptop market changed since those initial design plans from where it was at that time?LS: That’s a good question. I would say that there is more desire for performance in any given form factor. That’s where we’re doing this H-series, what we’re doing around gaming and content creation and offering more TDP but a lot more horsepower. I think Frank (Azor) said it very well – the idea of briging desktop class performance into the notebook form factor is one of the fastest growing areas in the market. We didn’t spend a lot of time on our commercial plans and designs today, but you can expect that our commercial offerings will be very important. Part of that is bringing security and manageability to the platform – businesses want more control over their machines.VolkerRißka, ComputerBase: If the design was being discussed three years ago, why are the new parts using Vega, and not Navi?LS: It's always how we integrate the components at the right time. Certainly the Vega architecture is well known, very well optimized. It was always planned that this would be Zen2 + Vega. But just to be clear, you will see Navi in our APUs, and those will be coming.GraphicsAnandTech: One of Intel’s marketing points has been that in order to get the best graphics in a laptop, such as the RTX 2080 and above, is to get an Intel laptop. Can you say if any of your OEM partners are going to partner the new Ryzen Mobile hardware with the best in the graphics market?LS: As you can imagine, I’m not going to preannounce designs from our OEM partners, but I would say that you should believe that we are on a clear mission, and that mission is to have AMD in more premium designs for both the consumer and commercial markets. That’s not just our focus today, in 2020, but beyond 2020 also.Dean Takahashi, VentureBeat: Is real time ray-tracing in graphics going to be as big as NVIDIA says it is?LS: I’ve said in the past that ray tracing is important, and I still believe that, but if you look at where we are today it is still very early. We are investing heavily in ray tracing and investing heavily in the ecosystem around it – both of our console partners have also said that they are using ray tracing. You should expect that our discrete graphics as we go through 2020 will also have ray tracing. I do believe though it is still very early, and the ecosystem needs to develop. We need more games and more software and more applications to take advantage of it. At AMD, we feel very good about our position on ray tracing.AnandTech: The rearchitect of Vega for 7nm has been given a +56% performance increase. Does this mean that there was a lot left on the table with the design for 14/12nm? I’m trying to understand how you were able to pull so much extra performance from a simple process node change.LS: When we put Vega into a mobile form factor with Ryzen 4000, we learned a lot about power optimization. 7nm was a part of it sure, but it was also a very power optimized design of that architecture. The really good thing about that is that what we learned is all applicable to Navi as well. David’s team put a huge focus on performance per watt, and that really comes out of the mobile form factor, and so I’m pleased with what they are doing. You will see a lot of that technology will also impact when you see Navi in a mobile form factor as well.AnandTech: Normally when you go after efficiency, there’s not always a positive correlation on frequency, but somehow you’re able to drive higher frequencies on the Mobile APUs on 7nm. Is there a special sauce?LS: A lot of hard work has gone into this design!Gordon Ung, PC World: Do you think that AMD has to have a high-end competitor in the discrete graphics market?LS: [laughs] I know those on Reddit want a high end Navi! You should expect that we will have a high-end Navi, and that it is important to have it. The discrete graphics market, especially at the high end, is very important to us. So you should expect that we will have a high-end Navi, although I don’t usually comment on unannounced products.How To Out-Intel IntelPC World: Breaking into desktop was relatively easy for AMD. For laptops, Intel has been circling the wagons for a while, so what are some of the challenges for AMD in this space? Can you address Thunderbolt 3 and CNVi-based Wi-Fi 6?LS: We've always thought it would be a deliberate rollout of technology. So with desktops and servers all about the CPU and took advantage of chiplet technology. As you go into the notebook form factor, we’ve integrated the CPU and the GPU and we’re pushing boundaries on battery life. We think the Ryzen Mobile 4000 series is going to be a very, very strong part for us. Very strong. If you look over the last eight quarters, we’ve gained share in the PC market, for every single quarter. We believe that with the Third Gen Ryzen Mobile we will take that market share to the next level with notebooks.I think we’re very bullish on what we can do, and it’s not just what we do, it’s what we can do with our OEMs. If you were to spend time with any of the OEMs I think you can also see that they’re doing a lot more designs in the AMD ecosystem, so we feel really good. We presented a couple of designs today with the Lenovo Slim 7 as well as the ASUS Zephyrus G14. There are a lot more machines coming out, in very nice form factors, all using the full power of the Ryzen 4000 series.VentureBeat: How does AMD make headway in commercial client market against Intel?LS: It's all a journey. What we have seen is the PC market is a good market, and if you see just how many PCs are sold, desktops and notebooks, we have made a lot of progress. Today there are a lot more products out with AMD inside, whether you look at large retailers or commercial manufacturers that are starting to incorporate AMD. I’m very optimizisic about what we can do, and of course bringing our notebooks into 7nm will certainly help build on our progress made with 2ndGen Ryzen Mobile, but the 3rdGen Ryzen Mobile will certainly help.AnandTech: Intel is a bigger company, and MDF (Marketing Development Funds) is a big tool in Intel’s arsenal. While having nice designs and design wins for AMD, from a marketing perspective, how do you compete against that?A: This is about bringing great technology to market but also having an ecosystem behind it. What I’ve learned is that success begets success, and so when we moved from 1st Gen Ryzen Mobile to 2nd Gen Ryzen Mobile, you saw significantly more designs in the market – I think we’ve probably had over 50 designs in the last year. OEMs are starting to understand the capabilities of our technology and we’ve been very very consistent with our roadmaps, and so you can imagine how important it is when we say there will be over 100 notebook designs with Ryzen Mobile 4000 in 2020 – there’s just a wider set of designs to now choose from. And yes, there is MDF involved, and there’s a whole retail marketing chain there, but I think our brand is doing pretty well, and we will continue to invest in the brand and invest in the ecosystem that will help promote AMD.PC World: Intel has put a lot of money into its mobile technologies, such as the one watt display panels, and is working very deep with the ecosystem and vendors that provide a lot of that optimized functionality. Is AMD also going to have to spend a similar amount of money to compete with the ecosystem that Intel has built, because it feels like there’s a lot still to do that Intel has a lead on.LS: The way to think about it is that the technology is key. The performance of Ryzen Mobile is significant – we are working with the ecosystem to how we optimize for panels and other parts, because that’s how we do things. But I think the ecosystem is also changing, and this idea that one person or one company is going to define what the next generation of laptops is going to look like, I just don’t think that’s going to be the case. If you look at what Microsoft is doing, or what Dell or HP are doing, they all have their own ideas about what the next generation form factor will look like. For AMD, we’re going after the volume areas of the market, so from that standpoint it’s the fact that we’re moving Ryzen Mobile up the stack and introducing Ryzen 4000 to ultra-thin devices up to H-series and commercial which is important to us. I think we have lots of opportunity to grow.VentureBeat: Intel has a shot at getting its act together on manufacturing and becoming more of a threat. Any comment?LS: You’ll have to ask Intel about their technology. But from our point of view, we never build a roadmap without expecting others to meet their roadmap. We've executed our roadmap from the previous five years and we’re extending it into the next 5 years, all while assuming our competition will be competitive and even beating their public targets. I think we’ve made some good choices, and this market is all about making the right choices at the right time, such as when you bring certain elements of technology to market in which order and how that is achieved. We need to continue executing on our cadence, and that has been one of our strengths. I’m expecting to have very stiff competition, whether you’re talking about process technology, microarchitecture, or packaging technologies. I’m also expecting us to do quite well, because that’s our job.PC World: Do you think you have to compete with Intel on their new foldable devices and other form factors that they’ve started pushing through lower power parts [like Lakefield]?LS: What you will see is that we will continue to push the envelope on form factor. We are working very closely with Microsoft on some of the features and functions that are required to enable those types of form factors. I think you can see what some of the work that we did with Surface, such as a sleek design, and we’ll see more of this going forward. The idea at AMD is that we know what we’re good at, and we’re really good at ensuring that we have differentiated performance, and that’s our focus when working together with OEMs. They’ll put these sorts of feature sets into differentiated designs and I think that’s really important to have unique designs based on AMD silicon.AMD’s BrandVentureBeat: There is a lot of discussion about how products like Threadripper have affected the market and the brand. How is the brand changing?LS: It’s not always clear cut just looking at one product and saying how the brand is changing – I mean we have a specific amount of market share but across Ryzen on the desktop, on the notebook, as well as Threadripper, there’s a lot more recognition of AMD and we always see new form factors for our hardware across all segments. Like I said, success begets success. If you think about what we did in desktop, 1stGen Ryzen was good, 2ndGen Ryzen was better, and now you look at 3rdGen Ryzen today, if you go to Amazon, AMD has 12 out of the top 12 best-selling desktop processors. That is tangible change. I think you’re going to see that on the notebook side as well – we’ve seen very very good sales through our partners as we’ve gone into the holiday season, and that’s only on the Ryzen 3000 series. I think we’re excited about what the Ryzen Mobile 4000 series will do in 2020.PC World: AMD has had the leadership in premium gaming desktops in the past, but is this is the first time that AMD will offer sizable competition in premium laptop designs?LS: This is a big foray into laptops – we are putting a lot of wood behind the arrow. I think we’ve had some good key designs in laptops in the past, but historically these have been at the low end of the market. What you’re going to see is a broader range of designs, of technology, and it’s because the products are really really good. We believe that Ryzen Mobile 4000 series are the best notebook processors available today, and we have some great OEM partners building some great machines.TSMC and Wafer SupplyAnandTech: Since the launch of 7nm, AMD has had a large demand and we’ve noticed AMD have trouble keeping some of its product in stock. There have also been stories about TSMC and lead times increasing, and we recently had a quote from CTO Mark Papermaster about the ability to predict at any given time how many wafers are needed. Can you speak about how AMD will approach its partnership and its orders with TSMC through 2020?LS: Today on stage I mentioned that we now have 20 products built on 7nm, either in production or in development, so we have bet big on TSMC’s 7nm process. We have great relationship with TSMC – they’ve supported us well, but wafer supply is tight. From our standpoint, we need to ensure that we get that prediction of what the demand will be early. In our desktop lines, when we first launched 3rd Gen Ryzen, there were some areas where we were out of stock at the high-end, particularly on the 3900X and 3950X. Now you’ll see that those CPUs are readily available, through the retailers. So it’s just a matter of when you are early in the cycle, making sure that you can call it correctly. That being the case, I think the technology is working really well, and we’re pleased with how 7nm has ramped for us.AT: Just to confirm, you said that wafer situation at TSMC was tight, or right?LS: Tight.AnandTech: To go beyond that, AMD has already spoken about how its Zen 3 products will be built on TSMC’s N7+. Does that help alleviate the situation, given how parts of your product portfolio will transition to a slightly different process?A: It’s fair to say that all the variants of TSMC’s 7nm share a lot of technology between them, whether that’s N7, N7P, or N7+. The main thing is that we forecast well and plan for success. That's what we're doing.On The Desktop: APUs and ThreadripperVentureBeat: With the launch of Renoir-based Ryzen Mobile 4000 APUs, where is AMD on the desktop APU variants?LS: It's a little bit early. When you take a look at our portfolio, we were very focused in 2019 about the desktop market and the server markets, and we also launched our first Navi-based chips with RDNA. I think going into 2020, well many of you have asked me about Zen 2 coming to notebooks. So we’re very excited about Renoir, the Ryzen Mobile 4000 series, and you should expect a lot more from is. It’s only January in 2020, so there’s a lot more to go.PC World: With Threadripper now at 64 cores, is there a limit to how far you can push the core count for consumers?LS: Is there a limit? [laughs] I think that for 3rd Gen Threadripper, 64 is the current limit! You might have seen in some of the data, that when you go from 16 to 32 cores, the scaling is actually quite good. For multithreaded applications, moving up to 64 there can be a bit of overhead and you won’t see perfect scaling. We believe that it will get better with optimization. Today there’s a lot of professional people that do not realize what you can do with 64 cores, so right now we’re working with the ecosystem to make sure that they’re able to really utilize all of these cores. We love seeing new applications, and we love working with the users that can really take advantage of it – we’re really trying to optimize those capabilities.Bullish on Arm and RISC-VAnandTech: AMD have previously had Arm based products, such as with Seattle, and the K12 project. Arm has put forward a roadmap for its leading enterprise cores to deliver performance gains of 25% year on year, which goes beyond AMD’s ‘above market’ target of 7%. Is there a time where AMD would look into doing a high-performance Arm design?LS: So we already work with Arm in some of our microcontrollers on our designs and products. From a server standpoint, we are not investing in Arm at this point. We think that there’s a huge market out there for x86. I do think Arm has a market and capability, but from our standpoint the focus on x86 is the right thing to do. There’s a lot more we can do in the datacenter, and we have plans to stay with x86 and our EPYC portfolio, going from 2ndGen EPYC and beyond.VentureBeat: Looking at the whole industry, what do you think of RISC-V as a challenger to the way of doing things?LS: There will be those that use it, and they've seen some good momentum. Our focus is very clear, it’s all about high-performance compute, and for that x86 is the leader.Zen 3AnandTech: AMD’s products have been on a very regular 12-14 month cadence for the last three years. Should we expect to see Zen 3 this year? In previous CES presentations you’ve shared 12-month roadmaps, but this year you only spoke about Q1 and Q2. Would you like to comment on Zen 3 or what’s coming?LS: You should expect that we’re going to be very aggressive with the CPU roadmap. We think Zen 2 is the best CPU core out there today, and we’re very proud of it. We’ve completed the family and Zen 3 is doing really well, we’re very pleased about it and you’ll hear more about it in 2020. Rather than ask me the question three times Ian [laughs], let me clear: you will see Zen 3 in 2020!Many thanks to Lisa and her team for their time.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15344/amd-at-ces-2020-qa-with-dr-lisa-su\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: CES 2020: Qualcomm Press Conference Live Blog 11am PT\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2020-01-06T18:45:00Z\n",
      "URL: https://www.anandtech.com/show/15323/ces-2020-qualcomm-press-conference-live-blog-11am-pt\n",
      "Content: 01:51PM EST- Hello all, we're here for the second press conference we're covering today, with many more to come. This time it's Qualcomm's turn to talk about the newest development in 2020 and how the company's product strategies will evolve over the coming year.02:00PM EST- The event is starting - intro video now.02:01PM EST- Cristiano Amon taking the stage.02:03PM EST- Recap over what QC did at the tech summit back in December.02:03PM EST- The Snapdraogn 865 redefined the user experience for 5G for 2020 flagships, as well as introducing the 765.02:04PM EST- The Snapdragon 765 was already announced in the Oppo Reno3 a little over a week ago - a fantastic schedule shortly after the release in Hawaii.02:05PM EST- \"Really excited for Snapdragon setting the bar for performance in 5G\"02:05PM EST- \"2020 is the year we expect 5G to scale\" - moving faster than 4G despite skepticism.02:05PM EST- Couple of quarters ago 5G was seen as hype, but now there's a turnaround in the view on 5G and its adoption.02:06PM EST- The device ecosystem has already moved on.02:06PM EST- 45 operators already have 5G deployed globally.02:06PM EST- 2 years faster than 4G to get the same level of penetration.02:07PM EST- Snapdragon 865 has seen a 2x design win increase over the 855 this year.02:08PM EST- A high ecosystem momentum in adoption.02:08PM EST- The Snapdragon 765 has 2.5x the design traction over the previous generation.02:09PM EST- Oppo, Xiaomi, and Motorola being mentioned as leading adopters of the new platforms.02:09PM EST- \"It's not been well understood how good the traction has been for mobile broadband\"02:10PM EST- 34 OEMS developing CPE solutions using the X55.02:10PM EST- \"We never had that large traction for fixed wireless for any G transition\"02:12PM EST- Expanding adoption of laptops using Windows on Snapdragon02:12PM EST- The 5G PC is now a reality.02:13PM EST- World's first 5G PC announced in partnership with Lenovo.02:14PM EST- Lenovo SVP Johnson Jia taking the stage to talk about the product.02:15PM EST- \"2020 is a year of 5G\" - \"Lenovo is proud to be one of the innovation leaders in 5G\"02:16PM EST- The Lenovo Yoga 5G PC02:16PM EST- Ultraportable 2-in-1 always connected Windows PC.02:17PM EST- Based on the 8cx SoC - likely paired with the new X55 modem.02:19PM EST- Lenovo is using a new patented 5G antenna system.02:20PM EST- New charging system that is more efficient and reduces the laptop's thermals when charging.02:20PM EST- The Yoga 5G laptop comes in spring 2020 starting 1499USD.02:22PM EST- Moving on to the next topic and \"exciting announcement\"02:22PM EST- Cristiano is now talking about the cloud and how 5G is changing it.02:22PM EST- Starting to see new use-cases with the new low latency and bandwidth of 5G02:24PM EST- Talking about the new Cloud AI 100 accelerator that offers up to 350 TOPS inference performance.02:25PM EST- Creating a new 5G edge box for the cloud scalers - apparently a big design win with a major hyperscaler.02:25PM EST- Moving onto automotive.02:26PM EST- Qualcomm has a long history of over 15 years in automotive.02:27PM EST- 100 million vehicles are using QC technology today.02:28PM EST- #1 position in infotainment design wins in 202002:28PM EST- 4 key focus areas in automotive.02:29PM EST- Telematics, Digital Cockpit, Cloud devie management, ADAS and autonomous driving.02:30PM EST- Peter Virk, from Jaguar and Land Rover taking the stage to talk more about connectivity.02:33PM EST- Using the Snapdragon 820am platform. Is this based on the 820? It really show the super long design cycles in automotive designs.02:34PM EST- Jaguar/Land Rover part wrapping up.02:35PM EST- Continuing about the evolution of automotive solutions and autonomous driving.02:35PM EST- We're now at level 2+ - just short of full self driving solutions at level 4+02:37PM EST- Short video about automotive R&D on the platform02:38PM EST- Announcing the new Qualcomm Snapdragon Ride Platform - scaling from level 1&2 up to 4+02:38PM EST- 700TOPs at 130W at the highest end02:38PM EST- Based on a new full new SoC \"Snapdragon Ride\"02:39PM EST- Passively air cooled, reducing cost significantly while improving reliability02:40PM EST- I think that this may be Cortex-A76AE based - I saw some Arm people before the event mentioning they'll have a new product with their IP in the announcements today.02:40PM EST- Expanding the partnership with General Motors.02:42PM EST- C-V2X being approved in China, the EU and the US around the 5.9GHz band by regulators.02:43PM EST- Also announcing a new product offering for automotive : Qualcomm Car-to-Cloud Services02:44PM EST- \"The car is becoming a service platform\"02:45PM EST- 2020 is about scaling 5G, More devices02:46PM EST- Redefining mobile computing, driving performance in the edge cloud02:46PM EST- And continuation of the transformation in automotive platforms02:46PM EST- Welcoming the \"Suunto 7\" using the new Snapdragon Wear 310002:47PM EST- Cristiano is wrapping up, and there's now a few Q&A.02:48PM EST- We're wrapping up - see you on the next press event - AMD is next at 2PM.02:48PM EST- .\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15323/ces-2020-qualcomm-press-conference-live-blog-11am-pt\n",
      "Title: An Interview with AMD’s CTO Mark Papermaster: ‘There’s More Room At The Top’\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-12-30T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/15268/an-interview-with-amds-cto-mark-papermaster-theres-more-room-at-the-top\n",
      "Content: On the back of a very busy 2019, AMD is gaining market share and is now a performance leader in a lot of CPU segments. The company has executed well not only with a good product but also at a time when its main competition is having manufacturing and production issues, which means AMD is ripe to push boundaries, gain market share, and make noise with a lot of wins. This year AMD’s Ryzen 3000 CPUs and EPYC Rome CPUs both have the new Zen 2 microarchitecture built on TSMC’s 7nm, something that might not have been possible without deep co-optimization between the two companies for a high performance product. AMD is also making strides with its relationships with enterprise OEMs. We touched on these topics (and more) with AMD’s CTO, Mark Papermaster.Dr. Ian CutressAnandTechMark PapermasterAMDMark and I have met several times over the last few years to discuss AMD’s strategy. His attitude is, perhaps correctly, a little reserved, but he isn’t shy of commending his teams design wins and is keen to talk up AMD’s partnerships and design strategies with corporate partners. In this interview, we cover product cadence, driving industry standards, partnerships between AMD and OEMs, and where the next of IPC could come from.As always with our interviews, the transcription has been tidied up for readability and the questions may be rearranged to keep certain topics together.AMD’s Release Schedule and Future ProductsDr. Ian Cutress: So far AMD’s rate of new products is on track to produce a new core almost every year. The roadmaps quite proudly showcase Zen 3 as almost ready, Zen 4 in development, and Zen 5 further out. Is this cadence sustainable?Mark Papermaster: We’re on a 12-18 month cadence, and we believe that is sustainable. It’s what the industry demands from us.IC: With the recent successes AMD has had, such as increasing market share in key areas and driving review, does that not perhaps give an opportunity to accelerate the cadence, hire more staff, or does it inevitably cause new ideas to form that slow down the rate of development?MP: There’s a natural cadence to where we are, and we have multiple design/implementation teams in play. As one team completes, they move onto the next project. You know I think it’s the right rate for us, because in some markets you have an increased expectation of a new product every year, like smartphones. For that market you have to catch the holiday season every year with pretty much a new core, and to a large extent they can do that. But that need to tie things off at the same time every year means that there might be performance still left on the table for that core, and that’s going to happen every generation. So we believe we’re on the right track to deliver for our high performance customers, and that’s what we’ve based AMD’s resurgence on.IC: Can you talk about how much of the roadmap has been penciled in for Zen 4 and post-Zen 4? Normally when we speak about these roadmaps, the 3-to-5-to-7 year roadmaps, the size of the company often determines the extent of the long-term vision, and over the last couple of years AMD has been quite active in promoting its three-year time span.MP: What we share publicly, the current roadmap, shows our trajectory and what we’re doing for the current generation, but also n+1 and n+2. You know, of course we have teams that are looking beyond n+2, but we don’t really talk publicly about what we’re looking at that far out. The biggest change for AMD compared to how we used to do things years back is that we have a constantly leapfrogging set of design teams, and this helps us to have this constant innovation process to say on the 12-to-18 month cycle, taking the time to make sure that we have the right performance for the hardware that we’ve come up with. If we get to a point where we have to square off a design due to time-to-market, then the ideas that don’t fit in that window get rolled right into the next generation team.IC: Recently an interview with an AMD executive related the current roadmap as a sort of tick-tock model, based on what Intel used to run for a decade. Do you have parity with this analysis?MP: I didn’t see the particular interview you’re referring to, but what I will say is that we’re not on a tick-tock model. What we’re doing is looking at each generation of CPU and marrying the best process variant that’s out there with the right set of IPC improvements, memory hierarchy, and all the things that we can put in there. We are committed to staying on the best possible pace of improvements each generation that we can. This is a formula that’s working well for us at AMD.IC: Should we expect Zen 2 to have a refresh, like a Zen 2+, like Zen 1 did?MP: I have nothing to say on any refresh on current designs, but we always look at where it makes sense and where we’re able to take opportunities to provide a bump in performance, power, or die area.Driving IPC and PerformanceIC: During a recent earnings call, Lisa mentioned that moving forward AMD won’t always be relying on taking the best process node advantage from TSMC, and the company will be driving architectural improvements as time goes on. In order to drive leadership through these last two generations and into Zen 2 and beyond, you’ve been driving microarchitectural IPC aggressively – is that in of itself sustainable, as we move into an era of diminishing return for increased core counts?MP: So clearly the industry has figured out how to take advantage of increasing cores. There is a rate and pace that as a CPU vendor you have to manage with this because you cannot increase the rate of change of the number of cores at a faster rate than the software can take advantage of it! But we’re clearly adding more engines on CPUs and GPUs, and those specific hardware engines will remain a force going forward. Now it clearly has to be done in a balance with memory and IO – we have to be able to provide a balanced system. We know this very well at AMD, and that’s what we’re going to keep doing.IC: One of the things that drives new generations of hardware for everyone, especially processor architecture, is improvements to the ISA and new instructions. But as we’re also seeing specific accelerators being bundled on packages, and additional security hardware and such, how is AMD going to drive that arm of the industry forward?MP: If you look at our chips today, we have a variety of specialized functions. Every AMD chip has a PSP, a platform security processor, which as you know is an isolated processor within our silicon for root of trust and providing security services. We also have accelerators in our cores designed for specific math functions. We also have a number of DSPs that we focus on that our notebook chips have and will have, and our client processors will have audio processors on there. So it’s a matter of ‘when does it make sense’ to have an accelerator or a specialized function on the silicon, or perhaps even as a chiplet? We’ve led the way in the chiplet architecture for various segments, so we understand when it makes sense to have something discrete and/or keep it under evaluation until it needs to come on-module or on-chip.IC: AMD is very proud to have an IPC advantage on the desktop today with Zen 2, but we continue to see Intel pushing the boundaries of new ISA instructions, such as vector instructions or bfloat16, because Intel has to cater for those edge cases and customers that really want the specific features. AMD has had much success in the past, such as driving the 64-bit x86 standards. Can we see AMD being a force behind defining future x86 instruction standards?MP: We continue to look at every facet of design. If you see the security innovations that we brought out with the Zen processor line, with memory encryption – we drove capability that did not require new extensions as it was under the covers and didn’t need to drive a set of software modifications. So we look at innovations that can benefit the end customer, and some of them do in fact need an extension so we continue to look at that. On the overall viewpoint for x86, we’re always looking at some extensions that drive wide adoption, and if that happens, we include it.IC: Everyone’s favorite topic when discussing new hardware is about increases in IPC. For Zen 2, we saw a very good 15% increase in IPC over the previous generation. Recently we heard about Forrest Norrod (SVP of Datacenter) talk about Zen 3, in that we should expect to see a very typical increase in IPC again, making another step-function improvement over Zen 2. Do you expect AMD to continue with these sorts of improvements?MP: So with our roadmap, and as you know I won’t go into specifics about IPC gains for future generations, but we are driving a model of high-performance compute, and there will be performance gains with each new generation. It’s all about picking the right set of IPC improvements, and the right process nodes and design rules for our core designs. There’s always a balance when we design our cores, so match power with efficiency, and at AMD we still expect to exceed the industry standard. We’ve stated before that the industry has been on a slow 7% annual growth rate in single threaded performance, and our goal is to beat that with every generation of our products. We’ve executed better than the industry with our recent products and we exceeded industry expectations.(It’s worth noting that at a 7% annual improvement needs to be met, if a new product comes out every 18 months, then to stay on the same trend each product needs a +10.7% improvement.)IC: As core performance increases, it puts extra stress on the caches inside the core. AMD has done well with respect to cache size and latency, such as the large L2 in the current designs. We now have a situation where your main competitor has increased its L1 cache size by 50%, in exchange for a 25% increase in latency right at the heart of the core, which was a big surprise to us. Can you talk about AMD’s cache design momentum, and your thoughts on this?MP: As your saw, our Zen design from first gen to second gen has improved our memory hierarchy and associated latencies. We have that focus, and I’m not going to pre-announce what’s in Zen 3, but the cache design is a part of that overall IPC uplift that we talk about, and part of the performance improvement that we focus on every generation. Latency matters for our customers, for sure.IC: One of the key marketing messages that AMD has had since Kaveri is to hit its 25x20 goal: to enable 25x more performance on its 2016 products by the end of 2020. We’ve been following this goal closely, and it still seems as if AMD has a way to go. Can you comment on this?MP: I’m actually really excited with how our team has rallied around this message. Our engineers actually called that goal, it wasn’t initially a marketing message at all. They’ve used it as a motivating factor in driving the notebook advantages of all the design and the process aspects that go into every generation. So yes, we’re excited about the progress we’ve made, and if you look at our second generation of Ryzen Mobile that came out, I think what you saw was excellent with improvements in battery life and performance. Then you have to look at the deep partnership with Microsoft on the Surface Laptop 3, and that enabled yet an even closer software and hardware partnership that benefits not only Microsoft but also the Windows ecosystem. That partnership has resulted in an even closer power management interlock across all of AMD’s mobile offerings with Windows.On-Package High Bandwidth Memory and Semi-Custom DesignsIC: Are we at a point in time where processors need on-package high-bandwidth memory? We’re seeing memory channel counts going up and memory frequencies going up, and we’ve transitioned from DDR3 to DDR4 and moving into DDR5 in the future, but there’s still a segment of the market that wants high-bandwidth memory as close to the processor as possible.MP: You bet, and we’ve been in this space for years. We led the way at AMD of bringing HBM to GPUs – it’s on a silicon interposer using a 2.5D packaging technique. What makes it critical is getting the right memory solution for the workload, and you also have to hit the right cost/performance targets. It’s a tricky balance that you have to make for every product. In high performance GPU computation, HBM is a must, and you see it on our MI roadmap. At every other point, we always look at the right solution, and both GDDR and DDR have great roadmaps that are being executed on, and we’ve been leveraging these technologies successfully. We will do the same moving forward, looking at each of our products and asking where the industry is going, what the workload requirements for each product are meant to be, and what are the cost/performance metrics of each memory solution.IC: We’ve seen some customers go down the AMD route already implement GDDR with your CPUs and APUs, such as with the Subor console and other console-like chips. If a HPC customer came along and wanted HBM on the CPU package, would that be something you would consider?MP: For our semi-custom business like that, when a customer has a volume opportunity and wants to partner deeply to leverage the building blocks that we have, then if it’s a great opportunity for both companies involved we will create a semi-custom solution. We continue to analyze those opportunities as they come up.IC: What level of investment is required for a partner to invest in a semi-custom chip? I know that’s a super broad question, but we’re trying to understand what a company like Zhongshan Subor (now defunct) had to invest to be able to get a semi-custom design. Is it millions, or 10s of millions? Surely someone with 50 grand isn’t going to be in semi-custom.MP: I’m unfortunately not able to answer that. These types of agreements we have in place are confidential. But you are correct, it’s more than 50k! As you know, if you want to be on the leading edge with these designs, just creating the masks can cost millions.IC: From our perspective, it’s hard to discern with these agreements how much the partner is putting in compared to what AMD is putting in: does the partner get a good deal if it’s a unique design and AMD sees interest in that design outside of the partnership, or are all the costs borne by the partner? The Zhongshan product was reported (albeit unverified), to be in the 30m range, which meant that if they funded all of that, they would have to sell a good million or two million units to break even, which didn’t seem feasible.MP: I can’t say much, but I hear what you are saying. Our semi-custom agreements, even if we’re collaborating just on hardware or it’s the full stack, ultimately remain confidential and it’s up to the customer about how much they want to say. All I can say is that our semi-custom model is alive, successful, and a key part of AMD’s offerings.Compute eXpress Link (CXL)IC: AMD is now a member of the CXL consortium, even though it’s also a member of pretty much every interconnect standard in the market, and in the past AMD has pushed CCIX. CXL has strong ties to PCIe 5.0, so can you talk about AMD’s expectations and deployments of these new interconnects?MP: AMD is a proponent of open standards that align with the industry, especially when it comes to common standards for IO and accelerators. You’re absolutely right in that we joined CCIX and we have been active there – but we’ve been active in almost all the consortia we are a part of. On CXL, we joined and we believe it’s actually a very strong consortium to leverage PCIe 5.0. We are also very strong participants in PCIe development efforts, so this aligns with our strategy of leveraging PCIe by also being able to craft, through a consortium, a lower latency interconnect standard that is the best way for the industry to drive alignment and leverage that PCIe foundation. This is what is going to drive the PHY (physical layer) development – it can be a huge cost but being part of both of these standards helps us amortize those PHY costs, as a path to both low-latency accelerators and memory compared to standard PCIe. So we think it’s a good consortium, and I do see it gaining momentum in the industry.IC: Do you expect CXL to align with PCIe extensively over its lifetime?MP: We’ll see – watch this space. When you look historically, often these types of consortia start off from one of the larger standards committees, they have in the past emerged into a broader standards committee in of themselves, but it’s way too early to make that call for CXL.IC: One concern over the diagrams shown for CXL is that in the future, there might be CXL-only accelerators, like CXL-only graphics cards and CPUs, that won’t support PCIe 5.0. What’s your opinion of this?MP: It’s good to know that concern – I don’t see that happening. Any device that is CXL compliant will also be PCIe compliant (and actually requires a PCIe handshake to initiate the protocol), so any additional limitation would be vendor specific. But we don’t see that happening.Frequencies and Moore’s LawIC: One of the concerns raised when moving to more advanced process nodes with similarly designed cores is a loss in frequency due to current density. When AMD moved to TSMC’s 7nm the company made some noise about how it was able to keep frequency parity with the 14nm/12nm designs from Global Foundries. Compared to previous generations of Ryzen, you’ve even pushed those frequencies higher. Elsewhere in the industry we’re seeing difficulty in keeping frequency parity on the newest process nodes – so even though PPA (power, performance, area) improves as a whole, how does AMD approach that issue for future process nodes? Are there any additional factors that we should know as the public and as engineers that follow this space that AMD has taken into account?MP: I think that when you look at all the commentary about Moore’s Law, you see pretty much universal alignment that we no longer have the old Moore’s Law we’d all come to know and love, the one with a frequency improvement in every generation and the advantages of power and area that came along with those frequency improvements. In this new era, it’s not that Moore’s Law is dead as such, but it’s not the same old Moore’s Law. There is somewhat universal alignment now that frequency is already not scaling with each successive node – I think that most in our industry accept that as a given.What that means at AMD is that we design our plans such that we can take a holistic view at all the levers we have to drive performance up. This is why we’ve innovated on chiplet approaches – we apply that to where it makes sense. That’s why we’re innovating on changes that we make to our memory hierarchy, and as I’ve already mentioned, carefully choosing the right type of memory we deploy for specific workloads. It’s this type of holistic system approach that we’ve adopted and going to add to as we move forward to make sure that even in this new era of Moore’s Law that we operate in, we can continue our drive to higher performance.New Generations of Infinity FabricIC: AMD’s Infinity Fabric (IF) is a key part of the messaging when it comes to AMD driving scalability between its products and AMD fellows do delight in talking about it. As we move to new faster connectivity standards, like PCIe 5.0 and even PCIe 6.0, will IF keep pace with driving bandwidth, or are we going to be blowing through the power budget just on interconnect?MP: What you flag is certainly a concern across not only the Infinity Fabric (which is AMD’s protocol and a key in our chiplet approach) but also flexibility and modularity that we have across our roadmap to get our latest generations of our intellectual property into all the segments we serve. So there is a reason why we increment and design a new version of IF for each generation of product – it is in fact the innovation required to keep growing the bandwidth and yet keeping the overall system optimized. You also have to do the same with the IO and interconnects that you drive off chip, like the PCIe and all these SERDES links. They require that same innovation. I think you’re seeing a lot of work on our design approach on both logic as well as our SERDES, for both long reach and short reach implementations.IC: With every new generation of Zen, would it be safe to assume that we should expect new generations of Infinity Fabric to partner them?MP: That would be my expectation.IC: In the market today we have dual socket Rome using IF for socket-to-socket communications, and we’ve been told that the GPU line will enable GPU-to-GPU infinity fabric links at some point. Is there a technical reason we don’t see CPU-to-GPU infinity fabric connectivity right now?MP: So we deploy IF in both CPU and GPU, and it enables us to scale very effectively. We leverage across CPU and GPU today, and it allows us to use elements of optimization that we can do using the protocol. We continue to look at where we can leverage that benefit, and where having an AMD CPU connected via IF to an AMD GPU makes sense.Manufacturing PartnershipsIC: Is AMD a key partner for TSMC’s 5nm production?MP: We haven’t announced a 5nm part at this point, but we work very closely with TSMC, and you’ve seen the kind of deep partnership that we have with them with 7nm across our product lines.IC: It has been announced that AMD is partnering with Samsung regarding mobile graphics. Is that purely a hardware relationship, or does it extend to manufacturing?MP: The partnership with Samsung is an IP joint venture. We’re partnering with them on that with our graphics IP. As you know, AMD is not in the smartphone business, so that joint venture will result in technology that is then deployed by Samsung into their products.IC: From our audience, one of the most common questions I need to put to you is about TSMC’s capacity. Can you shed some light into how this might affect AMD?MP: TSMC is a key partner for us, and they were with us at our second generation EPYC launch. I think that really helped people to understand the scale that TSMC has. The most rapid launch in the history of TSMC was its 7nm process node, which has had an asymptotic volume ramp and that was well ahead of our launch of Rome and EPYC. So we’re getting a great partnership with TSMC, as well as great supply. We did have some shortfalls on chips when we first launched our highest performing Ryzens, and that was simply demand outstripping what we had expected and what we had planned for. That wasn’t a TSMC issue at all.IC: We’ve seen reports of TSMC extending lead times for order requests? Does that put a spanner in the works for 2020/2021 launches?MP: Part of our close relationship with TSMC allows us some insight into what they’re able to offer and when; but we also have to leverage our planning team and they have to be on the ball with every product and every launch. Just so you know, that’s always going to be the way: lead times for not only popular nodes but also the leading edge nodes go up every generation. As a result, everyone, not only at AMD, has to get better and better at this.AMD’s Market ShareIC: A number of media and analysts have been looking at AMD’s rise in both product performance but also strategy when it comes to the enterprise. They’ve been waiting for AMD to catch up and provide healthy competition, which it is now doing, and now we’re looking at when your competition will catch up to you. Despite this, we still see AMD’s enterprise market share growing perhaps slower than expected. Is there a reason for that? Or are we still in a miasma of teaching people that AMD is back in business, or a victim of slow enterprise update cycles? Can you shed some light?MP: If you look at the server market, we said that we projected in about mid 2020 that we would hit double digit market share. People have asked us why does it take that long, and why we aren’t moving more quickly, and the fact is that it takes longer in the data center markets to bring the partners along and get through their entire certification cycle to get their customer work cases and applications optimized. We’re actually really incredibly pleased with the response from the OEMs and customers, and with our second generation EPYC, with Rome, we’re right on track with our expectations, and the market is growing too.What’s the Limit?IC: AMD’s current highest TDP processor on the market today is at 280W, such as the EPYC 7H12 built for HPC. Is there an upper limit to TDP expansion? We see Intel moving into the market with higher TDP chips, at 350-400W TDPs.MP: Collaboration with our OEM partners isn’t just about maximizing the power available for the CPU. You also work across both CPU and GPU, and that’s what we’re doing with Cray/HPE for the Frontier supercomputer. That’s really indicative of the kind of system optimization across hardware and the system and software stacks that we can do with OEM partners to really push up the roof in the HPC market.We announced that 7H12 part in our continued roll out of Rome, and you saw ATOS use it and we were really happy for them to see their placement to catch the Top500 listing as it was a race against time, but it just shows what you can do with great execution. But you know when you think about that time of integrated water cooled solution, it tells you that you have to grow these close partnerships, as we’re doing at AMD with our OEM customers, and there’s a lot more performance that can be had going forward. There’s a lot more room at the top!Many thanks to Mark and his team for their time.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15268/an-interview-with-amds-cto-mark-papermaster-theres-more-room-at-the-top\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AnandTech Year In Review 2019: Flagship Mobile\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2019-12-26T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15246/anandtech-year-in-review-2019-flagship-mobile\n",
      "Content: 2019 is coming to an end, and it’s time again to look back at what the industry has brought us. This year, we saw a lot of hardware improvements from all the various vendors, with a big focus on bringing out new distinctive designs. We’ve seen some exotic devices in the form of foldable phones for the first time ever, and even some more traditional designs dared to implement design cues such as mechanical pop-out cameras. While designs were sometimes the main differentiating factors, most of the time the key selling points of 2019 devices were big upgrades in their camera capabilities. Here we’ve seen huge leaps from almost all the vendors, and the year definitely will be remembered mainly for the innovations in photography.Triple Cameras Become A Must-Have in 2019Undoubtedly the biggest trend in 2019 devices, affecting both design as well as features, has been the standardisation of triple-camera setups. A trendstarted in 2018 by Huawei, it’s essentially become a main-stay of almost every flagship device in 2019, and a must-have for any phone which takes photography seriously. The feature is now so widespread that we’ve even seen it trickle down to the mid-range, and even sometimes lower end devices.In most cases, the new module being added to the camera array was the ultra-wide-angle unit. The UWA isn’t exactly a new innovation; LG was the first in the industry to employ it several years ago, and Huawei last year perfected it in the Mate 20 and 20 Pro by dramatically raising the bar in terms of picture quality.The adoption of the UWA by Samsung, Apple, OPPO, OnePlus, Sony, Xiaomi and many others means that the new camera perspective is ubiquitous for almost all 2019 devices, and it’s a fantastic addition to the capture experience not only for still pictures, but also for video recording.We’ve also seen some innovations on the telephoto modules by some vendors. Huawei and OPPO here are the two main vendors who attempted to differentiate themselves through implementing high level optical magnification in their units. This was mainly achieved by using a more innovative right-angled orientation of the camera sensor inside of the body of the phone, viewing the outside world through the help of a mirror prism. The system does work very well to achieve high magnification levels, however the limitation of the sensor size as well as the very small aperture of the optics systems generally results in lesser picture quality in terms of exposures or dynamic range, to the point that I found these systems to be more or less just simple gimmicks, rather than practical competitors more traditional optical 3x magnification modules.Sony’s IMX586 – An Absurd Amount of Design WinsThe very first IMX586 device we’ve reviewed this year was theHonor View20in January. Following that, we’ve seen a stupendous number of smartphones employing Sony’s new sensor. It’s been employed byHonor in many moreproduct ranges, it’s the sensor powering Xiaomi’s flagship devices,OnePlus’s 7-seriesuses the new sensor, OPPOfeatures it in their Reno devices, ASUS has it as the main sensor for their Zenfone 6 andROG Phone II, and it’s used in a plethora of other devices from Realme, Lenovo, Samsung, Vivo, Nubia, and others.I don’t think we’ve ever seen a single camera sensor have so much design wins in the industry as the number of phones featuring it is pretty absurd. Whilst the IMX586 isn’t the first modern high-megapixel mobile sensor, that title still goes to Huawei’s exclusive 40MP units introduced last year with the P20’s, it’s been the sensor that has popularized the new sensor technology for the mass-market.(Yes the Nokia 808's sensor was technically first; but with very different technology and no adoption)Sony here has triggered a new trend, and frankly I’m not sure it’s based as much on technical merit rather than the marketing power of the 48MP figure. As we’ve seen in numerous reviews this year, phones with the quad-Bayer high-MP module didn’t necessarily perform any better than more traditional lower resolution Bayer sensors. In fact, it seems a key selling point of the IMX586 was that it wasn’t necessarily a high-end sensor, and was thus cheaper to produce and sell to vendors, hence its popularity across a wide range of phones at different price-points.For 2020, it seems that Samsung LSI is aiming to disrupt the high-MP camera sensor market even more with their introduction of64MP and 108MP sensors. Thefirst devices with the new sensorgeneration have already been released, and I think the 108MP model featuring a new humongous 1/1.33” sensor size is very promising. It’s rumoured Samsung will be released their next-gen Galaxy-S with a sensor of this calibre, which will be very interesting to see how that pans out.Computational Photography: The New NormalComputational photography has been one of the key innovation points of 2018 – it’s been mostly been pioneered by Huawei with the introduction of a tripod-less night mode in last year’s devices. Google was a quick follower with the introduction of Night Sight in the Pixel 3, but other than these two vendors, it still was a rather rare or inexistent aspect to photography for most devices.2019 has now completely changed this around, and much like the new triple-camera setups, having computational photography modes exemplified by dedicated Night Mode has been a must for most vendors. The vastly improved computational processing power of new SoCs has made this possible for essentially all device vendors, however we’ve seen that software plays a much bigger role, and device vendors need to invest in a lot of R&D into enabling the features.[Mate 30 Pro] -[P30 Pro][S10+ (S)] -[S10+ (E)][iPhone 11 Pro] -[Pixel 4][G8X] -[Xperia 1]Samsung notably started the year with the release of the Galaxy S10 without such a mode, but was able to add it in a firmware update in May, continuously improving it over the course of the year, and also making it a key feature during the launch of the Note10 in August.Apple’s introduction of Night Mode as well as Deep Fusion also finally brought computational photography to iPhones – Deep Fusion particularly was an industry-first which brought new levels of detail to still photography.Other vendors are also hanging in there – the implementations here range from bad (Xiaomi), to excellent (OPPO). The software development aspect of the camera firmware and processing means that things are continuously changing, and a device reviewed earlier in the year most certainly won’t showcase the same results if it were to be tested now. This also puts a lot of pressure on vendors to be able to deliver a representative implementation early on as it’s rare for reviews to go back and be updated. Alongside always having the same environmental test conditions, this has been another key reason why I always re-test the complete device line-up, in order to have the most up-to-date comparisons between devices for each new review, so make sure to always check out the newest pieces even if it’s about some other device that might not interest you.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15246/anandtech-year-in-review-2019-flagship-mobile\n",
      "Title: Analyzing Intel’s Discrete Xe-HPC Graphics Disclosure: Ponte Vecchio, Rambo Cache, and Gelato\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-12-24T14:30:00Z\n",
      "URL: https://www.anandtech.com/show/15188/analyzing-intels-discrete-xe-hpc-graphics-disclosure-ponte-vecchio\n",
      "Content: It has been a couple of weeks since Intel formally provided some high-level detail on its new discrete graphics strategy. The reason for the announcements and disclosures centered around Intel’s contract with the Department of Energy to build Aurora, an exascale supercomputer at the Argonne National Laboratory. The DoE and Argonne want developers clued into the hardware early, so when the supercomputer is deployed it can be used with as little ‘learning time’ as possible. This means Intel had to flesh out some of its strategy, as well as lift the lid on its first announced discrete GPU product. Only time will tell if it’s a bridge too far, or over troubled water, but today we know it as Ponte Vecchio.Intel On Discrete Graphics: A Quick RecapWhile Intel has had a graphics portfolio for a couple of decades, those graphics solutions have been limited to embedded graphics and integrated graphics solutions. There was a slight attempt to move into the graphics space and play with the big boys,with the Intel i740, however that was a long time ago. Intel’s current graphics architecture, called ‘Gen’, is currently in use in hundreds of millions of mobile devices, and is present in a substantial number of desktop processors, even if a discrete GPU is being used instead.Intel has had high hopes for the graphics space before. Known as ‘Larrabee’, Intel attempted to engineer what was essentially x86 based graphics: using wide vector engines based on the same code path as Intel CPUs, the idea was to provide high-end graphics performance with the ease of programming in standard CPU code. While that product did actually run a number of graphics demos over the years, the hardware ended up being put to use in the high-performance computing market, where some developers saw the use of five-dozen 512-bit wide vector units absolutely fantastic for their simulations. This was the birth of AVX-512, which has lived on and now in Intel’s Xeon Scalable CPUs as well as consumer-grade Ice Lake laptop processors. The product that ‘Larrabee’ ended up as, Xeon Phi, scored a number of supercomputer wins and originally the Xeon Phi ‘Knights Hill’ product was destined to be put into Aurora in 2020. However the Xeon Phi program only lasted a few generations, with the final ‘Knights Mill’ hardware not being widely deployed and subsequently put to pasture.Fast forward several years, and some management adjustments, and Intel has decided once again to enter the big graphics market. This time they’re going with something more conventional, something that looks more like a traditional graphics design. While the project started somewhere around three years ago, the big announcement that Intel was serious was when the company hired Raja Koduri, AMD’s Chief Graphics Architect in December 2017, and then Jim Keller, renowned SoC Guru. Raja Koduri’s title, Chief Architect, and his two decade of experience in building graphics solutions at AMD and Apple showcased how serious Intel was with this.Since December 2017, Intel hasn’t said much about its new graphics plans. Under Ari Rauch, notable marketing figures and analysts were hired to be part of the team. Intel disclosed at its Architecture Day in December 2018 that the graphics solutions it would offer would be a full top-to-bottom implementation, covering low power integrated graphics all the way to the high-end. At the time Intel stated there would be two main GPU microarchitectures, all building from the ‘Xe’ architecture. Xeis meant to stand for ‘eXascale for Everyone’ (rather than x^2.718), with the marketing message that Intel wants to put high-end performance and efficiency anywhere it can.As part of HPC DevCon, and Intel’s announcement with the DoE/Argonne, the veil was lifted, and we were told very slightly more than just the high level information. We were lucky enough to speak with Raja Koduri in a worldwide exclusive for the event, as his first official 1-on-1 interview since he joined Intel. It is worth a read and gives his perspective on a lot of ideas, as well as some of the decisions he has made.https://www.anandtech.com/show/15130/anandtech-exclusive-an-interview-with-intels-raja-koduri-about-xeThis article is going to dive into Intel’s HPC DevCon disclosures about their graphics strategy. Here we are going to cover some of the blurb about Intel’s big plans, the new ‘third’ microarchitecture in Xecalled Xe-HPC, the new GPU product ‘Ponte Vecchio’, Intel’s new Memory Fabric, a breakdown of the oneAPI software stack as presented, and what all this means for the rest of Intel’s graphics platform.Exascale for EveryoneIntel says that it is hard not to notice the ‘insatiable’ demand for faster, more power efficient compute. Not only that, but certain people want that compute at scale, specifically at ‘exascale’. (It was disclosed at a high-performance supercomputing event, after all). For 2020 and beyond, Intel has designated this the ‘Exascale’ era in computing, where no amount of compute is good enough for leading edge research.On top of this, Intel points to the number of connected devices in the market. A few years ago analysts were predicting 50 B IoT devices by 2020-2023, and in this presentation Intel is saying that by mid-2020 and beyond, there will be 100 billion devices that require some form of intelligent compute. The move to implementing AI, both in terms of training and inference, means that performance and computational ability have to be ubiquitous: beyond the network, beyond the mobile device, beyond the cloud. This is Intel’s vision of where the market is going to go.Intel splits this up into four specific categories of compute: Scalar, Vector, Matrix, and Spatial. This is certainly one blub part of the presentation I can say I agree with, having done high-performance programming in a previous career. Scalar compute, is the standard day-to-day compute that most systems run on. Vector compute is moving to parallel instructions, while Matrix compute is the talking point of the moment, with things like tensor cores and AI chips all working to optimize matrix throughput. The other part of the equation is spatial compute, which is derived from the FPGA market: for sparse compute that is complex and can be optimized with its own non-standard compute engine, then an FPGA solves it. Obviously Intel’s goal here is to cover each of these four corners with dedicated hardware: CPU for Scalar, GPU for Vector, AI for Matrix, and FPGA for Spatial.One of the issues with hardware, as you move from CPU to FPGA, is that it becomes more and more specialized. A CPU for example can do Scalar, Vector, Matrix, and Spatial, in a pinch. It’s not going to be much good at some of those, and the power efficiency might be poor, but it can at least do them, as a launching point onto other things. With GPU, AI, and FPGA, these hardware specializations come with different amounts of complexity and a higher barrier to entry, but for those that can harness the hardware, large speed-ups are possible. In an effort to make compute more ubiquitous, Intel is pushing its oneAPI plan with a singular focal resource for all four types of hardware. More on this later.Intel’s Xearchitecture will be the underpinning for all of its GPU hardware. It represents a new fundamental redesign from its current graphics architecture, called ‘Gen’, and pulls in what the company has learned from products such as Larrabee/Xeon Phi, Atom, Core, Gen, and even Itanium (!). Intel officially disclosed that it has its first Xesilicon back from the fabs, and has performed power cycling and basic functionality testing with it, keen to promote that it is an actual thing.So far the latest ‘Gen’ graphics we have seen is the Gen11 graphics solution, which is on the newest Ice Lake consumer notebook processors. These are out in the market, ready to buy today, and feature performance 2x over the previous Gen9/Gen9.5 designs. (I should point out that Gen10 shipped in Cannon Lake but was disabled: this is the only graph ever where I’ve seen Intel officially acknowledge the existence of Gen10 graphics.) We have seen diagrams, either potentially from Intel or elsewhere, showing ‘Gen12’. It would appear that ‘Gen12’ was just a holding name for Xe, and doesn’t actually exist as an iteration of Gen. When we asked Raja Koduri about the future of Gen, he said that all the Gen developers are now working on Xe. There are still graphics updates to Gen, but the software developers that can be transferred to Xehave been already.If you’re only going to read one thing today, then I want to skip ahead to Raja’s final slide of what he presented at HPC DevCon. Putting a quite ambitious goal in front of the audience, it showed that Intel wants to be able to provide a 500x in performance per server node by the end of 2021 compared to the per-node performance in 2019.Now it is worth noting that this goal wasn’t specifically nailed down: are we comparing vector code running in scalar mode on a single 6-core Xeon Bronze in 2019 to an optimized dual-socket with six XeGPUs in 2021? 500x is a big bet to make, so I hope Intel is ready.In the next few pages, we’ll cover Xe, Ponte Vecchio, oneAPI, and Aurora.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15188/analyzing-intels-discrete-xe-hpc-graphics-disclosure-ponte-vecchio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 80-Core N1 Next-Gen Ampere, ‘QuickSilver’: The Anti-Graviton2\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-12-23T20:00:00Z\n",
      "URL: https://www.anandtech.com/show/15253/80core-n1-nextgen-ampere-quicksilver-the-antigraviton2\n",
      "Content: The drive to putting Arm into the server space has had its ups and downs. We’ve seen the likes ofAppliedMicro/Ampere,Calxeda, Broadcom/Cavium/Marvell,Qualcomm,Huawei,Fujitsu,Annapurna/Amazon, and evenAMD, deal with Arm-based silicon in the server market. Some of these designs have successful, others not so much, but Arm is pushing itsnew Neoverse N1 roadmapof cores into this space, aiming for high performance and for scale. We’ve already seen Amazon come into the market with itsN1-based Graviton2for its cloud services, but there’s going to be a counter product for every other cloud provider, with the new N1-based Next-Gen Ampere CPU, codenamed QuickSilver. We have some details ahead of the official release announcement in Q1 2020.Ampere, technically Ampere Computing, is currently in the market with its eMAG processors. Using custom Arm cores derived from its acquisition of AppliedMicro, it has achieved mild success in second tier cloud providers (such asPacket) as well as Android in the cloud-type services for a number of Chinese smartphone game providers. There’s also some success with telecommunications at the edge, but ultimately nothing with extreme demands. This is where the Next-Gen Ampere product comes in.Updated Roadmap for Jan 2020New Product, No (Marketing) Name YetThe new product doesn’t yet have a marketing name – we asked and they preferred it to be called ‘Next-Gen Ampere’ for now, or to use its SoC codename ‘QuickSilver’. What we were told is that the new product is a brand new ground-up design from Ampere, separate from the AppliedMicro IP acquisition. It plans to compete in the same space that Amazon’s Graviton2 currently sits at AWS, but as the main alternative to the other cloud providers that won’t have access to Graviton2.What we are getting today is some rough details of the new chip. Other features, such as exact SKUs to be launched, exact TDPs, exact frequencies, and pricing, are going to be disclosed at the official release announcement in 2020. Nonetheless, Ampere has exposed a lot of details.Next-Gen Ampere will be a monolithic chip built on TSMC’s 7nm process and featuring 80 cores. These cores are not custom like eMAG, but are built on Arm’s Neoverse N1 design, using paired clusters of cores connected by an Arm mesh IP (CMN-600). This is the same core as Graviton2, and as expected Ampere is keen to promote that their design is optimized for power, performance, latency, and throughput, as well as offering more cores and more of other things as well.N1 cores are single threaded (not multi-threaded), and the QuickSilver SoC is built with containers in mind: chip will support container level QoS commands such that shared resources are not taken by one particular customer, and additional RAS features will be in play to ensure consistent performance. Ampere actually hammered on this point a fair bit, stating that QuickSilver is built to ensure consistency in a multi-tenant environment to the point where deterministic performance is required, which suggests a container based frequency and cache control. Ampere says that the turbo frequency of its new chip will be consistent in order to enable this.Aside from 80 cores, QuickSilver will also have over 128 PCIe 4.0 lanes. Ampere wouldn’t state at this point exactly how many (details to come in Q1), but did state that it would have more on the market than any other server chip, x86 or Arm, will have in 2020. The current leader in this race is AMD, whose Rome EPYC processors have 128, and we confirmed that Quicksilver will have more than 128. This suddenly got very interesting, as this opens up a lot of possibilities for connectivity in specific markets, such as accelerators and storage. Ampere is a key partner in NVIDIA’s CUDA-on-Arm strategy in 2020, and so expect to see cloud instances using Quicksilver with access to lots of NVIDIA GPUs.Also on the new chip, there will be eight DDR4 memory channels, although an exact frequency support was not stated. Ampere said it would be more than the current eMAG support, which is DDR4-2666, and that the IP they are using for the DDR4 memory controller allows them to push up to the limits of the JEDEC DDR4 specification, so realistically here we are looking at DDR4-3200 memory out of the gate. It will be interesting if this includes 2 DIMMs per channel support as well.QuickSilver will support up to dual socket configurations, and Ampere states that it will use the CCIX protocol over PCIe 4.0 to enable socket-to-socket communications. Beyond this, the new chips will also support off-chip CCIX, for coherent accelerator attachments, or to add coherent storage class memory tiers. Ampere stated that their desire to use standardized interconnect protocols is helping drive their time-to-market, and with use of tried-and-tested IP blocks (like the DDR4 memory controller), that helps as well, along with taking advantage of the software and infrastructure that already circulates these standards. We were also told that all the work done to improve the performance on eMAG was transferable to QuickSilver, and that going forward Ampere’s strategy of building on the previous will be a key element for its customer base.At this point in time Ampere did not see the need to add-in any specific acceleration units to the design – aside from the limited work they could do on the core, they decided that there was no need for specific cryptography or AI acceleration engines at this time, and they will wait until such compute blocks become standardized. The amount of connectivity and coherency, according to Ampere, will assist any customer needing additional acceleration.On performance, Ampere is targeting a big leap over eMAG. The N1 cores from Arm are A76-like in their performance, and offer some configurability, such as a 512 KB to 1 MB L2 option. Ampere didn’t state exactly what size L2 they are using, but did state that they optimized for performance over die area in order to reduce memory/cache misses, which would imply that the full 1 MB L2 configuration is in play here. Ampere currently has silicon in house and early silicon sampling with key customers, and will provide a more rounded vision of performance with its full launch announcement in 2020.We were told that the TDP of Next-Gen Ampere will range from 200W+ for the full 80-core model for servers, down to 45W with silicon that will offer an aggressive core count and performance per watt for particular use cases at the edge, for fanless designs. We were able to confirm that Ampere is building a monolithic die with 80 cores, so the halo chip will be a fully enabled part, with other chips being cut down versions of this. Ampere wouldn’t commit to any smaller silicon designs at this stage, stating that their customers are mostly requesting high-core count and high-performance parts.Ampere’s Jeff Wittich, SVP of Products and ex-Intel, did state that the company is very thorough in its silicon design work, stating that he’s never seen so much pre-silicon design work for a product before. He cites an extensive amount of emulation for QuickSilver, especially when it comes to features of the SoC, and the company also does a lot of test chip designs as well to make sure what comes out at the right time ends up being what the company needs in a product with as few stepping adjustments as possible. I was surprised to hear this, given the relative size of Ampere and Wittich’s history, but given the recent praise on Graviton2 with its launch, QuickSilver does seem best poised to offer a direct competitor with higher-performance to other cloud providers in 2020.Arm-based Server CPU OfferingsAnandTechAmpereQuickSilverAmazonGraviton2MarvellThunderX2HuaweiKunpeng 920AmpereeMAGLaunchedQ1 2020Q4 2019Q2 2018Q3 2019Q3 2018Arm archµarchv8.2Neoverse N1v8.2Neoverse N1v8.1Vulkanv8.4TSV110v8.0SkylarkCores8064326432NodeTSMC7nmTSMC7nmTSMC16nmTSMC7nmTSMC16nmFreq?3.1 GHz2.5 GHz2.6 GHz3.3 GHzTDP200W+100W ??180 W125 WMemory8xDDR4-3200?8xDDR4-32008xDDR4-26668xDDR4-29338xDDR4-2666PCIe>128 4.0?56x 3.040x 4.032x 3.0CCIXYes?-Yes-MultiSocket2?241Ampere’s Future and RoadmapWe were able to also ask about Ampere’s future in our briefing. It’s no secret that questions are being asked as to Ampere’s future, having done two rounds of funding but not making a serious dent in the Arm server space while also being suspiciously quiet. We were told that ultimately Ampere has had its head down recently driving the new Next-Gen Ampere product, hence the relative quietness, even whenworkstation productscame to market but not a peep was heard from the company.As part of our discussions, Ampere did state that it is very secure in its funding. We were told that Ampere is working on an annual cadence with its processor portfolio, with products coming out in 2020, 2021, and 2022. High-volume manufacturing with Quicksilver will start at the end of Q1/Q2 2020, and the company actually has the next two generations of hardware completely funded. This allows the company to be candid with old and new customers as to where the product is going, but also allows them to work with customers to examine future workloads and determine the requirements for future products.Ampere's RoadmapAmpereeMagAmpereNext-GenNG+1NG+2ShippedSamplingIn DevelopmentDefined16nm7nm7nm5nmSkylarkQuickSilverQS+1QS+2ARM v8.0ARM v8.2Up to 32 CoresUp to 80 Cores8 x DDR4-26678 x DDR4-3200?42x PCIe 3.0>128 PCIe 4.075 W to 125 W45 W to 200+ W3.3 GHz Turbo? FrequencyMore IOCCIX AttachDual SocketImproved IPCAs to future products, Ampere did state that as they are now using LGA socketed processors for QuickSilver, then the next generation after this (which I’ll call QuickSilver+1) will be socket compatible and offer drop in support. Ampere sees this as a benefit for a quicker time-to-market, which is true, but also means that we should expect parity with memory and PCIe support for two generations, which is often welcomed in the server space. The generation after this, QuickSilver+2, is funded and is already in the definition phase. Beyond this Ampere is doing research and pathfinding, but based on the need to be Agile in an aggressive market space, Ampere doesn’t feel the need to start defining specifications in stone for products 3+ years away just yet.From an AnandTech point of view, we’re glad that Ampere reached out to us to talk about this so far in advance of the official launch in Q1: it looks like only a couple of other media were offered briefings. We’ve seenworkstation versions of eMAGhit the market, so we’re hoping to be sampled one of those ahead of the launch of the Next-Gen Product, if only to act as a reference point for performance claims. Then hopefully we can put it against the other competition in the market. Arm servers just got interesting.The carousel image for this article is the current eMAG product as part of the Packet bare-metal cloud offeringRelated ReadingArm Server CPUs: You Can Now Buy Ampere’s eMAG in a WorkstationAmpere Computing: Arm is Now an InvestorAmpere eMAG in the Cloud: 32 Arm Core Instance for $1/hrArm Announces Neoverse N1 & E1 Platforms & CPUs: Enabling A Huge Jump In Infrastructure PerformanceAmazon Announces Graviton2 SoC Along With New AWS Instances: 64-Core Arm With Large Performance Uplifts\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15253/80core-n1-nextgen-ampere-quicksilver-the-antigraviton2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: NVIDIA Details DRIVE AGX Orin: A Herculean Arm Automotive SoC For 2022\n",
      "Author: Ryan Smith\n",
      "Date Published: 2019-12-18T13:30:00Z\n",
      "URL: https://www.anandtech.com/show/15245/nvidia-details-drive-agx-orin-a-herculean-arm-automotive-soc-for-2022\n",
      "Content: While NVIDIA’s SoC efforts haven’t gone entirely to plan since the company first started on them over a decade ago, NVIDIA has been able to find a niche that works in the automotive field. Backing the company’s powerful DRIVE hardware, these SoCs have become increasingly specialized as the DRIVE platform itself evolves to meet the needs of the slowly maturing market for the brains behind self-driving cars. And now, NVIDIA’s family of automotive SoCs is growing once again, with the formal unveiling of the Orin SoC.First outlined as part of NVIDIA’s DRIVE roadmapat GTC 2018, NVIDIA CEO Jensen Huang took the stage at GTC China this morning to properly introduce the chip that will be powering the next generation of the DRIVE platform. Officially dubbed the NVIDIA DRIVE AGX Orin, the new chip will eventually succeed NVIDIA’s currently shipping Xavier SoC, which has been available for about the last year now. In fact, as has been the case with previous NVIDIA DRIVE unveils, NVIDIA is announcing the chip well in advance: the company isn't expecting the chip to be fully ready for automakers until 2022.What lies beneath Orin then is a lot of hardware, with NVIDIA going into some high-level details on certain parts, but skimming over others. Overall, Orin is a 17 billion transistor chip, almost double the transistor count of Xavier and continuing the trend of very large, very powerful automotive SoCs. NVIDIA is not disclosing the manufacturing process being used at this time, but given their timeframe, some sort of 7nm or 5nm process (or derivative) is pretty much a given. And NVIDIA will definitely need a smaller manufacturing process – to put things in comparison, the company’s top-end Turing GPU, TU102, takes up 754mm2 for 18.6B transistors, so Orin will pack in almost as many transistors as one of NVIDIA’s best GPUs today.NVIDIA ARM SoC Specification ComparisonOrinXavierParkerCPU Cores12x Arm \"Hercules\"8x NVIDIA Custom ARM \"Carmel\"2x NVIDIA Denver +4x Arm Cortex-A57GPU Cores\"Next-Generation\" NVIDIA iGPUXavier Volta iGPU(512 CUDA Cores)Parker Pascal iGPU(256 CUDA Cores)INT8 DL TOPS200 TOPS30 TOPSN/AFP32 TFLOPS?1.3 TFLOPs0.7 TFLOPsManufacturing Process7nm?TSMC 12nm FFNTSMC 16nm FinFETTDP~65-70W?30W15WThose transistors, in turn, will be driving several elements. Surprisingly, for today’s announcement NVIDIA has confirmed what CPU core they’ll be using. And even more surprisingly, it isn’t theirs. After flirting with both Arm and NVIDIA-designed CPU cores for several years now, NVIDIA has seemingly settled down with Arm. Orin will include a dozen of Arm’s upcomingHerculesCPU cores, which are from Arm’s client device line of CPU cores. Hercules, in turn, succeeds today’s Cortex-A77 CPU cores, with customers recently receiving the first IP for the core. For the moment we have very little information on Hercules itself, but Arm has previously disclosed that it will be a further refinement of the A76/A77 cores.I won’t spend too much time dwelling on NVIDIA’s decision to go with Arm’s Cortex-A cores after using their own CPU cores for their last couple of SoCs, but it’s consistent with the direction we’ve seen most of Arm’s other high-end customers take. Developing a fast, high-performance CPU core only gets harder and harder every generation. And with Arm taking a serious stab at the subject, there’s a lot of sense in backing Arm’s efforts by licensing their cores as opposed to investing even more money in further improving NVIDIA’s Project Denver-based designs. It does remove one area where NVIDIA could make a unique offering, but on the flip side it does mean they can focus more on their GPU and accelerator efforts.Speaking of GPUs, Jensen revealed very little about the GPU technology that Orin will integrate. Besides confirming that it’s a “next generation” architecture that offers all of the CUDA core and tensor functionality that NVIDIA has become known for, nothing else was stated. This isn’t wholly surprising since NVIDIA hasn’t disclosed anything about their forthcoming GPU architectures – we haven’t seen a roadmap there in a while – but it means the GPU side is a bit of a blank slate. Given the large gap between now and Orin’s launch, it’s not even clear if the architecture will be NVIDIA’s next immediate GPU architecture or the one after that, however given how Xavier’s development went and the extensive validation required for automotive, NVIDIA’s 2020(ish) GPU architecture seems like a safe bet.Meanwhile NVIDIA’s Deep Learning Accelerator (DLA) blocks will also be making a return. These blocks don’t get too much attention since they’re unique to NVIDIA’s DRIVE SoCs, but these are hardware blocks to further offload neural network inference, above and beyond what NVIDIA’s tensor cores already do. On the programmable/fixed-function scale they’re closer to the latter, with the task-specific hardware being a good fit for the power and energy-efficiency needs NVIDIA is shooting for.All told, NVIDIA expects Orin to deliver 7x the 30 INT8 TOPS performance of Xavier, with the combination of the GPU and DLA pushing 200 TOPS. It goes without saying that NVIDIA is still heavily invested in neural networks as the solution to self-driving systems, so they are similarly heavily investing in hardware to execute those neural nets.Rounding out the Orin package, NVIDIA’s announcement also confirms that the chip will offer plenty of hardware for supporting features. The chip will offer 4x 10 Gigabit Ethernet hosts for sensors and in-vehicle communication, and while the company hasn’t disclosed how many camera inputs the SoC can field, it will offer 4Kp60 video stream encoding and 8Kp30 decoding for H.264/HEVC/VP9. The company has also set a goal for 200GB/sec of memory bandwidth. Given the timeframe for Orin and what NVIDIA does for Xavier today, an 256-bit memory bus with LPDDR5 support sounds like a shoe-in, but of course this remains to be confirmed.Finally, while NVIDIA hasn’t disclosed any official figures for power consumption, it’s clear that overall power usage is going up relative to Xavier. While Orin is expected to be 7x faster than Xavier, NVIDIA is only claiming it’s 3x as power efficient. Assuming NVIDIA is basing all of this on INT8 TOPS as they usually do, then the 1 TOPS/Watt Xavier would be replaced by the 3 TOPS/Watt Orin, putting the 200 TOPS chip at around 65-70 Watts. Which is admittedly still fairly low for a single chip at a company that sells 400 Watt GPUs, but it could add up if NVIDIA builds another multi-processor board like theDRIVE Pegasus.Overall, NVIDIA certainly has some lofty expectations for Orin. Like Xavier before it, NVIDIA intends for various forms of Orin to power everything from level 2 autonomous cars right up to full self-driving level 5 systems. And, of course, it will do so while being able to provide the necessary ASIL-D level system integrity that will be expected for self-driving cars.But as always, NVIDIA is far from the only silicon vendor with such lofty goals. The company will be competing with a number of other companies all providing their own silicon for self-driving cars – ranging from start-ups to the likes of Intel – and while Orin will be a big step forward in single-chip performance for the company, it’s still very much the early days for the market as a whole. So NVIDIA has their work cut out for them across hardware, software, and customer relations.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15245/nvidia-details-drive-agx-orin-a-herculean-arm-automotive-soc-for-2022\n",
      "Title: The Snapdragon 865 Performance Preview: Setting the Stage for Flagship Android 2020\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2019-12-16T12:30:00Z\n",
      "URL: https://www.anandtech.com/show/15207/the-snapdragon-865-performance-preview-setting-the-stage-for-flagship-android-2020\n",
      "Content: Earlier this month we had the pleasure to attend Qualcomm’s Maui launch event of the new Snapdragon 865 and 765 mobile platforms. The new chipsets promise to bring a lot of new upgrades in terms of performance and features, and undoubtedly will be the silicon upon which the vast majority of 2020 flagship devices will base their designs on. We’ve covered the new improvements and changes of the new chipset inour dedicated launch article, so be sure to read that piece if you’re not yet familiar with the Snapdragon 865.As has seemingly become a tradition with Qualcomm, following the launch event we’ve been given the opportunity to have some hands-on time with the company’s reference devices, and had the chance to run the phones through our benchmark suite. The QRD865 is a reference phone made by Qualcomm and integrates the new flagship chip. The device offers insight into what we should be expecting from commercial devices in 2020, and today’s piece particularly focuses on the performance improvements of the new generation.Qualcomm Announces Snapdragon 865 and 765(G): 5G For All in 2020, All The DetailsQualcomm Windows on Snapdragon: New 7c & 8c SoCs for sub-$800 LaptopsQuick Bytes: Qualcomm’s Dynamic Spectrum Sharing Demo with 5G and 4GQuick Bytes: Qualcomm’s Prediction of 1.4 Billion 5G Smartphones by 2022Qualcomm Snapdragon Tech Summit Live Blog: Day OneQualcomm Snapdragon Tech Summit Live Blog Day Two: All About MobileQualcomm Snapdragon Tech Summit Day 3 Live Blog: ACPC and XRA quick recap of the Snapdragon 865 if you haven’t read the more thorough examination of the changes:Qualcomm Snapdragon Flagship SoCs 2019-2020SoCSnapdragon 865Snapdragon 855CPU1x CortexA77@ 2.84GHz1x512KB pL23x CortexA77@ 2.42GHz3x256KB pL24x Cortex A55@ 1.80GHz 4x128KB pL24MB sL3 @?MHz1x Kryo 485 Gold (A76 derivative)@ 2.84GHz 1x512KB pL23x Kryo 485 Gold (A76 derivative)@ 2.42GHz 3x256KB pL24x Kryo 485 Silver (A55 derivative)@ 1.80GHz 4x128KB pL22MB sL3 @ 1612MHzGPUAdreno 650@ 587 MHz+25% perf+50% ALUs+50% pixel/clock+0% texels/clockAdreno 640 @ 585 MHzDSP / NPUHexagon 69815 TOPSAI(Total CPU+GPU+HVX+Tensor)Hexagon 6907 TOPS AI(Total CPU+GPU+HVX+Tensor)MemoryController4x 16-bit CH@ 2133MHz LPDDR4X / 33.4GB/sor@2750MHz LPDDR5/ 44.0GB/s3MB system level cache4x 16-bit CH@ 1866MHz LPDDR4X 29.9GB/s3MB system level cacheISP/CameraDual 14-bit Spectra 480 ISP1x 200MP64MP ZSLor2x 25MP ZSL4K video & 64MPburst captureDual 14-bit Spectra 380 ISP1x 192MP1x 48MP ZSL or 2x 22MP ZSLEncode/Decode8K30 / 4K12010-bit H.265Dolby Vision, HDR10+, HDR10, HLG720p960 infinite recording4K60 10-bit H.265HDR10, HDR10+, HLG720p480Integrated Modemnone(Paired withexternal X55only)(LTE Category 24/22)DL = 2500 Mbps7x20MHz CA, 1024-QAMUL = 316 Mbps3x20MHz CA, 256-QAM(5G NR Sub-6 + mmWave)DL = 7000 MbpsUL = 3000 MbpsSnapdragon X24 LTE(Category 20)DL = 2000Mbps7x20MHz CA, 256-QAM, 4x4UL = 316Mbps3x20MHz CA, 256-QAMMfc. ProcessTSMC7nm (N7P)TSMC7nm (N7)The Snapdragon 865 is a successor to the Snapdragon 855 last year, and thus represents Qualcomm’s latest flagship chipset offering the newest IP and technologies. On the CPU side, Qualcomm has integrated Arm’s newest Cortex-A77 CPU cores, replacing the A76-based IP from last year. This year Qualcomm has decided against requesting any microarchitectural changes to the IP, so unlike the semi-custom Kryo 485 / A76-based CPUs which had some differing aspects to the design, the new A77 in the Snapdragon 865 represents the default IP configuration that Arm offers.Clock frequencies and core cache configurations haven’t changed this year – there’s still a single “Prime” A77 CPU core with 512KB cache running at a higher 2.84GHz and three “Performance” or “Gold” cores with reduced 256KB caches at a lower 2.42GHz. The four little cores remain A55s, and also the same cache configuration as well as the 1.8GHz clock. The L3 cache of the CPU cluster has been doubled from 2 to 4MB. In general, Qualcomm’s advertised 25% performance uplift on the CPU side solely comes from the IPC increases of the new A77 cores.The GPU this year features an updates Adreno 650 design which increases ALU and pixel rendering units by 50%. The end-result in terms of performance is a promised 25% upgrade – it’s likely that the company is running the new block at a lower frequency than what we’ve seen on the Snapdragon 855, although we won’t be able to confirm this until we have access to commercial devices early next year.A big performance upgrade on the new chip is the quadrupling of the processing power of the new Tensor cores in the Hexagon 698. Qualcomm advertises 15 TOPS throughput for all computing blocks on the SoC and we estimate that the new Tensor cores roughly represent 10 TOPS out of that figure.In general, the Snapdragon 865 promises to be a very versatile chip and comes with a lot of new improvements – particularly 5G connectivity and new camera capabilities are promised to be the key features of the new SoC. Today’s focus lies solely on the performance of the chip, so let’s move on to our first test results and analysis.New Memory Controllers & LPDDR5: A Big ImprovementOne of the larger changes in the SoC this generation was the integration of a new hybrid LPDDR5 and LPDDR4X memory controller. On the QRD865 device we’ve tested the chip was naturally equipped with the new LP5 standard. Qualcomm was actually downplaying the importance of LP5 itself: the new standard does bring higher memory speeds providing better bandwidth, however latency should be the same, and power efficiency benefits, while there, shouldn’t be overplayed. Nevertheless, Qualcomm did claim they focused more on improving their memory controllers, and this year we’re finally seeing the new chip address some of the weaknesses exhibited by the past two generations; memory latency.We had criticised Qualcomm’s Snapdragon 845 and 855 for having quite bad memory latency – ever since the company had introduced their system level cache architecture to the designs, this aspect of the memory subsystem had seen some rather mediocre characteristics. There’s been a lot of arguments in regards to how much this actually affected performance, with Qualcomm themselves naturally downplaying the differences. Arm generally notes a 1% performance difference for each 5ns of latency to DRAM, if the differences are big, it can sum up to a noticeable difference.()Looking at the new Snapdragon 865, the first thing that pops up when comparing the two latency charts is the doubled L3 cache of the new chip. It’s to be noted that it does look that there’s still some sort of logical partitioning going on and 512KB of the cache may be dedicated to the little cores, as random-access latencies start going up at 1.5MB for the S855 and 3.5MB for the S865.Further down in the deeper memory regions, we’re seeing some very big changes in latency. Qualcomm has been able to shave off around 35ns in the full random-access test, and we’re estimating that the structural latency of the chip now falls in at ~109ns – a 20ns improvements over its predecessor. While it’s a very good improvements in itself, it’s still a slightly behind the designs of HiSilicon, Apple and Samsung. So, while Qualcomm still is the last of the bunch in regards to its memory subsystem, it’s no longer trailing behind by such a large margin. Keep in mind the results of the Kirin 990 here as we go into more detailed analysis of memory-intensive workloads in SPEC on the next page.Furthermore, what’s very interesting about Qualcomm’s results in the DRAM region is the behaviour of the TLB+CLR Trash test. This test is always hitting the same cache-line within a page across different, forcing a cache line replacement. The oddity here is that the Snapdragon 865 here behaves very differently to the 855, with the results showcasing a separate “step” in the results between 4MB and ~32MB. This result is more of an artefact of the test only hitting a single cache line per page rather than the chip actually having some sort of 32MB hidden cache. My theory is that Qualcomm has done some sort of optimisation to the cache-line replacement policy at the memory controller level, and instead the test hitting DRAM, it’s actually residing at on the SLC cache. It’s a very interesting result and so far, it’s the first and only chipset to exhibit such behaviour. If it’s indeed the SLC, the latency would fall in at around 25-35ns, with the non-uniform latency likely being a result of the four cache slices dedicated to the four memory controllers.Overall, it looks like Qualcomm has made rather big changes to the memory subsystem this year, and we’re looking forward to see the impact on performance.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15207/the-snapdragon-865-performance-preview-setting-the-stage-for-flagship-android-2020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Early TSMC 5nm Test Chip Yields 80%, HVM Coming in H1 2020\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-12-12T00:05:00Z\n",
      "URL: https://www.anandtech.com/show/15219/early-tsmc-5nm-test-chip-yields-80-hvm-coming-in-h1-2020\n",
      "Content: Today at the IEEE IEDM Conference, TSMC is presenting a paper giving an overview of the initial results it has achieved on its 5nm process. This process is going to be the next step for any customer currently on the N7 or N7P processes as it shares a number design rules between the two. The new N5 process is set to offer a full node increase over the 7nm variants, and uses EUV technology extensively over 10+ layers, reducing the total steps in production over 7nm. The new 5nm process also implements TSMC’s next generation (5th gen) of FinFET technology.The Headline NumbersIf you’re only here to read the key numbers, then here they are. In the disclosure, TSMC is stating that their 5nm EUV process affords an overall with a ~1.84x logic density increase, a 15% power gain, or a 30% power reduction. The current test chip, with 256 Mb of SRAM and some logic, is yielding 80% on average and 90%+ in peak, although scaled back to the size of a modern mobile chip, the yield is a lot lower. The technology is currently in risk production, with high volume production scheduled for the first half of 2020. This means that chips built on 5nm should be ready in the latter half of 2020.FromWikiChipTSMC's 7nm process currently yields just shy of 100 million transistors per square millimeter (mTr/mm2) when using dense libraries, about 96.27 mTr/mm2. This means that the new 5nm process should be around 177.14 mTr/mm2.The Details on YieldAs part of any risk production, a foundry produces a number of test chips in order to verify that the process is working expected. For 5nm, TSMC is disclosing two such chips: one built on SRAM, and other combing SRAM, logic, and IO.For the SRAM chip, TSMC is demonstrating that it has both high current (HC) and high density (HD) SRAM cells, at a size of 25000 nm2and 21000 nm2respectively. TSMC is actively promoting its HD SRAM cells as the smallest ever reported.For the combined chip, TSMC is stating that the chip consists of 30% SRAM, 60% Logic (CPU/GPU), and 10% IO. In that chip are 256 mega-bits of SRAM, which means we can calculate a size. A 256 Mbit SRAM cell, at 21000 nm2, gives a die area of 5.376 mm2. TSMC states that this chip does not include self-repair circuitry, which means we don’t need to add extra transistors to enable that. If the SRAM is 30% of the chip, then the whole chip should be around 17.92 mm2.For this chip, TSMC has published an average yield of ~80%, with a peak yield per wafer of >90%. Knowing the yield and the die size, we can go toa common online wafer-per-die calculatorto extrapolate the defect rate. To make things simple, we assume the chip is square, we can adjust the defect rate in order to equal a yield of 80%. Using the calculator, a 300 mm wafer with a 17.92 mm2die would produce 3252 dies per wafer. An 80% yield would mean 2602 good dies per wafer, and this corresponds to a defect rate of 1.271 per sq cm.So, a 17.92 mm2die isn’t particularly indicative of a modern chip on a high performance process. The first chips on a new process are often mobile processors, especially high-performance mobile processors that can amortize the high cost of moving into a new process. These chips have been increasing in size in recent years, depending on the modem support. For example, the Kirin 990 5G built on 7nm EUV is over 100 mm2, closer to 110 mm2. One could point to AMD’s Zen 2 chiplet as more applicable chip, given it comes from a non-EUV process which is more amenable to moving to 5nm EUV, however something like this will come later and will use high performance libraries to not be as dense.In that case, let us take the 100 mm2die as an example of the first mobile processors coming out of TSMC’s process. Again, taking the die as square, a defect rate of 1.271 per cm2would afford a yield of 32.0%. This is pretty good for a process in the middle of risk production. A yield rate of 32.0% for a 100 mm2chip would even be sufficient for some early adopters wanting to get ahead of the game.(For anyone wanting to compare this defect density to the size of Zen 2 chiplet at 10.35x7.37mm, that equates to 41.0% yield.)(Note initially when I read it the first time, I saw this only in the context of the 5.376 mm2SRAM-only die. Doing the math, that would have afforded a defect rate of 4.26, or a 100mm2yield of 5.40%. This is very low. The paper is a little ambiguous as to which test chip the yields are referring to, hence my initial concern atonlya 5.4% yield. In conversing with David Schor from Wikichip, he says that even the 32.0% yield for 100 mm2calculated is a little low for risk production, unless you’re happy taking a lot of risk.)TSMC’s Test Chip: CPU and GPU FrequencyOf course, a test chip yielding could mean anything. A successful chip could just ‘turn on’, and the defect rate doesn’t take into account how well the process can drive power and frequency. As part of the disclosure, TSMC also gave some ‘shmoo’ plots of voltage against frequency for their example test chip.For CPU, the plot shows a frequency of 1.5 GHz at 0.7 volts, all the way up to 3.25 GHz at 1.2 volts.For GPU, the plot shows a frequency of 0.66 GHz at 0.65 volts, all the way up to 1.43 GHz at 1.2 volts.One could argue that these aren’t particularly useful: the designs of CPUs and GPUs are very different and a deeply integrated GPU could get a much lower frequency at the same voltage based on its design. Unfortunately TSMC doesn’t disclose what they use as an example CPU/GPU, although the CPU part is usually expected to be an Arm core (although it might only be a single core on a chip this size). It often depends on who the lead partner is for the process node.IO Demonstration: PAM4One of the key elements in future chips is the ability to support multiple communication technologies, and in the test chip TSMC also included a transceiver designed to enable high-speed PAM-4.We have already seen 112 Gb/s transceivers on other processes, and TSMC was able to do 112 Gb/s here with a 0.76 pJ/bit energy efficiency. Pushing the bandwidth further, TSMC was able to get 130 Gb/s still within tolerances in the eye diagram, but at a 0.96 pJ/bit efficiency. This bodes well for any PAM-4 based technologies, such as PCIe 6.0.Using EUV: A Reduction in Mask CountHaving spent a number of processes built upon 193nm-based ArF immersion lithography, the mask count for these more and more complex processors has been ballooning. What used to be 30-40 masks on 28 nm is now going above 70 masks on 14nm/10nm, with reports that some leading edge process technologies are already above 100 masks. With this paper, TSMC is saying that extensive use of EUV for over 10 layers of the design will actually, for the first time, reduce the number of process masks with a new process node.The benefit of EUV is the ability to replace four or five standard non-EUV masking steps with one EUV step. This comes down to the greater definition provided at the silicon level by the EUV technology. The flip side is that the throughput of a single EUV machine (175 wafers per hour per mask) is much slower than a non-EUV machine (300 wafers per hour per mask), however the EUV’s speed should be multiplied by 4-5 to get a comparison throughput. TSMC’s extensive use, one should argue, would reduce the mask count significantly. Ultimately it’s only a small drop.If we assume around 60 masks for the 16FFC process, the 10FF process is around 80-85 masks, and 7FF is more 90-95. With 5FF and EUV, that number goes back down to the 75-80 number, compared to the 110+ that it might have been without EUV. This simplifies things, assuming there are enough EUV machines to go around. Recent reports state that ASML is behind in shipping its 2019 orders, and plans to build another 25-27 in 2020 with demand for at least 50 machines.Transistor Types at 5nmPart of the IEDM paper describes seven different types of transistor for customers to use. We’ve already mentioned the new types, eVT at the high end and SVT-LL at the low end, however here are a range of options to be used depending on the leakage and performance required.The three main types are uLVT, LVT and SVT, which all three have low leakage (LL) variants. Then eLVT sits on the top, with quite a big jump from uLVT to eLVT.The Effect of Design-Technology Co-Optimization (DTCO)One of the features becoming very apparent this year at IEDM is the use of DTCO. In a nutshell, DTCO is essentially one arm of process optimization that occurs as a result of chip design – i.e. it can be very easy to design a holistic chip and put it onto silicon, but in order to get the best performance/power/area, it needs to be optimized for the process node for the silicon in question. The effects of this co-optimization can be dramatic: the equivalent of another process node jump in PPA is not something to be sniffed at, and it also means that it takes time to implement.One downside to DTCO is that when applied to a given process or design, it means that any first generation of a future process node is technically worse than the holistic best version of the previous generation, or at best, on parity, but a lot more expensive. So in order to better the previous process technology, at least one generation of DTCO has to be applied to the new node before it can even be made viable, making its roll-out take even longer.This is a persistent artefact of the world we now live in. Intel, TSMC, and to a certain extent Samsung, have to apply some form of DTCO to every new process (and every process variant) for specific products. For TSMC at least, certain companies may benefit from exclusive rights to certain DTCO improvements, to help those companies get additional performance benefits. But the fact that DTCO is needed just to draw parity means that we’re getting a further elongation of process node announcements: if it doesn’t come with a form of DTCO, it’s not worth announcing as no-body will want it.Thankfully in TSMC’s 5nm paper at IEDM, the topic of DTCO is directly addressed. The 5nm test chip has an element of DTCO applied, rather than brute-forcing the design rules, which has enabled scaling of the design rules for an overall 40% chip size reduction. So that overall test chip, at 17.92 mm2, would have been more like 25.1 mm2, with a yield of 73%, rather than 80%. It doesn’t sound like much, but in this case every little helps: with this element of DTCO, it enables TSMC to quote the 1.84x increase in density for 15+% speed increase/30% power reduction.Unfortunately, we don't have the re-publishing rights for the full paper. For those that have access to IEDM papers, search for36.7 5nm CMOS Production Technology Platform featuring full-fledged EUV, and High Mobility Channel FinFETs with Densest 0.021 µm2 SRAM Cells for Mobile SoC and High Performance Computing Applications, IEEE IEDM 2019One obvious data point that TSMC hasn't disclosed is the exact details on its fin pitch sizes, or contacted poly pitch (cpp), which are often quoted when disclosing risk production of new process nodes. We're hoping TSMC publishes this data in due course.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15219/early-tsmc-5nm-test-chip-yields-80-hvm-coming-in-h1-2020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Arm Server CPUs: You Can Now Buy Ampere’s eMAG in a Workstation\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-12-06T17:00:00Z\n",
      "URL: https://www.anandtech.com/show/15165/arm-server-cpus-you-can-now-buy-amperes-emag-in-a-workstation\n",
      "Content: One of the critical elements to all these new server-class Arm processors is availability. We are not yet at the point where these chips are freely sold on the open market: anyone who wants to use them needs to buy a server (or a rack of servers), or rent a cloud instance. One of the benefits of x86 in this space is that users can write code for x86 servers on other easily accessible hardware, then port it up to the big server iron. Well now it seems that one of the Arm licencees playing in the server space has a workstation based product in the hands of distributors ready for software developers to cut their teeth on the hardware.Over at Avantek, the Ampere eMAG 64-bit Arm Workstation is a single socket workstation design offered in an XL-ATX chassis with up to 512 GB of DDR4-2666 as well as an NVMe drive and some SATA ports. There are onboard video outputs from the IPMI interface, or a PCIe 3.0 x8 expansion slot could add in something else (assuming drivers are available).The workstation is only offered with a single CPU SKU, the eMAG 8180. This isn’t to be confused with Intel’s 8180: this one has more cores! The eMAG 8180 is a 32-core design running at 2.8 GHz with a turbo up to 3.0 GHz, with a TDP of 125 W. This is a first generation eMAG, which uses the old AppliedMicro Skylark microarchitecture, a custom design of Arm v8 with 32 MB of L3, 42 PCIe lanes, and eight memory channels.Official eMAG 8180 specifications - note the frequency here is higher.It looks like the workstation has decreased clocksAvantek offers the system with three optional graphics cards: AMD FirePro W2100, a Radeon Pro WX 5100, and the NVIDIA Quadro GV100. OS options are variants of Linux: Ubuntu, CentOS, SUSE SLES, and openSUSE.The base configuration requires the user to select at least a 240 GB SSD and 1x8GB of DRAM, which means a super low (!) price of $2,794.50 for the base model. Users who want a chassis with a window and LED lighting will need to shell out an extra $108, because it isn’t a proper workstation with LEDs, right?! A more sensible configuration with the W2100, 64 GB of DRAM, and 4x256GB of SSDs, comes to $4044.60. Validated purchasers can leave a review – so far none have been left. Perhaps we should ask for one for review.Source:AvantekRelated ReadingAmpere Computing: Arm is Now an InvestorAmpere eMAG in the Cloud: 32 Arm Core Instance for $1/hrArm Announces Neoverse N1 & E1 Platforms & CPUs: Enabling A Huge Jump In Infrastructure Performance\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15165/arm-server-cpus-you-can-now-buy-amperes-emag-in-a-workstation\n",
      "Title: Qualcomm Snapdragon Tech Summit Day 3 Live Blog: ACPC and XR\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-12-05T18:08:00Z\n",
      "URL: https://www.anandtech.com/show/15202/qualcomm-snapdragon-tech-summit-day-3-live-blog-acpc-and-xr\n",
      "Content: 01:13PM EST- The annual Qualcomm Snapdragon Tech Summit is in its final day, and the main focal points for discussion are the Windows on Snapdragon-based Always Connected PCs, with updates to the 8cx family of processors, and the into augmented reality and virtual reality with Qualcomm's efforts in XR being expanded. We're here ready to report, with the presentation set to start at 2pm ET / 7pm UTC.01:57PM EST- We're just getting seated, a few minutes to start02:05PM EST- Here we go: SVP/GM Handset products Jim Tran to the stage02:06PM EST- 820k views on QC yesterday02:07PM EST- (note, that wasn't on the YT stream, which has only 3200 - they've hired influencers to manage their social accounts for the week)02:07PM EST- Video time02:08PM EST- VP of XR, Hugo Swart to the stage02:08PM EST- How many of you have experienced a poor conference call?02:09PM EST- XR, or extended reality, can be better than reality02:09PM EST- especially for conference calls02:09PM EST- We have news that will change how we socialise that will combine XR, AI, and 5G02:09PM EST- The digital transformation has begun02:10PM EST- Everyone and everything will be connected02:10PM EST- XR = VR, AR, MR02:10PM EST- XR is the next mobile platform02:10PM EST- Hands free, immersive interactions02:10PM EST- (isn't that just.... reality?)02:11PM EST- Taking optimizations for XR and moving it to the main SoC02:11PM EST- Building reference designs02:11PM EST- enabling OEMs to come to market02:11PM EST- Make VR really mobile02:11PM EST- Oculus Go, Quest, Vive, many others in China02:11PM EST- Transported into a virtual world02:12PM EST- Kids call VR 'the new world'02:12PM EST- Qualcomm is creating a new world02:12PM EST- digital elements overlaid with the real world02:13PM EST- Use cases for industry to improve efficiency02:13PM EST- Snapdragon powers the top XR platforms in the world02:13PM EST- 30+ devices with Snapdragon02:13PM EST- Don't intend to stop02:13PM EST- POtential to be ubiquitous and everywher02:14PM EST- Every new Walmart employee is now trained in VR02:14PM EST- There's a demo in the demo room later02:14PM EST- How to face an angry customer02:14PM EST- also industrial logistics02:14PM EST- Tourism, retail02:15PM EST- Enabled by different device form factors02:15PM EST- Now we have standalone headsets02:15PM EST- Truly free and mobile02:15PM EST- QC created this category with partners02:15PM EST- It's here for the long term02:16PM EST- The next category, especially for 5G, is XR viewers02:16PM EST- processing is split between the glasses and a device in your pocket02:16PM EST- QC now has customers launching XR viewers02:17PM EST- Experiencing immersive video and games on the train02:17PM EST- Now Boundless XR02:17PM EST- Distributed processing: headset with nearby PC with 60 GHz connectivity02:17PM EST- Wi-Gig02:18PM EST- split processing02:18PM EST- tracking in the headset, heavy lifting on the PC02:18PM EST- announced at GDC, customers launching now02:18PM EST- Boundless XR with 5G as well02:19PM EST- Infrastructure and cloud via 5G does the heavy lifting02:19PM EST- Four new form factors: Standalone, Viewers, Boundless for PC, Boundless for 5G02:19PM EST- Market segments: Consumer, Enterprise, Operator02:20PM EST- Andrea Hogan to the stage, Senior Director of Partner Marketing02:21PM EST- 'We have had innovative announcements'02:21PM EST- 20 minutes of a partner showcase02:21PM EST- push the boundaries of the technology02:21PM EST- QC breaks through and the ecosystem leaps forward02:22PM EST- three new korean operators announced recently as partners02:22PM EST- 'XR is going to change the way we interact with the world'02:22PM EST- such as with gaming02:22PM EST- XR has been tied to gaming02:23PM EST- Consumers enjoy new game worlds02:23PM EST- Partnering with Unity02:23PM EST- Unity empowers everyone02:23PM EST- 34 billion installs of unity games in the last year02:23PM EST- Unity to the stage02:25PM EST- AR music experiences made with Unity02:25PM EST- Beat Saber02:25PM EST- shifts in human computer interactions02:26PM EST- low powered computing devices to ingest real world data02:26PM EST- moving from the cloud to the PC to the new form factors02:26PM EST- Devices need to interact and react02:27PM EST- Using your phone to interact with people02:28PM EST- Using XR glasses to check email and synced with the cloud02:28PM EST- Unity can build these spacial applications02:28PM EST- Unity is building the tools02:28PM EST- Unity AR Foundation02:29PM EST- Unity Project Tiny initiative - tool for XR developers to shrink apps02:29PM EST- Mixed Reality Studio (MARS) to build multi-device experiences with real-world data02:30PM EST- QC back to the stage02:30PM EST- Unity is the ultimate development platform for XR02:30PM EST- Now Enterprise XR02:30PM EST- A connected workforce02:31PM EST- Mitchell International to the stage, Automotive Physical Damage EVP02:32PM EST- Restoration of vehicles post accident02:33PM EST- connecting owners and repairers and insurers02:33PM EST- Also medical claims02:36PM EST- More change in automotive in the next 5 years than the last 5002:37PM EST- Sensors need to be correct and work fine when damaged02:37PM EST- A car mechanic is now more like a software engineer02:37PM EST- Using augmented reality for years in this market02:38PM EST- Toyota measuring paint thickness in XR02:38PM EST- It took 2 people a day. Now one person takes 4 hours with XR assistance02:38PM EST- Two cameras to see behind a trailer02:39PM EST- When these are repaired, they have to be repaired properly02:40PM EST- Combining artificial intelligence and XR02:40PM EST- New XR for enterprise efficiency02:40PM EST- PoC for vehicle repair02:41PM EST- XR Glasses02:42PM EST- Detects vehicle, show cases technical instructions in the glasses. Verbally interacting with the glasses through voice recognition and commands. All hands free02:42PM EST- Access to experts02:43PM EST- Demo in the demo lounge later today02:43PM EST- This is the first step02:43PM EST- QC back to the stage02:44PM EST- Many innovative companies and institutions deliver at scale with XR solutions02:44PM EST- fuelling business growth02:44PM EST- Accenture XR to the stage02:45PM EST- Accenture XR is global02:45PM EST- Four pillars02:45PM EST- Maximize sales02:46PM EST- Key Clients and projects02:46PM EST- Connected Worker, Immersive Learning02:48PM EST- Virtual Merchandising02:50PM EST- More demos in Vegas02:50PM EST- QC to the stage02:50PM EST- Operators offering XR content02:51PM EST- KDDI to the stage02:51PM EST- 5G+XR for consumers02:52PM EST- Focusing on two main assets: Visual Position Service, and Smartglasses02:53PM EST- Social integration with XR02:55PM EST- smart glasses in stores from 1st November02:55PM EST- More plans for 202002:56PM EST- Operator offering for small and medium businesses02:56PM EST- Deutsche Telekom to the stage02:56PM EST- AR FieldAdvisor02:56PM EST- Remote field support02:56PM EST- Solving key XR challenges for DT engineers02:57PM EST- nreal AR Glasses02:57PM EST- Beta is ready02:58PM EST- field technicians plug the smart glasses into the phone. Uses spatial computing interface to call an expert, two way video call02:58PM EST- (no mention of lag)03:02PM EST- Journey to new realities03:03PM EST- In the next few years, get a single device from AR to VR, replacing all your screens03:03PM EST- The next step is here03:03PM EST- New platform dedicated to XR03:03PM EST- Enable new experiences03:03PM EST- Qualcomm XR2 with 5G03:04PM EST- 8K video, 3K per eye, true mixed reality03:04PM EST- 3d reconstruction, semantic segmentation03:04PM EST- Holographic telepresence03:04PM EST- XR social experiences03:04PM EST- XR assistant03:05PM EST- Combines XR, 5G, and AI in a single platform03:06PM EST- built for dual display, dual compute, optimizations and algorithm that a smartphone chip can't do03:06PM EST- XR2 sits above XR103:07PM EST- XR2 for premium, XR1 for mainstream03:07PM EST- Meet the different needs of XR users03:07PM EST- Reference design for XR203:07PM EST- HMD accelerator program03:08PM EST- specs for XR2 will be withheld for a few months03:08PM EST- Head of XR to the stage03:09PM EST- Understanding XR complexity03:10PM EST- QC is best at high perf and low power03:10PM EST- low latency too03:10PM EST- Fueling Premium Quality VR03:11PM EST- AR learning03:11PM EST- moving in the virtual world03:11PM EST- Visuals, audio, interactivity03:12PM EST- XR2 definition was built thinking about these pillars03:13PM EST- Performance against Snapdragon 835.03:14PM EST- Audio, visuals, interactions03:14PM EST- Gaming in XR is very different in consumer electronic devices03:14PM EST- Requires more shader processing, more texel rates, higher resolutions03:16PM EST- Mixing graphics and watching videos in VR03:16PM EST- 360-degree videos03:16PM EST- Support 360-video at 8K60 decode03:16PM EST- 120 Hz refresh rates, HDR10+03:16PM EST- custom silicon for reduced latency in AR03:17PM EST- High perf and low power is crucial03:18PM EST- Support AptX audio03:19PM EST- Voice UI03:19PM EST- e.g. 'say open door' in a game and it opens doors03:19PM EST- Say 'Press F' to pay respects03:19PM EST- Audio context detection03:20PM EST- eg, hear when the pizza guy comes to the door03:20PM EST- Multi-player communication03:23PM EST- 7 camera support on XR203:23PM EST- Customers require more cameras03:24PM EST- Map the environment to a virtual world with XR203:25PM EST- They all require world class CPU, GPU, Hexagon DSP, AI Engine03:25PM EST- Controllers and Hand Tracking03:26PM EST- Mixed Reality03:26PM EST- Wear a VR glass and see levels of immersion (virtual or real)03:26PM EST- video see through03:27PM EST- 5G for high bandwidth, low latency03:27PM EST- Premium XR everywhere03:28PM EST- VR movie in XR2 glasses in 8K 36003:28PM EST- Enables No wires and cables03:30PM EST- XR in 5G with Pokemon Go ?03:31PM EST- Niantic to the stage03:32PM EST- Pokemon Go Fest requires infrastructure03:33PM EST- Continue to evolve the Pokemon Go platform to develop new experiences03:33PM EST- Announcing multi-year collab with Qualcomm and Pokemon Go for consumers03:33PM EST- Using QC APIs and Niantic's real world APIs03:33PM EST- Starts with XR2 and cloud mapping features03:33PM EST- 5G leverage03:34PM EST- Shared social experiences03:34PM EST- Encouraging people to go outside03:34PM EST- (I thought the benefit of 5G is that you *don't* need to be there)03:36PM EST- XR2 with Purina03:37PM EST- (So, you can have 50 cats without actually having cats?)03:39PM EST- Brainstorming product design with Purina03:39PM EST- Multi-user collab event03:39PM EST- People get avatars03:40PM EST- Eyetracking to give immersion03:40PM EST- 2D interfaces with interaction03:40PM EST- Conduct holographic meetings03:42PM EST- Virtual environments as well03:43PM EST- Using AI to simulate mouth movements based on audio03:44PM EST- Collaborative solutions03:44PM EST- Powered by XR2 and 5G. (Would a business use 5G like that? Or local network?)03:44PM EST- Does XR2 still require an external Wi-Fi 6 chip / Wi-Gig chip ?03:45PM EST- Don McGuire to the stage to talk about ACPC03:46PM EST- Miguel Nunes to the stage, PM for Compute Products03:47PM EST- Qualcomm: 'Leading the PC Industry to Mobile'03:48PM EST- 'we don't believe fans should be in PCs'03:48PM EST- there seems to be a number of chinese devices there03:49PM EST- 'Battery life that lasts for days'03:50PM EST- Three premium devices in the market03:50PM EST- Surface Pro X with SQ103:50PM EST- $500 x86 PC example03:51PM EST- so, Qualcomm is going after the $500 market? I'll hold them to that03:54PM EST- Video of a chunky PC with Snapdragon03:55PM EST- Snapdragon 8c and 7c announced03:55PM EST- More price points and perfromance levels03:56PM EST- A snapdragon for everyone03:56PM EST- Focusing on user experiences03:57PM EST- These areas matter for users beyond CPU/GPU03:57PM EST- Security across every tier03:58PM EST- Devices are now protected with hardware root of trust03:58PM EST- location access policies03:59PM EST- Gaze correction04:01PM EST- Uses 50x less power than x8604:01PM EST- 7c has LTE04:02PM EST- 5+ TOPs04:02PM EST- Octocore Kryo, Adreno GPU04:02PM EST- (looks like an 855?)04:03PM EST- 2x the battery life vs competitors04:03PM EST- SiP design, including modem and RF04:04PM EST- Wi-Fi too04:04PM EST- Now 8c04:05PM EST- 8c is 7nm04:05PM EST- 6+ TOPs. Faster freq over 7c ?04:06PM EST- Integrated X24 modem, or can be used with X55 for 5G04:07PM EST- Now talking ecosystem04:08PM EST- Customers want speed, durability, design, security04:09PM EST- CUstomers love all-day battery04:10PM EST- ACPC is now targeting enterprise, small business, students and developers04:10PM EST- lower cost enables new opportunities in emerging markets04:10PM EST- Adobe Creative Cloud Suite coming to ARM6404:11PM EST- Business model evolution04:11PM EST- unlocking the creator04:12PM EST- Accelerating a new wave of computing04:12PM EST- Windows remote desktop with low latency04:13PM EST- MS is committed to bring xCloud to Snapdragon04:14PM EST- Digital transformation is here today04:15PM EST- 5G mmWave can elevate enterprise use cases04:16PM EST- 5 Gbps indoors in initial tests for private 5G networks in an office04:17PM EST- Zoom video communications is a partner04:19PM EST- transcription services04:19PM EST- (this is just a software ad)04:21PM EST- Zoom is going Arm native, demo later today04:24PM EST- Qualcomm's 8c reference design video04:24PM EST- looks kinda cool04:25PM EST- 8cx optimized for business customers04:26PM EST- 'Snapdragon for everyone'04:26PM EST- Yes, QC has supply (a dig at Intel)04:30PM EST- That's a wrap.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15202/qualcomm-snapdragon-tech-summit-day-3-live-blog-acpc-and-xr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Success on Arm for HPC: We Found a Fujitsu A64FX Wafer\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-12-05T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15169/a-success-on-arm-for-hpc-we-found-a-fujitsu-a64fx-wafer\n",
      "Content: When speaking about Arm in the enterprise space, the main angle for discussion is on the CPU side. Having a high-performance SoC at the heart of the server has been a key goal for many years, and we have had players such as Amazon, Ampere, Marvell, Qualcomm, Huawei, and others play for the server market. The other angle to attack is for co-processors and accelerators. Here we have one main participant: Fujitsu. We covered the A64FX when the design was disclosed at Hot Chips last year, with its super high cache bandwidth, and it will be available on a simple PCIe card. The main end-point for a lot of these cards will be the Fugaku / Post-K supercomputer in Japan, where we expect it to hit a one of the top numbers on the TOP500 supercomputer list next year.After the design disclosure last year at Hot Chips, at Supercomputing 2018 we saw an individual chip on display. This year at Supercomputing 2019, we found a wafer.I just wanted to post some photos. Enjoy.The A64FX is the main recipient of the Arm Scalable Vector Extensions, new to Arm v8.2, which in this instance gives 48 computing cores with a 512-bit wide SIMD powered by 32 GiB of HBM2. Inside the chip is a custom network, and externally the chip is connected via a Tofu interconnect (6D/Torus), and the chip provides 2.7 TFLOPs of DGEMM performance. The chip itself is built on TSMC 7nm and has 8.786 billion transistors, but only 594 pins. Peak memory bandwidth is 1 TB/s.The chip is built for both high performance, high throughput, and high performance per watt, supporting FP64 through to INT8. The L1 data cache is designed for sustained throughput, and power management is tightly controlled on chip. Either way you slice it, this chip is mightily impressive. We even saw HPE deploy two of these chips in a single half-width node.Related ReadingHot Chips 2018: Fujitsu's A64FX Arm Core Live BlogARM Research Summit 2016 Keynote Live Blog\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15169/a-success-on-arm-for-hpc-we-found-a-fujitsu-a64fx-wafer\n",
      "Title: Qualcomm Announces Snapdragon 865 and 765(G): 5G For All in 2020, All The Details\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2019-12-04T18:50:00Z\n",
      "URL: https://www.anandtech.com/show/15178/qualcomm-announces-snapdragon-865-and-765-5g-for-all-in-2020-all-the-details\n",
      "Content: We’re here in Maui for the second day of Qualcomm’s fourth annual Snapdragon summit, for what is probably the most exciting part of the event, as we cover the disclosure of the intricate details of the brandnew Snapdragon 865 and 765 SoCs that the company had briefly announced yesterday.Indeed, this year, Qualcomm isn’t launching just one SoC, but rather two new platforms at the same time. The Snapdragon 865 is self-explanatory in its positioning; as a direct successor to the Snapdragon 855 we expect the new chip to represent the best Qualcomm is able to deliver, and be the silicon that powers most of 2020’s flagship devices. The new top-model this year is accompanied by the new Snapdragon 765 and 765G SoCs. As with other 7-series models since the launch of the new range, the new generation adds of the new features introduced with the new Snapdragon 865, at a lower performance level and a more affordable price for what is becoming an increasinglypopular device category.Qualcomm Snapdragon Flagship SoCs 2019-2020SoCSnapdragon 865Snapdragon 855CPU1x CortexA77@ 2.84GHz1x512KB pL23x CortexA77@ 2.42GHz3x256KB pL24x Cortex A55@ 1.80GHz 4x128KB pL24MB sL3 @?MHz1x Kryo 485 Gold (A76 derivative)@ 2.84GHz 1x512KB pL23x Kryo 485 Gold (A76 derivative)@ 2.42GHz 3x256KB pL24x Kryo 485 Silver (A55 derivative)@ 1.80GHz 4x128KB pL22MB sL3 @ 1612MHzGPUAdreno 650@ ? MHz+25% perf+50% ALUs+50% pixel/clock+0% texels/clockAdreno 640 @ 585 MHzDSP / NPUHexagon 69815 TOPSAI(Total CPU+GPU+HVX+Tensor)Hexagon 6907 TOPS AI(Total CPU+GPU+HVX+Tensor)MemoryController4x 16-bit CH@ 2133MHz LPDDR4X / 33.4GB/sor@2750MHz LPDDR5/ 44.0GB/s3MB system level cache4x 16-bit CH@ 1866MHz LPDDR4X 29.9GB/s3MB system level cacheISP/CameraDual 14-bit Spectra 480 ISP1x 200MPor64MP with ZSLor2x 25MP with ZSL4K video & 64MPburst captureDual 14-bit Spectra 380 ISP1x 48MP or 2x 22MPEncode/Decode8K30 / 4K12010-bit H.265Dolby Vision, HDR10+, HDR10, HLG720p960 infinite recording4K60 10-bit H.265HDR10, HDR10+, HLG720p480Integrated Modemnone(Paired withexternal X55only)(LTE Category 24/22)DL = 2500 Mbps7x20MHz CA, 1024-QAMUL = 316 Mbps3x20MHz CA, 256-QAM(5G NR Sub-6 + mmWave)DL = 7000 MbpsUL = 3000 MbpsSnapdragon X24 LTE(Category 20)DL = 2000Mbps7x20MHz CA, 256-QAM, 4x4UL = 316Mbps3x20MHz CA, 256-QAMMfc. ProcessTSMC7nm (N7P)TSMC7nm (N7)We’ll start off with the whole story on the Snapdragon 865, and in particular one surprising aspect what we didn’t expect from the SoC this year; its lack of an integrated modem, and what the story behind the design choice.No Modem Integration This Year?The one aspect of the new Snapdragon 865 that overshadows all other new characteristics is the fact that Qualcomm designed it without an integrated modem. Qualcomm already announced this change yesterday during the day 1 of the event, without much context into the matter. It’s been fun seeing the reactions of various media and commenters theorising as to why this would be. It’s also one of the very first aspects that we wanted to have clarified by Qualcomm:The choice to not integrate the modem this year was a highly practical one, stemming from the complexity of 5G and the platform. There are actually more nuances to this though, and one thing that Qualcomm wants to make clear, is that this isn’t just a matter of the company's technical ability to actually create a chip with an integrated 5G modem; it’s keen to point out the Snapdragon 765, also announced today, which does exactly this.Instead, the technical difficulty of the 5G modem platform is actually on the platform and device side itself. As this will be the very first wide-range 5G implementation of a lot of OEM vendors who use Qualcomm’s chips, there will be a large number of designs who will be integrating 5G for the first time. The problem is that this requires a quite large development increase for the vendors creating the devices: they need to make sure their RF systems, antenna designs, as well as certifications of the systems are in full order. The nature of the 5G design complexity means that this process in a device’s development cycle this time around is actually quite a lot more complicated and more time-consuming than what we’ve seen from past 4G phones.Qualcomm’s solution to the problem, in order to facilitate the vendor’s device development cycle, is to separate the modem from the rest of the application processor, at least for this generation. The X55 modem has had a lead time to market, being available earlier than the Snapdragon 865 SoC by several months. OEM vendors thus would have been able to already start developing their 2020 handset designs on the X55+S855 platform, focusing on getting the RF subsystems right, and then once the S865 becomes available, it would be a rather simple integration of the new AP without having to do much changes to the connectivity components of the new device design.Qualcomm’s explanation makes a lot of sense in practical terms, and would bring time-to-market advantages. The company explains that in the future, it would re-integrate the modem back into the SoC, and this generation’s choices just made more sense for today’s situation in the market. Qualcomm isn’t the only one to have made such a choice,Samsung’s Exynos 990 SoC makes the same exact design decisions, shipping the main SoC as a simple application processor without any modem, although we don’t have any official backstory on their rationale for the design choice.The decision of shipping AP+discrete modem does have some disadvantages though. Motherboard PCB complexity does go up this generation, more so than competitors solutions which are able to integrate 5G modems (Such as the HiSiliconKirin 990 5G, or the MediaTekDimensity 1000). We also expect some compromises in terms of battery efficiency due to the silicon overhead, however as a counter-argument, Apple’s iPhones always have had separate AP+modem solutions, and the latest generation this year had amongstthe strongest battery life performance of any device out there, even with a competing Intel modem.As for the X55 modem itself: It’s the same piece thatQualcomm announced earlier this year, and this time around promises full global 5G connectivity ability (the X50’s support was more limited). The modem chip is manufactured on the TSMC’s 7nm process node, and the most important fact about the new combination is that the Snapdragon 865 SoC is exclusively tied to the X55 modem. This means that Qualcomm is selling the 865 SoC only as a pair with the X55 modem, and they do not offer support with any past 4G modem. In theory vendors could use another modem, but since they only sell the new platform as a pair anyway, it would make very little sense for anybody to do this.One question that also came up is on whether vendors, for whatever reason, would still be able to develop 4G phones with the Snapdragon 865+X55 platform. Qualcomm says this would be possible if you essentially just ignore the X55’s 5G capabilities, but they don’t see any reason for any vendor to actually do this. In essence, bar any abnormal decision from some vendors, all Snapdragon 865 devices in 2020 will be full 5G devices.The Snapdragon 865 Application Processor: TSMC 7nm N7PWith the elephant in the room of the modem configuration being out of the way, we come to the actual Snapdragon 865 application processor.For me, the biggest surprise of the new chipset was Qualcomm’s revelation that the chip is the fact that it’s manufactured by TSMC on the improved 7nm “N7P” node – this is the same manufacturing process as used by Apple’s A13 chipset. For the longest time were expecting this generation to be manufactured on Samsung’s 7nm EUV process (7LPP), givenas the two companies had announcement nearly 2 years ago.It seems that the announcement was actually about the new Snapdragon 765, which does come on the 7LPP process node, and which we'll be covering in more detail later on in the article.Qualcomm only's comment is that their process node choices were based on various considerations, including volume. Reading between the lines, it’s possible that Qualcomm wasn’t as confident in Samsung’s ability to manufacture the large quantities needed for the new chip. I’ve also heard murmurs from other sources that Samsung’s process simply doesn’t have as good performance and leakage characteristics as TSMC, and in flagship parts, those aspects have to be taken higher consideration than in say a premium- or mid-range SoC. In any case, it’s a big blow to Samsung’s foundry business as having the design win this year was quite critical, as I do not expect them to win the 2021 flagship contract due to TSMC’s 5nm leadership.As for the choice between TSMC’s N7P and N7+ (+ is the EUV node), it seems HiSilicon is currently the only client for the node for the time being, with the Kirin 990 5G being the only chipset manufactured on the node until later in 2020. It’s most likely that TSMC here simply doesn’t yet have the EUV volume capacity and yields to fulfil Qualcomm’s demand at this point in time, essentially being in a similar situation as Samsung, with the only difference being that TSMC has a viable high-volume DUV-based process ready as an alternative.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15178/qualcomm-announces-snapdragon-865-and-765-5g-for-all-in-2020-all-the-details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Imagination Announces A-Series GPU Architecture: \"Most Important Launch in 15 Years\"\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2019-12-03T01:00:00Z\n",
      "URL: https://www.anandtech.com/show/15156/imagination-announces-a-series-gpu-architecture\n",
      "Content: There are very few companies in the world able to claim a history in the graphics market dating back to the “golden age” of the 90’s. Among the handful of survivors of that era are of course NVIDIA and AMD (formerly ATI), but there is also Imagination Technologies. The company has been around since the early days of graphics accelerators and has seen a shifting industry. In the early to mid-2000’s the company moved away from the desktop space, instead focusing on offering GPU IP optimized for mobile devices.The Mobile GPU IP Market SituationThe mobile GPU IP market these days is a quite barren landscape; Imagination’s long history along with Arm’s rise to a leadership position with the Mali GPUs has essentially created a duopoly, as smaller vendors such as Vivante have exited the market. The problem of duopolies like these is that there’s a risk of stagnation if either vendor falls behind in terms of technology, and most of the time this comes at the cost of the vendors who rely on licensing the IP.Over the last several years in particular, we’ve seen a larger shift in the mobile market dynamics as Qualcomm has increasingly managed to dominate over its competitors. The more vertically integrated company produces its own SoC designs using its own in-house GPU IP, and as a result isn’t reliant on technologies from Imagination or Arm (CPUs are a whole other story).More recently, we’ve seen Apple’s custom GPU efforts make huge leaps in performance and efficiency. While officially we have no concrete information on the behind-the-scenes details of Apple’s design, it’s commonly regarded as a design forked-off from Imagination IP back in 2015, which Apple continues to independently develop thanks to an architectural license. The relationship between Apple and Imagination nowadays is still quite unclear, butthe frictions from back in 2017, whichultimately lead to Imaginationputting itself to sale and beingbought by an equity firm with Chinese investors, seem to have calmed down.The more important situation to consider is that the two leading GPUs in the mobile market today – Apple and Qualcomm – are designs that are internally developed by the SoC vendors themselves. This is a bit troubling for everybody else in the market, as their reliance on IP licensing means they’re limited in what they’re able to do. MediaTek had suffered quite a lot in terms of market share versus Qualcomm for example (GPU being only one of many other reasons). Other vendors such as Samsung have evidently also realized their situation, and launched development of their own internal GPU architectures, with a more recent surprise development of actuallylicensing new GPU IP from AMDin order to compete with Apple’s designs.Introducing Imagination's new A-Series: \"Fastest GPU IP ever created\"With that quite long foreword and context in mind, we come full circle to Imagination as well as today’s announcement. Today, the company is revealing its new A-Series GPU IP, describing the new product as Imagination’s most important GPU and IP launch in 15 years. The wording here is extremely strong, and the goals of the new product seem pretty clear: reverse the trend of market share loss against custom GPUs in the market, and put licensed GPU IP back at the forefront of performance.A-Series is the company’s 10thgeneration GPU architecture and represents a big leap for the company. Even something simple, like the product names, have changed. Gone is the longstanding PowerVR branding, in favor of a more immediate focus on Imagination the company. We'll still see PowerVR around – Imagination is keeping it to describe their technology, particularly their tile-based deferred rendering approach – but after over 20 years it's no longer a brand in and of itself.As a note on the new GPU IP naming scheme: it’s a deliberate departure from the numerical numbering of the past, and instead is trying to represent a new clean start going forward. I asked, but unfortunately the name it isn’t meant to be a clever twist on a “Series 10” with the numbering now represented in hexadecimal, a pity I say!The new naming crosses the company’s whole GPU range going from the low-end, mid-range to high-end. Here we see the continuation of the XE, XM and XT suffixes; however as in the past, there are architectural differences between the IPs, with today’s news mostly covering the new XT series.As mentioned, the new A-Series promises to bring some exceptional leaps in performance compared to past generation designs, although it’s also important to point out some footnotes on the matter.An important metric that’s been showcased today is Imagination's claim that the new A-series XT design is 2.5x faster. \"Fast than what\" you might ask? The figure represents the new architecture’s performance leap in an ISO-area and process node comparison against Imagination's previous generation Series 9 GPU.The thing to note is that the company isn’t doing exact apples-to-apples comparisons between different generations of XT GPUs, but rather is making comparisons to the latest widely available “Series 9” on the market. This was an unfortunate, but yet practical choice to make given that currently there’s no publicly announced or available chips which make use of the company’s 8XT or 9XTP designs.The comparison data here is likely based on MediaTek’s Helio P90 with the GM9446 – technically part of the 9-series, but also architecturally based on the older Rogue architecture. Imagination wanted to be more transparent in regards to its area and performance claims for the A-Series, and the comparison with the 9XM Rogue-based GPU provides a much more recent and readily available baseline against which vendors can independently verify and compare metrics to.Key improvements of the A-Series include a massive overhaul over the GPU’s compute units and ALU architecture, promising a 4x increase over a 9XM Rogue GPU. AI is a first-class citizen in the A-Series and we’ll be seeing dedicated compute resources resulting into improvements of up to 8x in performance. Finally, the new architecture is said to bring a very large leap in terms of power efficiency, with the new design consuming 60% less power for a similar level of performance.The new architecture covered today spans four disclosed configurations in the XT series, as well as one XM configuration. The XE series, while having seen improvements, isn’t based on the new architecture, but rather a continuation from the previous generation.Imagination’s naming for the new A-Series is extremely straightforward in regards to understanding their positioning a performance. The AXT-64-2048 for example represents the highest end configuration of the architecture in the XT range, achieving 64 texels per clock and 2048 FP32 FLOPs per clock. AI TOP performance is quadruple the FLOPS, so in this case for an envisioned 1GHz design, the AXT-64-2048 reaches 2 TFLOPs in graphics and compute power, 8 TOPs in INT8 inference throughput, and 64 Gigatexel/s fill rate.It should also be noted that it's more correct to say Gigatexels/s instead of Gigapixels/s as in the marketing materials published - Imagination found that a lot of people didn't correctly grasp the meaning of texture fillrate and thus opted to simplify the marketing information in pixels/s.The figures scale down the range with corresponding performance with the AXT-48-1536, AXT-32-1024, AXT-16-512 and finally the AXM-8-256. The AXE-1-16 is a separate, smaller more customized microarchitecture.Imagination envisions the AXT-32-1024 to be the traditional sweet-spot target for a premium mobile GPU SoC in smartphones, while the larger configurations would possibly be used in larger form-factor devices.The company calls the A-Series “the GPU of everything”, and there’s indeed a ton of changes and features that make up the new IP. We’ll be covering a few aspects of the company’s disclosed big matrix of features above, starting off with one of the biggest changes, the GPU’s ALU architecture.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15156/imagination-announces-a-series-gpu-architecture\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Fireside Interview on 5G with Cristano Amon and Alex Katouzian, Qualcomm\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-12-02T13:00:00Z\n",
      "URL: https://www.anandtech.com/show/15187/fireside-interview-on-5g-with-cristano-amon-and-alex-katouzian-qualcomm\n",
      "Content: A little while back, ahead of Qualcomm’s upcoming Tech Summit, we spoke to President of Qualcomm Cristano Amon and SVP/GM of Qualcomm’s Mobile Technologies Alex Katouzian. Qualcomm’s march in the high-end smartphone space during 2019 has been impressive, capturing all the flagship devices from companies that don’t build their own silicon. 2019 has been a key arena for 5G as well, with over 150 5G smartphones coming to market all on Qualcomm solutions. Then there’s the little matter of Windows on Snapdragon, hopefully becoming a tour-de-force with the 8cx chipset.We quizzed Cristiano and Alex on all three sides of business and how it relates to Qualcomm’s partnerships, strategy, and future. The discussion provided a deeper insight into what lies ahead for Qualcomm, given how they see the market, and ended up as a nice little teaser for Qualcomm’s Tech Summit in a few days.Alex KatouzianQualcommDr. Ian CutressAnandTechCristiano AmonQualcommAs always with our transcriptions, they’re tidied up a little for readability, and sometimes the order is changed based on how topics bounce around.Many thanks to Gavin Bonshor for his transcriptionWindows on SnapdragonIan Cutress: Regarding the Windows on Snapdragon ecosystem, you have now been at it for three or four generations. In that time it has been a very Qualcomm driven enterprise, and yet there are other Arm hardware partners who might want to partake. Is there room in that ecosystem, or are you happy to drive the use case and form factor?Cristiano Amon: If you look at a company like Microsoft, historically it has a development process that is fairly focused on its primary silicon partners. For us they’ve been very focused on optimizing the solution to the hardware. When we started down this journey, with Windows on Arm, there was a lot of work for us to do, and a lot of work for Microsoft to do. When you say this has been Qualcomm driven, it has really been both Qualcomm and Microsoft driven, as we both do heavy lifting: we focus on creating the best hardware that we can, and in 8cx we have a great example of that as our dedicated platform for PCs to meet the performance requirements for users. For Microsoft, they have been optimizing the Windows performance not only for a CPU centric solution, but specifically to our SoC.A great example of this is when we show multitasking side-by-side with Intel. In our demo, it’s very easy to see Intel dropping frames for video playback and such, which means that Microsoft is working directly to enable our hardware acceleration technologies. On top of that, Microsoft is addressing some of the pain points that people like to bring up – we can run legacy win32 apps, and there are plenty of popular Windows tools now being ported directly to Arm, like office, and things like that, as well as being optimized for our hardware. So I feel that the approach that Microsoft has is very different to the idea of ‘how many pounds of low-end tablets can we support’, but it’s about delivering the connected PC experience that drives the future of both companies. Particularly with Microsoft, who also wants to encourage users onto Azure, oneDrive, Office365 and other things. Microsoft is very keen on working with Qualcomm on this, and they’re focused on making sure the experience drives the future of computing.IC: This is more a Microsoft question, but do you not feel it not assist Microsoft (and Qualcomm) to support more than just Snapdragon, to help build out the ecosystem?CA: If you think about problems to solve, Microsoft today has three suppliers: Intel, AMD, and Qualcomm. The PC space is well defined, but you can actually create the same or better experience with a fully rounded SoC that offers better battery life and connectivity! I find it interesting that when people say ‘Windows on Arm’, the Arm specific part is actually just a small piece of what we have to do. We prefer to say ‘Windows on Snapdragon’, because this is really what it is with our partnership with Microsoft.Now in the future, if Apple moves their SoC architecture to the Mac Book, Microsoft is going to want to have access to the best possible devices that can compete in the future of connectivity. In that is support for enterprise services, like Azure. So naturally we expect Microsoft to pick the best Arm SoC vendor, and focus their R&D efforts into supporting that solution. That’s my vantage point, and I don’t expect Microsoft is going to want to go after a low-end Arm notebook or tablet market in the same way.IC: Would it ever be beneficial, from your point of view, to see a ‘Windows on Exynos’, or ‘Windows on Kirin’ device, to help drive scale?CA: I’ll say no, and the reason is because we have to invest ahead of scale. We took a strategic decision that we’re going to go drive conversions of mobile PCs come hell or high water. We decided that we were going to invest and build a product that’s all about making the best PC experience possible, for consumers and for professional use cases. I think that if you think about it, the solution to the obstacle of success is the software ecosystem. You have to create the software ecosystem and you have to bring it to the enterprise, and I think consistency of performance is what we’re looking for right now. A multi-vendor market, with software R&D efforts split in several different directions, might not work for this sort of product category.IC: What are current sales like, for Windows on Snapdragon devices?Alex Katouzian: We’re easily shipping over 1000 pieces a day. Some might consider this small, but we think it’s a growing market with people who want a differentiated user experience.IC: With the current crop of Windows on Snapdragon devices, we already have Samsung with an 8cx device and a Lenovo Yoga 8cx in the works, one might argue that the key value that Windows on Snapdragon could bring is to the commercial enterprise space, with something like a Thinkpad. Are we there yet?CA: I’ll say that the first waves of devices were targeted towards the consumer. Then we had to wait until we could get Windows 10 Enterprise Edition, which came in around the Snapdragon 850 time frame. With the 8cx we now have the performance that compares with the devices in that commercial enterprise space, like the i5 family. So I believe we’re in the very beginning of this journey into the enterprise. If I go back to our previous conversation, I said that the enterprise market is the ultimate goal – this is where our interests are more aligned. Our focus here is 100% aligned with Microsoft, because of their enterprise cloud strategy. What our benefit is with the 8cx is commercialization – we now have the first of the 8cx devices being presented and as you have noticed these have been designed towards the enterprise market. The hardware attributes and functionality of the 8cx and the devices which have it are pointed towards the enterprise. We have a laser focus here, and we’ve been working actively with Microsoft to get this to happen.AK: From a ThinkPad perspective, we’re also working very closely with Lenovo on these type of products as well. They all see these platforms for the enterprise as a value-add to their business, especially with performance, battery, and connectivity. Then if you tie that back into the mobile data wireless operators, they also see a big advantage when we look at 5G connectivity, and having always-connected devices in the enterprise space, along with Qualcomm small cells. It’s like a three party or four party type of solution with a laser focus. This is where we are.5G and Modem IntegrationIC: Current 8cx solutions rely on external 5G modems (external to the main SoC) in order to support 5G connectivity. We’re still in a 4G/5G transition phase – would you expect an integrated 5G SoC/modem as a single piece of silicon in this sort of form factor?AK: Absolutely. If you go back to the 4G days when it was launched, we took all of the 3GPP specifications for 4G that were defined and we put it in a bit of silicon. Over the course of a couple of years, you hone in on the features and the functionality that are most useful – you figure out different partitioning and architectures for your modem to make it smaller, and then that gets moved onto the main SoC. That’s exactly what is going to happen with 5G as well. As you’ve already heard, we are going to have integrated 5G SoCs coming in 2020. The life of a standard mobile PC is in the region of 2-3 years, and you’re not going to have a new mobile SoC every year, so the next generation after 8cx is where you should look for an integrated solution.IC: We currently have a bifurcated strategy with 4G and 5G, given that some markets have been quicker to adopt different parts of the 5G standards than others. Is that strategy of separating 4G and 5G silicon expected to continue long term, or will there be a right time to go for a single integrated 5G philosophy across the board?AK: I think the good part right now is that we have a line, a product line of devices that span our entire portfolio. In PC devices, we have the 8cx at the top, but we also have a version of the 8cx called the 8c. It will be a little less powerful, but it targets parts of our line-up in-between the 8cx and the 850. And there’s another one coming, so you will have a whole line-up – some of them 4G, some of them 5G.IC: Is the smartphone market ready for 5G in the mid-range?CA: Yes. You have to look at all the stakeholders and why we’re so focused on scaling right now. There are three parts to this answer.First, think from the operator’s perspective. It’s very clear, you can argue, that you can have a conversation about how fast you are going to build coverage, how fast we are reforming the spectrum, but the trends are very clear: users are using more data, and the average data consumption per month is going up. If you look at 5G in Korea, there are 2 million subscribers, and the data from KT (Korea Telecom) is that average data per user has doubled. So from the operator perspective, you’re going to get to a point where it is uneconomical to give unlimited data plans on 4G, and there is a need to switch for 5G, as 5G offers you a lower cost per bit and gives the user to have a premium experience with less congestion. Operators want to move you onto 5G for a variety of reasons, but that one is key from a user experience perspective.IC: We’ve seen a few operators show resistance to providing affordable 5G plans. From my perspective, aside from the cost of current devices, that’s somewhat of a limiting factor.CA: Depending on the market there may be some segmentation of price plans, and I think that network slicing is going to enable this. I feel that every new technology that hits the market is expected to have a price premium for that extra performance. But overall, the economics, from the operator perspective, there’s one element on the revenue side and that’s how it is priced to the consumers. But in terms of operating costs, 5G brings about a different cost equation. Especially if you can build out knowing how the customers will use data in the future.It’s hard for me to say what the operators will exactly do, but I’ll tell you that we’re still at an initial scale-out. The thing is, having worked at an operator, it’s all about transitions from one generation of technology to the next. At the beginning of those transitions, you have a non-ideal situation, based on hardware and deployment. Even in Europe today, we still have operators with 2G. As operators have migrated from 2G to 3G then to 4G, you have to adjust that cost equation, paying maintenance, electricity, paying all those things, and in the beginning of a transition, you have to run multiple networks, and then by getting people on board helps smooth the deployment out to future sites. So I think you’re going to see the equation of the economics being amortised as more users sign up, along with CapEx (capital expenditure) and OpEx (operational expenditure). How they do that deployment, in a transition phase, depends on the technology, but ultimately it’s a drive to a lower cost per bit. 5G is designed for this, and the data usage model will change: customers might not have a fixed line into the home in the more, and use 5G for that last mile of cable. This is why scale is so important.For part two of the question ‘is the smartphone market ready for 5G in the mid-range’, we should consider that we now have a very mature smartphone market. In a mature smartphone market like the United States, the number of people with at least a ‘good’ device is high, and people buy knew phones every 2.5-3 years. In Europe it’s more 3-4 years. So if you’re going to buy a phone right now, and you already know about 5G and heard that 5G is coming, you can end up with a device for 2-3 years that won’t support the fastest speeds or get the best user experience. Users who know about 5G will want a 5G phone with how fast 5G is being supported.The third part of my answer is that the device ecosystem is mature ahead of the network: the devices are ready and the network is catching up. Unlike the transitions to 3G and to 4G, where the operators were first, here at Qualcomm we have been accelerating and driving the device adoption of 5G. So even if you never use the 5G modem in the device, it’s still a very competitive unit, such as the Samsung Galaxy S10 5G, from the industrial design, battery life, and all these things. With the device ecosystem being ready, the operators are getting there, and with more phones enabling 5G today, users aren’t going to want to be stuck on a 4G smartphone.The last piece that you don’t see right away with one of these new devices is the 5G – there is a bigger difference between premium and the mid-range here. If I have a Samsung high tier device, or even a mid-tier device, and then I go buy the premium tier, what I will see is the premier tiere has a better camera, a higher end display, maybe a bigger screen with more screen area, more memory, better graphics, and the games probably play a little better. The services on both devices are still kind of the same – I still have WhatsApp, I still have Instagram. Some might perform better, some might be worse. With 5G, it’s all a little different. When it comes to the question of say, streaming 4K video, it’s a very binary result: you either have it or you don’t. If I want to stream some games, you either can or you cannot. It’s a very binary proposition. The evolution of social media with live broadcasting, such as Facebook Live, means that the entire developer ecosystem says that if the capabilities are there, users want to have it.When you put all of this together, you get to the same conclusion that we got to, and I think that forms the key basis of our announcements this year. Mission one has been to get 5G launched, and get all the flagships supporting 5G. We have our flagship modem, and we have our second generation 5G that we will show at our Tech Summit later this year, and we’re going to impress you! But then that still isn’t enough – we need to scale 5G, and we need to scale it right now. The transition is going to be faster than we’ve seen with 4G, and that’s why you’ll see our 7-series and 6-series with 5G and we’ll keep going.IC: Currently we’re seeing a mix of discrete 5G solutions and integrated 5G solutions, whether it’s in a smartphone or other devices. There has been some commentary regarding performance in both methods, such that some discrete options can offer 2x the bandwidth of an integrated solution. What is Qualcomm’s approach here?CA: You have to think about 5G a little differently to 4G. I won’t comment on our competitors, as they’ll do what they want to do, but I can tell you that 5G gives you the flexibility to have differences in the performance as you design the modem. Despite this, there is one very important thing on 5G is average speed – if you remember when Qualcomm pioneered Gigabit LTE or even 2 Gigabit LTE, you had a significant improvement in peak speeds, but the average speeds don’t improve at the same rate. It’s very different with 5G – with 5G the average data rates go up and, what you’re going to see as we start moving our modems onto our 7-series chipsets is that we’re not going to compromise, average data rates aren’t going to be compromised, and latencies aren’t going to be compromised. You can still have different choices, as you’ll see from our portfolio, we’ll still have full support for millimeter wave, and full support for Sub 6 GHz. We are going to unveil all the details of our modem in the 7-series chipsets at our Tech Summit. But we have a plan, and you’ll see a pattern.5G For Both mmWave and Sub 6 GHzIC: With different areas of the world starting with mmWave or Sub 6 GHz, and slowly transitioning to support both, what difficulties are there in supporting both in mobile devices?CA: The fact that we have had these two elements of 5G starting off differently in different areas has been a mistake. It has played out in the marketing strategies of certain companies, especially when you talk about millimeter wave support. Some companies didn’t have that solution, or focused on Sub 6 GHz first, and so this whole NSA/SA separation occurred and some of the early hardware was labeled as NSA, as SA, as mmWave, as Sub 6 GHz and so on. 5G has been designed to have all of this! So in the end everyone will support all of this. It is almost as crazy as what we saw in the early days of 4G, with TDD and FDD LTE. It’s almost that crazy.What you’re going to see is millimeter wave and Sub 6 GHz deployed in every geographical region. In the United States we deployed millimeter wave, and in Korea they went Sub 6 GHz first then millimeter wave is coming in 2020. In speaking with Korea Telecom, some of the Korean industry applications, for example from Hyundai, is that the amount of data coming out of machines in the production lines are of the order of multiple terabytes per day. With that you either need a fixed connection, which could be expensive, or you manage an over-the-air connection to give you flexibility, but you cannot do it without millimeter wave. It’s the same with Japan, they will deploy millimeter wave in 2020. For Europe we have it licensed or in testing at sites in Italy, the UK, Germany, and others. So it’s fair to say that when you get to the end of 2020, or even during the second half of 2020, you’re going to see Sub 6 GHz and millimeter wave both deployed where 5G is deployed.I want to go back to comment on NSA/SA. Millimeter wave is a lot more complex than Sub 6 GHz, because you require densification of the network. With that in mind, it is natural to see it get deployed in the United States first, because that was the first region that they made use of 5 GHz on LAA. The deployment of LAA caused the operators to identify suitable cell sites, and the same sites can be used to deploy millimeter wave. That’s why part of the conversation you’ll see in the USA, and in Germany, will be around having access to traffic lights. Because the hard part of deployment is just identifying places for cell sites, this is what makes millimeter wave more difficult to deploy. Sub 6 GHz is easy, because we can put upgrades into existing 4G sites.But at the end of the day, how the 5G network is deployed will revolve around dynamic spectrum sharing. We have the low band, as low as 800 MHz or 600 MHz, and then moving up to 1.8-1.9 GHz to 3.5 GHz. This is what we’re going to see in 2020, with the reform of dynamic spectrum sharing, and you will see Sub 6 GHz with hotspots of millimeter wave. I also have a view that industrial use cases, because of the performance and latency requirements, will want millimeter wave as a mandatory requirement. It’s fascinating to watch.It is worth noting that once you start building 5G, coverage will grow, as with any other generation of wireless technology. Coverage isn’t everywhere immediately. With a design where you keep dual connectivity with 5G and LTE, so if you lose 5G the connectivity with LTE is still there. It has the highest level of reliability for connectivity. So every operator, including China, when they started services, they started with NSA, then as they built out the coverage they moved to SA but at the same time with private deployments you have your own SA core. So SA and NSA all happened at the same time, but for some reason it became a distinction about what version of 5G people had, and it became a mess. We’re going to see Sub 6 GHz and millimeter wave both in every single deployment.IC: As we move to 2020, do you feel support for both Sub 6 GHz and mmWave is going to be required, even for mid-tier devices?CA: It all comes back to the question of binary services: you need the hardware to enable the services. Right now we’re still in that early phase when many people are asking if they need that much speed, or that low latency. That question will disappear when the services start to change. But the other way to think about it is that it will take longer for mmWave to get deployed, so you may have a transition when in some cases mmWave isn’t going to be available. But as soon as you build coverage, people will see these binary effects.IC: One limitation of mmWave is the antenna design. Qualcomm has been very vocal about its range of antennas and RF front ends. Can you speak to recent improvements and progressions on this front?CA: One thing you’re going to see us talking more and more about is expanded features. We did announce an expanded feature set for our X55 modem, and you’re going to see more at our Tech Summit. At that event we’re going to be talking about something called Smart Transmit, and we have been proving this difficult problem despite the complexity of 5G, despite the complexity of the antenna, despite the complexity of multi-user MIMO. You have to start thinking at the system level and take advantage not only of digital signaling but also analogue signaling in how you design it, and Smart Transmit is a very clear result of this. If you have the time, you don’t have to take Qualcomm’s word for it – I highly encourage people to go check out the Samsung Note 10 5G FCC filings as they’re public and while it doesn’t say Smart Transmit on the web page, and it wasn’t launched with Smart Transmit in the FCC filings. But in those notes, they talk about the technology, how Qualcomm was able to support Smart Transmit without the ability of external sensors to deal with proximity detection, how Qualcomm will deal with power management, thermal management, and all this complexity.AK: With Smart Transmit you cannot talk about 5G without having an end-to-end modem and antenna solution. It’s impossible otherwise. Anyone who is claiming Smart Transmit without this doesn’t understand the need for the integrated system development. The only way to understand is if you have all the modem technology, can follow 3GPP specifications, and you drive the specifications. You have to make sure the front-end of your RF solution, and your antenna, is all in line to make 5G great, otherwise it doesn’t happen.CA: One of the things you probably heard about in the early deployments of mmWave was the thermal issues – there was a suite of features that we implemented, but it took time for the OEMs to use them. But even when you think of a mmWave antenna array, how you can have different power levels om different antenna and how the complexity is so big, that’s the reason I think we’re going to redefine how we talk about these things. There are going to be features that people will come to expect because that’s how you think about the system as a whole. That’s just the reality of 5G as it’s a more complex RF environment.Now we deal with an enormous amount of skepticism from many analysts, that we’ll never be able to succeed in the front-end RF business because of mature players such as Skyworks, Qorvo, Murata, and Avago. They said that you (us, Qualcomm) will not be able to do this. We have come out with a platform, and out of the 150 designs using Qualcomm 5G modems, all 150 with no exceptions are using our front-end RF. This is for both mmWave and for Sub 6 GHz. This is not a coincidence – if you look at the performance comparisons, we’ve proved the point of what we have been saying: this is a different ballgame to 3G and 4G.IC: Can you summarize the upcoming Qualcomm Tech Summit?CA: You will see one of the most significant upgrades in premium tier platforms and what you’ll see is how we create the new benchmark in performance for our 5G modem strategy.Many thanks to Cristiano and Alex for their time. Qualcomm’s Tech Summit is taking place December 3-5th, and we’ll be there with the latest news.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15187/fireside-interview-on-5g-with-cristano-amon-and-alex-katouzian-qualcomm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Junk or Treasure? We Tested Amazon’s Cheapest Black Friday Desktop PC, only $60!\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-12-01T14:15:00Z\n",
      "URL: https://www.anandtech.com/show/15186/amazons-cheapest-black-friday-desktop-pc\n",
      "Content: Some Black Friday deals are wild. A store might offer only a couple of units of a particular TV, discounted by 66%. There might be a few pieces of a flagship smartphone at your local electronics store at half price. These are designed to entice customers through the door, and if you’re brave enough, ensure the cold for up to 12 hours to get that bargain of the year. But one of the key observations about looking at Amazon’s Computing and Components section every Black Friday, particularly this year, is that most of the discounts are for complete trash. After the headline external storage discounts, it’s just page after page of USB cables and smartphone holders. But one thing did catch my eye: an entire PC, for only £57 / $61! How can an entire x86 desktop PC be sold for so little? We did the only thing worth doing: we purchased it.So let’s put this into context – Amazon sells tablets, desktop PCs, smartphones, and all other sorts of computing gadgets. A simple Atom based desktop small form factor PC can be around $160, or perhaps an Android based HTPC for the same amount. Amazon’s only Fire 7 tablets are £50/$50, reduced for Black Friday down to £30/$30, but if you want something a bit beefier, the Fire HD 8 is $80 regular/$50 on sale. The point is that while tablets regular feature in the mid double digits for pricing, a full desktop certainly doesn’t. So what makes this one so cheap?Our £56.96 unit sold on Black Friday, which normally retails at £99.99, is old. It’s also a refurbished PC. If you take out the 20% UK sales tax, it’s only £47.46. It weighs 7.12 kg / 15.7 lbs, and we got it on next day delivery, so posting it probably costs around half that value. How much horsepower can a £25 / $30 PC actually get you in that instance? I did mention it is a full x86 system, not anything from Arm.*For those that aren't aware, because different US states have different amount of sales tax / VAT, the prices quoted online do not include any sales tax. We are quoting US prices here without sales tax, as we usually do. UK prices included sales tax unless otherwise indicated.This PC uses a CPU that was launched in 2009. The Pentium E6500 inside is a first generation Core microarchitecture called (Wolfdale-3M with 2MB L2) processor with two cores running at 2.93 GHz, and is built on a Dell custom Socket 775 motherboard. This CPU was built on Intel’s 45nm process, and features only 228 million transistors (compared to the billions today), and has a TDP of 65 W. It uses DDR3 memory, but is so old it doesn’t even have AVX instructions: SSSE3 is all you can get here. Technically the recommended retail value of this processor when new was $68-$75, which was a low-end chip even for 2009.The listing on Amazon is for a refurbished Dell Optiplex 780 – an office form factor machine that is very typical of one you might see in an office that hasn’t been updated yet (this is probably where this unit came from). The listing for the machine promises a few things: a CPU at 2.6 GHz, 4 GB of DDR3, a 160 GB HDD, and 802.11abg Wi-Fi, as well as Windows 10. What we received was a 2.93 GHz processor (woohoo!), 2x2 GB of DDR3, a 250 Gb HDD (woohoo!), no Wi-Fi (boo), and a full copy of Windows 10. The fact that this comes will a full blown copy of Windows 10 Pro, which even at its cheapest is around $20, astounds me. Even if the whole unit is a refurb, that’s the one part that is most likely new: and given that the value of the contents are around $30, that only leaves $10 for the actual hardware.Instead of Wi-Fi, we get a 10/100 ethernet port. There are plenty of USB ports, and the front has a CD/DVD tray. The processor doesn’t have any integrated graphics, but there’s no add-in card here: this machine has a separate onboard Intel GMA4500 chip providing outputs, either by a VGA/D-Sub cable or, perhaps surprisingly for a 2009-derived system, DisplayPort. That’s Dell, always thinking ahead (said no-one, except today). There’s also a COM header and an LPT port at the back.Inside is where some fun begins. There’s a half-height PCIe x16 slot, if anyone wants to put in a cheap graphics card, but we can open up the heatsink to see the processor underneath.The heatsink is actually a lot beefier than I expected – it has a full copper core, and is screwed down really quite tight. As for the CPU, looking at eBay, an E6500 CPU can be had for as little as $4.61. The fastest DDR3 s775 processor on the market is a Q9650, although that is a 95 W processor – if you wanted to upgrade this thing, then a Q9550S at 65 W TDP might be your best bet, although these sell for as much as this entire system! A Q9550S would however get you four cores at 2.83 GHz.Everything else pretty much looks normal. The integrated power supply is a Dell unit rated for 235 W, which is easily sufficient for this system although there’s no telling how old it is – the unit is the exact model which was designed for the Optiplex 780.If all of this, for this price, has seemed great so far, then the memory section is going to annoy you to no end. It annoys me, that’s for sure.The listed said 4 GB of DRAM, which in this modern age I assumed would be one module, and we’d end up with a system running in single channel bandwidth. For this price, I assumed nothing less. However, on opening the unit I was greeted with two modules of 2 GB. These are Samsung models actually built for HP (it’s a refurbished system, or code for mix-and-match), and are DDR3-1333 speeds. The system runs them at DDR3-1066 7-7-7, so there’s plenty of headroom for them.But what angered me most was the placement. This motherboard has four DDR3 slots, which indicates dual channel memory, at two modules per channel. Whoever refurbished this machine knows nothing about dual channel memory, and put both modules into the slots for the same channel!Upon entering the BIOS, you end up being greeted with this:‘Number of Active Channels: Single’ – despite two memory modules being inserted.If anything needed more memory bandwidth in 2019/2020, it’s going to be a 10 year old dual-core processor. I was livid.Now, moving one memory module over to the other slot is easy enough. Upon asking Twitter if I should test in single or dual channel, the answer I got was ‘test both, but if single is a lot worse, gut the person selling the machine’. It’s always a fun day when someone’s stupid mistake doubles your workload! So for this quick test, I’m going to take it from the view point of a single channel machine. Just because some idiot putting together the machine doesn’t know how memory works. Mind you, for $10 of hardware, should I have expected anything less?Just to prove it can be in DC modeUpon getting the machine, it booted straight into Windows. Someone had clearly been through the install procedure already, and it gave me Windows 10 Pro 1903 with a standard desktop and we were ready to go.Because this is just a quick test, and I’m on the road for two weeks the day after the unit arrived, we only had time for a small number of tests to gauge the performance. For the cost of this system, what I really should be comparing it against is a tablet, but the nearest modern thing we have are the cheapest Atoms or Athlon APUs.BenchmarkingSo we start with the benchmarks that everyone recognizes: CineBench.CineBench R15For CineBench R15, we actually have a long history of data. If you ignore the fact that some of these were done on Windows 7 or Windows 10, we could compare it to Intel’s final ever single core desktop processor, the Celeron G465 on Sandy Bridge, or one of AMD’s low end A-series APUs built for AM4, the A6-9500, which uses an Excavator design with one module but two threads.So perhaps surprisingly, moving to dual channel in CB R15 gave us some ST performance, but not any more MT performance. For ST and MT this E6500 is ahead of the single core 1.9 GHz Sandy Bridge, but even slightly newer Pentiums can easily go above and beyond.Cinebench R20CB20 is one of our newest benchmarks, built on a design that is meant to scale better as core counts are going up to 32 and 64 cores per CPU. Unfortunately we’ve only been running this test for a short while, so only a few of our CPUs have any data. We can still pick from the dual core Celeron G3950 (Kaby Lake), and AMD’s 15W FX-8800P from the Biostar SFF motherboard launched earlier this year.This time around moving to dual channel gives a sizeable MT benefit, whereas ST didn’t move much at all. AMD’s 15W Excavator CPU gives better performance here, but it is a much newer processor.Time to Load GIMPOne of our popular tests in the latest suite is our test that probes how long it takes to load the image editing software GIMP. For this test we use the post-first-load time, which already has some things cached, and then run it over 10 successive trials. Fast modern day processors can do the job in just over three seconds, however this test shows its teeth with the slower end of the spectrum.Compared to the Sandy Bridge single core G465, the E6500 actually does better here with two cores. It even makes it faster than Intel’s best mobile excavator parts, and the dual channel result is better than an AM4 processor. Modern CPUs, even the Ryzen 5 1400, are somewhat better.Gaming: Counter Strike Source TimeDemoWith this system having Intel GMA4500 graphics, it was relatively hard to decide what to test. It does support DirectX 9.0, but anything mildly strenuous is going to make this system crying for mercy. In that regard, we went back to an old staple: Counter Strike Source. The Source engine allows for timedemo playback, although this is more a CPU test than a graphics test: the engine attempts to playback the timedemo as fast as possible, which is often at a slightly faster framerate than the actual gameplay during the recording.For our test we took a 72 second round of de_dust2 with 30 extreme difficulty bots as our playback, and ran it at various resolutions. The settings were for low resolution models, low resolution textures, low shaders, medium shadows, no AA, with multi-core rendering enabled.The only comparison point I have for this test is the Core i7-8565U that I am currently using as my work laptop, which has Intel UHD620 Graphics.CS:S is known for having lots of low poly tweaks which could be applied on top of these to get higher frame rates, but I’m surprised at just how smooth the E6500 was at low resolution gameplay. It’s just a darned shame that the people who shipped this PC only put it in single channel mode – the gains at 800x600 were above 30%, just by physically moving a memory module!ConclusionSure, this PC by most metrics is a pile of trash: a dual core 2009 processor, a mechanical hard drive, and memory in the wrong holes. But for $60, with a full blown copy of Windows 10 Pro? It’s a treasure. Attach a monitor, keyboard, mouse, network cable/USB Wi-Fi, and you are golden.One of Anand’s quotes that I like bringing up is the one where he said ‘there are no bad products, only bad prices’. This Dell Optiplex 780, in the Black Friday sale for $60, was a good price. Just as long as you change that memory stick to dual channel.Now that the sales are over, this system is back to it's full price. Any sales widgets below will be of different systems.A More Modern Dell\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15186/amazons-cheapest-black-friday-desktop-pc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: MediaTek Announces Dimensity 1000 SoC: Back To The High-End With 5G\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2019-11-26T05:30:00Z\n",
      "URL: https://www.anandtech.com/show/15155/mediatek-announces-dimensity-1000-soc-back-to-the-highend-with-5g\n",
      "Content: Today MediaTek is announcing the new Dimensity 1000 SoC – the company’s new 5G flagship SoC for 2019 and early 2020. The announcement really isn’t too much of a surprising new reveal, as the “5G SoC” wasannounced some time ago in Maywith the company describing its 5G capabilities as well as the inclusion of the new Cortex-A77 and Mali-G77 IPs. What MediaTek does today however is to officially name the new chipset, and go into a few more details on the specifications.First of all, the naming of the chipset is part of rebrand for the company’s SoC line-up going forward. The “Dimensity” name is meant to usher in a new generation and differentiate the new 5G SoC from the previous 4G Helio line-up.“We chose the name Dimensity to highlight how our 5G solutions are driving new waves of innovation and experiences, much like the fabled fifth dimension,”- MediaTek President Joe Chen.MediaTek SoCsSoCDimensity 1000Helio G90(Helio G90T)CPU4xCortex A77@ 2.6GHz4x Cortex A55 @ 2.0GHz2x Cortex A76 @ 2.0GHz(2.05GHz)6x Cortex A55 @ 2.0GHzGPUMali-G77MP9@ ? MHzMali G76 MP4 @ 720MHz(800MHz)APU / NPU / AI Proc. / Neural IP\"3rd gen APU\"2 \"big\" + 3 \"small\" + 1 \"tiny\"4.5TOPs total perf2x APU+1TOPs total perfMemory4x 16b LPDDR4xLPDDR4X @ 2133MHzISP/Camera80MPor32MP + 16MP1x 48MP(64MP)or2x 24+16MPEncode/Decode2160p60H.264 & HEVC& AV1(Decode)2160p30H.264 & HEVCIntegrated Modem5G Sub-6DL = 4600Mbps200MHz 2CA, 256-QAM,4x4 MIMOUL = 2500Mbps200MHz 2CA, 256-QAM,2x2 MIMOLTE Category 19 DLCategory 12/13DL = 600Mbps3x20MHz CA, 256-QAM, 4x4 MIMOUL = 150Mbps2x20MHz CA,64-QAMConnectivityWiFi 6 (802.11ax)+ Bluetooth 5.1+ Dual Band GNSSMfc. ProcessN712FFCAt the heart of the Dimensity SoC lies the new Cortex-A77 CPU. MediaTek was notoriously first to announce a product based on the new IP, only days after Arm had themselves publicly revealed the new microarchitecture. Today MediaTek goes into more detail, and explains that the CPU configuration on the Dimensity 1000 (Let’s just call it the D1000) is a 4x Cortex-A77 setup running at 2.6GHz alongside 4x Cortex-A55’s at 2.0GHz. The 2.6GHz clock rate is pretty muchspot on where we expected the first A77 configuration to land at, and should make the D1000 extremely competitive on the high-end of the SoC market.The fact that this is a SoC with 4 big cores further enforces the notion that this is a flagship chip, and is a departure from MediaTek’s CPU strategy of the previous few years. The SoC also employs a new Mali-G77 GPU in an MP9 configuration. This setup is 19% smaller than the MP11 in the Exynos 990, but MediaTek also doesn't disclose any clock frequency, so the end performance likely isn't fully determined yet. Nevertheless, it's a very competitive GPU configuration.The chip continues to support LPDDR4 - so while we'll see LP5 from the competition, it still makes sense for MediaTek to use LP4 this generation.MediaTek also employs its own third-generation neural processing unit which it calls “APU 3.0”. It says it’s able to double the performance over its previous generation chipsets. MediaTek fared quite well in some benchmarks here due to its quite complete software stack supporting all the new NNAPI features, whilst other vendors such as Qualcomm or Samsung still have incomplete solutions.ISP capabilities have been improved and the new SoC will support camera sensors with resolutions of up to 80MP at 24fps, or multi-camera setups with 32+16MP sensors. Media encoding capabilities fall in at 4K60, but here the biggest surprise lies in the chipset's support for AV1 video decoding. As far as we're aware, this make the D1000 the very first consumer mobile SoC to support the format, which is a great leap forward in terms of future-proofing the devices which are based on the new chip.Most importantly what justifies the Dimensity 1000 adopting a whole new branding is the fact that it’s a 5G with a new integrated modem. MediaTek discloses it supports 5G Sub-6 in both SA and NSA, and is capable of download speeds of up to 4600Mbps thanks to 2x carrier aggregation, as well as upload speeds of up to 2500Mbps. We currently don’t have information on the 4G capabilities and if the carrier aggregation differs to the 5G abilities.MediaTek describes its 5G modem implementation as extremely power efficient and claims it’s ahead of competing solutions in this regard, an interesting claim which hopefully we’ll maybe get to test in 2020.The company says we’ll be seeing devices based on the new chipset later this year as well as in early 2020. Overall, it seems like MediaTek’s new push into 5G is a bigger than usual effort to try to re-enter the flagship device market, and based on pricing, the Dimensity 1000 should end up as a competitive offering.Related Stories:MediaTek Announces 7nm 5G With Cortex-A77 CPU, Mali-G77 GPU ComingIntel and MediaTek Announce Partnership To Bring 5G Modems to PCsMediaTek Announces New Helio G90 Series SoCs: Gaming Focused Mid-RangeSamsung Announces Exynos 990: 7nm EUV, M5, G77, LPDDR5 Flagship SoC alongside Exynos 5123 5G ModemHuawei Announces Kirin 990 and Kirin 990 5G: Dual SoC Approach, Integrated 5G Modem\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15155/mediatek-announces-dimensity-1000-soc-back-to-the-highend-with-5g\n",
      "Title: AMD Pre-Announces 64-core Threadripper 3990X: Time To Open Your Wallet\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-11-25T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/15151/amd-preannounces-64-core-threadripper-3990x\n",
      "Content: Ever since AMD announced its latest enterprise platform, Rome, and the EPYC 7002 series, one question that high-end desktop users have been wondering is when the 64-core hardware will filter down into more mainstream markets. White today AMD is announcing their Threadripper 3000 platform with 24-core and 32-core processors, the other part of AMD’s announcement today is that yes, they will be selling 64-core hardware to the masses, in the form of the Threadripper 3990X.AMD isn’t giving too many details away just yet. As we predicted, there was room at the top of AMD’s naming strategy to expose more Threadripper hardware: one does not simply stop as the 3970X being the most powerful processor, and the 3990X will certainly take the mantle. AMD is announcing today that the 3990X will have 64 cores, 128 threads, and will have the full 256 MB of L3 cache.The 3990X will be the high-end desktop equivalent of the EPYC 7742. This means that inside it will have eight chiplets, each with 8 cores enabled. This is compared to the 3960X/3970X being announced today, with 24 and 32 cores respectively, which only have four chiplets. We can tell that the four chiplet designs are that way by the L3 cache: when only four chiplets are active, it has 128 MB of L3 cache, however with all eight chiplets, the 3990X will have 256 MB of L3 cache. That’s a sizeable processor, and seemingly unthinkable for a consumer part. However, as we’ve learned from AMD since introducing Ryzen, they like to go aggressive and offer some level of parity between consumer and enterprise hardware.One thing that will differentiate the 3990X from the EPYC hardware will be memory and PCIe count. We fully expect (although not confirmed) that the 3990X will have quad channel memory and 64 PCIe 4.0 lanes, compared to EPYC which has eight channel memory and 128 PCIe lanes. AMD has also confirmed that the 3990X will have a TDP above and beyond the EPYC 7742's, with the 3990X coming in at 280W TDP. If this seems familiar, then this is the same TDP as the 24-core and 32-core Threadripper parts. As a result, we do expect the per-core frequency of the 3990X to be higher than the EPYC, but lower than the other Threadrippers.In our testing of the 3970X 32-core hardware, we saw that in the 280W TDP we had around 75W reserved for non-core activities, and 205W for the cores. Non-core activities in this instance means PCIe, Infinity Fabric, and memory channels. Moving up to the 3990X means double the IF connections, but the others stay the same. So even if that means we reserve 100W for non-core activities, that leaves 180 W for 64 cores, or around 3 W each per core when at full load. Based on what we know about Zen 2 frequency scaling with power, around 6 W per core gives 4.0 GHz, so 3 W per core should offer low-to-mid 3.0 GHz all-core frequencies.This prediction actually fits well: AMD’s 240 W EPYC 7742 is a 2.35 GHz base, 3.2 GHz turbo, so we should expect frequencies north of that. There’s also the EPYC 7H12,a new part recently announcedto cater for the high frequency market. Like the 3990X, it also has a 280W TDP, but a 2.6 GHz base frequency, and a 3.3 GHz turbo. There is no official pricing on the 7H12 as yet.AMD HEDT SKUsAnandTechCores/ThreadsBase/TurboL3DRAM1DPCPCIeTDPSRPThird Generation ThreadripperTR 3990X64 / 1282.6+ / 3.3+ ?256 MB4 x ?64 ?280 WarmTR 3980X ?*48 / 96 ??256 MB4 x ?64 ?280 W ?legTR 3970X32 / 643.7 / 4.5128 MB4x320064280 W$1999TR 3960X24 / 483.8 / 4.5128 MB4x320064280 W$1399Second Generation ThreadripperTR 2990WX32 / 643.0 / 4.264 MB4x293364250 W$1799TR 2970WX24 / 483.0 / 4.264 MB4x293364250 W$1299TR 2950X16 / 323.5 / 4.432 MB4x293364180 W$899TR 2920X12 / 243.5 / 4.332 MB4x293364180 W$649Ryzen 3000Ryzen 9 3950X16 / 323.5 / 4.732 MB2x320024105 W$749* TR 3980X is a theorized part due to a hole in AMD's naming. Specifications are guesses based on trends and potential hardware support.AMD is set to launch the Threadripper 3990X in 2020. Unfortunately we don’t get any more info than that at this time, but if we consider the 24-core 3960X at $1399, the 32-core 3970X at $1999, I can easily see this processor being at least $3999, if not more. The EPYC 7742 has an MSRP of $6950, and the 7H12 is higher than that, so the 3990X is going to cost a pretty penny by comparison.Related ReadingAMD Q4: 16-core Ryzen 9 3950X, Threadripper Up To 32-Core 3970X, Coming November 25thDetails About 3rd Generation Ryzen Threadripper Appear: 24 and 32 Cores, Up to 280 WTRX40 Chipset For Upcoming AMD Ryzen Threadripper ListedSpotted at Computex: A Ryzen PC in a Threadripper RETAIL boxComputer Upgrade King Shows Off a Compact 32-Core Ryzen Threadripper PCAMD’s New 280W 64-Core Rome CPU: The EPYC 7H12AMD Rome Second Generation EPYC Review: 2x 64-core Benchmarked\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15151/amd-preannounces-64-core-threadripper-3990x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: New NVIDIA GPU Variant Found at Supercomputing 2019: Tesla V100S\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-11-22T20:00:00Z\n",
      "URL: https://www.anandtech.com/show/15146/new-nvidia-gpu-variant-at-supercomputing-2019\n",
      "Content: NVIDIA announced a number of things at Supercomputing, such as an Arm server referece design. Despite the show being the major hub event for high-performance computing/supercomputers, it isn’t often the location where NVIDIA launches a GPU. Nonetheless we saw a new model of NVIDIA’s high performance Tesla V100 at multiple booths at Supercomputing.The new GPU we saw was called the V100S (or V100s). Firstly, the name: I didn’t realise it was new/unannounced until it was pointed out to me. The way it was written on a few of the billboards looks like it is just referring to ‘multiple V100 units’, but a couple of companies confirmed to be that it is a new product. For these vendors, they were actually told before the show that NVIDIA was planning to announce it there, and were surprised that the CEO Jensen Huang did not mention it in his off-site two hour presentation to press and partners.Nonetheless, NVIDIA’s partners had printed the billboards, built the displays, built the systems, and hadn’t been told *not* to show it off. So they did. I was informed to look out for the gold shroud at one particular booth – they were differentiating by having the standard V100 with a green shroud, and their V100S will have a gold shroud. This is despite the gold shroud units also just say ‘V100’, which is meant to signify the family of the card.Finding out what is different about this card has actually been a task – none of my usual contacts seem to know exact numbers, although a couple confirmed it was ‘faster memory’, referring to the on package HBM2. I’m still looking into exact frequency changes, and presumably the knock on effects on TDP, but as it stands ‘faster memory’ is the only information I have. There might also be a price difference for anyone interested in these variants.One thought is that NVIDIA might not actually announce the V100S as a separate model, but just a higher memory version of the V100 and customers will just have to check exactly what the memory frequency is when they purchase – just like different consumer cards can have different memory speeds. No-one was discussing exact launch timing, but it seemed NVIDIA’s partners were deep into validation, if not already offering them to select customers.Related ReadingNVIDIA Unveils DGX-2H Server with 450W Tesla V100 GPUs16GB NVIDIA Tesla V100 Gets Reprieve; Remains in ProductionNVIDIA Bumps All Tesla V100 Models to 32GB, Effective ImmediatelyNVIDIA Formally Announces PCIe Tesla V100: Available Later This Year\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15146/new-nvidia-gpu-variant-at-supercomputing-2019\n",
      "Title: The Dell XPS 13 7390 2-in-1 Review: The Ice Lake Cometh\n",
      "Author: Brett Howse\n",
      "Date Published: 2019-11-15T16:30:00Z\n",
      "URL: https://www.anandtech.com/show/15092/the-dell-xps-13-7390-2in1-review-the-ice-lake-cometh\n",
      "Content: It’s difficult to overstate how important the XPS 13 is to Dell’s lineup, and to the industry as a whole. This is the device that reshaped the entire market with the advent of the InfinityEdge display back in 2015 which transformed the laptop landscape in an instant, rendering all other devices as dull and out of date. But other manufacturers are relentless, and Dell’s early design lead was never going to last forever. Other laptops have arguably caught, and even surpassed the XPS 13 over the last couple of generations. But Dell’s latest model of their flagship 13-inch laptop hopes to take the reins back.Dell has made some serious updates to the 7390 model, which we're checking out today on the 2-in-1 version of the laptop. The biggest update is a shot in the arm that all manufacturers have been anxiously awaiting:Intel’s Ice Lake platform, dubbed the 10th generation Core, and based on the long-delayed 10 nm Intel process. Over the previous generation Intel has seen some serious competition from their x86 neighbor, and AMD’s Zen core has brought AMD back from the brink, offering competitive performance, and in laptop guise. All of which comes coupled with a potent Vega GPU which has run circles around Intel’s UHD 620 found in all of the 8th generation U-Series processors powering so many laptops.Intel is looking to change that with Ice Lake, offering not only improved CPU performance thanks to the Sunny Cove CPU architecture, but a much-improved GPU as well, with the Gen 11 graphics. The vast majority of previous generation U-Series processors offered just 24 Execution Units (EUs), with some premium devices offering Intel Iris Graphics which had 48 EUs in the 15-Watt range, but Ice Lake improves on that significantly with 64 EUs on the G7-suffix processors, 48 EUs on the G4 range, and 32 EUs on the lowest-tier G1 lineup. Our Dell XPS 13 7390 features the top of the line offering in the 15-Watt envelope with the Core i7-1065G7, meaning it also sports the 64 EU GPU.But an improved processor can’t be the only defining change, as all manufacturers will be offering Ice Lake in one form or another. Dell has also taken their InfinityEdge display to the next level, bumping the overall screen size from 13.3 to 13.4 inches, and moving to a 1920x1200 16:10 aspect ratio, which dramatically reduces the bezel on the bottom of the display. In addition, Dell is offering a 3840x2400 panel which hits 90% of the P3-D65 gamut, and is HDR 400 and Dolby Vision certified. If you need it, the displays also offer touch and pen support as well.Dell XPS 13 7390 2-in-1As Reviewed: Core i7-1065G7 / 16GB / 512 GB / $1749.99CPUIntel Core i3-1005G12C / 4T 1.2 GHz - 3.4 GHzIntel UHD Graphics 32 EUs 300 MHz - 900 MHz15W TDPIntel Core i5-1035G14C / 8T 1.00 GHz - 3.6 GHzIntel UHD Graphics 32 EUs 300 Mhz - 1.05 GHz15W TDPIntel Core i7-1065G74C / 8T 1.3 GHz - 3.9 GHzIntel Iris Plus Graphics 64 EUs 300 MHz - 1.1 GHz15W TDPMemory4 / 8 / 16 / 32 GB LPDDR4X-3733Display13.4-inch 1920x1200 IPS100% sRGB 500-nitTouch and Pen supportOptional13.4-inch 3840x2400 IPS90% P3-D65 500-nit HDR 400 CertifiedTouch and Pen supportStorage256GB / 512GB / 1TB PCIe x4 NVMeWirelessKiller AX1650 2x2:2 Wi-Fi6Bluetooth 5.0I/O2 x Thunderbolt 3 (DP / Power Delivery / 4-lanes PCIe)Micro SD3.5mm headset jackWebcam720PBattery51 Wh Li-Ion45 W Type-C AC AdapterDimensions297 x 207 x 7-13 mm11.69 x 8.15 x 0.28-0.51 inchesWeight1.32 Kg / 2.9 lbsMSRP (USD)i3 / 4GB / 256GB - $999.99+i5 / 8GB / 256GB - $1299.99+i7 / 16GB / 256GB - $1469.99+i7 / 16GB / 512GB - $1699.99+The changes don’t stop there. Despite the almost 85% screen-to-body ratio, Dell is still offering the webcam at the top of the display, unlike the original InfinityEdge design, and the latest XPS 13 2-in-1 is 8% thinner than before.As a modern, premium laptop, the XPS 13 also offers two USB-C Thunderbolt 3 ports, although there’s no legacy USB-A port which may hinder some people. Dell does ship the laptop with an adapter, but having a dongle on-hand when you need it is one thing to be prepared for.There’s also WiFi 6, based on Intel’s wireless card but with the Killer software as well. The latest XPS 13 offers an instant-wake feature, and an integrated fingerprint reader in the power button for Windows Hello support.Dell is offering a lot in the XPS 13 2-in-1. They revolutionized the laptop back in 2015, but the competition has fought back. Let’s dig into the latest model and see how the XPS 13 7390 stacks up.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15092/the-dell-xps-13-7390-2in1-review-the-ice-lake-cometh\n",
      "Title: The AMD Ryzen 9 3950X Review: 16 Cores on 7nm with PCIe 4.0\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-11-14T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/15043/the-amd-ryzen-9-3950x-review-16-cores-on-7nm-with-pcie-40\n",
      "Content: Deciding between building a mainstream PC and a high-end desktop has historically been very clear cut: if budget is a concern, and you're interested in gaming, then typically a user looks to the mainstream. Otherwise, if a user is looking to do more professional high-compute work, then they look at the high-end desktop. Over the course of AMD’s recent run of high-core count Ryzen processors that line has blurred. This year, that line has disappeared. Even in 2016, mainstream CPUs used to top out at four cores: today they now top out at sixteen.Does anyone need sixteen cores? Yes.Does everyone need sixteen cores? No.There are two fundamental drivers for most PC builders: cost and performance. Users who want a gaming machine are going to put their dollars in what gives them the best gaming performance. Users that want to edit video are going to look at content creation focused hardware. For those in the business world, the added incentive of extra performance is being able to offset or amortize those costs with an improved work rate. For the video editor needing a week per video, if they can spend +40% to reduce the render time by half then it can pay off over a short period of time.As we move through 2019, users are doing more with their systems. Even at the low end, users might have double monitors where they game and watch their favourite streamer at the same time. High end users might reserve certain cores for different tasks, ensuring that there’s always some horsepower for the high-throughput tasks or virtual machines. Even though processors became ‘multi-core’ over a decade ago, we all as users are only recently adjusting how we do things to be more parallel, and the hardware is coming up to match our demands.To that end, AMD’s Ryzen processors have been timely. The first generation mainstream Ryzen hardware in 2017 was a breath of fresh air in a market that had become sufficiently stale to be unexciting. With the color drained, AMD’s Ryzen enabled up to eight cores on a single CPU, and at the time aimed to throw its weight against Intel’s hardware in the class above. The new architecture didn’t push ahead on day one clock for clock, but it enabled a different paradigm at an obscenely reasonable price point.Enter round 2, andZen 2. Earlier this yearAMD pushed again, this time putting 12 cores in the market for the same price as 8, or what had been the 4-core price point only three years prior. In three years we had triple the cores for the same price, and these cores also have more raw performance. The frequency wasn’t as high as the competition, but this was offset by that raw clock-for-clock throughput and ultimately where the competition now offered eight cores, AMD offered 12 at a much lower power consumption to boot.Today is round 2 part 2: taking that same 12-core processor, and adding four more cores (for a 50% increase in price), and not only going after the best consumer processor Intel has to offer, but even the best high-end desktop processor. This is AMD squeezing Intel’s product portfolio like never before. What exactly is mainstream, anyway?AMD’s new Ryzen 9 3950X has a suggested retail price of $749. For that AMD is advertising sixteen of its latest Zen 2 cores built on TSMC’s 7nm process, running at a 3.5 GHz base frequency and a 4.7 GHz single-core turbo frequency. The TDP of the chip is rated at 105 watts and it has 24 PCIe 4.0 lanes as well as dual memory channels that support up to 128 GB of DDR4-3200.AMD 'Matisse' Ryzen 3000 Series CPUsAnandTechCoresThreadsBaseFreqBoostFreqL2CacheL3CachePCIe4.0ChipletsIO+CPUTDPPrice(SEP)Ryzen 93950X16C32T3.54.78 MB64 MB16+4+41+2105W$749Ryzen 93900X12C24T3.84.66 MB64 MB16+4+41+2105W$499Ryzen 9390012C24T3.14.36 MB64 MB16+4+41+265WOEMRyzen 73800X8C16T3.94.54 MB32 MB16+4+41+1105W$399Ryzen 73700X8C16T3.64.44 MB32 MB16+4+41+165W$329Ryzen 53600X6C12T3.84.43 MB32 MB16+4+41+195W$249Ryzen 536006C12T3.64.23 MB32 MB16+4+41+165W$199Ryzen 53500X6C6T3.64.13 MB32 MB16+4+41+165WOEMIt wasn’t too long ago that this price range used to be the realm of AMD’s high-end desktop Threadripper processors, which started at 8 cores and we up to 32 cores. AMD is now shifting that paradigm as well, with this 16-core chip being at $749, andAMD’s next generation Threadripper 3000 processorsstarting at 24-cores at $1399. When AMD CEO Dr. Lisa Su was asked earlier this year what would happen given the drive to more cores for the mainstream processors, her response was ‘as Ryzen goes up, Threadripper goes up-up’. This is the realization of that.It is worth noting that the price is likely to be higher at retail initially, as demand is expected to be high and stock levels haven’t been defined – given the popularity of the 12-core chip, it would seem that users wanting the mainstream platform always want the best.Going AM4: The Battle with MotherboardsWhen the AM4 platform was first launched, technically with pre-Zen hardware, it supported four cores. The same platform now goes all the way up to sixteen cores, which is no small task. The flip side of this comes down to motherboard support: some AM4 motherboards were not designed with high-power sixteen core processors in mind. Some motherboards built on the AM4 socket were for the budget market, and will struggle when it comes to this 16-core part.AMD has attempted to at least segment its AM4 market a little. Only the latest AM4 chipset, the X570 chipset, has official support for the Ryzen 3000-series PCIe 4.0 connections. In order to enable the PCIe 4.0 lanes on the processor as qualified by AMD, users will have to purchase an X570 motherboard, otherwise these lanes will run at half speed (PCIe 3.0) in non-X570 motherboards.The quality of the motherboard is likely to affect turbo frequencies as well. AMD’s turbo algorithms are influenced in part by the ability of the power delivery to push current through from the power supply. We are seeing X570 motherboards range from $170 all the way up to $999. This isn’t saying that doubling the cost of the motherboard will double the ability to turbo, but as seen with the previous Ryzen 3000 series chips, the motherboard choice (as well as the cooling it uses) will matter.All the X570 motherboards we’ve tested recently are up to the task of taming the Ryzen 9 3950X. Here’s a list of what we’ve tested:ASUS ROG Crosshair VIII ImpactASRock X570 Phantom Gaming-ITX/TB3GIGABYTE X570 Aorus XtremeMSI MEG X570 GodlikeASUS Pro WS X570-AceMSI MEG X570 AceUsers looking at motherboards have to find the right mix of capacity, cost, and features. We did a visual inspection ofall 35+ launch models.Toe-to-Toe: Intel Core i9-9900KS / Core i9-9980XE / Core i9-10980XEWith the mainstream and high-end desktop market now seemingly merging, there are many angles to consider different competitive parts between Intel and AMD. If we compete purely on PCIe lanes, then we might put the Core i9-9900KS (8-cores) up against the 3950X (16-cores), although there is a big price difference ($513 vs. $749). If we compare on pricing, the nearest processor to the 3950X would be either the 9900KS (mainstream) or the Core i9-10940X ($729), however while 3950X has more cores than either, but doesn’t have as many PCIe lanes/memory lanes as the 10940X. If we go for core count, then Intel’s sixteen Core i9-9960X would be the obvious candidate, although this CPU is a lot more expensive (until Intel reduces the price) and is technically an X299 processor, so has more PCIe lanes and memory channels.Unlocked CPU Pricingand Select OthersAMD(MSRP Pricing)CoresAnandTechCoresIntel*(OEM Pricing)$900-$99918/36Core i9-10980XE ($979)$800-$899Ryzen 9 3950X ($749)16/32$700-$79914/28Core i9-10940X ($784)$600-$69912/24Core i9-10920X ($689)$500-$59910/208/16Core i9-10900X ($590)Core i9-9900KS ($513)Ryzen 9 3900X ($499)12/24$400-$4998/16Core i9-9900K/F ($488)Ryzen 7 3800X ($399)8/16$350-$3998/8Core i7-9700K/F ($374)Ryzen 7 3700X ($329)8/16$300-$349$250-$2996/6Core i5-9600K ($262)Ryzen 5 3600X ($249)6/12$200-$249Ryzen 5 3600 ($199)6/12Below $2004/4Core i3-9350K ($173)*Intel quotes OEM/tray pricing. Retail pricing will sometimes be $20-$50 higher.There is no easy comparison between any of the processors. AMD is pushing the boundaries of the mainstream dual channel memory processor regime, and Intel doesn't have an equivalent in that space. Intel can match it in the high-end desktop space, but therein lays other issues with PCIe lane counts and memory channel support disparity between the two, as well as Intel’s current retail options being high-priced variants. Intel’s published next generation hardware is set to be launched sometime in November, and with it a number of price cuts, however given the known differences between Intel’s current and Intel’s next generation processor line, the performance gain is not expected to be particularly big.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15043/the-amd-ryzen-9-3950x-review-16-cores-on-7nm-with-pcie-40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Intel Core i9-9900KS Review: The 5 GHz Consumer Special\n",
      "Author: Dr. Ian Cutress\n",
      "Date Published: 2019-10-31T14:45:00Z\n",
      "URL: https://www.anandtech.com/show/14979/the-intel-core-i9-9900ks-review\n",
      "Content: Intellikes 5.0 GHz processors. The one area where it claims a clear advantage over AMD is in its ability to drive the frequency of its popular 14nm process. Earlier this week,we reviewed the Core i9-9990XE, which is a rare auction only CPU but with 14 cores at 5.0 GHz, built for the high-end desktop and high frequency trading market. Today we are looking at its smaller sibling, the Core i9-9900KS, built in numbers for the consumer market: eight cores at 5.0 GHz. But you’ll have to be quick, as Intel isn’t keeping this one around forever.The Battle of the BitsEvery time a new processor comes to market, several questions get asked: how many cores, how fast, how much power? We’ve come through generations of promises of many GHz and many cores for little power, but right now we have an intense battle on our hands. The red team is taking advantage of a paradigm shift in computing with an advanced process node to offer many cores at a high power efficiency as well as at a good frequency. In the other corner is team blue, which has just equipped its arsenal by taking advantage of its most aggressive binning of 14nm yet, with the highest frequency processor for the consumer market, enabled across all eight cores and to hell with the power. Intel’s argument here is fairly simple:Do you want good all-around, or do you want the one with the fastest raw speed?The Intel Core i9-9900KS is borne from the battle. In essence it looks like an overclocked Core i9-9900K, however by that logic everything is an overclocked version of something else. In order for Intel to give a piece of silicon off the manufacturing like the name of a Core i9-9900KS rather than a Core i9-9900K requires additional binning and validation, to the extent where it has taken several months from announcement just for Intel to be happy that they have enough chips for demand that will meet the warranty standards.At the time Intel launched its 9thGeneration Core desktop processors, like the Core i9-9900K, I perhaps would not have expected them to launch something like the Core i9-9900KS. It’s a big step up in the binning, and I’d be surprised if Intel gets one chip per wafer that hits this designation. Intel announced the Core i9-9900KS after AMD had launched its Zen 2 Ryzen 3000 family, offering 12 cores with an all core turbo around 4.2 GHz and a +10% IPC advantage over Intel’s Skylake microarchitecture (and derivatives) for a lower price per core. In essence, Intel’s Core i9-9900K consumer flagship processor had a chip that was pretty close to it in performance with several more cores.Intel is pushing the Core i9-9900KS as the ultimate consumer processor. With eight cores all running at 5.0 GHz, it is promising fast response and clock rates without any slowdown. Intel has many marketing arguments as to why the KS is the best processor on the market, especially when it comes to gaming: having a 5.0 GHz frequency keeps it top of the pile for gaming where frequency matters (low resolution), and many games don’t scale beyond four cores, let alone eight, and so the extra cores on the competition don’t really help here. It will be interesting to see where the 9900KS comes out in standard workload tests however, where cores can matter.Intel’s 9thGeneration Core ProcessorsThe Intel Core i9-9900KS now sits atop of Intel’s consumer product portfolio. The processor is the same 8-core die as the 9900K, unlocked with UHD 620 integrated graphics, but has a turbo of 5.0 GHz. All cores can turbo to 5.0 GHz. The length of the turbo will be motherboard dependent, however.Intel 9th Gen Core 8-Core Desktop CPUsAnandTechCoresBaseFreqAll-Core TurboSingleCore TurboFreqIGPDDR4TDPPrice(1ku)i9-9900KS8 / 164.0 GHz5.0 GHz5.0 GHzUHD 6302666127 W$513i9-9900K8 / 163.6 GHz4.7 GHz5.0 GHzUHD 630266695 W$488i9-9900KF8 / 163.6 GHz4.7 GHz5.0 GHz-266695 W$488i7-9700K8 / 83.6 GHz4.6 GHz4.9 GHzUHD 630266695 W$374i7-9700KF8 / 83.6 GHz4.6 GHz4.9 GHz-266695 W$374The Core i9-9900KS has an tray price of $513 (when purchased in 1000 unit bulk), which means we’re likely to see an on-shelf price of $529-$549, depending on if it gets packaged in its dodecanal box that our review sample came in.Compared to the Core i9-9900K or Core i9-9900KF, the Core i9-9900KS extends its 5.0 GHz all through from when 2 cores are active to 8 cores are active. There is still no Turbo Boost Max 3.0 here, which means that all cores are guaranteed to hit this 5.0 GHz number. The TDP is 127 W, which is the maximum power consumption of the processor at its base frequency, 4.0 GHz. Above 4.0 GHz Intel does not state what sort of power to expect. We have this testing further in the review.CompetitionAt present, Intel is competing against two major angles with the Core i9-9900KS. On the one side, it already has the Core i9-9900K, which if a user gets a good enough sample, can be overclocked to emulate the 9900KS. Intel does not offer warranty on an overclocked CPU, so there is something to be taken into account – the warranty on the Core i9-9900KS is only a limited 1 year warranty, rather than the standard 3 years it offers to the majority of its other parts, which perhaps indicates the lengths it went to for binning these processors.From AMD, the current 12-core Ryzen 9 3900X that is already in the market has become a popular processor for users going onto 7nm and PCIe 4.0. It offers more PCIe lanes from the CPU to take advantage of PCIe storage and such, and there are a wealth of motherboards on the market that can take advantage of this processor. It also has an MSRP around the same price, at $499, although is often being sold for much higher due to availability.AMD also has the 16-core Ryzen 9 3950X coming around the corner, promising slightly more performance than the 3900X, and aside from the $749 MSRP, it’s going to be an unknown on availability until it gets released in November.The CompetitionIntel i9-9900KSIntel i9-9900KAnandTechAMD2920XAMD3950XAMD3900XAMD3800X88Cores12161281616Threads243224164.03.6Base3.53.53.83.98 x 5.02 x 5.0Turbo4.34.74.64.52 x 26662 x 2666DDR44 x 29332 x 32002 x 32002 x 32003.0 x163.0 x16PCIe3.0 x644.0 x244.0 x244.0 x24127 W95 WTDP180 W105 W105 W105 W$513$486Price$649$749$499$399It’s worth noting here that while Intel has committed to delivering ‘10nm class’ processors on the desktop in the future, it currently has made zero mention of exactly when this is going to happen. Offering a limited edition all-core 5.0 GHz part like the Core i9-9900KS into the market is a brave thing indeed – it will have to provide something similar or better when it gets around to producing 10nm processors for this market. We saw this once before, when Intel launched Devil’s Canyon: super binned parts that ultimately ended up being faster than those that followed on an optimized process, because the binning aspect ended up being a large factor. Intel either has extreme confidence in its 10nm process for the desktop family, or doesn’t know what to expect.This ReviewIn our review, we’re going to cover the usual benchmarking scenarios for a processor like this, as well as examine Intel’s relationship with turbo and how much a motherboard manufacturer can affect the performance.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/14979/the-intel-core-i9-9900ks-review\n",
      "Title: SiFive Announces First RISC-V OoO CPU Core: The U8-Series Processor IP\n",
      "Author: Andrei Frumusanu\n",
      "Date Published: 2019-10-30T14:00:00Z\n",
      "URL: https://www.anandtech.com/show/15036/sifive-announces-first-riscv-ooo-cpu-core-the-u8series-processor-ip\n",
      "Content: In the last few year’s we’ve seen an increasing amount of talk about RISC-V and it becoming real competitor to the Arm in the embedded market. Indeed, we’ve seen a lot of vendors make the switch from licensing Arm’s architecture and IP designs to the open-source RISC-V architecture and either licensed or custom-made IP based on the ISA. While many vendors do choose to design their own microarchitectures to replace Arm-based microcontroller designs in their products, things get a little bit more complicated once you scale up in performance. It’s here where SiFive comes into play as a RISC-V IP vendor offering more complex designs for companies to license – essentially a similar business model to Arm’s – just that it’s based on the new open ISA.Today’s announcement marks a milestone in SiFive’s IP offering as the company is revealing its first ever out-of-order CPU microarchitecture, promising a significant performance jump over existing RISC-V cores, and offering competitive PPA metrics compared to Arm’s products. We’ll be taking a look at the microarchitecture of the new U8 Series CPU and how it’s built and what it promises to deliver.As a bit of background on the company, SiFive was founded in 2015 by the researchers who invented the RISC-V instruction set at UC Berkeley back in 2010. The company’s goal was to develop and implement CPUs and IP based on the RISC-V ISA and produce the first hardware based on the technology. The company first full-blown CPU IP that was able to run a full OS such as Linux was the U54 series which was released in 2017, and ever since SiFive has been in an upward trend of success and hypergrowth.Introducing the U8-Series - A Scalable Out-of-Order RISC-V CPU CoreUp until now, it’s been relatively unsurprising that if you’re designing a new CPU based on a new ISA, you first start out small and then iterate as you continue to add more complexity to your design. SiFive’s U5 and U7 series as such have been relatively more simplistic in-order CPU microarchitectures. While offering functionality and being very cost-effective options and alternatives compared to Arm’s low-end and microcontroller cores, they really weren’t up to the task of more complex workloads that needed more raw performance.The new U8-Series addresses these concerns by massively improving the performance that can be delivered by the new microarchitecture – outpacing the U54 and U74 by factors of up to 5-4x, a quite significant performance jump that we don’t usually see very often in the industry.The new CPU IP’s performance promises to vastly expand SiFive’s and the RISC-V’s ecosystem viability in end-point products, and really be able to offer alternatives to the embedded Arm products in the world today and in the future.SiFive’s design goals for the U8-Series are quite straightforward: Compared to an Arm Cortex-A72, the U8-Series aims to be comparable in performance, while offering 1.5x better power efficiency at the same time as using half the area. The A72 is quite an old comparison point by now, however SiFive’s PPA targets are comparatively quite high, meaning the U8 should be quite competitive to Arm’s latest generation cores.\n",
      "\n",
      "Article already exists: https://www.anandtech.com/show/15036/sifive-announces-first-riscv-ooo-cpu-core-the-u8series-processor-ip\n",
      "Total number of articles found: 449\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import psycopg2\n",
    "import re\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "class General:\n",
    "    def __init__(self, main_url, start_page, end_page, final_file):\n",
    "        self.main_url = main_url\n",
    "        self.start_page = start_page\n",
    "        self.end_page = end_page\n",
    "        self.final_file = final_file\n",
    "\n",
    "    def write_to_file(self, data):\n",
    "        with open(self.final_file, 'a') as file:\n",
    "            file.write(data + \"\\n\")\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub('<[^<]+?>', '', text)  # Remove HTML tags\n",
    "        text = re.sub('\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "        text = text.strip()  # Remove leading and trailing spaces\n",
    "        return text\n",
    "\n",
    "    def normalize_url(self, url):\n",
    "        parsed = urlparse(url)\n",
    "        normalized = parsed._replace(fragment=\"\", query=\"\").geturl().rstrip('/')\n",
    "        return normalized\n",
    "\n",
    "class AnandTech(General):\n",
    "    def __init__(self, main_url, start_page, end_page, final_file, db_params):\n",
    "        super().__init__(main_url, start_page, end_page, final_file)\n",
    "        self.db_params = db_params\n",
    "        self.conn = psycopg2.connect(**db_params)\n",
    "        self.cur = self.conn.cursor()\n",
    "        self.create_table()\n",
    "\n",
    "    def create_table(self):\n",
    "        create_table_query = '''\n",
    "        CREATE TABLE IF NOT EXISTS News_articles (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            title TEXT,\n",
    "            author TEXT,\n",
    "            date_published TEXT,\n",
    "            url TEXT UNIQUE,\n",
    "            content TEXT\n",
    "        );\n",
    "        '''\n",
    "        self.cur.execute(create_table_query)\n",
    "        self.conn.commit()\n",
    "\n",
    "    def article_exists(self, url):\n",
    "        normalized_url = self.normalize_url(url)\n",
    "        query = \"SELECT EXISTS(SELECT 1 FROM News_articles WHERE url = %s)\"\n",
    "        self.cur.execute(query, (normalized_url,))\n",
    "        return self.cur.fetchone()[0]\n",
    "\n",
    "    def insert_article(self, title, author, date_published, url, content):\n",
    "        normalized_url = self.normalize_url(url)\n",
    "        if self.article_exists(normalized_url):\n",
    "            print(f\"Article already exists: {normalized_url}\")\n",
    "            return\n",
    "        \n",
    "        if title and author and content and url:\n",
    "            insert_query = '''\n",
    "            INSERT INTO News_articles (title, author, date_published, url, content) \n",
    "            VALUES (%s, %s, %s, %s, %s) ON CONFLICT (url) DO NOTHING;\n",
    "            '''\n",
    "            self.cur.execute(insert_query, (title, author, date_published, normalized_url, self.clean_text(content)))\n",
    "            self.conn.commit()\n",
    "\n",
    "    def parse(self):\n",
    "        article_count = 0\n",
    "\n",
    "        for page in range(self.start_page, self.end_page + 1):\n",
    "            url = f\"{self.main_url}&CurrentPage={page}&q=arm&sort=date\"\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            articles = soup.find_all('div', class_='cont_box1')\n",
    "\n",
    "            for article in articles:\n",
    "                article_count += 1\n",
    "                link_tag = article.find('a')\n",
    "                if link_tag:\n",
    "                    article_url = self.normalize_url('https://www.anandtech.com' + link_tag['href'])\n",
    "                    self.process_article(article_url)\n",
    "\n",
    "        print(f\"Total number of articles found: {article_count}\")\n",
    "\n",
    "    def process_article(self, article_url):\n",
    "        response = requests.get(article_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        title_tag = soup.find('title')\n",
    "        title = self.clean_text(title_tag.get_text(strip=True)) if title_tag else 'Title not found'\n",
    "\n",
    "        content_div = soup.find('div', class_='articleContent')\n",
    "        content = self.clean_text(content_div.get_text(strip=True)) if content_div else 'Content not found'\n",
    "\n",
    "        script_json_ld = soup.find('script', type='application/ld+json')\n",
    "        if script_json_ld:\n",
    "            json_str = re.sub(r'[\\n\\t\\r]', '', script_json_ld.string)\n",
    "            try:\n",
    "                data = json.loads(json_str)\n",
    "                if isinstance(data, dict):\n",
    "                    data_list = [data]\n",
    "                elif isinstance(data, list):\n",
    "                    data_list = data\n",
    "                else:\n",
    "                    data_list = []\n",
    "                \n",
    "                for item in data_list:\n",
    "                    author_info = item.get('author', {})\n",
    "                    if isinstance(author_info, dict):\n",
    "                        author = author_info.get('name', 'Author not found')\n",
    "                    elif isinstance(author_info, list) and author_info:\n",
    "                        author = author_info[0].get('name', 'Author not found') if isinstance(author_info[0], dict) else 'Author not found'\n",
    "                    else:\n",
    "                        author = 'Author not found'\n",
    "                    \n",
    "                    date_published = item.get('datePublished', 'Date not found')\n",
    "                    self.print_article(title, author, date_published, article_url, content)\n",
    "                    self.insert_article(title, author, date_published, article_url, content)\n",
    "                    break\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from {article_url}: {e}\")\n",
    "        else:\n",
    "            print(\"No JSON-LD script found in the article.\")\n",
    "\n",
    "    def print_article(self, title, author, date_published, url, content):\n",
    "        print(f\"Title: {title}\\nAuthor: {author}\\nDate Published: {date_published}\\nURL: {url}\\nContent: {content}\\n\")\n",
    "\n",
    "# Database Parameters\n",
    "db_params = {\n",
    "    'dbname': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': '1234',\n",
    "    'host': 'localhost'\n",
    "}\n",
    "\n",
    "# Main URL adjusted to include base search query 'q=arm' and sort parameter\n",
    "main_url = 'https://www.anandtech.com/SearchResults?q=Arm'\n",
    "parser = AnandTech(main_url, 1, 30, 'AnandTech_articles.txt', db_params)\n",
    "parser.parse()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8edde7",
   "metadata": {},
   "source": [
    "# \"RogerKam/roberta_fine_tuned_sentiment_newsmtsc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30411123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Database connection parameters\n",
    "db_host = \"localhost\"\n",
    "db_name = \"postgres\"\n",
    "db_user = \"postgres\"\n",
    "db_password = \"1234\"\n",
    "\n",
    "# Connect to your database\n",
    "conn = psycopg2.connect(host=db_host, dbname=db_name, user=db_user, password=db_password)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Modify the table to include new columns\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS article_sentiments_RogerKam (\n",
    "        article_id INTEGER PRIMARY KEY REFERENCES news_articles(id),\n",
    "        sentiment TEXT,\n",
    "        url TEXT,\n",
    "        author TEXT,\n",
    "        date_published DATE,\n",
    "        title TEXT\n",
    "    );\n",
    "''')\n",
    "conn.commit()\n",
    "\n",
    "# Update SQL query to fetch additional fields\n",
    "sql_query = \"SELECT id, content, url, author, date_published, title FROM news_articles\"\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model_id = \"RogerKam/roberta_fine_tuned_sentiment_newsmtsc\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "max_token_limit = tokenizer.model_max_length\n",
    "priotity_label = 'negative'\n",
    "\n",
    "# Your existing sentiment_analysis function remains the same\n",
    "def sentiment_analysis(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = softmax(outputs.logits, dim=1)\n",
    "    # Assuming the sentiment labels are ordered as [\"negative\", \"neutral\", \"positive\"]\n",
    "    sentiment_labels = ['negative', 'neutral', 'positive']\n",
    "    sentiment_score, sentiment_index = torch.max(predictions, dim=1)\n",
    "    max_score_index = torch.argmax(predictions[:, 1:]) + 1  # Exclude \"negative\" from max score calculation\n",
    "    if sentiment_index == 0:\n",
    "        sentiment = priotity_label\n",
    "    else:\n",
    "        sentiment = sentiment_labels[max_score_index.item()]\n",
    "    return sentiment\n",
    " \n",
    " \n",
    " #Function to find the maximum sentiment label for a combined sentence\n",
    "def max_sentiment_label(sentiment_labels):\n",
    "    max_sentiment = max(sentiment_labels, key=sentiment_labels.count)\n",
    "    return max_sentiment\n",
    "# Fetch data, perform sentiment analysis for each sentence, and store results\n",
    "try:\n",
    "    cursor.execute(sql_query)\n",
    "    records = cursor.fetchall()\n",
    "    for record in records:\n",
    "        article_id, content, url, author, date_published, title = record\n",
    "        sentences = [content[i:i + max_token_limit] for i in range(0, len(content), max_token_limit)]\n",
    "        \n",
    "        results = []\n",
    "        for sentence in sentences:\n",
    "            sentiment = sentiment_analysis(sentence)\n",
    "            if sentiment == priotity_label:\n",
    "                results = [sentiment]\n",
    "                break\n",
    "            else:\n",
    "                results.append(sentiment)\n",
    "                \n",
    "        combined_sentiment = priotity_label if priotity_label in results else max_sentiment_label(results)\n",
    "\n",
    "        # Update insert query to include new fields\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO article_sentiments_RogerKam (article_id, sentiment, url, author, date_published, title)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (article_id) DO UPDATE SET\n",
    "        sentiment = EXCLUDED.sentiment,\n",
    "        url = EXCLUDED.url,\n",
    "        author = EXCLUDED.author,\n",
    "        date_published = EXCLUDED.date_published,\n",
    "        title = EXCLUDED.title\n",
    "        \"\"\"\n",
    "        cursor.execute(insert_query, (article_id, combined_sentiment, url, author, date_published, title))\n",
    "        conn.commit()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Database error: {e}\")\n",
    "finally:\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c57a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
